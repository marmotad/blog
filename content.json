{"meta":{"title":"marmotad","subtitle":"blog","description":"myBlog","author":"John Doe","url":"https://marmotad.github.io","root":"/blog/"},"pages":[],"posts":[{"title":"应用编排与管理(控制器)","slug":"应用编排与管理-控制器","date":"2022-02-10T02:31:33.000Z","updated":"2022-02-18T05:43:33.371Z","comments":true,"path":"2022/02/10/应用编排与管理-控制器/","link":"","permalink":"https://marmotad.github.io/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/","excerpt":"","text":"应用编排与管理应用程序版本升级时的在线应用更新操作也在实践中形成了灰度更新、蓝绿部署和金丝雀部署等解决方案。但这类的编排任务由传统的人工或工具化编排进化到了由ReplicaSet、Deployment或ReplicaSet控制器实现的半自动化编排机制，而HPA（Horizontal Pod Autoscaler）和VPA（Vertical Pod Autoscaler）控制器更是让这类任务彻底走向完全自动化。本章着重于介绍无状态应用控制器ReplicaSet和Deployment、系统应用控制器DaemonSet、单次任务控制器Job和定时作业控制器CronJob等。 Kubernetes控制器基础我们可以把API Server想象成存储Kubernetes资源对象的数据库系统，它仅支持预置的数据存储方案，每个方案对应于一种资源类型，客户端将API创建的、符合数据存储方案的数据项称为资源对象。但这些基于数据方案创建并存储于API Server中的仅是对象的定义。例如，一个Pod对象的定义并不代表某个以容器形式运行的应用，它仅停留在“纸面上”，我们还需要某个程序以特定的步骤调用容器运行时接口，按照Pod对象的定义创建出具体的应用容器来。这一类负责把API Server上存储的对象定义实例化到集群上的程序就是控制器。控制器需要运行为守护进程：一方面，注册监视API Server上隶属该控制器类型的对象定义（spec）的变动，及时将变动反映到集群中的对象实例之上；另一方面，通过控制循环（control loop，也可称为控制回路）持续监视集群上由其负责管控的对象实例的实际状态，在因故障、更新或其他原因导致当前状态（Status）发生变化而与期望状态（spec）时，通过实时运行相应的程序代码尝试让对象的真实状态向期望状态迁移和逼近。 控制器与Pod资源本质上讲，Kubernetes的核心就是控制理论，控制器中实现的控制回路是一种闭环（反馈）控制系统，该类型的控制系统基于反馈回路将目标系统的当前状态与预定义的期望状态相比较，二者之间的差异作为误差信号产生一个控制输出作为控制器的输入，以减少或消除目标系统当前状态与期望状态的误差，如图。这种控制循环在Kubernetes上也称为调谐循环（reconciliation loop）。 对Kubernetes来说，无论控制器的具体实现有多么简单或多么复杂，它基本都是通过定期重复执行如下3个步骤来完成控制任务。 1）从API Server读取资源对象的期望状态和当前状态。2）比较二者的差异，而后运行控制器中的必要代码操作现实中的资源对象，将资源对象的真实状态修正为Spec中定义的期望状态，例如创建或删除Pod对象，以及发起一个云服务API请求等。3）变动操作执行成功后，将结果状态存储在API Server上的目标资源对象的status字段中。图8-3给出了Kubernetes控制循环工作示意图。 任务繁重的Kubernetes集群上同时运行着数量巨大的控制循环，每个循环都有一组特定的任务要处理，为了避免API Server被请求淹没，需设定控制回路以较低的频率运行，默认每5分钟一次。同时，为了能及时触发由客户端提交的期望状态的更改，控制器向API Server注册监视受控资源对象，这些资源对象期望状态的任何变动都会由Informer组件通知给控制器立即执行而无须等到下一轮的控制循环。控制器使用工作队列将需要运行的控制循环进行排队，从而确保在受控对象众多或资源对象变动频繁的场景中尽量少地错过控制任务。出于简化管理的目的，Kubernetes将数十种内置的控制器程序整合成了名为kube-controller-manager的单个应用程序，并运行为独立的单体守护进程，它是控制平面的重要组件，也整个Kubernetes集群的控制中心。提示Kubernetes可用的控制器有attachdetach、bootstrapsigner、clusterrole-aggregation、cronjob、csrapproving、csrcleaner、csrsigning、daemonset、deployment、disruption、endpoint、garbagecollector、horizontalpodautoscaling、job、namespace、node、persistentvolume-binder、persistentvolume-expander、podgc、pvc-protection、replicaset、replicationcontroller、resourcequota、route、service、serviceaccount、serviceaccount-token、statefulset、tokencleaner和ttl等数十种。工作负载范畴的控制器资源类型包括ReplicationController、ReplicaSet、Deployment、DaemonSet、StatefulSet、Job和CronJob等，它们各自代表一种类型的Pod控制器资源，分别实现不同的应用编排机制。通常，一个工作负载控制器资源通常应该包含3个基本的组成部分。 标签选择器：匹配并关联Pod对象，并据此完成受其管控的Pod对象的计数。 期望的副本数：期望在集群中精确运行受控的Pod对象数量。 Pod模板：用于新建Pod对象使用的模板资源。注意 DaemonSet控制器用于确保集群中每个工作节点或符合条件的每个节点上都运行着一个Pod副本，而非某个预设的精确数量值，因而不具有上面组成部分中的第二项。例如，如图8-4所示的Deployment控制器eshop-deploy对象使用app=eshop为标签选择器，以过滤当前名称空间中的Pod对象，它期望能够匹配到的Pod对象副本数量精确为4个。 将eshop-deploy对象创建到集群上之后，Deployment控制器将根据该对象的定义标签选择器过滤符合条件的Pod对象并对其进行计数，少于指定数量的缺失部分将由控制器通过Pod模板予以创建，而多出的副本也将由控制器请求终止及删除。通常，对于那些以Deployment、DaemonSet或StatefulSet控制器编排的且需要长期运行的容器应用，其应用更新、回滚和扩缩容也是编排操作的核心任务。但这类任务所导致的Pod对象的变动势必会影响透过Service来访问应用服务的客户端，如图8-5所示。 显然，生产环境中的应用编排过程通常不能影响或过度影响当前正在获取服务的用户体验，达成该类目标也是应用程序控制器的核心功能之一，事实上，Deployment等甚至允许用户自定义更新策略来自定义应用升级过程。 Pod模板资源Pod模板资源是Kubernetes API的常用资源类型，常用于为控制器指定自动创建Pod资源对象时所需的配置信息。内嵌于控制器的Pod模板的配置信息中不需要apiVersion和kind字段，除此之外的其他内容跟定义自主式Pod对象所支持的字段几乎完全相同，这包括metadata和spec及其内嵌的其他各字段。工作负载控制器类的资源的spec字段通常都要内嵌replicas、selector和template字段，其中template便是用于定义Pod模板。下面是一个定义在ReplicaSet资源中的模板资源示例，它基于ikubernetes/demoapp:v1.0镜像简单定义了一个应用，并同时配置了存活探针和就绪探针。 123456789101112131415161718192021222324252627282930313233apiVersion: apps/v1kind: ReplicaSetmetadata: name: replicaset-demospec: minReadySeconds: 3 replicas: 2 selector: matchLabels: app: demoapp release: stable template: metadata: labels: app: demoapp release: stable spec: containers: - name: demoapp image: ikubernetes/demoapp:v1.0 ports: - name: http containerPort: 80 livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 initialDelaySeconds: 5 readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 initialDelaySeconds: 15 如上示例中，spec.template字段在定义时仅给出了metadata和spec两个字段，它的使用方法与自主式Pod资源几乎完全相同。一个特别的建议是，生产环境中运行的Pod对象务必要添加存活探针和就绪探针，否则Kubernetes无法准确判定应用的存活状态和就绪状态，而只能把处于运行中的容器进程一律视为在健康运行，而健康运行的容器进程则一律视为就绪。显然，定义在Pod模板中的存储卷资源将由当前模板创建出的所有Pod实例共享使用，因此定义时务必要确保该存储卷允许多路客户端同时访问，以及多路写操作时的数据安全性。应用编排是Kubernetes的核心功能，因而控制器资源及其使用的Pod模板也随之成为最常用的两种资源类型。 ReplicaSet控制器Kubernetes较早期的版本中仅有ReplicationController一种类型的Pod控制器，后来又陆续引入了更多的控制器实现，这其中就包括用来取代ReplicationController的新一代实现ReplicaSet。事实上，ReplicaSet除了支持基于集合的标签选择器，以及它的滚动更新（RollingUpdate）机制要基于更高级的Deployment控制器实现之外，目前ReplicaSet的其余功能基本与ReplicationController相同。考虑到Kubernetes强烈推荐使用ReplicaSet控制器，且表示ReplicationController不久后即将废弃，因而本节重点介绍ReplicaSet控制器。 功能分析ReplicaSet（简称RS）是工作负载控制器类型的一种实现，隶属于名称空间级别，主要用于编排无状态应用，核心目标在于确保集群上运行有指定数量的、符合其标签选择器的Pod副本。ReplicaSet规范由标签选择器、期望的副本数和Pod模板3个主要因素所定义，它在控制循环中持续监视同一名称空间中运行的Pod对象，并在每个循环中将标签选择器筛选出的Pod数量与期望的数量相比较，通过删除多余的Pod副本或借助于模板创建出新的Pod来确保该类Pod对象数量能始终吻合所期望的数量。标签选择器是ReplicaSet判断一个Pod对象是否处于其作用域的唯一标准，Pod模板仅在补足缺失数量的Pod对象时使用，这意味着由其他Pod规范所创建的Pod对象也存在进入某个ReplicaSet作用域的可能性。因而，我们要精心设计同一名称空间中使用的标签选择器，以竭力避免它们以相同的条件出现在不同的控制器对象之上，这种原则同样交叉适用于其他类型的控制器对象。ReplicaSet规范中的副本数量、标签选择器，甚至是Pod模板都可以在对象创建后随时按需进行修改。降低期望的Pod副本数量会导致删除现有的Pod对象，而增加该数量值会促使ReplicaSet控制器根据模板创建出新的Pod对象。修改标签选择器会导致ReplicaSet在当前名称空间中匹配Pod标签，这可能会让它无法再匹配到现有Pod副本的标签，进而触发必要的删除或创建操作。另外，ReplicaSet不会关注筛选到的现存Pod对象或者由其自身创建的Pod对象中的实际内容，因此Pod模板的改动也仅会对后来新建的Pod副本有影响。事实上，ReplicaSet所支持的更新机制也正是建立在Pod模板更新后以“删除后的自动重建”机制之上。相较于手动创建和管理Pod对象来说，ReplicaSet控制器能够实现以下功能。 确保Pod对象的数量精确反映期望期：ReplicaSet对象需要确保由其控制运行的Pod副本数量精确吻合配置中定义的期望值，否则会自动补足所缺或终止所余。 确保Pod健康运行：探测到由其管控的Pod对象健康状态检查失败或因其所在的工作节点故障而不可用时，自动请求控制平面在其他工作节点创建缺失的Pod副本。 弹性伸缩：应用程序业务规模因各种原因时常存在明显波动，如波峰或波谷期间，可以通过改动ReplicaSet控制器规范中的副本数量动态调整相关Pod资源对象的数量，甚至是借助HPA控制器实现Pod资源规模的自动伸缩。 但ReplicaSet并非是用户使用无状态应用控制器的最终形态，Deployment控制器基于ReplicaSet实现了滚动更新、自动回滚、金丝雀部署甚至是蓝绿部署等更为高级和自动化的任务编排功能，因而成为用户在编排无状态应用时更高级的选择。 ReplicaSet基础应用ReplicaSet由kind、apiVersion、metadata、spec和status这5个一级字段组成，它的基本配置框架如下面的配置规范所示。 12345678910111213141516apiVersion: apps/v1kind: ReplicaSetmetadata: name: … namespace: …spec: minReadySeconds &lt;integer&gt; # Pod就绪后多少秒内，任一容器无崩溃方可视为“就绪” replicas &lt;integer&gt; # 期望的Pod副本数，默认为1 selector: # 标签选择器，必须匹配template字段中Pod模板中的标签 matchExpressions &lt;[]Object&gt; # 标签选择器表达式列表，多个列表项之间为“与”关系 matchLabels &lt;map[string]string&gt; # map格式的标签选择器 template: # Pod模板对象 metadata: # Pod对象元数据 labels: # 由模板创建出的Pod对象所拥有的标签，必须要能够匹配前面定义的标签选择器 spec: # Pod规范，格式同自主式Pod …… ReplicaSet规范中用于定义标签选择器的selector字段为必先字段，它支持matchLabels和matchExpressions两种表示格式。前者使用字符串映射格式，以key: value形式表达要匹配的标签；后者支持复杂的表达式格式，支持基于“等值（运算符=和!=）”和基于“集合”（运算符为in和notin等）的表示方法，同时定义二者时的内生逻辑为“与”关系。Pod模板中定义的标签必须要能匹配到其所属ReplicaSet对象的标签选择器，否则，ReplicaSet将因始终不具有足额的Pod副本数而无限创建下去，这相当于程序代码中无终止条件的死循环。另外，minReadySeconds字段用于指定在Pod对象启动后的多长时间内其容器未发生崩溃等异常情况即被视为“就绪”，默认为值0秒，表示一旦就绪性探测成功，即被视作可用。将“Pod模板资源”一节中的示例保存于资源清单文件中，例如replicaset-demo.yaml，而后即可使用类似如下命令将其创建到集群上来观察其运行特性。 12~$ kubectl apply -f replicaset-demo.yaml replicaset.apps/replicaset-demo created ReplicaSet对象的详细描述信息会输出对象的重点信息，例如标签选择器、Pod状态、Pod模板和相关的事件等。ReplicaSet控制器会追踪作用域的各Pod的运行状态，并把它们归类到Running、Waiting、Succeeded和Failed这4种状态之中。default名称空间中并未存在使用app: replicaset-demo这一标签的Pod对象，因此replicaset-demo需要根据指定的Pod模板创建出replicas字段指定数量的Pod实例，它们的名称以其所属的ReplicaSet对象的名称为前缀。如下的命令输出中可知，replicaset-demo成功创建出的两个Pod实例均处于健康运行状态。 123456789101112131415161718192021222324~$ kubectl describe replicasets/replicaset-demoName: replicaset-demoNamespace: defaultSelector: app=demoapp,release=stableLabels: &lt;none&gt;Annotations: Replicas: 2 current / 2 desiredPods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 FailedPod Template: Labels: app=replicaset-demo Containers: demoapp: Image: ikubernetes/demoapp:v1.0 Port: 80/TCP Host Port: 0/TCP Liveness: http-get http://:80/livez delay=5s timeout=1s period=10s #success=1 #failure=3 Readiness: http-get http://:80/readyz delay=15s timeout=1s period=10s #success=1 #failure=3 Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 36s replicaset-controller Created pod: replicaset-demo-z6bqt Normal SuccessfulCreate 36s replicaset-controller Created pod: replicaset-demo-vwb5g 我们也可以单独打印ReplicaSet对象的简要及扩展信息来了解其运行状态，例如期望的Pod副本数（DESIRED）、当前副本数（CURRENT）和就绪的副本数（READY），以及使用的镜像和标签选择器等，如下面的命令及结果所示。 123~$ kubectl get replicasets/replicaset-demo -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORreplicaset-demo 2 2 2 77s demoapp …… app=demoapp,… 通常，就绪的副本数量与期望的副本数量相同便意味着该ReplicaSet控制器以符合期望的状态运行于集群之上，由其编排的容器应用可正常借助专用的Service对象向客户端提供服务。长期运行中的Kubernetes系统环境存在着不少导致Pod对象数目与期望值不符合的可能因素，例如作用域内Pod对象的意外删除、Pod对象标签的变动、ReplicaSet控制器的标签选择器变动，甚至是作用域内Pod对象所在的工作节点故障等。ReplicaSet控制器的调谐循环能实时监控到这类异常，并及时启动调谐操作。任何原因导致的标签选择器匹配Pod对象缺失，都会由ReplicaSet控制器自动补足，我们可通过手动修改replicaset-demo标签选择器作用域内任一现有Pod对象标签，使得匹配失败，则该控制器的标签选择器会触发控制器的Pod对象副本缺失补足机制，其操作步骤如下。 步骤1：获取replicaset-demo标签选择器作用域内的一个Pod对象： 123~$ pod=$(kubectl get pods -l app=demoapp,release=stable -o jsonpath=&#123;.items[1].metadata.name&#125;)~$ echo $podreplicaset-demo-z6bqt 步骤2：删除该Pod对象的任意一个标签，例如app，使其无法再匹配到replicaset-demo的标签选择器： 12~$ kubectl label pod $pod app-pod/replicaset-demo-z6bqt labeled 步骤3：验证replicaset-demo是否将此前的Pod对象替换为了新建的Pod对象： 1234~$ kubectl get pods -l app=demoapp,release=stableNAME READY STATUS RESTARTS AGEreplicaset-demo-fcrkl 0/1 Running 0 4sreplicaset-demo-vwb5g 1/1 Running 0 5m49s 步骤4：可以看到此前的Pod对象依然存在，但它成为自主式Pod对象，而代表上级引用关系的metadata.ownerReferences字段变成空值，于是下面的命令便不再有返回值。 1~$ kubectl get pods $pod -o jsonpath=&#123;.metadata.ownerReferences&#125; 由此可见，通过修改Pod资源的标签即可将其从控制器的管控之下移出，若修改后的标签又能被其他控制器资源的标签选择器命中，则该Pod对象又成为隶属另一控制器的副本。若修改其标签后的Pod对象不再隶属于任何控制器，它就成了自主式Pod。另一方面，一旦被标签选择器匹配到的Pod对象数量因任何原因超出期望值，多余的部分也将被控制器自动删除。例如，我们可以为此前移出作用域的Pod对象重新添加app标签，让其能够再次匹配到replicaset-demo的标签选择器，这将触发控制器删除多余的Pod对象，如下面的命令结果所示。 1234567~$ kubectl label pods $pod app=demoapppod/replicaset-demo-z6bqt labeled~$ kubectl get pods -l app=demoapp,release=stableNAME READY STATUS RESTARTS AGEreplicaset-demo-fcrkl 1/1 Terminating 0 2m17sreplicaset-demo-vwb5g 1/1 Running 0 8m2sreplicaset-demo-z6bqt 1/1 Running 0 8m2s 应用更新与回滚ReplicaSet不会校验作用域内处于活动状态的Pod对象的内容，改动Pod模板的定义对已经创建完成的活动对象无效，但在用户手动删除其旧版本的Pod对象后能够自动以新代旧，实现控制器下的应用更新。通过修改Pod中某容器的镜像文件版本进行应用程序的版本升级是最常见的应用更新场景。尽管ReplicaSet资源的Pod模板可随时按需修改，但它仅影响其后新建的Pod对象，对已有的Pod副本不产生作用，因此，ReplicaSet自身并不会自动触发更新机制，它依赖于用户的手动触发机制。Deployment控制器是建立在ReplicaSet之上的，专用于支持声明式更新功能的更高级实现。在配置清单replicaset-demo.yaml中定义的replicaset-demo资源中，修改Pod模板中的容器使用的镜像文件为更高的版本，例如下面示例性配置片段中的ikubernetes/demoapp: v1.1，而后将变动的配置清单重新应用到集群上便可完成ReplicaSet控制器资源的更新。 123456containers:- name: demoapp image: ikubernetes/demoapp:v1.1 ports: - name: http containerPort: 80 将更新后的配置清单应用到集群之中，可以发现，现有各Pod对象中demoapp容器的镜像版本与replicaset-demo的Pod模板中的镜像版本存在差异。 12~$ kubectl apply -f replicaset-demo.yamlreplicaset.apps/replicaset-demo configured 首先，我们可以使用如下命令获取活动对象replicaset-demo的Pod模板中定义的镜像文件及版本信息： 12~$ kubectl get replicasets/replicaset-demo -o jsonpath=&#123;.spec.template.spec.containers[0].image&#125;ikubernetes/demoapp:v1.1 接着，通过如下命令获取replicaset-demo控制下的所有Pod对象中的demoapp容器的镜像文件及版本信息： 1234$ kubectl get pods -l app=demoapp,release=stable \\-o jsonpath=&#x27;&#123;range .items[*]&#125;[&#123;.metadata.name&#125;, &#123;.spec.containers[0].image&#125;]&#123;&quot;\\n&quot;&#125;&#123;end&#125;&#x27;[replicaset-demo-vwb5g, ikubernetes/demoapp:v1.0][replicaset-demo-z6bqt, ikubernetes/demoapp:v1.0] 上面两个命令及返回结果证实了“更新Pod模板不会对现在的Pod对象产生实质影响”的结论。另一方面，Pod中定义的容器及镜像的字段是不可变字段，我们无法在Pod创建完成后动态更新其容器镜像，因而接下来只有手动将replicaset-demo的现有Pod对象移出其标签选择器作用域（修改标签或删除Pod对象）来触发基于新的Pod模板新建Pod对象，以完成应用的版本更新。 常见更新机制常见的更新机制有如下两种。 1）单批次替换一次性替换所有Pod对象（见图8-6）：也称为重建式更新（recreate），是最为简单、高效的更新方式，但会导致相应的服务在一段时间内（至少一个Pod对象更新完成并就绪）完全不可用，因而一般不会用在对服务可用性有较高要求的生产环境中。 2）多批次替换一次仅替换一批Pod对象（见图8-7）：也称为滚动更新，是一种略复杂的更新方式，需要根据实时业务量和Pod对象的总体承载力做好批次规划，而后待一批Pod对象就绪后再更新另一批，直到全部完成为止；该策略实现了不间断服务的目标，但更新过程中会出现不同的应用版本并存且同时提供服务的状况。 接下来，我们在replicaset-demo之上分别进行更新测试来验证这两种方式的更新效果。我们先为replicaset-demo作用域内的各Pod对象创建一个ClusterIP类型的Service对象，以方便客户端在更新过程中进行请求测试，以下配置保存于service-for-replicaset-demo.yaml清单文件中。 123456789101112131415apiVersion: v1kind: Servicemetadata: name: demoapp namespace: defaultspec: type: ClusterIP selector: app: demoapp release: stable ports: - name: http port: 80 protocol: TCP targetPort: 80 接下来，将上面配置清单中的Service对象demoapp通过如下命令创建到集群之上，随后的应用测试将以之作为访问入口。 12~$ kubectl apply -f service-for-replicaset-demo.yaml service/demoapp created 重建式更新测试 步骤1：在管理节点上打开一个新的终端，创建一个临时的客户端Pod并发起持续性的请求测试，以验证单批次更新过程中是否会发生服务中断。 123~$ kubectl run pod-$RANDOM --image=ikubernetes/admin-toolbox:v1.0 -it \\ --rm --command -- /bin/sh[root@pod-28426 /]# 此时，在临时Pod的交互式接口中运行如下循环进行请求测试，立即可以看到v1.0版本的demoapp的响应结果； 12[root@pod-28426 /]# while true; do curl --connect-timeout 1 \\ demoapp.default.svc; sleep 1; done 步骤2：删除replicaset-demo作用域内的所有Pod对象，而后观察其更新结果。 123~$ kubectl delete pods -l app=demoapp,release=stablepod &quot;replicaset-demo-vwb5g&quot; deletedpod &quot;replicaset-demo-z6bqt&quot; deleted 步骤3：使用如下命令查看是否生成具有同样标签的新Pod对象。在如下命令结果中的任何一个新Pod对象就绪之前，curl命令返回结果会出现一定数量的请求超时，这是单批次更新的必然结果；验证完成后，应该停止测试循环。 1234~$ kubectl get pods -l app=demoapp,release=stable NAME READY STATUS RESTARTS AGEreplicaset-demo-mjc5x 0/1 Running 0 10sreplicaset-demo-w5lxw 0/1 Running 0 10s 步骤4：验证这些新的Pod对象中demoapp容器是否更新为指定的新镜像文件及版本。 12345~$ kubectl get pods -l app=demoapp,release=stable \\ -o jsonpath=&#x27;&#123;range .items[*]&#125;[&#123;.metadata.name&#125;, &#123;.spec.containers[0]. image&#125;]&#123;&quot;\\n&quot;&#125;&#123;end&#125;&#x27;[replicaset-demo-mjc5x, ikubernetes/demoapp:v1.1][replicaset-demo-w5lxw, ikubernetes/demoapp:v1.1] 事实上，修改Pod模板时，不仅能替换镜像文件的版本，甚至可以将其替换为其他应用程序的镜像，只不过此类需求并不多见。若同时改动的还有Pod模板中的其他字段，在新旧更替的过程中，它们也将随之被应用。 滚动式更新测试 步骤1：同前一节中的测试方式相似，我们需要在管理节点上打开一个新的终端，创建一个临时的客户端Pod以发起持续性的请求测试，以验证滚动更新过程中是否会发生服务中断。 123~$ kubectl run pod-$RANDOM --image=ikubernetes/admin-toolbox:v1.0 -it \\ --rm --command -- /bin/sh[root@pod-10196 /]# 此时，在临时Pod的交互式接口中运行如下循环进行请求测试，立即可以看到v1.0版本的demoapp的响应结果； 12[root@pod-10196 /]# while true; do curl --connect-timeout 1 \\ demoapp.default.svc; sleep 1; done 步骤2：更新replicaset-demo的Pod模板中demoapp容器使用ikubernetes/demoapp:v1.2镜像。本次，我们使用更便捷的kubectl set image命令。 12~$ kubectl set image replicasets/replicaset-demo demoapp=&quot;ikubernetes/demoapp:v1.2&quot;replicaset.apps/replicaset-demo image updated 步骤3：将replicaset-demo作用域的仅有的两个Pod对象分成两个批次进行更新。为了便于识别待删除对象，下面的命令获取现有的相关两个Pod对象的名称保存在数组中，并打印出相关的Pod对象名称。 1234~$ pods=($(kubectl get pods -l app=demoapp,release=stable \\ -o jsonpath=&quot;&#123;range .items[*]&#125;&#123;.metadata.name&#125;&#123;&#x27;\\t&#x27;&#125;&#123;end&#125;&quot;))~$ echo $&#123;pods[@]&#125;replicaset-demo-l857r replicaset-demo-r9t8f 步骤4：尝试删除一个Pod对象，以触发启动更新操作。随后，立即运行一个交互式的监视命令持续监视replicaset-demo作用域内各Pod对象的状态变动，可以看到旧版本Pod对象的删除及新Pod创建过程中的事件。 12345678~$ kubectl delete pods $&#123;pods[1]&#125;pod &quot;replicaset-demo-r9t8f&quot; deleted~$ kubectl get pods -l app=demoapp,release=stable -wNAME READY STATUS RESTARTS AGEreplicaset-demo-l857r 1/1 Running 0 143mreplicaset-demo-r9t8f 1/1 Terminating 0 143mreplicaset-demo-zxsh7 0/1 Running 0 8sreplicaset-demo-zxsh7 1/1 Running 0 20s 在删除命令执行后的新建Pod对象replicaset-demo-zxsh7就绪之前，客户端持续发出访问请求的所有响应均应该来自未删除的旧版本Pod对象。新Pod就绪后才能由相应的Service对象demoapp识别为Ready状态的后端端点，并路由请求报文至该端点，此时响应报文来自一新一旧两个版本的Pod对象，下面的内容就截取自相关测试命令的返回结果。 12iKubernetes demoapp v1.2 !!……, ServerName: replicaset-demo-zxsh7, ServerIP: 10.244.3.16!iKubernetes demoapp v1.1 !!……, ServerName: replicaset-demo-l857r, ServerIP: 10.244.1.41! 步骤5：再删除另一个旧版本的Pod对象，待替换的新Pod就绪后，测试命令的响应内容均来自于新版本的Pod对象，滚动更新也就全部完成了。 12~$ kubectl delete pods $&#123;pods[0]&#125;pod &quot;replicaset-demo-l857r&quot; deleted 由以上测试过程可知，滚动更新过程不会导致服务中断，唯一的问题在于两个版本有短暂的共存期，若两个版本使用了不同的数据库格式，则需要禁 止新版本执行写操作，以免数据异常。必要时，用户还可以将Pod模板改回旧的版本进行应用的“降级”或“回滚”，它的操作过程与上述过程类似，不同之处仅是将镜像文件改为过去曾使用过的历史版本。 应用扩容与缩容改动ReplicaSet控制器对象配置中期望的Pod副本数量（replicas字段）会由控制器实时做出响应，从而实现应用规模的水平伸缩。replicas的修改及应用方式同Pod模板，不过，kubectl提供了一个专用的子命令scale用于实现应用规模的伸缩，它支持从资源清单文件中获取新的目标副本数量，也可以直接在命令行通过–replicas选项读取，例如将replicaset-demo控制器的Pod副本数量提升至4个： 12~$ kubectl scale replicasets/replicaset-demo --replicas=4replicaset.apps/replicaset-demo scaled 由下面显示的rs-example资源的状态可以看出，将其Pod资源副本扩展至5个的操作已经成功完成： 123~$ kubectl get replicasets/replicaset-demoNAME DESIRED CURRENT READY AGEreplicaset-demo 4 4 4 3h ReplicaSet缩容的方式与扩容方式相同，我们只需要明确指定目标副本数量即可。例如： 12345~$ kubectl scale replicasets/replicaset-demo --replicas=1replicaset.apps/replicaset-demo scaled~$ kubectl get replicasets/replicaset-demo NAME DESIRED CURRENT READY AGEreplicaset-demo 1 1 1 3h 另外，kubectl scale命令还支持在现有Pod副本数量符合指定值时才执行扩展操作，这仅需要为命令使用–current-replicas选项即可。例如，下面的命令表示如果replicaset-demo目前的Pod副本数量为2，就将其扩展至3个： 12~$ kubectl scale replicasets/replicaset-demo --current-replicas=2 --replicas=3error: Expected replicas to be 2, was 1 但由于replicaset-demo控制器现存的副本数量是1个，上面的扩容操作不会真正执行，而是仅返回了错误提示。 高级更新策略除联合使用多个ReplicaSet外，我们还能为应用更新功能模拟实现更加灵活和更易于维护的滚动更新、金丝雀部署和蓝绿部署等。1. 滚动更新ReplicaSet上的应用更新也能够不改变现有资源（简称为rs-old）的定义，而是借助创建一个有着新版本Pod模板的新ReplicaSet资源（简称为rs-new）实现。新旧版本的ReplicaSet使用了不同的标签选择器，它们筛选相同的Pod标签，但至少会有一个标签匹配到不同的值，余下的标签各自匹配相同值，相关的Service对象的标签选择器会匹配这些拥有相同值的标签。我们可以设计用rs-old和rs-new共同筛选app、release和version标签，其中app和release分别匹配相同值，例如app=demoapp、release=stable，而version则匹配不同值，如rs-old匹配version=v1.0，而rs-new匹配version=v1.1。同时，Service的标签选择器则筛选app=demoapp和release=stable，以便能匹配到更新期间两个不同ReplicaSet作用域内不同版本的Pod对象。具体如图8-8所示。 rs-new的初始副本数为0，在更新过程中，我们以特定的分批（每个批次简称1个单位或步长）策略逐步增加rs-new的replicas字段值，并同步降低rs-old的replicas字段值，直到rs-new副本数为期望的数量，而rs-old的副本数为0时更新过程结束，如图8-9所示。 具体操作时，我们可以采取如下3种不同的策略进行整个滚动更新过程： 先于rs-new上增加1个单位的Pod副本，待全部就绪后再于rs-old上降低1个单位的副本数，待所有旧Pod成功终止后进行下一批次； 先于rs-old上降低1个单位的Pod副本，待所有旧Pod成功终止后再于rs-new上增加1个单位的副本数，再待所有新Pod对象就绪后进行下一批次； 以同步的方式进行，rs-new上新增1个单位的Pod对象，与此同时，rs-old上降低1个单位的副本数，等新Pod全部就绪且旧Pod全部成功终止后进行下一批次。 由此可见，第一和第三种策略会导致更新过程中，新旧两个ReplicaSet资源作用域内的Pod对象总和超出用户期望的副本数，而第二种和第三种策略会使得更新过程中Service的可用后端端点数缺少1个单位，但第三种策略能够更快地完成更新过程。因而，选择更新策略就存在两种重要的判断标准：一是Kubernetes集群资源是否可承载短时间内Pod数量的增加；另一个是支撑相应服务请求总量所依赖的Pod实例数。无论采取哪种滚动策略，我们都可以让更新过程在完成第一批次后暂停一段时长，根据新版本发现的问题以及路由到新版本应用上的用户体验和反馈，来判断是继续完成余下批次的更新操作，还是撤回此前一个批次的更新操作。显然，这种方式能够降低更新过程中的风险，它通过放出的一只“金丝雀”（canary）避免了更大范围的更新故障。另外，我们可保留最近一个范围内的副本数为0的旧版本的ReplicaSet资源于更新历史中，以便按需对比历史更新中所做出的改动，随时按需以类似于“回滚”的更新策略应用至历史中的任一版本。但显然上述的这些操作步骤过于烦琐，以手动方式操作极易出错，幸运的是，更高级别的Pod控制器Deployment能自动实现滚动更新和回滚，并为用户提供了自定义更新策略的接口，这些内容我们将在8.3节中展开说明。2. 蓝绿部署滚动更新过程中，会存在两个不同版本的应用同时向客户端提供服务，且更新和回滚过程耗时较长。另一种更为妥帖的更新方式是，在旧版本ReplicaSet资源运行的同时直接创建一个全Pod副本的新版本ReplicaSet，待所有的新Pod就绪后一次性地将客户端流量全部迁至新版本之上，这种更新策略也称为蓝绿部署（Blue-Green Deployment）。显然，为了避免更新过程中新旧版本ReplicaSet资源的Pod完全并存时Service将流量发往不同版本的Pod对象，我们需要设定Service使用的标签选择器仅能匹配到其中一个版本的Pod对象。最简单的实现方式是让Service与ReplicaSet使用完全相同的标签选择器，但每次更新过程中，在新版本所有Pod就绪之后，修改其标签选择器与新版本的ReplicaSet的标签选择器相同，如图8-10所示。 Service将所有客户端流量代理至新版本的Pod上运行一段时长之后，若确定运行正常，即可将旧版本ReplicaSet的副本数置零后保存到历史版本序列中。相较于滚动更新来说，蓝绿部署实现步骤要简单很多，用户完全能够以手动方式完成。例如，下面的配置清单通过环境变量的方式定义了一个可复用的ReplicaSet资源规范，其中的DEPLOY代表部署类型blue或green，而VERSION则用于表示demoapp的程序版本号，它保存在replicaset-blue-green.yaml文件中。 12345678910111213141516171819202122232425apiVersion: apps/v1kind: ReplicaSetmetadata: name: rs-$&#123;DEPLOY&#125;spec: minReadySeconds: 3 replicas: 2 selector: matchLabels: app: demoapp ctr: rs-$&#123;DEPLOY&#125; version: $&#123;VERSION&#125; template: metadata: labels: app: demoapp ctr: rs-$&#123;DEPLOY&#125; version: $&#123;VERSION&#125; spec: containers: - name: demoapp image: ikubernetes/demoapp:$&#123;VERSION&#125; ports: - name: http containerPort: 80 而下面的这个配置清单以类似的方式定义了一个方便复用的Service资源规范，其中的环境变量的作用与前一个配置清单中的环境变量相同，该配置保存在service-blue-green.yaml文件中。 12345678910111213141516apiVersion: v1kind: Servicemetadata: name: demoapp-svc namespace: defaultspec: type: ClusterIP selector: app: demoapp ctr: rs-$&#123;DEPLOY&#125; version: $&#123;VERSION&#125; ports: - name: http port: 80 protocol: TCP targetPort: 80 为了测试蓝绿部署的效果，我们先将replicaset-blue-green.yaml配置清单中的ReplicaSet资源以rs-blue的名称部署为待更新的老版本，它使用1.0的demoapp镜像，创建的Pod对象名称均以rs-blue为前缀。 12~$ DEPLOY=&#x27;blue&#x27; VERSION=&#x27;v1.0&#x27; envsubst &lt; replicaset-blue-green.yaml | kubectl apply -f -replicaset.apps/rs-blue created 提示envsubst是一个shell命令，能够从标准输入接收文本，完成环境变量替换。接下来，将service-blue-green.yaml配置清单中的Service资源demoapp-svc部署到集群上，它使用同rs-blue对象相同的标签选择器，等rs-blue作用域内的至少一个Pod就绪后即可接受客户端请求。 12~$ DEPLOY=&#x27;blue&#x27; VERSION=&#x27;v1.0&#x27; envsubst &lt; service-blue-green.yaml | kubectl apply -f -service/demoapp-svc created 随后，在新终端中启动一个用于测试的临时Pod对象，在其接口使用curl命令发起持续性访问请求。 1234~$ kubectl run pod-$RANDOM --image=ikubernetes/admin-toolbox:v1.0 -it \\ --rm --command -- /bin/sh[root@pod-30411 /]#[root@pod-30411 /]# while true; do curl --connect-timeout 1 demoapp-svc; sleep 1; done 待rs-blue期望的两个Pod对象均能正常提供服务后，即可假设需要更新到新的demoapp版本。此时，我们需要先基于replicaset-blue-green.yaml配置清单创建名为rs-green的新版本ReplicaSet。 12~$ DEPLOY=&#x27;green&#x27; VERSION=&#x27;v1.1&#x27; envsubst &lt; replicaset-blue-green.yaml | kubectl apply -f - replicaset.apps/rs-green created 随后，等到rs-green的两个Pod均就绪后，将Service对象demoapp-svc的标签选择器修改为匹配新版本ReplicaSet对象rs-green作用域内的所有Pod，可通过如下命令完成。 12~$ DEPLOY=&#x27;green&#x27; VERSION=&#x27;v1.1&#x27; envsubst &lt; service-blue-green.yaml | kubectl apply -f - service/demoapp-svc configured 这时，我们可以在专用于发起请求测试的终端上看到所有的响应报文均来自新版本的Pod中的容器应用demoapp。最后，将rs-blue的Pod副本数设置为0即可。 12~$ kubectl scale replicasets/rs-blue --replicas=0replicaset.apps/rs-blue scaled 显然，由于蓝绿部署要求两个及以上版本应用的Pod同时在线，对于应用规模较大而集群资源较为紧张的场景就成为“不可能”任务，而滚动更新则不具有这方面的问题。下面我们将着力介绍可用于声明式更新功能的Deployment控制器。 Deployment控制器Deployment（简写为deploy）是Kubernetes控制器的一种高级别实现，它构建于ReplicaSet控制器之上，如图8-11所示。它可用于为Pod和ReplicaSet资源提供声明式更新，并能够以自动方式实现8.2节中介绍的跨多个ReplicaSet对象的滚动更新功能。相比较来说，Pod和ReplicaSet是较低级别的资源，以至于很少被直接使用。 Deployment控制器资源的主要职责同样是为了保证Pod资源健康运行，其大部分功能通过调用ReplicaSet控制器实现，并增添了部分特性。 事件和状态查看：必要时可以查看Deployment对象的更新进度和状态。 版本记录：将Deployment对象的更新操作予以保存，以便后续可能执行的回滚操作使用。 回滚：更新操作启动后的任一时刻（包括完成后）发现问题，都可以通过回滚机制将应用返回到前一个或由用户指定的历史记录中的版本。 暂停和启动：更新过程中能够随时暂停和继续完成后面的步骤。 多种更新方案：一是Recreate，即重建更新机制，单批次更新所有Pod对象；另一个是RollingUpdate，即滚动更新机制，多批次逐步替换旧有的Pod至新的版本。 Deployment资源的扩缩容机制与ReplicaSet相同，修改.spec.replicas即能实时触发其规模变动操作。另外，kubectl scale是专用于扩展特定控制器类型的应用规模的命令，包括Deployment、ReplicaSet和StatefulSet等。 Deployment基础应用Deployment是标准的API资源类型，它以ReplicaSet资源为基础资源进行应用编排，并能够自动实现策略式滚动更新或单批次重建式更新，因而它的spec字段中嵌套使用的字段包含了ReplicaSet控制器支持的所有字段，而Deployment也正是利用这些信息完成其二级资源ReplicaSet对象的创建。另外，Deployment还支持几个专用于定义部署及相关策略的字段，具体使用说明如下。 123456789101112131415161718apiVersion: apps/v1 # API群组及版本kind: Deployment # 资源类型特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；Deployment隶属名称空间级别spec: minReadySeconds &lt;integer&gt; # Pod就绪后多少秒内任一容器无崩溃方可视为“就绪” replicas &lt;integer&gt; # 期望的Pod副本数，默认为1 selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板的标签 template &lt;object&gt; # Pod模板对象 revisionHistoryLimit &lt;integer&gt; # 滚动更新历史记录数量，默认为10 strategy &lt;Object&gt; # 滚动更新策略 type &lt;string&gt; # 滚动更新类型，可用值有Recreate和Rollingupdate rollingUpdate &lt;Object&gt; # 滚动更新参数，专用于RollingUpdate类型 maxSurge &lt;string&gt; # 更新期间可比期望的Pod数量多出的数量或比例 maxUnavailable &lt;string&gt; # 更新期间可比期望的Pod数量缺少的数量或比例 progressDeadlineSeconds &lt;integer&gt; # 滚动更新故障超时时长，默认为600秒 paused &lt;boolean&gt; # 是否暂停部署过程 若无须自定义更新策略等相关配置，除了资源类型之外，Deployment资源的基础配置格式几乎与ReplicaSet完全相同。下面是一个配置清单示例，它定了一个名为deployment-demo的Deployment资源，为了便于复用，我们把镜像标签以环境变量VERSION进行标识。 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1kind: Deploymentmetadata: name: deployment-demospec: replicas: 4 selector: matchLabels: app: demoapp release: stable template: metadata: labels: app: demoapp release: stable spec: containers: - name: demoapp image: ikubernetes/demoapp:$&#123;VERSION&#125; ports: - containerPort: 80 name: http livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 initialDelaySeconds: 5 readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 initialDelaySeconds: 15 同其他类型资源的创建方式类似，Deployment资源规范同样使用kubectl apply或kubectl create命令进行创建，但为了真正、全面地体现Deployment的声明式配置功能，建议统一使用声明式的管理机制创建和更新Deployment资源。 12~$ VERSION=&#x27;v1.0&#x27; envsubst &lt; deployment-demo.yaml | kubectl apply --record -f -deployment.apps/deployment-demo created kubectl get deployments命令可以列出创建的Deployment对象的简要状态信息，下面命令结果显示出的字段中，UP-TO-DATE表示已经满足期望状态的Pod副本数量，而AVAILABLE则表示当前处于就绪状态并已然可向客户端提供服务的副本数量。 123~$ kubectl get deployments/deployment-demoNAME READY UP-TO-DATE AVAILABLE AGEdeployment-demo 4/4 4 4 36s Deployment资源会由控制器自动创建下级ReplicaSet资源，并自动为其生成一个遵循[DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]格式的名称，其中的hash值由Deployment控制器根据Pod模板计算生成。另外，Deployment还会将用户定义在Pod模板上的标签应用到下级ReplicaSet资源之上，并附加一个pod-template-hash的标签，标签值即Pod模板的hash值。 123~$ kubectl get replicasets -l app=demoapp,release=stable --show-labelsNAME DESIRED CURRENT READY AGE LABELSdeployment-demo-b479b6f9f …… app=demoapp,pod-template-hash=b479b6f9f, release=stable Pod对象则使用同上级ReplicaSet资源一样的标签，包括pod-template-hash，而各Pod对象的名称同样遵循ReplicaSet对象对Pod命名的格式，它以ReplicaSet对象的名称为前缀，后跟5位随机字符。下面使用awk过滤出了get pods命令结果中的以deployment-demo开头的所有Pod资源，并显示了它们的标签。 12345~$ kubectl get pods --show-labels | awk &#x27;/^deployment-demo-/&#123;print $1,$NF&#125;&#x27;deployment-demo-b479b6f9f-5phpr app=demoapp,pod-template-hash=b479b6f9f, release=stabledeployment-demo-b479b6f9f-kqk2r app=demoapp,pod-template-hash=b479b6f9f, release=stabledeployment-demo-b479b6f9f-lbsp4 app=demoapp,pod-template-hash=b479b6f9f, release=stabledeployment-demo-b479b6f9f-sbnbj app=demoapp,pod-template-hash=b479b6f9f, release=stable 事实上，Deployment及下级ReplicaSet真正使用的标签选择器也包含pod-template-hash标签，这正是确保Deployment通过多ReplicaSet资源进行滚动更新时，确保各ReplicaSet不会交叉引用同一组Pod对象的一种途径。 Deployment更新策略Deployment只需要由用户指定在Pod模板中要改动的内容，例如容器镜像文件的版本，余下的步骤可交由Deployment控制器自动完成。未定义更新策略的Deployment资源，将以默认方式配置更新策略，资源详细描述能够输出更新策略的相关配置信息，下面以deployment-demo资源为例来了解默认的更新策略。 123456789101112~$ kubectl describe deployments/deployment-demoName: deployment-demo……Annotations: deployment.kubernetes.io/revision: 1Selector: app=demoapp,release=stableReplicas: 4 desired | 4 updated | 4 total | 4 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surge……OldReplicaSets: &lt;none&gt;NewReplicaSet: deployment-demo-b479b6f9f (4/4 replicas created)Events: …… Deployment控制器支持滚动更新（rolling updates）和重新创建（recreate）两种更新策略，默认使用滚动更新策略。重建式更新类同前文中ReplicaSet的第一种更新方式，即先删除现存的Pod对象，而后由控制器基于新模板重新创建出新版本资源对象。通常，只有当应用的新旧版本不兼容（例如依赖的后端数据库的格式不同且无法兼容）时才会使用recreate策略。但重建策略会导致应用在更新期间不可用，因而建议用户使用蓝绿部署的方式进行，除非系统资源不足以支撑蓝绿部署的实现。Deployment控制器的滚动更新操作并非在同一个ReplicaSet控制器对象下删除并创建Pod资源，而是将它们分置于两个不同的控制器之下，当前ReplicaSet对象的Pod副本数量不断减少的同时，新ReplicaSet对象的Pod对象数量不断增加，直到现有ReplicaSet对象的Pod副本数为0，而新控制器的副本数量变得完全符合期望值，如图8-9所示。新旧版本之间区别彼此Pod对象的关键标签为pod-template-hash。多批次更新模式的默认间隔标准是前一批次的所有Pod对象均已就绪，方可启动后一批次的更新。而Deployment还提供了两个配置滚动更新批次的字段，以允许用户自定义更新过程的滚动速率，这两个字段分别用于定义滚动更新期间的Pod总数可向上或向下偏离期望值的幅度。 spec.strategy.rollingUpdate.maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比；例如，如果期望值为10，maxSurge属性值为2，则表示Pod对象总数至多不能超过12个。 spec.strategy.rollingUpdate.maxUnavailable：升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比；默认值为1，这意味着如果期望值是10，则升级期间至少要有9个Pod对象处于正常提供服务的状态。如8.2.5节中的描述，我们通过组织maxSurge和maxUnavailable两个属性协同工作，可组合定义出3种不同的策略完成多批次的应用更新。 先增新，后减旧：将maxSurge设定为小于等于期望值的正整数或相对于期望值的一个百分比，而maxUnavailable的值为0。 先减旧，后增新：将maxUnavailable设定为小于等于期望值的正整数或相对于期望值的一个百分比，而maxSurge的值为0。 同时增减（少减多增）：将maxSurge和maxUnavailabe字段的值同时设定为小于等于期望值的正整数或相对于期望值的一个百分比，二者可以使用不同值。注意maxSurge和maxUnavailable属性的值不可同时为0，否则Pod对象的副本数量在符合用户期望的数量后无法做出合理变动以进行滚动更新操作。显然，deployment-demo的详细描述显示出，Deployment默认为滚动更新设置了同时增减的策略，增减的幅度为期望值的25%，它通过两个批次的创建和3个批次的删除即能完成整个应用的更新，具体过程如图8-12所示。不过，若Pod对象的整体副本数小于4的话，就只能按一次1个Pod对象的方式进行。 Deployment还支持使用spec.minReadySeconds字段来控制滚动更新的速度，其默认值为0，表示新建的Pod对象一旦“就绪”将立即被视作可用，随后即可开始下一轮更新过程。而为该字段指定一个正整数值能够定义新建的Pod对象至少要成功运行多久才会被视作可用，即就绪之后还要等待minReadySeconds指定的时长才能开始下一批次的更新。在一个批次内新建的所有Pod就绪后但转为可用状态前，更新操作会被阻塞，并且任何一个Pod就绪探测失败，都会导致滚动更新被终止。因此，为minReadySeconds赋予一个合理的正整数值，不仅能够减缓滚动更新的速度，还能够让Deployment提前发现一部分程序Bug导致的升级故障。Deployment可保留一部分滚动更新历史（修订记录）中旧版本的ReplicaSet对象，如图8-13所示。Deployment资源可保存的历史版本数量由spec.revisionHistoryLimit属性进行定义。 为了保存升级历史，需要在创建Deployment对象时为命令使用–record选项。尽管滚动更新以节约系统资源著称，但它也存在着一些劣势。直接改动现有环境，会为系统引入不确定性风险，而且一旦在更新过程中遇到问题，回滚操作的过程会较为缓慢。有鉴于此，金丝雀部署可能是较为理想的实现方式。当然，如果不考虑系统资源的可用性，那么传统的蓝绿部署将是更好的选择。 应用更新与回滚Pod模板内容的变动是触发Deployment执行更新操作的必要条件。对于声明式配置的Deployment来说，Pod模板的修改尤其适合使用apply和patch命令进行，不过，若仅是修改容器镜像，set image命令则更为易用。接下来通过更新此前创建的deployment-demo资源来了解Deployment更新操作过程的执行细节。为了使得升级过程更易于观测，这里先使用kubectl patch命令为Deployment的spec.minReadySeconds字段定义一个等待时长，例如30秒： 12~$ kubectl patch deployments/deployment-demo -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;minReadySeconds&quot;:30&#125;&#125;&#x27;deployment.apps/deployment-demo patched 修改Deployment控制器的minReadySeconds、replicas和strategy等字段的值并不会触发Pod资源的更新操作，因为它们不属于template的内嵌字段，对现存的Pod对象不产生任何影响。接下来，我们让Pod模板中的demoapp容器使用ikubernetes/demoapp:v1.1镜像文件，以触发deployment-demo启动滚动更新，下面先尝试使用kubectl apply命令完成更新操作： 12~ $ VERSION=&#x27;v1.1&#x27; envsubst &lt; deployment-demo.yaml | kubectl apply --record -f - deployment.apps/deployment-demo configured kubectl rollout status命令可用于打印滚动更新过程中的状态信息： 1~$ kubectl rollout status deployments/deployment-demo 另外，我们还可以使用kubectl get deployments -w命令监控其更新过程中Pod对象的变动过程： 1~$ kubectl get deployments/deployment-demo -w 滚动更新时，deployment-demo会创建一个新的ReplicaSet控制器对象来管控新版本的Pod对象，升级完成后，旧版本的ReplicaSet会保留在历史记录中，但它的Pod副本数被降为0。 1234~$ kubectl get replicasets -l app=demoapp,release=stableNAME DESIRED CURRENT READY AGEdeployment-demo-59d9f4475b 4 4 4 1m32sdeployment-demo-b479b6f9f 0 0 0 12m deployment-demo标签选择器作用域内的Pod资源对象也随之更新为以新版本ReplicaSet名称deployment-demo-59d9f4475b为前缀的Pod副本。另一方面，因各种原因导致滚动更新无法正常进行，例如镜像文件获取失败等，或者更新后遇到的应用程序级故障，例如新版本Pod中的应用触发了未知Bug等，都应该将应用回滚至之前版本用户指定的历史记录中的版本。我们此前曾分别执行了deployment-demo资源的一次部署和一次更新操作，因此修订记录（revision history）分别记录有这两次操作，它们各有一个修订标识符，最大标识符为当前使用的版本。kubectl rollout history命令能够打印Deployment资源的修订历史： 12345~$ kubectl rollout history deployments/deployment-demodeployment.apps/deployment-demo REVISION CHANGE-CAUSE1 kubectl apply --record=true --filename=-2 kubectl apply --record=true --filename=- 从某种意义上说，回滚亦是更新操作。因而，在deployment-demo之上执行回滚操作意味着将当前版本切换回前一个版本，但历史记录中，其REVISION记录也将随之变动，回滚操作会被当作一次滚动更新追加到历史记录中，而被回滚的条目则会被删除。因而，deployment-demo回滚后修订标识符将从1变为3。回滚操作可使用kubectl rollout undo命令完成： 12~$ kubectl rollout undo deployments/deployment-demo deployment.apps/deployment-demo rolled back 回滚完成后，我们可根据客户端的访问结果来验证deployment-demo是否回滚完成，或者根据当前ReplicaSet对象是否恢复到指定的历史版本进行验证。 123~$ kubectl get replicasets | grep &quot;^deployment-demo&quot;deployment-demo-59d9f4475b 0 0 0 11mdeployment-demo-b479b6f9f 4 4 4 13m 另外，在kubectl rollout undo命令上使用–to-revision选项指定revision号码还可回滚到历史记录中的特定版本。需要注意的是，如果此前的滚动更新过程处于“暂停”状态，回滚操作就需要先将Pod模板的版本改回之前，然后“继续”更新，否则，其将一直处于暂停状态而无法回滚。 金丝雀发布Deployment资源允许用户控制更新过程中的滚动节奏，例如“暂停”或“继续”更新操作，尤其是借助于前文讲到的maxSurge和maxUnavailable属性还能实现更为精巧的过程控制。例如，在第一批新的Pod资源创建完成后立即暂停更新过程，此时，仅有一小部分新版本的应用存在，主体部分还是旧的版本。然后，通过应用层路由机制根据请求特征精心筛选出小部分用户的请求路由至新版本的Pod应用，并持续观察其是否能稳定地按期望方式运行。默认，Service只会随机或轮询地将用户请求分发给所有的Pod对象。确定没有问题后再继续进行完余下的所有Pod资源的滚动更新，否则便立即回滚至第一步更新操作。这便是所谓的金丝雀部署，如图8-14所示。 为了尽可能降低对现有系统及其容量的影响，基于Deployment的金丝雀发布过程通常建议采用“先增后减且可用Pod对象总数不低于期望值”的方式进行。首次添加的Pod对象数量取决于其接入的第一批请求的规则及单个Pod的承载能力，视具体需求而定，为了能更简单地说明问题，接下来采用首批添加1个Pod资源的方式。我们将Deployment控制器的maxSurge属性的值设置为1，并将maxUnavailable属性的值设置为0就能完成设定： 1234~]$ kubectl patch deployments/deployment-demo \\ -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;strategy&quot;:&#123;&quot;rollingUpdate&quot;: &#123;&quot;maxSurge&quot;: 1, &quot;maxUnavailable&quot;: 0&#125;&#125;&#125;&#125;&#x27;deployment.apps/deployment-demo patched 随后，修改Pod模板触发deployment-demo资源的更新过程，进行第一批次更新后立即暂停该部署操作，则新生成的第一批Pod对象便是“金丝雀”，如图8-15所示。暂停Deployment资源的更新过程，需要将其spec.pause字段的值从false修改为true，这可通过修改资源规范后再次应用（apply）完成，也可通过kubectl rollout pause命令进行。例如，下面将deployment-demo资源的Pod模板中的容器镜像进行了修改以触发其更新，但同时使用shell操作符&amp;&amp;随后立即执行了暂停命令： 1234~$ VERSION=&#x27;v1.2&#x27; envsubst &lt; deployment-demo.yaml | kubectl apply --record -f - &amp;&amp; \\ kubectl rollout pause deployments/deployment-demodeployment.apps/deployment-demo configureddeployment.apps/deployment-demo paused 处于“暂停”状态中的Deployment资源的滚动状态也会暂停于某一批更新操作中，我们可以通过状态查看命令打印相关的信息： 12~ $ kubectl rollout status deployments/deployment-demoWaiting for deployment &quot;deployment-demo&quot; rollout to finish: 1 out of 4 new replicas have been updated... 相关的Pod列表也能够显示出旧版本ReplicaSet的所有Pod副本仍在正常运行，而同时新版本ReplicaSet对象也有了一个Pod实例，相关Service对象能够在其就绪后将一定比例的客户端流量引入到该Pod之上。运行足够长的一段时间后，若确认新版本应用没有必须通过回滚才能解决的问题，随后即可使用kubectl rollout resume命令继续后续更新步骤，以完成滚动更新过程。 12~ $ kubectl rollout resume deployments/deployment-demodeployment.apps/deployment-demo resumed kubectl rollout status命令监控到滚动更新过程完成后，即可通过deployment-demo资源及其作用域内的ReplicaSet和Pod对象的相关信息来了解其结果状态。然而，如果“金丝雀”遇险，回滚操作便成了接下来的紧要任务。 StatefulSet控制器无状态应用进程客户端的每次连接均可独立地处理，一次请求和响应即构成一个完整的事务，它们不受已完成的连接或现有其他连接的影响，且意外中断或关闭时仅需要重新建立连接即可，因而，无状态应用的Pod对象可随时由其他由同一模板创建的Pod平滑替代，这也正是Deployment控制器编排应用的方式。 功能分析Kubernetes系统使用专用的StatefulSet控制器编排有状态应用。StatefulSet表示一组具有唯一持久身份和稳定主机名的Pod对象，任何指定该类型Pod的状态信息和其他弹性数据都存放在与该StatefulSet相关联的永久性磁盘存储空间中。StatefulSet旨在部署有状态应用和集群化应用，这些应用会将数据保存到永久性存储空间，它适合部署Kafka、MySQL、Redis、ZooKeeper以及其他需要唯一持久身份和稳定主机名的应用。一个典型的、完整可用的StatefulSet资源通常由两个组件构成：Headless Service和StatefulSet资源。Headless Service用于为各Pod资源固定、唯一的标识符生成可解析的DNS资源记录，StatefulSet用于编排Pod对象，并借助volumeClaimTemplate以静态或动态的PV供给方式为各Pod资源提供专有且固定的存储资源。对于拥有N个副本的StatefulSet资源来说，它会以{0…N–1}依次对各Pod对象进行编号及顺序创建，当前Pod对象就绪后才会创建下一个，删除则以相反的顺序进行，每个Pod删除完成后才会继续删除前一个。Pod资源的名称格式为$(statefulset name)-$(ordinal)，例如名称为web的StatefulSet资源生成的Pod对象的名称依次为web-0、web-1、web-2等，其域名后缀则由相关的Headless Service资源给出，格式为$(service name).$(namespace).svc.cluster.local。Kubernetes 1.7及其之后的版本也支持StatefulSet并行管理Pod对象。配置了volumeClaimTemplate的StatefulSet资源会为每个Pod对象基于存储卷申请配置一个专用的PV，动静供给机制都支持，只是静态供给依赖于管理员的事前配置，如图8-16所示。而删除Pod对象甚至是StatefulSet控制器，并不会删除其相关的PV资源以确保数据可用性，因而Pod对象由节点故障或被驱逐等原因被重新调度至其他节点时，先前同名Pod实例专用的PV及其数据可安全复用。 与Deployment略有不同的是，StatefulSet对应用规模的扩容意味着按索引顺序增加更多的Pod资源，而缩容则表示按逆序依次删除索引号最大的Pod资源，直到规模数量满足目标设定值为止。多数有状态应用都不支持规模性安全、快速的缩减操作，因此StatefulSet控制器不支持并行缩容机制，而是要严格遵守一次仅能终止一个Pod资源的法则，以免导致数据讹误。通常也意味着，存在错误且未恢复的Pod资源时，StatefulSet资源会拒绝启动缩容操作。此外，缩容操作导致的Pod资源终止同样不会删除其相关的PV，以确保数据可用。StatefulSet也支持用户自定义的更新策略，它兼容支持之前版本中的OnDelete策略，以及新的RollingUpdate策略。RollingUpdate是默认的更新策略，更新过程中，更新顺序与终止Pod资源的顺序相同，由索引号最大的开始，终止一个Pod对象并完成其更新后继续进行前一个。此外，StatefulSet资源的滚动更新还支持分区（partition)机制，用户可基于某个用于分区的索引号对Pod资源进行分区，所有大于等于此索引号的Pod对象会被滚动更新，如图8-17所示，而小于此索引号的则不会被更新，而且，即便在此期间该范围内的某Pod对象被删除，它也一样会被基于旧版本的Pod模板重建。 若给定的分区号大于副本数量，意味着不存在大于此分区号的Pod资源索引号，因此，所有的Pod对象均不会被更新，这对于期望暂存发布、金丝雀发布或分段发布来说是有用的设定。 StatefulSet基础应用完整的StatefulSet资源需要由Headless Service和StatefulSet共同构成，StatefulSet资源规范中通过必选字段spec.serviceName指定关联的Headless类型的Service对象名称，但管理该Service是用户的责任，StatefulSet仅是强依赖于它，而不会自动管理它。下面是StatefulSet资源的规范格式及简要说明。 12345678910111213141516171819202122apiVersion: apps/v1 # API群组及版本kind: StatefulSet # 资源类型的特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；StatefulSet隶属名称空间级别spec: replicas &lt;integer&gt; # 期望的Pod副本数，默认为1 selector &lt;object&gt; # 标签选择器，需匹配Pod模板中的标签，必选字段 template &lt;object&gt; # Pod模板对象，必选字段 revisionHistoryLimit &lt;integer&gt; # 滚动更新历史记录数量，默认为10 updateStrategy &lt;Object&gt; # 滚动更新策略 type &lt;string&gt; # 滚动更新类型，可用值有OnDelete和Rollingupdate rollingUpdate &lt;Object&gt; # 滚动更新参数，专用于RollingUpdate类型 partition &lt;integer&gt; # 分区指示索引值，默认为0 serviceName &lt;string&gt; # 相关的Headless Service的名称，必选字段 volumeClaimTemplates &lt;[]Object&gt; # 存储卷申请模板 apiVersion &lt;string&gt; # PVC资源所属的API群组及版本，可省略 kind &lt;string&gt; # PVC资源类型标识，可省略 metadata &lt;Object&gt; # 卷申请模板元数据 spec &lt;Object&gt; # 期望的状态，可用字段同PVC podManagementPolicy &lt;string&gt; # Pod管理策略，默认的OrderedReady表示顺序创 #建并逆序删除，另一可用值Parallel表示并行模式 下面的配置清单示例中定义了一个名为demodb的Headless Service，以及一个同样名为demodb的StatefulSet资源，后者使用了存储卷申请模板，为Pod对象从fast-rbd存储类中请求动态供给并绑定PV。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263apiVersion: v1kind: Servicemetadata: name: demodb namespace: default labels: app: demodbspec: clusterIP: None ports: - port: 9907 selector: app: demodb---apiVersion: apps/v1kind: StatefulSetmetadata: name: demodb namespace: defaultspec: selector: matchLabels: app: demodb serviceName: &quot;demodb&quot; replicas: 2 template: metadata: labels: app: demodb spec: containers: - name: demodb image: ikubernetes/demodb:v0.1 ports: - containerPort: 9907 name: db env: - name: DEMODB_DATADIR value: &quot;/demodb/data&quot; livenessProbe: initialDelaySeconds: 5 periodSeconds: 10 httpGet: path: /status port: db readinessProbe: initialDelaySeconds: 15 periodSeconds: 30 httpGet: path: /status?level=full port: db volumeMounts: - name: data mountPath: /demodb/data volumeClaimTemplates: - metadata: name: data spec: accessModes: [ &quot;ReadWriteOnce&quot; ] storageClassName: &quot;rbd&quot; resources: requests: storage: 1Gi 示例中用到的demodb是一个仅用于测试的分布式键值存储系统，支持持久化数据存储，它由一个Leader和一到多个Followers组成，Followers定期从Leader查询并请求同步数据。Leader支持读写请求，而各Followers节点仅支持只读操作，它们会把接收到的写请求通过307响应码重定向给Leader节点。用于读写请求的URI分别为/get/KEY和/set/KEY，/status则用于输出状态，/status?level=full能够以200响应码返回持有的键数量，否则响应以500状态码返回。demodb仅可由StatefulSet控制器编排运行，并且在程序中将Leader的名称固定为demodb-0，依赖的Headless Service的名称也固定为demodb，因此StatefulSet和Headless Service资源的名称必须要使用demodb。默认情况下，StatefulSet资源使用OrderedReady这一Pod管理策略，它以串行的方式逐一创建各Pod实例及相关的PV，下面在创建后打印的statefulsets/demodb资源详细描述中的各事件的时间点也反映了这种事实。 1234567891011121314151617181920~$ kubectl apply -f demodb.yamlservice/demodb createdstatefulset.apps/demodb created~$ kubectl describe statefulsets/demodb Name: demodbNamespace: defaultSelector: app=demodbLabels: &lt;none&gt;Annotations: Replicas: 2 desired | 2 totalUpdate Strategy: RollingUpdate Partition: 0Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed……Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 2m22s statefulset-controller create Claim data-demodb-0 Pod demodb-0 in StatefulSet demodb success Normal SuccessfulCreate 2m22s statefulset-controller create Pod demodb-0 in StatefulSet demodb successful Normal SuccessfulCreate 97s statefulset-controller create Claim data-demodb-1 Pod demodb-1 in StatefulSet demodb success Normal SuccessfulCreate 97s statefulset-controller create Pod demodb-1 in StatefulSet demodb successful 如前所述，由StatefulSet资源创建的Pod对象拥有固定且唯一的标识符，它们基于唯一的索引序号及相关的StatefulSet对象的名称生成，格式为&lt;statefulset name&gt;-&lt;ordinal index&gt;，例如上面事件信息中显示出由statefuls/demodb所创建的demodb-0和demodb-1两个Pod对象的名称即遵循该格式。事实上，这类Pod对象的主机名也与其资源名称相同，以demodb-0为例，下面的命令打出的主机名称正是Pod资源的名称标识。 12~$ kubectl exec demodb-0 -- hostnamedemodb-0 Headless Service的DNS名称解析会由ClusterDNS以该Service对象关联各Pod对象的IP地址加以响应。而StatefulSet创建的各Pod对象的名称则以相关Headless Service资源的DNS名称为后缀，具体格式为$(pod_name).$(svc_name).$(namespace).svc.cluster.local，例如demodb-0和demodb-1的资源名称分别为demodb-0.demodb.default.svc.cluster.local和demodb-1.demodb.default.svc.cluster.local。下面在一个新的专用终端创建一个临时的、基于Pod对象的交互客户端进行测试。 1~$ kubectl run client --image=ikubernetes/admin-toolbox:v1.0 -it --rm --command -- /bin/sh 首先，请求解析Pod的FQDN格式主机名称，它会返回相应Pod对象的IP地址； 123456[root@client /]# nslookup -query=A demodb-0.demodbServer: 10.96.0.10Address: 10.96.0.10#53Name: demodb-0.demodb.default.svc.cluster.localAddress: 10.244.1.208 接着，创建一个测试文件，将之存储到demodb存储服务以发起数据存储测试。我们知道，CoreDNS默认以roundrobin的方式响应对同一个名称的解析请求，因而以名称方式发往demodb这一Headless Service的请求会轮询到demodb-0和demodb-1之上。 123[root@client /]# echo &quot;Advanced Kubernetes Practices&quot; &gt; /tmp/mydata[root@client /]# curl -L -XPUT -T /tmp/mydata http://demodb:9907/set/mydataWRITE completed 调度至从节点（demodb-1）的写请求会自动重定向给主节点（demodb-0），且主节点数据存储完成后将自动同步至各个从节点；我们可从服务请求读取数据，或者直接从demodb-1读取数据，以进行测试。 1234[root@client /]# curl http://demodb:9907/get/mydataAdvanced Kubernetes Practices[root@client /]# curl http://demodb-1.demodb:9907/get/mydataAdvanced Kubernetes Practices demodb的所有节点会将数据存储在/demodb/data目录下，每个键被映射为一个子目录，数据存储在该子目录下的content文件中。 12~$ kubectl exec demodb-0 -- cat /demodb/data/mydata/contentAdvanced Kubernetes Practices 而各Pod对象的/demodb/data目录挂载到一个由statefulset/demodb存储卷申请模板创建的PVC之上，每个PVC又绑定在由存储类fast-rbd动态供给的PV之上。各PVC的名称由volumeClaimTemplate对象的名称与Pod对象的名称组合而成，格式为$(volume-ClaimTemplate_name).(Pod_name)，如下面的命令结果所示。 1234~$ kubectl get pvc -l app=demodbNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEdata-demodb-0 Bound pvc-de6e81c1-… 2Gi RWO fast-rbd 4m50sdata-demodb-1 Bound pvc-e95d67ca-… 2Gi RWO fast-rbd 4m5s StatefulSet资源作用域内的Pod资源因被节点驱逐，或因节点故障、应用规模缩容被删除，甚至是手动误删除时，它挂载的由存储卷申请模板创建的PVC卷并不会被删除。因而，经StatefulSet资源重建或规模扩容回原来的规模后，每个Pod对象依然有固定的标识符并可关联到此前的PVC存储卷上。 扩缩容与滚动更新StatefulSet资源也支持类似于Deployment资源的应用规模的扩容、缩容以及更新机制。扩缩容通过简单地修改StatefulSet资源的副本数来改动期望的Pod资源数量就能完成，例如，下面的命令能将statefulsets/demodb中的Pod副本数量扩展至4个。 12~$ kubectl scale statefulsets/demodb --replicas=4statefulset.apps/demodb scaled StatefulSet资源的扩容过程与创建过程管理Pod对象的策略相同，默认为顺次进行，而且其名称中的序号也将以现有Pod资源的最后一个序号为基准向后进行。若定义了存储卷申请模板，扩容操作所创建的每个Pod对象也会各自关联所需要的PVC存储卷。与扩容操作相对，将其副本数量调低即能完成缩容操作，例如，下面的命令能够将StatefulSet资源demodb的副本数量缩减至3个。 12~$ kubectl patch statefulsets/demodb -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;&#x27; statefulset.apps/demodb patched 缩容过程中终止Pod资源的默认策略与删除机制相似，它会根据Pod对象的可用索引号逆序逐一进行，直到余下的数量满足期望的值为止。因缩容而终止的Pod资源的存储卷并不会被删除，因此，如果缩减规模后再将其扩展回来，此前的数据依然可用，且Pod资源名称不变。如前所述，在应用更新方面，StatefulSet资源自Kubernetes 1.7版本开始支持自动更新机制，其更新策略则由spec.updateStrategy字段定义，默认为RollingUpdate，即滚动更新。kubectl set image命令也支持修改StatefulSet资源上Pod模板中的容器镜像，因而，触发statefulsets/demodb上的应用升级可使用类似如下一条命令完成。 12~$ kubectl set image statefulsets/demodb demodb-shard=&quot;ikubernetes/demodb:v0.2&quot;statefulset.apps/demodb image updated 滚动更新StatefulSet资源的Pod对象以逆序的方式从其最大索引编号逐一进行，滚动条件为当前更新循环中的各个新Pod资源已然就绪。通常，对于主从复制类的集群应用来说，这种方式能保证担当主节点的Pod资源在最后进行更新，以确保其兼容性。例如，触发statefulsets/demodb更新后，可以看到类似如下命令中首先更新索引编号最大的Pod对象demodb-1的操作。 12345~$ kubectl get pods -l app=demodbNAME READY STATUS RESTARTS AGEdemodb-0 1/1 Running 0 5m42sdemodb-1 1/1 Running 0 4m42sdemodb-2 0/1 ContainerCreating 0 5s StatefulSet资源滚动更新过程中的状态同样可以使用kubectl rollout history命令获取。更新完成后，我们可使用如下命令，确认相关Pod对象使用的容器镜像都已经变更为指定的新版本。 123456~$ kubectl get pods -l app=demodb -o \\ jsonpath=&#x27;&#123;range .items[*]&#125;&#123;.metadata.name&#125;: &#123;.spec.containers[0].image&#125;&#123;&quot;\\n&quot;&#125; &#123;end&#125;&#x27;demodb-0: ikubernetes/demodb:v0.2demodb-1: ikubernetes/demodb:v0.2demodb-2: ikubernetes/demodb:v0.2 滚动更新过程不会影响相应的数据服务，此前的生成的数据键mydata及其数据在更新过程中同样可以正常访问，这在8.4.2节的交互式客户端测试结果中能够得到验证。但是，更新demodb-0期间写操作会有短暂的不可用区间。 12[root@client /]# curl http://demodb:9907/get/mydataAdvanced Kubernetes Practices 进一步地，StatefulSet资源支持使用分区编号（.spec.updateStrategy.rollingUpdate.partition字段值）将其Pod对象分为两个部分，仅那些索引号大于等于分区编号的Pod对象会被更新，默认的分区编号为0，因而滚动更新时，所有的Pod对象都是待更新目标。于是，在更新操作之前，将partition字段的值置为Pod资源的副本数量N（或大于该值）会使得所有的Pod资源（索引号区间为0到N–1）都不再处于可直接更新的分区之内，那么这之后设定的更新操作不会真正执行而是被“暂存”起来，直到降低分区编号至现有Pod资源索引号范围内，才开始触发真正的滚动更新操作。来看下面的例子。首先，将statefulsets/demodb的分区别编号设置为现有的Pod数量值3： 123~$ kubectl patch statefulsets/demodb -p \\ &#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;&#x27;statefulset.apps/demodb patched 而后，更新statefulsets/demodb的Pod模板中的容器镜像为ikubernetes/demodb:v0.3。 12~$ kubectl set image statefulsets/demodb demodb-shard=&#x27;ikubernetes/demodb:v0.3&#x27;statefulset.apps/demodb image updated 接下来，我们验证出最大编号的Pod对象demodb-2的容器镜像并未因执行更新而发生变化，根据更新策略来说，这意味着其他更小索引号的Pod对象更不会发生任何变动： 12~$ kubectl get pods/demodb-2 -o jsonpath=&#x27;&#123;.spec.containers[0].image&#125;&#x27; ikubernetes/demodb:v0.2 再接着，将分区编号降为statefulsets/demodb上的最大索引编号2之后可以验证，仅demodb-2执行了更新操作；如下第二条命令可于分区编号更改后，略等一段时间后再执行： 123456789~$ kubectl patch statefulsets/demodb -p \\ &#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:2&#125;&#125;&#125;&#125;&#x27;statefulset.apps/demodb patched~$ kubectl get pods -l app=demodb -o \\ jsonpath=&#x27;&#123;range .items[*]&#125;&#123;.metadata.name&#125;: &#123;.spec.containers[0].image&#125; &#123;&quot;\\n&quot;&#125;&#123;end&#125;&#x27;demodb-0: ikubernetes/demodb:v0.2demodb-1: ikubernetes/demodb:v0.2demodb-2: ikubernetes/demodb:v0.3 demodb-2就像是一只“金丝雀”，安然渡过一定时长的测试期间后，我们便可继续其他Pod资源的更新操作。若后续待更新的Pod资源数量较少，我们可直接将partition字段的值设置为0，从而让StatefulSet逆序完成后续所有Pod资源的更新。而待更新的Pod资源较多时，也可以将Pod资源以线性或指数级增长的方式来分阶段完成更新操作，操作过程仅仅是分多次更改partition字段值，例如将statefulsets/demodb控制器的分区号以较慢的节奏依次设置为1和0来完成剩余Pod资源的线性分步更新，如图8-18所示。 StatefulSet支持的另一更新策略是OnDelete，这类似于手动更新机制，它以用户的手动删除操作为触发时间点完成应用更新。 StatefulSet的局限性应用于生产环境的分布式有状态应用的各实例间的关系并非像本节示例中的demodb那样简单，它们在拓扑上通常是基于复杂分布式协议的成员关系，例如ZooKeeper集群成员基于ZAB协议的Leader/Follower关系以及etcd集群成员基于Raft协议的对等（peer）关系等。这些分布式有状态应用的内生拓扑结构存在区别，对持久存储的依赖需求也有所不同，并且集群成员的增加、减少以及在故障后的恢复操作通常都会依赖一系列复杂且精细的步骤才能完成，于是StatefulSet控制器无法为其封装统一、标准的管理操作。于是，用户就不得不配置某个特定的有状态应用，在其YAML配置清单中通过“复杂的运维代码”手动编写相关的运维逻辑，例如下面的这段代码便是以StatefuSet资源来编排etcd应用时，在其Pod模板中编写的仅实现了简单功能的运维代码。这看上去既奇怪又低效——每个用户不得不学习相关应用的运维知识并重复“造轮子”，而StatefulSet对此却也爱莫能助。 123456789101112131415161718192021command:- &quot;/bin/sh&quot;- &quot;-ecx&quot;- | IP=$(hostname -i) PEERS=&quot;&quot; for i in $(seq 0 $(($&#123;CLUSTER_SIZE&#125; - 1))); doPEERS=&quot;$&#123;PEERS&#125;$&#123;PEERS:+,&#125;$&#123;SET_NAME&#125;-$&#123;i&#125;=http://$&#123;SET_NAME&#125;-$&#123;i&#125;.$&#123;SET_NAME&#125;:2380&quot; done # start etcd. If cluster is already initialized the `--initial-*` options will be ignored. exec etcd --name $&#123;HOSTNAME&#125; \\ --listen-peer-urls http://$&#123;IP&#125;:2380 \\ --listen-client-urls http://$&#123;IP&#125;:2379,http://127.0.0.1:2379 \\ --advertise-client-urls http://$&#123;HOSTNAME&#125;.$&#123;SET_NAME&#125;:2379 \\ --initial-advertise-peer-urls http://$&#123;HOSTNAME&#125;.$&#123;SET_NAME&#125;:2380 \\ --initial-cluster-token etcd-cluster-1 \\ --initial-cluster $&#123;PEERS&#125; \\ --initial-cluster-state new \\ --data-dir /var/run/etcd/default.etcd 面对这种境况，CoreOS为Kubernetes引入了一个称为Operator的新概念和新组件，它借助CRD（Customed Resource Definition）创建自定义资源类型来完整描述某个有状态应用集群，并相应创建自定义的控制器来编排这些自定义资源类型所创建的各个资源对象。简单来讲，Operator就是一个开发规范和SDK，它合理地利用Kubernetes API的CRD功能扩展出二级抽象，又巧妙地回归到Kubernetes的“控制器”逻辑，从而提供了一个有状态应用的实现接口，用户可利用它开发专用于管理某个特定有状态应用的运维控制器，并按需回馈给社区。目前，Operator社区中涌现了大量的特定实现，例如coreos/etcd-operator、oracle/mysql-operator和jenkinsci/jenkins-operator等，有些分布式应用的可用Operator实现甚至不止一种。Operator官方维护着etcd、Rook、Prometheus和Vault几个Operator，并通过https://github.com/operator-framework/awesome-operators维护着主流的Operator项目列表。这意味着，在Kubernetes系统上部署分布式有状态应用的常用方式是使用Operator，而非自定义StatefulSet资源，我们将在第12章中再举例说明Operator的用法。 DaemonSet控制器Deployment仅用于保证在集群上精确运行多少个工作负载的实例，但有些系统级应用却需要在集群中的每个节点上精确运行单个实例，这就是DaemonSet控制器的核心功用所在。系统级工作负载的副本数量取决于集群中的节点数，而非由用户通过replicas进行定义，更重要的是，后续新加入集群的工作节点也会由DaemonSet对象自动创建并运行为一个相关Pod，而从集群移除节点时，该类Pod对象也将被自动回收且无须重建。此外，管理员也可以使用节点选择器或节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。简单来说，DaemonSet就是一种特殊的控制器，它有着特定的应用场景，通常用于运行那些执行系统级操作任务的应用，例如：▪运行集群存储的守护进程，例如在每个节点上运行的glusterd可用于接入Gluster集群；▪在每个节点上运行日志收集守护进程，例如fluentd、filebeat和logstash等；▪在每个节点上运行监控系统的代理守护进程，例如Prometheus Node Exporter、collectd、Datadog agent、New Relic agent，或Ganglia gmond等。提示以kubeadm部署的Kubernetes集群上，kube-proxy便是由DaemonSet控制器所编排；另外，Flannel网络插件运行在各节点之上的代理程序也使用了该类型的控制器。既然是需要在集群内的每个节点或部分节点运行工作负载的单个实例，那么，也就可以把应用直接运行为工作节点上的系统级守护进程，只是这么一来也就失去了托管给Kubernetes所带来的便捷性。另外，当必须把Pod对象以单实例运行在固定的几个节点并且需要先于其他Pod启动时，才有必要使用DaemonSet控制器，否则就应该使用Deployment控制器。 DaemonSet资源基础应用DaemonSet是标准的API资源类型，它的spec字段中嵌套使用的字段也需要使用selector、template和minReadySeconds，并且它们各自的功能和用法基本相同，但DaemonSet不支持使用replicas，毕竟DaemonSet不是基于期望的副本数，而是基于节点数量来控制Pod资源数量，但template是必选字段。另外，DaemonSet也支持策略式更新，它支持OnDelete和RollingUpdate两种策略，也能够为滚动更新保存修订记录。DaemonSet资源的简要配置规范如下。 1234567891011121314apiVersion: apps/v1 # API群组及版本kind: DaemonSet # 资源类型特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；DaemonSet资源隶属名称空间级别spec: minReadySeconds &lt;integer&gt; # Pod就绪后多少秒内任一容器无崩溃方可视为“就绪” selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板的标签 template &lt;object&gt; # Pod模板对象 revisionHistoryLimit &lt;integer&gt; # 滚动更新历史记录数量，默认为10 updateStrategy &lt;Object&gt; # 滚动更新策略 type &lt;string&gt; # 滚动更新类型，可用值有OnDelete和Rollingupdate rollingUpdate &lt;Object&gt; # 滚动更新参数，专用于RollingUpdate类型 maxUnavailable &lt;string&gt; # 更新期间可比期望的Pod数量缺少的数量或比例 下面的资源清单（daemonset-demo.yaml）示例中定义了一个DaemonSet资源，用于在每个节点运行一个Prometheus node_exporter进程以收集节点级别的监控数据，该进程共享节点的Network和PID名称空间。 123456789101112131415161718192021222324252627282930313233343536373839apiVersion: apps/v1kind: DaemonSetmetadata: name: daemonset-demo namespace: default labels: app: prometheus component: node-exporterspec: selector: matchLabels: app: prometheus component: node-exporter template: metadata: name: prometheus-node-exporter labels: app: prometheus component: node-exporter spec: containers: - image: prom/node-exporter:v0.18.0 name: prometheus-node-exporter ports: - name: prom-node-exp containerPort: 9100 hostPort: 9100 livenessProbe: tcpSocket: port: prom-node-exp initialDelaySeconds: 3 readinessProbe: httpGet: path: &#x27;/metrics&#x27; port: prom-node-exp scheme: HTTP initialDelaySeconds: 5 hostNetwork: true hostPID: true Prometheus node_exporter默认监听TCP协议的9100端口，基于HTTP协议及/metrics输出指标数据，我们可以将daemonset-demo创建到集群之上后，向任一节点IP发起访问，进行测试来验证。 12345678~$ kubectl apply -f daemonset-demo.yaml daemonset.apps/daemonset-demo created~$ curl -s 172.29.9.11:9100/metrics # HELP go_gc_duration_seconds A summary of the GC invocation durations.# TYPE go_gc_duration_seconds summarygo_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125; 8.729e-06go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125; 3.1308e-05…… DaemonSet资源在其详细描述信息输出了相关Pod对象的状态，包括应该在集群上运行的副本数和实际运行的副本数及相关的状态等。 123456Desired Number of Nodes Scheduled: 3Current Number of Nodes Scheduled: 3Number of Nodes Scheduled with Up-to-date Pods: 3Number of Nodes Scheduled with Available Pods: 3Number of Nodes Misscheduled: 0Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed 偶尔也存在需要将Pod对象以单一实例形式运行在集群中的部分工作节点，例如有些拥有特殊硬件节点需要运行特定的监控代理程序等。这仅需要在Pod模板的spec字段中嵌套使用nodeSelector字段，并确保其值定义的标签选择器与部分特定工作节点的标签匹配即可。另外，考虑到大多数系统级应用的特殊性，DaemonSet资源的各Pod实例通常需要被单独访问而不能隐藏在某个Service对象之后，例如无论是监控代理程序或日志采集代理程序所在的节点都需要由其服务器端各自识别并单独进行通信。因此，各节点上的Pod应用推送数据至服务端，使用Headless Service或者直接让Pod应用共享节点的网络名称空间，并监听一个端口（例如node_exporter的9100端口）是满足这种需求的常见做法。 DaemonSet更新策略DaemonSet自Kubernetes 1.6版本起也开始支持更新机制，相关配置定义在spec.update-Strategy嵌套字段中。目前，它支持RollingUpdate和OnDelete两种更新策略。▪RollingUpdate为默认的策略，工作逻辑类似于Deployment控制器上的同名策略，不过节点难以临时弹性增设，因而DaemonSet仅能支持使用maxUnavailabe属性定义最大不可用Pod资源副本数（默认值为1）。▪Ondelete是在相应节点的Pod资源被删除后重建为新版本，从而允许用户手动编排更新过程。将此前创建的daemonset-demo中Pod模板的容器镜像修改为prom/node-exporter:v0.18.1便能测试其更新过程： 123~$ kubectl set image daemonsets/daemonset-demo \\ prometheus-node-exporter=&quot;prom/node-exporter:v0.18.1&quot;daemonset.apps/daemonset-demo image updated 按照默认的RollingUpdate策略，daemonset-demo资源将采用一次更新一个Pod对象，待新建Pod对象就绪后再更新下一个Pod对象的方式进行，资源相关的事件中会详细展示出其更新过程。 1234567891011~$ kubectl describe daemonsets daemonsets/daemonset-demo……Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulDelete 7m44s daemonset-controller Deleted pod: daemonset-demo-btl8x Normal SuccessfulCreate 7m39s daemonset-controller Created pod: daemonset-demo-5z8q8 Normal SuccessfulDelete 6m6s daemonset-controller Deleted pod: daemonset-demo-hf9lv Normal SuccessfulCreate 6m3s daemonset-controller Created pod: daemonset-demo-bw5qp Normal SuccessfulDelete 4m34s daemonset-controller Deleted pod: daemonset-demo-gzxd2 Normal SuccessfulCreate 4m27s daemonset-controller Created pod: daemonset-demo-l9qgg 规模较大的集群中，我们也可以增大RollingUpdate策略中maxUnavailable属性的值来加快其滚动过程，例如设置为20%、25%甚至是50%等。DaemonSet控制器的滚动更新机制同样支持借助minReadySeconds来自定义Pod对象必须处于“就绪”状态多少时长才能视作“可用”。另外，DaemonSet资源的更新操作也支持回滚，包括回滚至REVISION历史记录中的任何一个指定的版本等。而对于需要精心组织每个实例更新过程才能确保其升级过程可靠进行的应用来说，我们就不得不使用OnDelete策略来替换默认的RollingUpdate策略。OnDelete策略的实施逻辑较为简单，这里就不再给出具体操作过程。 Job控制器与Deployment及DaemonSet控制器管理的守护进程类的服务应用所不同的是，Job控制器常用于管理那些运行一段时间就能够“完成”的任务，例如计算或备份操作。容器中的进程正常运行完成而结束后不需要再重启，而是由控制器把该Pod对象置于Completed（完成）状态，并能够在超过用户指定的生存周期后由系统自行删除。但是，若容器中的进程因“错误”（而非完成）而终止，则需要依据配置来确定其重启与否，通常，未运行完成的Pod对象因其所在的节点故障而意外终止后会被重新创建。Job控制器的Pod对象的状态转换如图8-19所示。 实践中，有的作业任务可能需要运行不止一次，用户可以配置它们以串行或并行方式运行。总结起来，这种类型的Job资源对象主要有两种。▪单工作队列的串行式Job：将一个作业串行执行多次直到满足期望的次数，如图8-20所示；这种Job也可理解为并行度为1的作业执行方式，在某个时刻仅有一个Pod资源对象存在。 ▪多工作队列的并行Job：这种方式中，可以设置工作队列数（即作业数），每个队列仅负责运行一个作业，如图8-21中的左图所示；也可以用有限的工作队列运行较多的作业，即工作队列数少于总作业数，它相当于运行着多个串行作业队列。如图8-21中的右图所示，工作队列数即同时可运行的Pod资源数。 具体运行中，我们需要根据作业的特性来选择合适的并行度及编排策略，对于有严格次序要求或者拥有“层进”特性的作业，单工作队列串行执行是其唯一可行的选择，反之，适度的并行能够提升作业运行速度。 Job资源基础应用作为标准的API资源类型之一，Job规范同样由apiVersion、kind、metadata和spec等字段组成，由系统自行维护的status字段用于保存资源的当前状态，该资源的基本定义格式如下。 12345678910111213apiVersion: batch/v1 # API群组及版本kind: Job # 资源类型特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；Job资源隶属名称空间级别spec: selector &lt;object&gt; # 标签选择器，必须匹配template字段中Pod模板的标签 template &lt;object&gt; # Pod模板对象 completions &lt;integer&gt; # 期望的成功完成的作业次数，成功运行结束的Pod数量 ttlSecondsAfterFinished &lt;integer&gt; # 终止状态作业的生存时长，超期将被删除 parallelism &lt;integer&gt; # 作业的最大并行度，默认为1 backoffLimit &lt;integer&gt; # 将作业标记为Failed之前的重试次数，默认为6 activeDeadlineSeconds &lt;integer&gt; # 作业启动后可处于活动状态的时长 定义Job资源时，spec字段内嵌的必要字段仅有template一个，Job会为其Pod对象自动添加job-name=JOB_NAME和controller-uid=UID标签，并使用标签选择器完成对controller-uid标签的关联。例如，下面的资源清单（job-example.yaml）中定义了一个名为job-demo的Job资源： 1234567891011121314151617apiVersion: batch/v1kind: Jobmetadata: name: job-demospec: template: spec: containers: - name: myjob image: alpine:3.11 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 60&quot;] restartPolicy: Never completions: 3 ttlSecondsAfterFinished: 3600 backoffLimit: 3 activeDeadlineSeconds: 300 注意Pod模板中的spec.restartPolicy默认为Always，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为Never或OnFailure。出于运行一段时长后可终止的目的，该示例中的Pod模板通过借助alpine镜像运行一个睡眠60秒（sleep 60）的应用来模拟该功能。将job-demo资源创建到集群之上便可查看相关的任务状态，如下第二条命令显示的简要状态信息中，COMPLETIONS字段（m/n）表示期望完成的作业数（n）和已经完成的作业数（m），DURATION为作业完成所运行的时长。 12345~$ kubectl apply -f job-demo.yaml job.batch/job-demo created~$ kubectl get jobs/job-demoNAME COMPLETIONS DURATION AGEjob-demo 1/2 66s 66s 相关的Pod资源能够以Job资源的名称为标签进行筛选，对于串行运行的作业来说，不同时刻能筛选出的Pod数量可能存在差异。下面的显示命令运行于第一次作业完成后，而第二次作业刚启动之时： 1234~$ kubectl get pods -l job-name=job-demoNAME READY STATUS RESTARTS AGEjob-demo-lb4vw 0/1 Completed 0 68sjob-demo-xn7zq 1/1 Running 0 6s Job资源的详细描述中能够获得进一步的信息，包括为Pod自动添加的标签、使用的标签选择器、作业并行度、各Pod的相关状态及相应事件等。 123456789101112131415161718192021222324~$ kubectl describe jobs/job-demo Name: job-demoNamespace: defaultSelector: controller-uid=42101d29-8a2b-45bf-b003-13af317c1300Labels: controller-uid=42101d29-8a2b-45bf-b003-13af317c1300 job-name=job-demoAnnotations: Parallelism: 1Completions: 2Start Time: Sun, 20 Sep 2020 12:00:33 +0800Completed At: Sun, 20 Sep 2020 12:02:37 +0800Duration: 2m4sActive Deadline Seconds: 300sPods Statuses: 0 Running / 2 Succeeded / 0 FailedPod Template: Labels: controller-uid=42101d29-8a2b-45bf-b003-13af317c1300 job-name=job-demo Containers: ……Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 3m51s job-controller Created pod: job-demo-lb4vw Normal SuccessfulCreate 2m49s job-controller Created pod: job-demo-xn7zq Normal Completed 107s job-controller Job completed 由上面命令结果可知，Job的默认使用的并行度为1，这也是为什么上面示例中的两个作业要先后执行而非同时执行的原因，这意味着多次作业需要以串行方式运行，作业的总时长至少要相当于各任务各自的执行时长之和。Job资源运行完成后便不再占用系统资源，用户可将其按需保留、手动删除或者设置相应属性执行自动删除操作。job-demo资源留给用户检查相关资源信息的时间窗口为3600秒（spec.ttlSecondsAfterFinished），超出该时长后，该作业将由控制器自行删除，而未定义该字段的作业将会一直保留。现实中的作业未必能有精确的运行时长，若某Job资源的Pod程序因存在Bug或其他原因导致的作业无法“完成”并退出，而其restatPolicy又定义为了重启，则该Pod可能会一直处于重启和错误的循环当中。为此，Job控制器提供了两个属性用于抑制这种情况的发生：▪.spec.activeDeadlineSeconds ：用于为Job指定最大活动时间长度，超出此时长的作业将被终止并标记为失败；▪.spec.backoffLimit ：将作业标记为失败状态之前的重试次数，默认值为6。由此可见，任务的总体可运行时长（activeDeadlineSeconds）也必须足够容纳作业的预测的总体运行时长。另外，不存在严格意义上先后次序的多次作业，适度的并行将能够显著提升其运行速度。 并行式Job与扩容机制将并行度属性.spec.parallelism设置为大于1的值，并设置总任务数.spec.completion属性大于并行度，便能够让Job资源以并行方式运行多任务。下面示例中定义了一个2路并行且总体运行10次任务的Job资源规范： 123456789101112131415161718apiVersion: batch/v1kind: Jobmetadata: name: job-para-demospec: template: spec: containers: - name: myjob image: alpine:3.11 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep 60&quot;] restartPolicy: OnFailure completions: 10 parallelism: 2 ttlSecondsAfterFinished: 3600 backoffLimit: 3 activeDeadlineSeconds: 1200 按照并行Job的运行法则，job-para-demo资源将允许最多同时运行两个Pod，这相当于存在两路虚拟作业管道，每个虚拟管道串行运行分配而来的Job。 123456~$ kubectl apply -f job-para-demo.yaml job.batch/job-para-demo created~$ kubectl get pods -l job-name=job-para-demoNAME READY STATUS RESTARTS AGEjob-para-demo-8fxj2 1/1 Running 0 10sjob-para-demo-mblzj 1/1 Running 0 10s Job资源的作业并行度支持运行时修改，因而，我们还能够通过修改parallelism属性的值来动态提升作业并行度以实现Job资源扩容之目的。例如，下面的命令将尚未完成的job-para-demo并行度从2提升到了5： 12~$ kubectl patch jobs/job-para-demo -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;parallelism&quot;:5&#125;&#125;&#x27;job.batch/job-para-demo patched 于是，pod-para-demo资源的并行度提升为5，Kubernetes系统为job-para-demo资源同时运行的Pod资源数量也随之提升到了5个，例如，对于刚启动不久的job-para-demo资源执行如下面的命令可生成类似如下运行于5个Pod作业的结果。 123456789~$ kubectl get pods -l job-name=job-para-demo -wNAME READY STATUS RESTARTS AGEjob-para-demo-8fxj2 0/1 Completed 0 82sjob-para-demo-d6z98 1/1 Running 0 7sjob-para-demo-hbh99 1/1 Running 0 20sjob-para-demo-lfvj5 1/1 Running 0 7sjob-para-demo-mblzj 0/1 Completed 0 82sjob-para-demo-nq8h7 1/1 Running 0 7sjob-para-demo-ss9t4 1/1 Running 0 20s 另外，Job资源详细描述中，相关事件的发生时间点也是辅助了解Pod对象并行运行状态的有效辅助信息。 CronJob控制器CronJob资源用于管理Job资源的运行时间，它允许用户在特定的时间或以指定的间隔运行Job，它适合自动执行特定的任务，例如备份、报告、发送电子邮件或清理类的任务等。换句话说，CronJob能够以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及周期性运行的方式：▪仅在未来某时间点将指定的作业运行一次；▪在指定的周期性时间点重复运行指定的作业。CronJob资源使用的时间格式类似于Linux系统上的crontab，稍具不同之处是，CronJob资源在指定时间点时，通配符“?”和“*”的意义相同，它们都表示任何可用的有效值。CronJob资源使用Job对象来完成任务，它每次运行时都会创建一个Job对象，并使用类似于Job资源的创建、管理和扩容方式。Cronjob也是Kubernetes系统标准的API资源，其资源规范的基本格式如下。 123456789101112131415apiVersion: batch/v1beta1 # API群组及版本kind: CronJob # 资源类型特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；CronJob资源隶属名称空间级别spec: jobTemplate &lt;Object&gt; # Job作业模板，必选字段 metadata &lt;object&gt; # 模板元数据 spec &lt;object&gt; # 作业的期望状态 schedule &lt;string&gt; # 调度时间设定，必选字段 concurrencyPolicy &lt;string&gt; # 并发策略，可用值有Allow、Forbid和Replace failedJobsHistoryLimit &lt;integer&gt; # 失败作业的历史记录数，默认为1 successfulJobsHistoryLimit &lt;integer&gt; # 成功作业的历史记录数，默认为3 startingDeadlineSeconds &lt;integer&gt; # 因错过时间点而未执行的作业的可超期时长 suspend &lt;boolean&gt; # 是否挂起后续的作业，不影响当前作业，默认为false 下面资源清单（cronjob-demo.yaml）定义了一个名为cronjob-demo的CronJob资源示例，它每隔2分钟运行一次由jobTemplate定义的示例任务，每次任务以单路并行的方式执行1次，每个任务的执行不超过60秒，且完成后600秒的Job将会被删除。 12345678910111213141516171819202122232425262728apiVersion: batch/v1beta1kind: CronJobmetadata: name: cronjob-demo namespace: defaultspec: schedule: &quot;*/2 * * * *&quot; jobTemplate: metadata: labels: app: mycronjob-jobs spec: parallelism: 1 completions: 1 ttlSecondsAfterFinished: 3600 backoffLimit: 3 activeDeadlineSeconds: 60 template: spec: containers: - name: myjob image: alpine command: - /bin/sh - -c - date; echo Hello from CronJob, sleep a while…; sleep 10; restartPolicy: OnFailure startingDeadlineSeconds: 300 将cronjob-demo资源创建到集群上后便可通过资源对象的相关信息了解运行状态。下面第二条命令结果中的SCHEDULE是指其调度时间点，SUSPEND表示后续任务是否处于挂起状态，即暂停任务的调度及运行，ACTIVE表示活动状态的Job对象的数量，而LAST SCHEDULE则表示前一次调度运行至此刻的时长。 12345~$ kubectl apply -f cronjob-demo.yamlcronjob.batch/cronjob-demo created~$ kubectl get cronjobs/cronjob-demoNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEcronjob-demo */2 * * * * False 1 6s 69s 我们可借助示例中Job模板上定义的标签过滤出名称空间中相关的Job对象。一段时长后，cronjob-demo创建的Job对象可能会存在多个，但示例中Job模板的配置会使得Job控制器自动删除那些完成后超过3600秒的、由cronjob-demo生成的Job对象。另外，CronJob资源默认仅会在历史记录中保留最近运行成功的3个以及运行失败的1个Job，因此，最终保留多少个Job也取决于CronJob中的历史记录定义，而历史记录中保存的Job数也支持由用户自定义其配置。 123456~$ kubectl get jobs -l controller=cronjob-demoNAME COMPLETIONS DURATION AGEcronjob-demo-1589970720 1/1 24s 6m28scronjob-demo-1589970840 1/1 27s 4m28scronjob-demo-1589970960 1/1 20s 2m28scronjob-demo-1589971080 0/1 27s 27s CloJob在Pod中运行，并会保留处于Completed状态的Pod日志。由CronJob资源通过模板创建的Job对象的名称以CronJob自身的名称为前缀，以Job创建时的时间戳为后缀，而各Job对象相关的Pod对象的名称则随机生成。已完成的CronJob资源相关Pod的状态为Completed，而失败的作业状态则存在RunContainerError、CrashLoopBackOff或其他表示失败的状态。可选的spec.startingDeadlineSeconds字段指示当CronJob由于某种原因错过了计划时间的情况下而允许延迟启动的最长时间（以秒为单位），错过的CronJob将被视为处于Failed状态。而未定义该字段值，则意味着CronJob永远不会超时，这将会导致CronJob资源存在同时运行多个实例的可能性。CronJob资源的Job对象可能不支持同时运行多个实例，用户可基于.spec.concurrencyPolicy属性来控制多个CronJob并存的机制，它的默认值为Allow，即允许不同时间点的多个CronJob实例同时运行。其他两个可用值中，Forbid用于禁止前后两个CronJob同时运行，如果前一个尚未结束，则后一个不能启动（跳过），Replace用于让后一个CronJob取代前一个，即终止前一个并启动后一个。 Pod中断预算尽管Deployment等一类的控制器能确保相应Pod对象的副本数量不断逼近期望的数量，但它却无法保证在某一时刻一定存在指定数量或一定百分比的Pod对象，然而这种需求在某些强调服务可用性的场景中是必备的。于是，Kubernetes自1.4版本起引入了PDB（PodDisruptionBudget，Pod中断预算）类型的资源，用于为那些自愿的中断做好预算方案，限制可自愿中断的最大Pod副本数或确保最少可用的Pod副本数，以确保服务的高可用性。Pod对象创建后会一直存在，除非用户有意将其销毁，或者出现了不可避免的硬件或系统软件错误。非自愿中断是指那些由不可控的外界因素导致的Pod中止而退出的情形，例如硬件或系统内核故障、网络故障以及节点资源不足导致Pod对象被驱逐等；而那些由用户特地执行的管理操作导致的Pod中断则称为自愿中断，例如排空节点、人为删除Pod对象、由更新操作触发的Pod对象重建等。用户可以为那些部署在Kubernetes的任何应用程序创建一个对应PDB对象以限制自愿中断时最大可以中断的副本数或者最少应该保持可用的副本数，从而保证应用自身的可用性。PDB资源的核心目标在于保护由控制器管理的应用，这必然意味着PDB将使用等同于相关控制器对象的标签选择器以精确关联至目标Pod对象。PDB支持的控制器类型包括Deployment、ReplicaSet和StatefulSet等。同时，PDB对象也可以用来保护那些纯粹是由定制的标签选择器自由选择的Pod对象。并非所有的自愿中断都会受到PDB的约束，例如，删除Deployment或者Pod的操作就会绕过PDB。另外，尽管那些因删除或更新操作导致不可用的Pod也会计入预算，但是控制器（例如Deployment）滚动更新时并不会真的被相关联的PDB资源所限制。因此，用户应当明确遵守PDB的限制法则，而不能直接删除PDB相关的Pod或者控制器资源。但管理员在维护集群时对节点执行的排空操作会受到PDB的限制。PDB也是标准的API资源类型，其资源规范如下所示。 12345678910apiVersion: batch/v1beta1 # API群组及版本kind: CronJob # 资源类型特有标识metadata: name &lt;string&gt; # 资源名称，在作用域中要唯一 namespace &lt;string&gt; # 名称空间；CronJob资源隶属名称空间级别spec: selector &lt;Object&gt; # 标签选择器，通常要与目标控制器资源相同 minAvailable &lt;string&gt; # 至少可用的Pod对象百分比，100%意味着不支持自愿中断 maxUnavailable &lt;string&gt; # 至多不可用的Pod对象百分比，0意味着不支持自愿中断； # minAvailable和maxUnavailable互斥，不能同时定义 下面的配置清单示例定义了名为pdb-demo的PDB资源，它对8.3.1节中由Deployment资源deployment-demo创建的Pod对象设置了PDB限制，要求其最少可用Pod对象数量为3个。 1234567891011apiVersion: policy/v1beta1kind: PodDisruptionBudgetmetadata: name: pdb-demo namespace: defaultspec: maxUnAvailable: 1 selector: matchLabels: app: demoapp release: stable pdb-demo资源对象创建完成后，我们能够从其YAML格式的资源规范状态信息中了解到该资源的当前状态。 12345678910~$ kubectl apply -f pdb-demo.yaml poddisruptionbudget.policy/pdb-demo created~$ kubectl get pdb/pdb-demo -o yaml……status: currentHealthy: 4 desiredHealthy: 3 disruptionsAllowed: 1 expectedPods: 4 observedGeneration: 1 接下来，我们可通过在1～2个节点上模拟驱逐deployment-demo资源作用域内的两个或以上数量的Pod对象模拟自愿中断过程，并监控各Pod对象被终止的过程来验证PDB资源对象的控制功效。首先，我们先了解deployment-demo作用域内各Pod对象在集群节点上的分布状态，下面的命令结果显示出，它有两个Pod对象同时运行在节点k8s-node02.ilinux.io之上： 123456~$ kubectl get pods -l app=demoapp,release=stable -o wide | awk &#x27;&#123;print $1,$7&#125;&#x27;NAME NODEdeployment-demo-b479b6f9f-dn8cc k8s-node02.ilinux.iodeployment-demo-b479b6f9f-ndt8t k8s-node03.ilinux.iodeployment-demo-b479b6f9f-pm994 k8s-node03.ilinux.iodeployment-demo-b479b6f9f-qcwj4 k8s-node02.ilinux.io 接下来，我们使用命令排空该节点以使得该deployment-demo资源作用域内的Pod对象有两个同时被中止，从而查看其触发pdb-demo的状态： 1234567891011$ kubectl drain k8s-node02.ilinux.io --ignore-daemonsetsnode/k8s-node02.ilinux.io already cordonedevicting pod default/deployment-demo-b479b6f9f-dn8ccevicting pod default/deployment-demo-b479b6f9f-qcwj4error when evicting pod &quot;deployment-demo-b479b6f9f-dn8cc&quot; (will retry after 5s): Cannot evict pod as it would violate the pod&#x27;s disruption budget.……error when evicting pod &quot;deployment-demo-b479b6f9f-dn8cc&quot; (will retry after 5s): Cannot evict pod as it would violate the pod&#x27;s disruption budget.evicting pod default/deployment-demo-b479b6f9f-dn8ccpod/deployment-demo-b479b6f9f-qcwj4 evictedpod/deployment-demo-b479b6f9f-dn8cc evictednode/k8s-node02.ilinux.io evicted 测试完成后，关闭节点k8s-node02.ilinux.io的SchedulingDisabled状态： 12~$ kubectl uncordon k8s-node02.ilinux.ionode/k8s-node02.ilinux.io uncordoned 从测试中排空命令的返回结果可以看出，同时执行驱逐deployment-demo作用域内的两个Pod对象的操作时，一个Pod能立即完成驱逐，但另一个Pod的驱逐操作被pdb-demo所阻塞，直到deployment-demo请求补足该Pod副本的请求在其他节点创建完成并就绪后，第二个Pod的驱逐操作才能得以完成。事实上，PDB资源对多实例的有状态应用来说尤为有用，如Consul、ZooKeeper或etcd等，用户可借助PDB资源来防止自愿中断场景中将实例的数量减少到低于法定数量（quorum），以避免错误的写操作。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"}]},{"title":"ConfigMap和Secret","slug":"ConfigMap","date":"2022-02-10T01:24:05.000Z","updated":"2022-02-18T05:57:55.910Z","comments":true,"path":"2022/02/10/ConfigMap/","link":"","permalink":"https://marmotad.github.io/2022/02/10/ConfigMap/","excerpt":"","text":"应用配置容器化应用配置容器化应用配置的常见方式容器镜像一般由多个只读层叠加组成，构建完成后无法进行修改，另一方面，“黑盒化”运行的容器使用隔离的专用文件系统，那么，如何为容器化应用提供配置信息呢？传统实践中，通常有这么几种途径。 启动容器时直接向应用程序传递参数。 将定义好的配置文件硬编码（嵌入）于镜像文件中。 通过环境变量传递配置数据。 基于存储卷传送配置文件。 命令行参数Dockerfile中的ENTRYPOINT和CMD指令用于指定容器启动时要运行的程序及其相关的参数。其中，CMD指令以列表形式指定要运行的程序及其相关的参数，若同时存在ENTRYPOINT指令，则CMD指令中的列表所有元素均被视作由ENTRYPOINT指定程序的命令行参数。另外，在基于某镜像创建容器时，可以通过向ENTRYPOINT中的程序传递额外的自定义参数，甚至还可以修改要运行的应用程序本向。例如，使用docker run命令创建并启动容器的格式为： 1docker run [OPTIONS] IMAGE [COMMAND] [ARG...] 其中的[COMMAND]即为自定义运行的程序，[ARG]则是传递给程序的参数。若定义相关的镜像文件时使用了ENTRYPOINT指令，则[COMMAND]和[ARG]都会被当作命令行参数传递给ENTRYPOINT指令中指定的程序，除非为docker run命令额外使用–entrypoint选项覆盖ENTRYPOINT指令而指定运行其他程序。在Kubernetes系统上创建Pod资源时，也能够向容器化应用传递命令行参数，甚至指定运行其他应用程序，相关的字段分别为pods.spec.containers[].command和pods.spec.containers[].args，该话题在Pod资源的相关话题中已有过介绍。 将配置文件嵌入镜像文件用户在Dockerfile中使用COPY指令把定义好的配置文件复制到镜像文件系统上的目标位置，或者使用RUN指令调用sed或echo一类的命令修改配置文件，从而达到为容器化应用提供自定义配置文件之目的。 通过环境变量向容器注入配置信息通过环境变量为镜像提供配置信息是最常见的容器应用配置方式之一，例如使用MySQL官方提供的镜像文件启动MySQL容器时使用的MYSQL_ROOT_PASSWORD环境变量，它用于为MySQL服务器的root用户设置登录密码。在基于此类镜像启动容器时，通过docker run命令的-e选项向环境变量传值即能实现应用配置，命令的使用格式为docker run -e SETTING1=foo -e SETTING2=bar … &lt;image name&gt;。非云原生的应用程序容器化时通常会借助entrypoint启动脚本以在启动时获取到这些环境变量，并在启动容器应用之前，通过sed或echo等一类命令将变量值替换到配置文件中。一般说来，容器的entrypoint启动脚本应该为这些环境变量提供默认值，以便在用户未为环境变量传值时也能基于此类必需环境变量的镜像启动容器。使用环境变量这种配置方式的优势在于配置信息的动态化供给，不过有些应用程序的配置也可能会复杂到难以通过键值格式的环境变量完成。也可以让容器的entrypoint启动脚本通过网络中的键值存储系统获取配置参数，常用的该类存储系统有Consul或etcd等，它们能够支持多级嵌套的数据结构，因而能够提供较之环境变量更为复杂的配置信息。不过，这种方式为容器化应用引入了额外的依赖条件。Kubernetes系统支持在为Pod资源配置容器时使用spec.containers.env为容器的环境变量传值从而完成应用的配置，我们在第4章中已经对该话题进行了说明并给出了使用示例。 通过存储卷向容器注入配置信息Docker存储卷能够将宿主机之上的任何文件或目录映射进容器文件系统上，因此，可以事先将配置文件放置于宿主机之上的某特定路径中，而后在启动容器时进行加载。这种方式灵活易用，但也依赖于用户事先将配置数据提供在宿主机上的特定路径。而且在多主机模型中，若容器存在被调度至任一主机运行的可能性时，用户还需要将配置共享在任一宿主机以确保容器能正确获取到它们。Kubernetes系统把配置信息保存于标准的API资源ConfigMap和Secret中，Pod资源可通过抽象化的同名存储卷插件将相关的资源对象关联为存储卷，而后引用该存储卷上的数据赋值给环境变量，或者由容器直接挂载作为配置文件使用。ConfigMap和Secret资源是Kubernetes系统上的“一等公民”，也是配置Pod中容器应用最常用的方式。 容器环境变量在运行时配置Docker容器中应用程序的第二种方式是在容器启动时向其传递环境变量。Docker原生的应用程序应该使用很小的配置文件，并且每一项参数都可由环境变量或命令行选项覆盖，从而能够在运行时完成任意的按需配置。然而，目前只有极少一部分应用程序是为容器环境原生设计，毕竟为容器原生重构应用程序工程浩大，且旷日持久。好在有利用容器启动脚本为应用程序预设运行环境的方法可用，通行的做法是在制作Docker镜像时，为ENTRYPOINT指令定义一个脚本，它能够在启动容器时将环境变量替换至应用程序的配置文件中，而后由此脚本启动相应的应用程序。基于这类镜像运行容器时，即可通过向环境变量传值的方式来配置应用程序。在Kubernetes中使用此类镜像启动容器时，也可以在Pod资源或pod模板资源的中定义，通过为容器配置段使用env参数来定义使用的环境变量列表。事实上，即便容器中的应用本身不处理环境变量，也一样可以向容器传递环境变量，只不过它不被使用罢了。通过环境变量配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构建的列表。每个环境变量通常由name和value（或valueFrom）字段构成。 name &lt;string&gt;：环境变量的名称，必选字段。 value &lt;string&gt;：环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。 valueFrom &lt;Object&gt;：环境变量值的引用源，例如当前Pod资源的名称、名称空间、标签等，不能与非空值的value字段同时使用，即环境变量的值要么源于value字段，要么源于valueFrom字段，二者不可同时提供数据。valueFrom字段可引用的值有多种来源，包括当前Pod资源的属性值，容器相关的系统资源配置、ConfigMap对象中的Key以及Secret对象中的Key，它们分别要使用不同的嵌套字段进行定义。 fieldRef &lt;Object&gt;：当前Pod资源的指定字段，目前支持使用的字段包括metadata.name、metadata.namespace、metadata.labels、metadata.annotations、spec.nodeName、spec.serviceAccountName、status.hostIP和status.podIP等。 configMapKeyRef &lt;Object&gt;：ConfigMap对象中的特定Key。 secretKeyRef &lt;Object&gt;：Secret对象中的特定Key。 resourceFieldRef &lt;Object&gt;：当前容器的特定系统资源的最小值（配额）或最大值（限额），目前支持的引用包括limits.cpu、limits.memory、limits.ephemeral-storage、requests.cpu、requests.memory和requests.ephemeral-storage。 下面是定义在资源清单文件env-demo.yaml中的Pod资源，它配置容器通过环境变量引用当前Pod资源及其所在的节点的相关属性值。fieldRef字段的值是一个对象，它一般由apiVersion（创建当前Pod资源的API版本）或fieldPath嵌套字段所定义。事实上，这正是5.7节讲述的downwardAPI的一种应用示例。 12345678910111213141516171819202122232425262728apiVersion: v1kind: Podmetadata: name: env-demo labels: purpose: demonstrate-environment-variablesspec: containers: - name: env-demo-container image: busybox command: [&quot;httpd&quot;] args: [&quot;-f&quot;] env: - name: HELLO_WORLD value: just a demo - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_NODE_IP valueFrom: fieldRef: fieldPath: status.hostIP - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace restartPolicy: OnFailure 创建上面资源清单中定义的Pod对象env-demo，而后打印它的环境变量列表，命令及其结果如下。 12345678~]$ kubectl exec env-demo printenvPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binHOSTNAME=env-demoMY_NODE_NAME=k8s-node02.ilinux.ioMY_NODE_IP=172.16.0.67MY_POD_NAMESPACE=defaultHELLO_WORLD=just a demo…… 这两种配置方式有着一个共同的缺陷：无法在容器应用运行过程中更新环境变量从而达到更新应用的目的。这通常意味着用户不得不为production、development和stage等不同的环境分别配置Pod资源。好在，用户还有ConfigMap资源可用。 应用程序配置管理与ConfigMap资源ConfigMap资源用于在运行时将配置文件、命令行参数、环境变量、端口号以及其他配置工件绑定至Pod的容器和系统组件。ConfigMap使配置更易于更改和管理，并防止将配置数据硬编码到Pod配置清单中。但ConfigMap资源用于存储和共享非敏感、未加密的配置信息，若要在集群中使用敏感信息，则必须使用Secret资源。简单来说，一个ConfigMap对象就是一系列配置数据的集合，这些数据可注入到Pod的容器当中为容器应用所使用，注入的途径有直接挂载存储卷和传递为环境变量两种。ConfigMap支持存储诸如单个属性一类的细粒度的信息，也可用于存储粗粒度的信息，例如将整个配置文件保存在ConfigMap对象之中。 创建ConfigMap对象ConfigMap是Kubernetes标准的API资源类型，它隶属名称空间级别，支持命令式命令、命令式对象配置及声明式对象配置3种管理接口。命令式命令的创建操作可通过kubectl create configmap进行，它支持基于目录、文件或字面量（literal）值获取配置数据完成ConfigMap对象的创建。该命令的语法格式如下所示。 1kubectl create configmap &lt;map-name&gt; &lt;data-source&gt; 命令中的&lt;data-source&gt;就是可以通过直接给定的键值、文件或目录（内部的一到多个文件）来获取的配置数据来源，但无论是哪一种数据供给方式，配置数据都要转换为键值类型，其中的键由用户在命令行给出或是文件类型数据源的文件名，且仅能由字母、数字、连接号和点号组成，而值则是字面量值或文件数据源的内容。 字面量值数据源为kubectl create configmap命令使用–from-literal选项可在命令行直接给出键值对来创建ConfigMap对象，重复使用此选项则可以一次传递多个键值对。命令格式如下： 1kubectl create configmap configmap_name --from-literal=key-1=value-1 … 例如，下面的命令创建demoapp-config时传递了两个键值对，一个是demoapp.host= 0.0.0.0，一个是demoapp.port=8080。 12$ kubectl create configmap demoapp-config --from-literal=demoapp.host=&#x27;0.0.0.0&#x27; \\ --from-literal=demoapp.port=&#x27;8080&#x27; --namespace=&#x27;default&#x27; ConfigMap对象仅是Kubernetes API存储中的数据，并没有与之相关联的其他组件存在，因而无须status字段来区分期望的状态（desired state）和当前状态（current state）。我们从下面的get configmap命令中输出的demoapp-config对象YAML格式信息可以看出，ConfigMap资源没有spec和status字段，而是直接使用data字段嵌套键值数据。 1234567891011~$ kubectl get configmaps demoapp-config -o yamlapiVersion: v1data: demoapp.host: 0.0.0.0 demoapp.port: &quot;8080&quot;kind: ConfigMapmetadata: creationTimestamp: &quot;2020-08-13T06:18:30Z&quot; managedFields: …… name: demoapp-config namespace: default resourceVersion: &quot;2660869&quot; selfLink: /api/v1/namespaces/default/configmaps/demoapp-config uid: e86036cc-e677-4529-87ce-64f58e72ecc7 显然，若要基于配置清单创建ConfigMap资源时，仅需要指定apiVersion、kind、metadata和data这4个字段，以类似上面的格式定义出相应的资源即可。 文件数据源ConfigMap资源也可用于为应用程序提供大段配置，这些大段配置通常保存于一到多个文本编码的文件中，可由kubectl create configmap命令通过–from-file选项一次加载一个配置文件的内容为指定键的值，多个文件的加载可重复使用–from-file选项完成。命令格式如下，省略键名时，将默认使用指定的目标文件的基名。 12kubectl create configmap &lt;configmap_name&gt; \\ --from-file[=&lt;key-name&gt;]=&lt;path-to-file&gt; 例如，下面的命令可以把事先准备好的Nginx配置文件模板保存于ConfigMap对象nginx-confs中，其中一个直接使用myserver.conf文件名作为键名，而另一个myserver-status.cfg对应的键名则自定义为status.cfg。 12~$ kubectl create configmap nginx-confs --from-file=./nginx-conf.d/myserver.conf \\ --from-file=status.cfg=./nginx-conf.d/myserver-status.cfg --namespace=&#x27;default&#x27; 我们可以从nginx-confs对象的配置清单来了解各键名及其相应的键值。 123456789101112131415161718192021~ $ kubectl get configmap nginx-confs -o yamlapiVersion: v1data: status.cfg: | # “|”是键名及多行键值的分割符，多行键值要进行固定缩进 location /nginx-status &#123; # 该缩进范围内的文本块即为多行键值 stub_status on; access_log off; &#125; myserver.conf: | server &#123; listen 8080; server_name www.ik8s.io; include /etc/nginx/conf.d/myserver-*.cfg; location / &#123; root /usr/share/nginx/html; &#125; &#125;kind: ConfigMap…… 通过这种方式创建的ConfigMap资源可以直接以键值形式收纳应用程序的完整配置信息，各个文件的内容以键值的形式保存于专用的键名称之下。当需要配置清单保留ConfigMap资源的定义，而键数据又较为复杂时，也需要以类似上面命令输出结果中的格式，将配置文件内容直接定义在配置清单当中。 目录数据源对于配置文件较多且又无须自定义键名称的场景，可以直接在kubectl create configmap命令的–from-file选项上附加一个目录路径就能将该目录下的所有文件创建于同一ConfigMap资源中，各文件名为即为键名。命令格式如下。 1kubectl create configmap &lt;configmap_name&gt; --from-file=&lt;path-to-directory&gt; 下面的命令把nginx-conf.d目录下的所有文件都保存于nginx-config-files对象中，从命令格式也不难揣测出，我们无法再为各文件内容自定义其键名称。 1~$ kubectl create configmap nginx-config-files --from-file=./nginx-conf.d/ 此目录中包含myserver.conf、status.cfg和gzip.cfg这3个配置文件，它们会被分别存储为3个键值数据，如下面的命令及其结果所示。 123456789101112131415161718192021222324252627~$ kubectl describe configmap nginx-config-filesName: nginx-config-filesNamespace: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====myserver-gzip.cfg: # 键值数据1，describe命令的输出中键和值使用“----”分割符----gzip on;……myserver.conf: # 键值数据2----server &#123; ……&#125;myserver-status.cfg: # 键值数据3----location /nginx-status &#123; stub_status on; access_log off;&#125;Events: &lt;none&gt; 注意，describe命令和get -o yaml命令都可显示由文件创建而成的键与值，但二者使用的键和值之间的分隔符不同。另外需要说明的是，基于字面量值和基于文件创建的方式也可以混合使用。例如下面的命令创建demoapp-confs对象时，使用–from-file选项加载demoapp-conf.d目录下的所有文件（共有envoy.yaml和eds.conf两个），又同时使用了两次–from-literal选项分别以字面量值的方式定义了两个键值数据。 12~$ kubectl create configmap demoapp-confs --from-file=./demoapp-conf.d/ \\ --from-literal=demoapp.host=&#x27;0.0.0.0&#x27; --from-literal=demoapp.port=&#x27;8080&#x27; 该对象共有4个数据条目，它们分别是来自于demoapp-conf.d目录下的envoy.yaml和eds.conf，以及命令行直接给出的demoapp.host和demoapp.port，这可以从下面命令的结果中得以验证。 123~$ kubectl get configmaps/demoapp-confsNAME DATA AGEdemoapp-confs 4 12s ConfigMap资源配置清单基于配置文件创建ConfigMap资源时，它所使用的字段包括通常的apiVersion、kind和metadata字段，以及用于存储数据的关键字段data。例如下面的示例所示。 1234567891011apiVersion: v1kind: ConfigMapmetadata: name: configmap-demo namespace: defaultdata: host: 0.0.0.0 port: &quot;10080&quot; app.config: | threads = 4 connections = 1024 若键值来自文件内容，使用配置文件创建ConfigMap资源的便捷性远不如直接通过命令行进行创建，因此我们可先使用命令行加载文件或目录的方式进行创建，在创建完成后使用get -o yaml命令获取到相关信息后进行编辑留存。 通过环境变量引用ConfigMap键值Pod资源配置清单中，除了使用value字段直接给定变量值之外，容器环境变量的赋值还支持通过在valueFrom字段中嵌套configMapKeyRef来引用ConfigMap对象的键值，它的具体使用格式如下。 1234567env:- name &lt;string&gt; # 要赋值的环境变量名 valueFrom: # 定义变量值引用 configMapKeyRef: # 变量值来自ConfigMap对象的某个指定键的值 key &lt;string&gt; # 键名称 name &lt;string&gt; # ConfigMap对象的名称 optional &lt;boolean&gt; # 指定的ConfigMap对象或者指定的键名称是否为可选 这种方式赋值的环境变量的使用方式与直接赋值的环境变量并无区别，它们都可用于容器的启动脚本或直接传递给容器应用等。下面是保存于配置文件configmaps-env-demo.yaml的资源定义示例，它包含了两个资源，彼此间使用“—”相分隔。第一个资源是名为demoapp-config的ConfigMap对象，它包含了两个键值数据；第二个资源是名为configmaps-env-demo的Pod对象，它在环境变量PORT和HOST中分别引用了demoapp-config对象中的demoapp.port和demoapp.host的键的值。 12345678910111213141516171819202122232425262728293031apiVersion: v1kind: ConfigMapmetadata: name: demoapp-config namespace: defaultdata: demoapp.port: &quot;8080&quot; demoapp.host: 0.0.0.0---apiVersion: v1kind: Podmetadata: name: configmaps-env-demo namespace: defaultspec: containers: - image: ikubernetes/demoapp:v1.0 name: demoapp env: - name: PORT valueFrom: configMapKeyRef: name: demoapp-config key: demoapp.port optional: false - name: HOST valueFrom: configMapKeyRef: name: demoapp-config key: demoapp.host optional: true demoapp支持通过环境变量HOST和PORT为其指定监听的地址与端口。将上面配置文件中的资源创建完成后，我们便可以来验证Pod资源监听的端口等配置信息是否为demoapp-config对象中定义的内容，如下面的命令及结果所示。 1234567~$ kubectl apply -f configmaps-env-demo.yamlconfigmap/demoapp-config createdpod/configmaps-env-demo created~$ kubectl exec configmaps-env-demo -- netstat -tnl Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 需要注意的是，被引用的ConfigMap资源必须事先存在，否则将无法在Pod对象中启动引用了ConfigMap对象的容器，但未引用或不存在ConfigMap资源的容器将不受影响。另外，ConfigMap是名称空间级别的资源，它必须与引用它的Pod资源在同一空间内。提示在容器清单中的command或args字段中引用环境变量要使用$(VAR_NAME)的格式。若ConfigMap资源中存在较多的键值数据，而且其大部分甚至是全部键值数据都需要由容器进行引用时，为容器逐一配置相应的环境变量将是一件颇为劳心费神之事，而且极易出错。对此，Pod资源支持在容器中使用envFrom字段直接将ConfigMap资源中的所有键值一次性地导入。 12345envFrom: - prefix &lt;string&gt; # 为引用的ConfigMap对象中的所有变量添加一个前缀名 configMapRef: # 定义引用的ConfigMap对象 name &lt;string&gt; # ConfigMap对象的名称 optional &lt;boolean&gt; # 该ConfigMap对象是否为可选 envFrom字段值是对象列表，用于同时从多个ConfigMap对象导入键值数据。为了避免从多个ConfigMap引用键值数据时产生键名冲突，可以为每个引用中将被导入的键使用prefix字段指定一个特定的前缀，例如HTCFG_一类的字符串，于是ConfigMap对象中的PORT键名将成为容器中名为HTCFG_PORT的变量。注意如果键名中使用了连字符“-”，转换为变量名的过程会自动将其替换为下划线“_”。例如，把上面示例中的配置清单转为如下形式的定义（configmap-envfrom-demo.yaml配置文件）后，引用ConfigMap进行配置的效果并无不同。 12345678910111213141516171819202122apiVersion: v1kind: ConfigMapmetadata: name: demoapp-config-for-envfrom namespace: defaultdata: PORT: &quot;8090&quot; HOST: 0.0.0.0---apiVersion: v1kind: Podmetadata: name: configmaps-envfrom-demo namespace: defaultspec: containers: - image: ikubernetes/demoapp:v1.0 name: demoapp envFrom: - configMapRef: name: demoapp-config-for-envfrom optional: false 由envFrom从ConfigMap对象一次性引入环境变量时无法自定义每个环境变量的名称，因此，ConfigMap对象中的键名称必须要与容器中的应用程序引用的变量名保持一致。待Pod资源创建完成后，可通过查看其环境变量验证其导入的结果 123456~$ kubectl apply -f configmaps-envfrom-demo.yaml configmap/demoapp-config-for-envfrom createdpod/configmaps-envfrom-demo created~$ kubectl exec configmaps-envfrom-demo -- printenv | grep -E &#x27;^(PORT|HOST)\\b&#x27;HOST=0.0.0.0PORT=8090 值得提醒的是，从ConfigMap对象导入环境变量时若省略了可选的prefix字段，各变量名将直接引用ConfigMap资源中的键名。若不存在键名冲突的可能性，例如从单个ConfigMap对象导入变量或在ConfigMap对象中定义键名时已添加了特定前缀时，省略前缀的定义既不会导致键名冲突，又能保持变量的简洁。 ConfigMap存储卷使用环境变量导入ConfigMap对象中来源于较长的内容文件的键值会导致占据过多的内存空间，而考虑此类数据通常用于为容器应用提供配置文件，将其内容直接以文件格式进行引用是为了更好地选择。Pod资源的configMap存储卷插件专用于以存储卷形式引用ConfigMap对象，其键值数据是容器中的ConfigMap存储卷挂载点路径或直接指向的配置文件。 挂载整个存储卷基于ConfigMap存储卷插件关联至Pod资源上的ConfigMap对象可由内部的容器挂载为一个目录，该ConfigMap对象的每个键名将转为容器挂载点路径下的一个文件名，键值则映射为相应文件的内容。显然，挂载点路径应该以容器加载配置文件的目录为其名称，每个键名也应该有意设计为对应容器应用加载的配置文件名称。在Pod资源上以存储卷方式引用ConfigMap对象的方法非常简单，仅需要指明存储卷名称及要引用的ConfigMap对象名称即可。下面是在配置文件configmaps-volume-demo.yaml中定义的Pod资源，它引用了前面创建的ConfigMap对象nginx-config-files，并由nginx-server容器挂载至Nginx加载配置文件模块的目录/etc/nginx/conf.d之下。 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: configmaps-volume-demo namespace: defaultspec: containers: - image: nginx:alpine name: nginx-server volumeMounts: - name: ngxconfs mountPath: /etc/nginx/conf.d/ readOnly: true volumes: - name: ngxconfs configMap: name: nginx-config-files optional: false 此Pod资源引用的nginx-config-files中包含3个配置文件，其中myserver.conf定义了一个虚拟主机www.ik8s.io，并通过include指令包含/etc/nginx/conf.d/目录下以myserver-为前缀、以.cfg为后缀的所有配置文件，例如在nginx-config-files中包含的myserver-status.cfg和myserver-gzip.cfg，如图6-1所示。 创建此Pod资源后，在Kubernetes集群中的某节点直接向Pod IP的8080端口发起访问请求，即可验证由nginx-config-files资源提供的配置信息是否生效，例如通过/nginx-status访问其内置的stub status。 123456~$ POD_IP=$(kubectl get pods configmaps-volume-demo -o go-template=&#123;&#123;.status.podIP&#125;&#125;)~$ curl http://$&#123;POD_IP&#125;:8080/nginx-statusActive connections: 1server accepts handled requests 1 1 1Reading: 0 Writing: 1 Waiting: 0 当然，我们也可以直接于Pod资源configmaps-volume-demo之上的相应容器中执行命令来确认文件是否存在于挂载点目录中： 1234~$ kubectl exec configmaps-volume-demo -- ls /etc/nginx/conf.d/myserver-gzip.cfgmyserver-status.cfgmyserver.conf 我们还可以在容器中运行其他命令来进一步测试由ConfigMap对象提供的配置信息是否已生效，以示例中的Nginx为例，我们可运行如下的配置测试与打印命令进行配置信息的生效确认。 123456789101112131415~$ kubectl exec configmaps-volume-demo -- nginx -T……# configuration file /etc/nginx/conf.d/myserver.conf:server &#123; listen 8080; server_name www.ik8s.io; ……&#125;# configuration file /etc/nginx/conf.d/myserver-gzip.cfg:……# configuration file /etc/nginx/conf.d/myserver-status.cfg:…… 由上面两个命令的结果可见，nginx-config-files中的3个文件都被添加到了容器中，并且实现了由容器应用Nginx加载并生效。 挂载存储卷中的部分键值有些应用场景中，用户很可能期望仅向容器中的挂载点暴露Pod资源关联的ConfigMap对象上的部署键值，这在通过一个ConfigMap对象为单个Pod资源中的多个容器分别提供配置时尤其常见。例如前面曾创建了一个名为demoapp-confs的ConfigMap对象，它包含有4个键值，其中的envoy.yaml和eds.conf可为envoy代理提供配置文件，而demoapp.port能够为demoapp（通过环境变量）定义监听的端口。下面配置清单示例定义的Pod资源中定义了两个容器，envoy和demoapp，demoapp-confs为envoy容器提供两个配置文件，为demoapp容器提供了一个配置参数。 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: Podmetadata: name: configmaps-volume-demo2 namespace: defaultspec: containers: - name: proxy image: envoyproxy/envoy-alpine:v1.14.1 volumeMounts: - name: appconfs mountPath: /etc/envoy readOnly: true - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: PORT valueFrom: configMapKeyRef: name: demoapp-confs key: demoapp.port optional: false volumes: - name: appconfs configMap: # 存储卷插件类型 name: demoapp-confs items: # 要暴露的键值数据 - key: envoy.yaml path: envoy.yaml mode: 0644 - key: lds.conf path: lds.conf mode: 0644 optional: false configMap卷插件中的items字段的值是一个对象列表，可嵌套使用3个字段来组合指定要引用的特定键。 key &lt;string&gt;：要引用的键名称，必选字段。 path &lt;string&gt;：对应的键在挂载点目录中映射的文件名称，它可不同于键名称，必选字段。 mode &lt;integer&gt;：文件的权限模型，可用范围为0～0777。 上面的配置示例（configmap-volume-demo2.yaml）中，把envoy.yaml和eds.conf两个键名分别映射为/etc/envoy目录下的两个与键同名的文件，且均使用0644的权限。 独立挂载存储卷中的单个键值前面的两种方式中，无论是装载ConfigMap对象中的所有还是部分文件，挂载点目录下原有的文件都会被隐藏。对于期望将ConfigMap对象提供的配置文件补充在挂载点目录下的需求来说，这种方式显然难以如愿。以Nginx应用为例，基于nginx:alpine启动的容器的/etc/nginx/conf.d目录中原本就存在一些文件（例如default.conf等），有时候我们需要把nginx-config-files这个ConfigMap对象中的全部或部分文件装载进此目录中而不影响其原有的文件。事实上，此种需求可以通过在容器上的volumeMounts字段中使用subPath字段来解决，该字段用于支持从存储卷挂载单个文件或单个目录而非整个存储卷。例如，下面的示例就单独挂载了两个文件在/etc/nginx/conf.d目录中，但保留了目录下原有的文件。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: configmaps-volume-demo3 namespace: defaultspec: containers: - image: nginx:alpine name: nginx-server volumeMounts: - name: ngxconfs mountPath: /etc/nginx/conf.d/myserver.conf subPath: myserver.conf readOnly: true - name: ngxconfs mountPath: /etc/nginx/conf.d/myserver-gzip.cfg subPath: myserver-gzip.cfg readOnly: true volumes: - name: ngxconfs configMap: name: nginx-config-files 基于上述配置创建了Pod资源后，即可通过命令验证/etc/nginx/conf.d目录中原有文件确实能够得以保留，如下面的命令及其结果所示。 1234~$ kubectl exec configmaps-volume-demo3 -- ls /etc/nginx/conf.d/default.confmyserver-gzip.cfgmyserver.conf 接下来也可将该Pod资源创建于集群上，验证myserver主机的配置，正常情况下，它应该启动了页面压缩功能，但因未装载myserver-status.cfg配置而不支持内置的status页面，感兴趣的读者可自行完成测试。 容器应用重载新配置相较于环境变量来说，使用ConfigMap资源为容器应用提供配置的优势之一在于支持容器应用动态更新其配置：用户直接更新ConfigMap对象，而后由相关Pod对象的容器应用重载其配置文件即可。细心的读者或许已经发现，挂载有ConfigMap存储卷的容器上，挂载点目录中的文件都是符号链接，它们指向了挂载点目录中名为..data隐藏属性的子目录，而..data自身也是一个符号链接，它指向了名字形如..2020_05_15_03_34_10.435155001这样的以挂载操作时的时间戳命名的临时隐藏目录，该目录才是存储卷的真正挂载点。例如，查看Pod对象configmaps-volume-demo的容器中的挂载点目录下的文件列表，它将显示出类似如下结果。 1234567~ $ kubectl exec -it configmaps-volume-demo -- ls -lA /etc/nginx/conf.dtotal 0drwxr-xr-x 2 root root 79 Apr 14 03:34 ..2020_05_15_03_34_10.435155001lrwxrwxrwx 1 root root 31 Apr 14 03:34 ..data -&gt; ..2020_05_15_03_34_10.435155001lrwxrwxrwx 1 root root 24 Apr 14 03:34 myserver-gzip.cfg -&gt; ..data/myserver-gzip.cfglrwxrwxrwx 1 root root 26 Apr 14 03:34 myserver-status.cfg -&gt; ..data/myserver-status.cfglrwxrwxrwx 1 root root 20 Apr 14 03:34 myserver.conf -&gt; ..data/myserver.conf 这种两级符号链接设定的好处在于，当引用的ConfigMap对象中的数据发生改变时，它将被重新挂载至一个以当前时间戳命名的新的临时目录下，而后将..data指向这个新的挂载点便达到了同时更新存储卷上所有文件数据的目的。例如，使用kubectl edit cm命令直接在ConfigMap对象nginx-config-files中的myserver-status.cfg配置段增加“allow 127.0.0.0/8;”和“deny all;”两行，稍等片刻之后再次查看configmap-volume-demo中容器挂载点目录中的文件列表，结果是其挂载点已经指向新的位置，例如下面的命令及其结果所示。 12345678910111213141516171819~$ kubectl edit configmaps/nginx-config-files -n default……data: …… myserver-status.cfg: | location /nginx-status &#123; stub_status on; access_log off; allow 127.0.0.0/8; deny all; &#125;……~ $ kubectl exec -it configmaps-volume-demo -- ls -lA /etc/nginx/conf.dtotal 0drwxr-xr-x 2 root root 79 Apr 14 03:45 ..2020_05_15_03_45_25.239510550lrwxrwxrwx 1 root root 31 Apr 14 03:45 ..data -&gt; ..2020_05_15_03_45_25.239510550lrwxrwxrwx 1 root root 24 Apr 14 03:34 myserver-gzip.cfg -&gt; ..data/myserver-gzip.cfglrwxrwxrwx 1 root root 26 Apr 14 03:34 myserver-status.cfg -&gt; ..data/myserver-status.cfglrwxrwxrwx 1 root root 20 Apr 14 03:34 myserver.conf -&gt; ..data/myserver.conf ConfigMap对象中的数据更新同步至应用容器后并不能直接触发生效新配置，还需要在容器上执行应用重载操作。例如Nginx可通过其nginx -s reload命令完成配置文件重载，如下面的命令所示。 12~$ kubectl exec configmaps-volume-demo -- nginx -s reload2020/05/15 03:52:50 [notice] 32#32: signal process started 新增的两行配置信息对/nginx-status这个URL施加了访问控制机制，它仅允许来自本地回环接口上的访问请求，因而此容器之外访问/nginx-status页面的请求将会被拒绝。对于不支持配置文件重载操作的容器应用来说，仅那些在ConfigMap对象更新后创建的Pod资源中的容器会应用到新配置，因此手动重启旧有的容器之前会存在配置不一致的问题。即使对于支持重载操作的应用来说，由于新的配置信息并非同步推送进所有容器中，而且在各容器中进行的手动重载操作也未必能同时进行，因此在更新时，短时间内仍然会存在配置不一致的现象。还有，以单个文件形式独立挂载ConfigMap存储卷中的容器并未采用两级链接的方式进行文件映射，因此存储卷无法确保所有挂载的文件可以被同时更新至容器中。为了确保配置信息的一致性，目前这种类型的挂载不支持文件更新操作。有些云原生应用支持配置更新时的自动重载功能，例如Envoy支持基于XDS协议订阅文件系统上的配置文件，并在该类配置文件更新时间戳发生变动时自动重载配置。然而，采用联合挂载多层叠加且进行写时复制的容器隔离文件系统来说，这种时间戳的更新未必能够触发内核中的通知机制，也就难以触发应用程序的自动重载功能。总结起来，在Pod资源中调用ConfigMap对象时要注意以下几个问题。 以存储卷方式引用的ConfigMap对象必须先于Pod对象存在，除非在Pod对象中把它们统统标记为optional，否则将会导致Pod无法正常启动；同样，即使ConfigMap对象存在，但引用的键名不存在时，也会导致同样的错误。 以环境变量方式引用的ConfigMap对象的键不存在时会被忽略，Pod对象可以正常启动，但错误引用的信息会以InvalidVariableNames事件记录于日志中。 ConfigMap对象是名称空间级的资源，能够引用它的Pod对象必须位于同一名称空间。 Kubelet仅支持那些由API Server管理的Pod资源来引用ConfigMap对象，因而那些由kubelet在节点上通过–manifest-url或–config选项加载配置清单创建的静态Pod，以及由用户直接通过kubelet的RESTful API创建的Pod对象。ConfigMap无法替代配置文件，它仅在Kubernetes系统上代表对应用程序配置文件的引用，我们可以将它类比为在Linux主机上表示/etc目录及内部文件的一种方法。 Secret资源：向容器注入配置信息出于增强可移植性的需求，我们应该从容器镜像中解耦的不仅有配置数据，还有默认口令（例如Redis或MySQL服务的访问口令）、用于SSL通信时的数字证书和私钥、用于认证的令牌和ssh key等，但这些敏感数据不宜存储于ConfigMap资源中，而是要使用另一种称为Secret的资源类型。将敏感数据存储在Secret中比明文存储在ConfigMap或Pod配置清单中更加安全。借助Secret，我们可以控制敏感数据的使用方式，并降低将数据暴露给未经授权用户的风险。Secret对象存储数据的机制及使用方式都类似于ConfigMap对象，它们以键值方式存储数据，在Pod资源中通过环境变量或存储卷进行数据访问。不同的地方在于，Secret对象仅会被分发至调用了该对象的Pod资源所在的工作节点，且仅支持由节点将其临时存储于内存中。另外，Secret对象的数据存储及打印格式为Base64编码的字符串而非明文字符，用户在创建Secret对象时需要事先手动完成数据的格式转换。但在容器中以环境变量或存储卷的方式访问时，它们会被自动解码为明文数据。注意Base64编码并非加密机制，其编码的数据可使用base64 –decode一类的命令进行解码。Secret对象以非加密格式存储于etcd中，管理员必须精心管控对etcd服务的访问以确保敏感数据的机密性，包括借助于TLS协议确保etcd集群节点间以及API Server间的加密通信和双向身份认证等。此外还要精心组织Kubernetes API Server服务的访问认证和授权，因为拥有创建Pod资源的用户都可以使用Secret资源并能够通过Pod对象中的容器访问其数据。目前，Secret资源主要有两种用途：一是作为存储卷注入Pod对象上，供容器应用程序使用；二是用于kubelet为Pod里的容器拉取镜像时向私有仓库提供认证信息。不过，后面使用ServiceAccount资源自建的Secret对象是一种更安全的方式。 创建Secret资源类似于Config Map资源，创建Secret对象时也支持使用诸如字面量值、文件或目录等数据源，而根据其存储格式及用途的不同，Secret对象还会划分为如下3种类别。 generic：基于本地文件、目录或字面量值创建的Secret，一般用来存储密码、密钥、信息、证书等数据。 docker-registry：用于认证到Docker Registry的Secret，以使用私有容器镜像。 tls：基于指定的公钥/私钥对创建TLS Secret，专用于TLS通信中；指定公钥和私钥必须事先存在，公钥证书必须采用PEM编码，且应该与指定的私钥相匹配。 这些类别也体现在kubectl create secret generic|docker-registry|tls命令之中，每个类别代表一个子命令，并分别有着各自专用的命令行选项。 通用Secret通用类型的Secret资源用于保存除用于TLS通信之外的证书和私钥，以及专用于认证到Docker注册表服务之外的敏感信息，包括访问服务的用户名和口令、SSH密钥、OAuth令牌、CephX协议的认证密钥等。使用Secret为容器中运行的服务提供用于认证的用户名和口令是一种较为常见的应用场景，以MySQL或PostgreSQL代表的开源关系型数据库系统的镜像就支持通过环境变量来设置管理员用户的默认密码。此类Secret对象可以直接使用kubectl create secret generic –from-literal=key=value命令，以给定的字面量值直接进行创建，通常用户名要使用username为键名，而密码则要使用password为键名。例如下面的命令，以root/iLinux分别为用户名和密码创建了一个名为mysql-root-authn的Secret对象： 12~$ kubectl create secret generic mysql-root-authn --from-literal=username=root \\ --from-literal=password=iLinux 由下面获取Secret对象资源规范的命令及其输出结果可以看出：未指定类型时，以generic子命令创建的Secret对象是Opaque类型，其键值数据会以Base64编码格式保存和打印。 1234567891011~$ kubectl get secrets/mysql-root-authn -o yamlapiVersion: v1data: password: aUxpbnV4 username: cm9vdA==kind: Secretmetadata: name: mysql-root-authn namespace: default ……type: Opaque 但Kubernetes系统Secret对象的Base64编码数据并非加密格式，许多相关的工具程序可轻松完成解码，例如将上面命令结果中的password字段的值可交由下面所示的Base64命令进行解码。 12~$ echo aUxpbnV4 | base64 -diLinux 将用户名和密码用于Basic认证时，需要在创建命令中额外使用–type选项明确定义Secret对象的类型，该选项值固定为”kubernetes.io/basic-auth”，并要求用户名和密码各自的键名必须为username和password，如下面Secret对象的创建和显示命令结果所示。 12345~$ kubectl create secret generic web-basic-authn --from-literal=username=ops \\ --from-literal=password=iK8S --type=&quot;kubernetes.io/basic-auth&quot;~$ kubectl get secrets/mysql-ops-authnNAME TYPE DATA AGEweb-basic-authn kubernetes.io/basic-auth 2 1m 有些应用场景仅需要在Secret中保存密钥信息即可，用户名能够以明文的形式由客户端直接提供而无须保存于Secret对象中。例如，在Pod或PV资源上使用的RBD存储卷插件以CephX协议认证到Ceph存储集群时，使用内嵌的user字段指定用户名，以secretRef字段引用保存有密钥的Secret对象，且创建该类型的Secret对象需要明确指定类型为kubernetes.io/rbd，如下面的命令所示。 12$ kubectl create secret generic ceph-kube-secret --type=&quot;kubernetes.io/rbd&quot; \\ --from-literal=key=&#x27;AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA==&#x27; 对于文件中的敏感数据，可以在命令上使用–from-file选项以直接将该文件作为数据源，例如创建用于SSH认证的Secret对象时就可以直接从认证的私钥文件加载认证信息，其键名需要使用ssh-privatekey，而类型标识为kubernetes.io/ssh-auth。下面的命令先创建出一对用于测试的认证密钥，而后将其私钥创建为Secret对象。 1234~$ ssh-keygen -t rsa -P &quot;&quot; -f $&#123;HOME&#125;/.ssh/id_rsa~$ kubectl create secret generic ssh-key-secret --from-file=ssh-privatekey=$&#123;HOME&#125;/.ssh/id_rsa \\ --type=&quot;kubernetes.io/ssh-auth&quot; Kubernetes系统上还有一种专用于保存ServiceAccount认证令牌的Secret对象，它存储有Kubernetes集群的私有CA的证书（ca.crt）以及当前Service账号的名称空间和认证令牌。该类资源以kubernetes.io/service-account-token为类型标识，并附加专用资源注解kubernetes.io/service-account.name和kubernetes.io/service-account.uid来指定所属的ServiceAccount账号名称及ID信息。kube-system名称空间中默认存在许多该类型的Secret对象，下面的第一个命令先获取到以node-controller开头的Secret资源（ServiceAccount/node-controller资源的专用Secret）的名称，而后第二个命令以YAML格式打印该资源的详细规范。下面命令用于打印kube-system名称空间下的Secret/node-controller资源对象的信息。 123456789101112131415~$ secret_name=$(kubectl get secrets -n kube-system | awk &#x27;/^node-controller/&#123;print $1&#125;&#x27;)~ $ kubectl get secrets $secret_name -o yaml -n kube-systemapiVersion: v1data: ca.crt: …… namespace: a3ViZS1zeXN0ZW0= token: ……kind: Secretmetadata: annotations: kubernetes.io/service-account.name: node-controller kubernetes.io/service-account.uid: 54dedc06-09db-4024-b756-e4e64ed1a1cf name: node-controller-token-6wlvm namespace: kube-system ……type: kubernetes.io/service-account-token 还有一种专用于Kubernetes集群自动引导（bootstrap）过程的Secret类型，最早由kubeam引入，类型标识为bootstrap.kubernetes.io/token，它需要由auth-extra-groups、description、token-id和token-secret等专用键名来指定所需的数据。由kubeadm部署的集群上，会在kube-system名称空间中默认生成一个以bootstrap-token为前缀的该类Secret对象。 12345678910111213141516~$ bs_name=$(kubectl get secrets -n kube-system | awk &#x27;/^bootstrap-token/&#123;print $1&#125;&#x27;)~$ kubectl get secret $bs_name -o yaml -n kube-system apiVersion: v1data: auth-extra-groups: …… description: …… token-id: ZG5hY3Y3 token-secret: YjE1MjAzcm55ODV2ZW5kdw== usage-bootstrap-authentication: dHJ1ZQ== usage-bootstrap-signing: dHJ1ZQ==kind: Secretmetadata: name: bootstrap-token-dnacv7 namespace: kube-system ……type: bootstrap.kubernetes.io/token 以配置清单创建以上各种类型的通用Secret对象，除Opaque外，都需要使用type字段明确指定类型，并在data字段中嵌套使用符合要求的字段指定所需要数据。 TLS Secret为TLS通信场景提供专用数字证书和私钥信息的Secret对象有其专用的TLS子命令，以及专用的选项–cert和–key。例如，为运行于Pod中的Nginx应用创建SSL虚拟主机之时，需要事先通过Secret对象向相应容器注入服务证书和配对的私钥信息，以供nginx进程加载使用。出于测试的目的，我们先使用类似如下命令生成私钥和自签证书。 1234~$ openssl rand -writerand $HOME/.rnd~$ (umask 077; openssl genrsa -out nginx.key 2048)~$ openssl req -new -x509 -key nginx.key -out nginx.crt \\ -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=www.ilinux.io 而后即可使用如下命令将这两个文件创建为secret对象。无论用户提供的证书和私钥文件使用什么名称，它们一律会分别转换为以tls.key（私钥）和tls.crt（证书）为其键名。 12~$ kubectl create secret tls nginx-ssl-secret --key=./nginx.key --cert=./nginx.crtsecret &quot;nginx-ssl-secret&quot; created 该类型的Secret对象的类型标识符为kubernetes.io/tls，例如下面命令结果所示。 1234567891011~$ kubectl get secret nginx-ssl -o yamlapiVersion: v1data: tls.crt: …… tls.key: ……kind: Secretmetadata: name: nginx-ssl-secret namespace: default ……type: kubernetes.io/tls Docker Registry Secret当Pod配置清单中定义容器时指定要使用的镜像来自私有仓库时，需要先认证到目标Registry以下载指定的镜像，pod.spec.imagePullSecrets字段指定认证Registry时使用的、保存有相关认证信息的Secret对象，以辅助kubelet从需要认证的私有镜像仓库获取镜像。该字段的值是一个列表对象，它支持指定多个不同的Secret对象以认证到不同的Resgistry，这在多容器Pod中尤为有用。创建这种专用于认证到镜像Registry的Secret对象有其专用的docker-registry子命令。通常，认证到Registry的过程需要向kubelet提供Registry服务器地址、用户名和密码，以及用户的E-mail信息，因此docker-registry子命令需要同时使用以下4个选项。 –docker-server：Docker Registry服务器的地址，默认为https://index.docker.io/v1/。 –docker-user：请求Registry服务时使用的用户名。 –docker-password：请求访问Registry服务的用户密码。 –docker-email：请求访问Registry服务的用户E-mail。 这4个选项指定的内容分别对应使用docker login命令进行交互式认证时所使用的认证信息，下面的命令创建了名为local-registry的docker-registry Secret对象。 12~$ kubectl create secret docker-registry local-registry --docker-username=Ops \\ --docker-password=Opspass --docker-email=ops@ilinux.io 该类secret对象打印的类型信息为kubernetes.io/dockerconfigjson，如下面的命令结果所示。 123~$ kubectl get secrets local-registryNAME TYPE DATA AGElocal-registry kubernetes.io/dockerconfigjson 1 7s 另外，创建docker-registry Secret对象时依赖的认证信息也可使用–from-file选项从dockercfg配置文件（例如/.dockercfg）或JSON格式的Docker配置文件（例如/.docker/config.json）中加载，但前者的类型标识为kubernetes.io/dockercfg，后者的类型则与前面使用字面量值的创建方式相同。在Pod资源上使用docker-registry Secret对象的方法有两种。一种方法是使用spec.imagePullSecrets字段直接引用；另一种是将docker-registry Secret对象添加到某特定的ServiceAccount对象之上，而后配置Pod资源通过spec. serviceAccountName来引用该服务账号。第二种方法的实现我们放在ServiceAccount资源的相关话题中进行介绍，这里先以下面的示例说明第一种方法的用法。 1234567891011apiVersion: v1kind: Podmetadata: name: secret-imagepull-demo namespace: defaultspec: imagePullSecrets: - name: local-registry containers: - image: registry.ilinux.io/dev/myimage name: demoapp 上面的配置清单仅是一个示例，付诸运行时，需要由读者将引用的Secret对象中的内容及清单资源的镜像等修改为实际可用的信息。当运行的多数容器镜像均来自私有仓库时，为每个Pod资源在imagePullSecrets显式定义一或多个引用的Secret对象实在不是一个好主意，我们应该将docker-registry Secret对象的引用定义在一个特定的ServiceAccount之上，而后由各相关的Pod资源进行引用才是更好的选择。 Secret资源清单Secret资源是标准的Kubernetes API资源类型之一，但它仅是存储于API Server上的数据定义，无须区别期望状态与现实状态，无须使用spec和status字段。除了apiVersion、kind和metadata字段，它可用的其他字段如下。 data &lt;map[string]string&gt;：key:value格式的数据，通常是敏感信息，数据格式需是以Base64格式编码的字符串，因此需要用户事先完成编码。另外，不同类型的Secret资源要求使用的嵌套字段（键名）也不尽相同，甚至ServiceAccount专用类型的Secret对象还要求使用专用的注解信息。 stringData &lt;map[string]string&gt;：以明文格式（非Base64编码）定义的键值数据。无须用户事先对数据进行Base64编码，而是在创建为Secret对象时自动进行编码并保存于data字段中。stringData字段中的明文不会被API Server输出，但使用kubectl apply命令进行创建的Secret对象，其注解信息可能会直接输出这些信息。 type &lt;string&gt;：仅为了便于编程处理Secret数据而提供的类型标识。下面是保存于配置文件secrets-demo.yaml中的Secret资源定义示例，它使用stringData提供了明文格式的键–值数据，从而免去了事先手动编码的麻烦。 12345678apiVersion: v1kind: Secretmetadata: name: secrets-demostringData: username: redis password: redisp@sstype: Opaque 为保存于配置清单文件中的敏感信息创建Secret对象时，用户需要先将敏感信息读出并转换为Base64编码格式，再将其创建为清单文件，过程烦琐，反而不如命令式创建来得便捷。不过，如果存在多次创建或者重构之需，将其保存为配置清单也是情势所需。 使用Secret资源类似于Pod资源使用ConfigMap对象的方式，Secret对象可以注入为容器环境变量，也能够通过Secret卷插件定义为存储卷并由容器挂载使用。但是，容器应用通常会在发生错误时将所有环境变量保存于日志信息中，甚至有些应用在启动时会将运行环境打印到日志中。另外，容器应用调用第三方程序为子进程时，这些子进程能够继承并使用父进程的所有环境变量。这都有可能导致敏感信息泄露，因而通常仅在必要的情况下才使用环境变量引用Secret对象中的数据。 环境变量Pod资源以环境变量方式消费Secret对象也存在两种途径：① 一对一地将指定键的值传递给指定的环境变量；② 将Secret对象上的全部键名和键值一次性全部映射为容器的环境变量。前者在容器上使用env.valueFrom字段进行定义，而后者则直接使用envFrom字段，如下面给出的详细配置格式所示。 123456789101112131415containers:- name: … image: … env: - name: &lt;string&gt; # 变量名，其值来自某Secret对象上的指定键的值 valueFrom: # 键值引用 secretKeyRef: name: &lt;string&gt; # 引用的Secret对象的名称，需要与该Pod位于同一名称空间 key: &lt;string&gt; # 引用的Secret对象上的键，其值将传递给环境变量 optional: &lt;boolean&gt; # 是否为可选引用 envFrom: # 整体引用指定的Secret对象的全部键名和键值 - prefix: &lt;string&gt; # 将所有键名引用为环境变量时统一添加的前缀 secretRef: name: &lt;string&gt; # 引用的Secret对象名称 optional: &lt;boolean&gt; # 是否为可选引用 下面Pod资源配置清单（secrets-env-demo.yaml）示例中，容器mariadb运行时初始化root用户的密码，引用自此前创建的Secret对象mysql-root-authn中的password键的值。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: secrets-env-demo namespace: defaultspec: containers: - name: mariadb image: mariadb imagePullPolicy: IfNotPresent env: - name: MYSQL_ROOT_PASSWORD valueFrom: secretKeyRef: name: mysql-root-authn key: password mariadb的镜像并不支持从某个文件中加载管理员root用户的初始密码，这里也就只能使用环境变量赋值的方式来引用Secret对象中的敏感数据。下面完成测试步骤，首先将清单中的Pod对象创建在集群上： 12~$ kubectl apply -f secrets-env-demo.yaml pod/secrets-env-demo created 而后使用保存在mysql-root-authn对象中的password字段的值iLinux作为密码进行数据库访问，如下面命令所示。 12345678910~$ kubectl exec -it secrets-env-demo -- mysql -uroot -piLinuxWelcome to the MariaDB monitor. Commands end with ; or \\g.Your MariaDB connection id is 8Server version: 10.4.12-MariaDB-1:10.4.12+maria~bionic mariadb.org binary distributionCopyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.Type &#x27;help;&#x27; or &#x27;\\h&#x27; for help. Type &#x27;\\c&#x27; to clear the current input statement.MariaDB [(none)]&gt; 命令结果表明使用MySQL客户端工具以root用户和iLinux密码认证到容器mariadb的操作成功完成，经由环境变量向容器传递Secret对象中保存的敏感信息得以顺利实现。 Secret存储卷Pod资源上的Secret存储卷插件的使用方式同ConfigMap存储卷插件非常相似，除了其类型及引用标识要替换为secret及secretName之外，几乎完全类似于ConfigMap存储卷，包括支持使用挂载整个存储卷、只挂载存储卷中指定键值以及独立挂载存储卷中的键等使用方式。下面是定义在配置清单文件secrets-volume-demo.yaml中的Secret资源使用示例，它将nginx-ssl-secret对象关联为Pod对象上名为nginxcert的存储卷，而后由容器ngxservrer挂载至/etc/nginx/certs目录下。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: secrets-volume-demo namespace: defaultspec: containers: - image: nginx:alpine name: ngxserver volumeMounts: - name: nginxcerts mountPath: /etc/nginx/certs/ readOnly: true - name: nginxconfs mountPath: /etc/nginx/conf.d/ readOnly: true volumes: - name: nginxcerts secret: secretName: nginx-ssl-secret - name: nginxconfs configMap: name: nginx-sslvhosts-confs optional: false ConfigMap对象nginx-sslvhosts-confs中存储有证书文件tls.cert和私钥文件tls.key，这些文件是可调用容器通过挂载nginx-ssl-secret在/etc/nginx/certs/目录下生成的，并根据证书与私钥文件定义了一个SSL类型的虚拟主机。并且，所有发往80端口的流量都会被重定向至SSL虚拟主机。其中的关键配置部分如下所示。 123456789101112131415161718192021server &#123; listen 443 ssl; server_name www.ik8s.io; ssl_certificate /etc/nginx/certs/tls.crt; ssl_certificate_key /etc/nginx/certs/tls.key; ssl_session_timeout 5m; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; ssl_prefer_server_ciphers on; location / &#123; root /usr/share/nginx/html; &#125;&#125;server &#123; listen 80; server_name www.ilinux.io; return 301 https://$host$request_uri; &#125; 我们知道，由Pod资源引用的所有ConfigMap和Secret对象必须事先存在，除非它们被显式标记为optional: true。因此，在创建该Pod对象之前，我们需要事先生成其引用的ConfigMap对象nginx-sslvhosts-confs，相关的所有配置文件保存在nginx-ssl-conf.d/目录下，因而直接运行如下命令即可完成创建。 1~$ kubectl create configmap nginx-sslvhosts-confs --from-file=./nginx-ssl-conf.d/ 而后，将上面资源清单文件中定义的Pod资源创建于集群之上，待其正常启动后可查看容器挂载点目录中的文件，以确认其挂载是否成功完成，或直接向Pod中的Nginx服务发起访问请求进行验证。 12~$ kubectl apply -f secrets-volume-demo.yaml pod/secrets-volume-demo created 而后，使用openssl s_cleint命令向该Pod对象的IP地址发起TLS访问请求，确认其证书是否为前面自签生成的测试证书。 12~$ podIP=$(kubectl get pods secrets-volume-demo -o jsonpath=&#123;.status.podIP&#125;)~$ openssl s_client -connect $podIP:443 -state 不过，这里的测试请求使用了IP地址而非证书中的主体名称www.ilinux.io，因而证书的验证会失败，但我们只需关注证书内容即可，尤其是证书链中显示的信息。若能成功证明响应中的证书来自nginx-ssl-secret对象中保存的自签证书，也就意味着通过存储卷方式向容器提供敏感信息的操作成功了。 应用Downward API存储卷配置信息除了通过ConfigMap和Secret对象向容器注入配置信息之外，应用程序有时候还需要基于所运行的外在系统环境信息设定自身的运行特性。例如nginx进程可根据节点的CPU核心数量自动设定要启动的worker进程数，JVM虚拟机可根据节点内存资源自动设定其堆内存大小等。这种功能有点类似于编程中的反射机制，它旨在让对象加载与自身相关的重要环境信息并据此做出运行决策。Kubernetes的Downward API支持通过环境变量与文件（downwardAPI卷插件）将Pod及节点环境相关的部分元数据和状态数据注入容器中，它们的使用方式同ConfigMaps和Secrets类似，用于完成将外部信息传递给Pod中容器的应用程序。然而，Downward API并不会将所有可用的元数据统统注入容器中，而是由用户在配置Pod对象自行选择需要注入容器中的元数据。可选择注入的信息包括Pod对象的IP、主机名、标签、注解、UID、请求的CPU与内存资源量及其限额，甚至是Pod所在的节点名称和节点IP等。Downward API的数据注入方式如图6-2所示。 但是与ConfigMap和Secret这两个标准的API资源类型不同的是，Downward API自身便是一种附属于API Server之上API，在Pod资源的定义中可直接进行引用而无须事先进行任何资源定义。 环境变量式元数据注入类似于ConfigMap或Secret资源，容器能够在环境变量valueFrom字段中嵌套fieldRef或resourceFieldRef字段来引用其所属Pod对象的元数据信息。不过，通常只有常量类型的属性才能够通过环境变量注入容器中，毕竟进程启动完成后无法再向其告知变量值的变动，于是环境变量也就不支持中途的更新操作。在容器规范中，可在环境变量中配置valueFrom字段内嵌fieldRef字段引用的信息包括如下这些。 metadata.name：Pod对象的名称。 metadata.namespace：Pod对象隶属的名称空间。 metadata.uid：Pod对象的UID。 metadata.labels[‘‘]：Pod对象标签中的指定键的值，例如metadata.labels[‘mylabel’]，仅Kubernetes 1.9及之后的版本才支持。 metadata.annotations[‘‘]：Pod对象注解信息中的指定键的值，仅Kubernetes 1.9及之后的版本才支持。容器上的计算资源需求和资源限制相关的信息，以及临时存储资源需求和资源限制相关的信息可通过容器规范中的resourceFieldRef字段引用，相关字段包括requests.cpu、limits.cpu、requests.memory和limits.memory等。另外，可通过环境变量引用的信息有如下几个。 status.podIP：Pod对象的IP地址。 spec.serviceAccountName：Pod对象使用的ServiceAccount资源名称。 spec.nodeName：节点名称。 status.hostIP：节点IP地址。 另外，还可以通过resourceFieldRef字段引用当前容器的资源请求及资源限额的定义，因此它们包括requests.cpu、requests.memory、requests.ephemeral-storage、limits.cpu、limits.memory和limits.ephemeral-storage这6项。下面的资源配置清单示例（downwardAPI-env.yaml）中定义的Pod对象通过环境变量向容器demoapp中注入了Pod对象的名称、隶属的名称空间、标签app的值以及容器自身的CPU资源限额和内存资源请求等信息。 1234567891011121314151617181920212223242526272829303132333435363738394041apiVersion: v1kind: Podmetadata: name: downwardapi-env-demo labels: app: demoappspec: containers: - name: demoapp image: ikubernetes/demoapp:v1.0 command: [ &quot;/bin/sh&quot;, &quot;-c&quot;, &quot;env&quot; ] resources: requests: memory: &quot;32Mi&quot; cpu: &quot;250m&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;500m&quot; env: - name: THIS_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: THIS_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: THIS_APP_LABEL valueFrom: fieldRef: fieldPath: metadata.labels[&#x27;app&#x27;] - name: THIS_CPU_LIMIT valueFrom: resourceFieldRef: resource: limits.cpu - name: THIS_MEM_REQUEST valueFrom: resourceFieldRef: resource: requests.memory divisor: 1Mi restartPolicy: Never 该Pod对象创建并启动后向控制台打印所有的环境变量即终止运行，它仅用于测试通过环境变量注入信息到容器的使用效果。我们先根据下面的命令创建出配置清单中定义的Pod资源Pod/downwardapi-env-demo。 12~$ kubectl apply -f downwardapi-env-demo.yamlpod/downwardapi-env-demo created 等该Pod对象的状态转为Completed之后即可通过控制台日志获取注入的环境变量，如下面的命令及结果所示。 123456~$ kubectl logs downwardapi-env-demo | grep &quot;^THIS_&quot;THIS_CPU_LIMIT=1THIS_APP_LABEL=demoappTHIS_MEM_REQUEST=32THIS_POD_NAME=downwardapi-env-demoTHIS_POD_NAMESPACE=default 示例最后一个环境变量的定义中还额外指定了一个divisor字段，它用于为引用的值指定一个除数，以对引用的数据进行单位换算。CPU资源的divisor字段默认值为1，它表示为1个核心，相除的结果不足1个单位时则向上圆整（例如0.25向上圆整的结果为1），它的另一个可用单位为1m，即表示1个微核心。内存资源的divisor字段默认值也是1，不过它意指1个字节，此时32MiB的内存资源则要换算为33554432予以输出。其他可用的单位还有1KiB、1MiB、1GiB等，于是在将divisor字段的值设置为1MiB时，32MiB的内存资源换算的结果即为32。注意未给容器定义资源请求及资源限额时，通过downwardAPI引用的值则默认为节点的可分配CPU及内存资源量。 存储卷式元数据注入downwardAPI存储卷能够以文件方式向容器中注入元数据，将配置的字段数据映射为文件并可通过容器中的挂载点访问。事实上，6.4.1节中通过环境变量方式注入的元数据信息也都可以使用存储卷方式进行信息暴露，但除此之外，我们还能够在downwardAPI存储卷中使用fieldRef引用下面两个数据源。 metadata.labels：Pod对象的所有标签信息，每行一个，格式为label-key=”escaped-label-value”。 metadata.annotations：Pod对象的所有注解信息，每行一个，格式为annotation-key=”escaped-annotation-value”。 下面的资源配置清单示例（downwardapi-volumes-demo.yaml）中定义的Pod资源通过downwardAPI存储卷向容器demoapp中注入了Pod对象隶属的名称空间、标签、注解以及容器自身的CPU资源限额和内存资源请求等信息。存储卷在容器中的挂载点为/etc/podinfo目录，因而注入的每一项信息均会映射为此路径下的一个文件。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748kind: PodapiVersion: v1name: downwardapi-volume-demometadata: labels: zone: zone1 rack: rack100 app: demoapp annotations: region: ease-cnspec: containers: - name: demoapp image: ikubernetes/demoapp:v1.0 resources: requests: memory: &quot;32Mi&quot; cpu: &quot;250m&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;500m&quot; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: false volumes: - name: podinfo downwardAPI: defaultMode: 420 items: - fieldRef: fieldPath: metadata.namespace path: pod_namespace - fieldRef: fieldPath: metadata.labels path: pod_labels - fieldRef: fieldPath: metadata.annotations path: pod_annotations - resourceFieldRef: containerName: demoapp resource: limits.cpu path: &quot;cpu_limit&quot; - resourceFieldRef: containerName: demoapp resource: requests.memory divisor: &quot;1Mi&quot; path: &quot;mem_request&quot; 创建资源配置清单中定义的Pod对象后即可测试访问由downwardAPI存储卷映射的文件pod_namespace、pod_labels、pod_annotations、limits_cpu和mem_request等。 12~$ kubectl apply -f downwardapi-volume-demo.yaml pod/downwardapi-volume-demo created 待Pod对象正常运行后即可测试访问上述的映射文件，例如访问/etc/podinfo/pod_labels文件以查看Pod对象的标签列表： 1234~$ kubectl exec downwardapi-volume-demo -- cat /etc/podinfo/pod_labels app=&quot;demoapp&quot;rack=&quot;rack100&quot;zone=&quot;zone1&quot; 如命令结果所示，Pod对象的标签信息每行一个地映射于自定义的路径/etc/podinfo/pod_labels文件中，类似地，注解信息也以这种方式进行处理。如前面的章节所述，标签和注解支持运行时修改，其改动的结果也会实时映射进downwardAPI生成的文件中。例如，为downwardapi-volume-demo对象添加新的标签： 12~$ kubectl label pods/downwardapi-volume-demo release=&quot;Canary&quot;pod/downwardapi-volume-demo labeled 而后再次查看容器内的pod_labels文件的内容，由如下的命令结果可知新的标签已经能够通过相关的文件获取到。 12345~$ kubectl exec downwardapi-volume-demo -- cat /etc/podinfo/pod_labelsapp=&quot;demoapp&quot;rack=&quot;rack100&quot;release=&quot;Canary&quot;zone=&quot;zone1&quot; downwardAPI存储卷为Kubernetes上运行容器化应用提供了获取外部环境信息的有效途径，这对那些非云原生应用在不进行代码重构的前提下获取环境信息，以进行自身配置等操作时尤为有用。事实上，5.6节中Longhorn存储系统在其Longhorn Manager相关的资源清单中就使用了downwardAPI。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes存储卷与数据持久化","slug":"Kubernetes存储卷与数据持久化","date":"2022-02-09T12:57:32.000Z","updated":"2022-02-11T01:08:54.195Z","comments":true,"path":"2022/02/09/Kubernetes存储卷与数据持久化/","link":"","permalink":"https://marmotad.github.io/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/","excerpt":"","text":"存储卷与数据持久化存储卷基础Pod本身有生命周期，其应用容器及生成的数据自身均无法独立于该生命周期之外持久存在，并且同一Pod中的容器可共享PID、Network、IPC和UTS名称空间，但Mount和USER名称空间却各自独立，因而跨容器的进程彼此间默认无法基于共享的存储空间交换文件或数据。因而，借助特定的存储机制甚至是独立于Pod生命周期的存储设备完成数据持久化也是必然之需。 存储卷概述存储卷是定义在Pod资源之上可被其内部的所有容器挂载的共享目录，该目录关联至宿主机或某外部的存储设备之上的存储空间，可由Pod内的多个容器同时挂载使用。Pod存储卷独立于容器自身的文件系统，因而也独立于容器的生命周期，它存储的数据可于容器重启或重建后继续使用。图5-1展示了Pod容器与存储卷之间的关系。 每个工作节点基于本地内存或目录向Pod提供存储空间，也能够使用借助驱动程序挂载的网络文件系统或附加的块设备，例如使用挂载至本地某路径上的NFS文件系统等。Kubernetes系统具体支持的存储卷类型要取决于存储卷插件的内置定义，如图5-2所示，不过Kubernetes也支持管理员基于扩展接口配置使用第三方存储。另外，Kubernetes甚至还支持一些有着特殊功用的存储卷，例如将外部信息投射至Pod之中的ConfigMap、Secret和Downward API等。 存储卷并非Kubernetes上一种独立的API资源类型，它隶属于Pod资源，且与所属的特定Pod对象有着相同的生命周期，因而通过API Server管理声明了存储卷资源的Pod对象时也会相应触发存储卷的管理操作。在具体的执行过程中，首选由调度器将该Pod对象绑到一个工作节点之上，若该Pod定义存储卷尚未被挂载，Controller Manager中的AD控制器（Attach/Detach Controller）会先借助相应的存储卷插件把远程的存储设备附加到该目标节点，而由内置在kubelet中的Pod管理器（Pod Manager）触发本地的存储卷操作实现，它借助存储卷管理器（Volume Manager）调用存储卷插件进行关联并驱动相应存储服务，并完成设备的挂载、格式化和卸载等操作。存储卷独立于Pod对象中容器的生命周期，从而使得容器重启或更新之后数据依然可用，但删除Pod对象时也必将删除其存储卷。Kubernetes系统内置了多种类型的存储卷插件，因而能够直接支持多种类型存储系统（即存储服务方），例如CephFS、NFS、RBD、iscsi和vSphereVolume等。定义Pod资源时，用户可在其spec.volumes字段中嵌套配置选定的存储卷插件，并结合相应的存储服务来使用特定类型的存储卷，甚至使用CS或flexVolume存储卷插件来扩展支持更多的存储服务系统。对Pod对象来说，卷类型主要是为关联适配的存储系统时提供相关的配置参数。例如，关联节点本地的存储目录与关联GlusterFS存储系统所需要的配置参数差异巨大，因此指定了存储卷类型也就限定了其关联到的后端存储设备。目前，Kubernetes支持的存储卷可简单归为以下类别，它们也各自有着不少的实现插件。 1）临时存储卷：emptyDir。 2）本地存储卷：hostPath和local。 3）网络存储卷： 云存储——awsElasticBlockStore、gcePersistentDisk、azureDisk和azureFile。 网络文件系统——NFS、GlusterFS、CephFS和Cinder。 网络块设备——iscsi、FC、RBD和vSphereVolume。 网络存储平台——Quobyte、PortworxVolume、StorageOS和ScaleIO。 4）特殊存储卷：Secret、ConfigMap、DownwardAPI和Projected。 5）扩展支持第三方存储的存储接口（Out-of-Tree卷插件）：CSI和FlexVolume。 Kubernetes内置提供的存储卷插件可归类为In-Tree类型，它们同Kubernetes源代码一同发布和迭代，而由存储服务商借助于CSI或FlexVolume接口扩展的独立于Kubernetes代码的存储卷插件则统称为Out-Of-Tree类型，集群管理员也可根据需要创建自定义的扩展插件，目前CSI是较为推荐的扩展接口，如图5-3所示。 尽管网络存储基本都具有持久存储能力，但它们都要求Pod资源清单的编写人员了解可用的真实网络存储的基础结构，并且能够准确配置用到的每一种存储服务。例如，要创建基于Ceph RBD的存储卷，用户必须要了解Ceph集群服务器（尤其是Monitor服务器）的地址，并且能够理解接入Ceph集群的必要配置及其意义。 配置Pod存储卷在Pod中定义使用存储卷的配置由两部分组成：一部分通过.spec.volumes字段定义在Pod之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储系统的访问接口进行定义；另一部分是嵌套定义在容器的volumeMounts字段上的存储卷挂载列表，它只能挂载当前Pod对象中定义的存储卷。不过，定义了存储卷的Pod内的容器也可以选择不挂载任何存储卷。 1234567891011121314spec: volumes: - name &lt;string&gt; # 存储卷名称标识，仅可使用DNS标签格式的字符，在当前Pod中必须唯一 VOL_TYPE &lt;Object&gt; # 存储卷插件及具体的目标存储系统的相关配置 containers: - name: … image: … volumeMounts: - name &lt;string&gt; # 要挂载的存储卷的名称，必须匹配存储卷列表中某项的定义 mountPath &lt;string&gt; # 容器文件系统上的挂载点路径 readOnly &lt;boolean&gt; # 是否挂载为只读模式，默认为“否” subPath &lt;string&gt; # 挂载存储卷上的一个子目录至指定的挂载点 subPathExpr &lt;string&gt; # 挂载由指定模式匹配到的存储卷的文件或目录至挂载点 mountPropagation &lt;string&gt; # 挂载卷的传播模式 Pod配置清单中的.spec.volumes字段的值是一个对象列表，每个列表项定义一个存储卷，它由存储卷名称（.spec.volumes.name &lt;String&gt;）和存储卷对象（.spec.volumes.VOL_TYPE &lt;Object&gt;）组成，其中VOL_TYPE是使用的存储卷类型名称，它的内嵌字段随类型的不同而不同，具体参数需要参阅Pod上各存储卷插件的相关文档说明。定义好的存储卷可由当前Pod资源内的各容器进行挂载。Pod中仅有一个容器时，使用存储卷的目的通常在于数据持久化，以免重启时导致数据丢失，而只有多个容器挂载同一个存储卷时，“共享”才有了具体的意义。挂载卷的传播模式（mountPropagation）就是用于配置容器将其挂载卷上的数据变动传播给同一Pod中的其他容器，甚至是传播给同一个节点上的其他Pod的一个特性，该字段的可用值包括如下几项。 None：该挂载卷不支持传播机制，当前容器不向其他容器或Pod传播自己的挂载操作，也不会感知主机后续在该挂载卷或其任何子目录上执行的挂载变动；此为默认值。 HostToContainer：主机向容器的单向传播，即当前容器能感知主机后续对该挂载卷或其任何子目录上执行的挂载变动。 Bidirectional：主机和容器间的双向传播，当前容器创建的存储卷挂载操作会传播给主机及使用了同一存储卷的所有Pod的所有容器，也能感知主机上后续对该挂载卷或其任何子目录上执行的挂载变动；该行为存在破坏主机操作系统的危险，因而仅可用于特权模式下的容器中。 临时存储卷Kubernetes支持的存储卷类型中，emptyDir存储卷的生命周期与其所属的Pod对象相同，它无法脱离Pod对象的生命周期提供数据存储功能，因此通常仅用于数据缓存或临时存储。不过，基于emptyDir构建的gitRepo存储卷可以在Pod对象的生命周期起始时，从相应的Git仓库中克隆相应的数据文件到底层的emptyDir中，也就使得它具有了一定意义上的持久性。 emptyDir存储卷emptyDir存储卷可以理解为Pod对象上的一个临时目录，类似于Docker上的“Docker挂载卷”，在Pod启动时被创建，而在Pod对象被移除时一并被删除。因此，emptyDir存储卷只能用于某些特殊场景中，例如同一Pod内的多个容器间的文件共享，或作为容器数据的临时存储目录用于数据缓存系统等。emptyDir存储卷嵌套定义在.spec.volumes.emptyDir字段中，可用字段主要有两个。 medium：此目录所在的存储介质的类型，可用值为default或Memory，默认为default，表示使用节点的默认存储介质；Memory表示使用基于RAM的临时文件系统tmpfs，总体可用空间受限于内存，但性能非常好，通常用于为容器中的应用提供缓存存储。 sizeLimit：当前存储卷的空间限额，默认值为nil，表示不限制；不过，在medium字段值为Memory时，建议务必定义此限额。下面是一个使用了emptyDir存储卷的简单示例，它保存在volumes-emptydir-demo.yaml配置文件中。 123456789101112131415161718192021222324252627282930apiVersion: v1kind: Podmetadata: name: volumes-emptydir-demo namespace: defaultspec: initContainers: - name: config-file-downloader image: ikubernetes/admin-box imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;wget -O /data/envoy.yaml https://raw. githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/ master/chapter4/envoy.yaml&#x27;] volumeMounts: - name: config-file-store mountPath: /data containers: - name: envoy image: envoyproxy/envoy-alpine:v1.13.1 command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;envoy -c /etc/envoy/envoy.yaml&#x27;] volumeMounts: - name: config-file-store mountPath: /etc/envoy readOnly: true volumes: - name: config-file-store emptyDir: medium: Memory sizeLimit: 16Mi 在该示例清单中，为Pod对象定义了一个名为config-file-store的、基于emptyDir存储插件的存储卷。初始化容器将该存储卷挂载至/data目录后，下载envoy.yaml配置文件并保存于该挂载点目录下。主容器将该存储卷挂载至/etc/envoy目录，再通过自定义命令让容器应用在启动时加载的配置文件/etc/envoy/envoy.yaml上，如图5-4所示。 Pod资源的详细信息中会显示存储卷的相关状态，包括其是否创建成功（Events字段中输出）、相关的类型及参数（Volumes字段中输出），以及容器中的挂载状态等信息（Containers字段中输出）。如下面的命令结果所示。 12345678910111213141516171819~$ kubectl describe pods volumes-emptydir-demo……Init Containers: config-file-downloader: …… Mounts: /data from config-file-store (rw) ……Containers: envoy: Mounts: /etc/envoy from config-file-store (ro) ……Volumes: config-file-store: Type: EmptyDir (a temporary directory that shares a pod&#x27;s lifetime) Medium: Memory SizeLimit: 16Mi…… 为Envoy下载的配置文件中定义了一个监听所有可用IP地上TCP 80端口的Ingress侦听器，以及一个监听所有可用IP地址上TCP的9901端口的Admin接口，这与Envoy镜像中默认配置文件中的定义均有不同。下面命令的结果显示它吻合自定义配置文件的内容。 12345678~$ kubectl exec volumes-emptydir-demo -- netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN tcp 0 0 0.0.0.0:9901 0.0.0.0:* LISTEN ~$ podIP=$(kubectl get pods/volumes-emptydir-demo -o jsonpath=&#123;.status.podIP&#125;)~$ curl $podIP:9901/listenerslistener_0::0.0.0.0:80 emptyDir卷简单易用，但仅能用于临时存储。另外存在一些类型的存储卷构建在emptyDir之上，并额外提供了它所没有功能，例如将于下一节介绍的gitRepo存储卷。 gitRepo存储卷gitRepo存储卷可以看作是emptyDir存储卷的一种实际应用，使用该存储卷的Pod资源可以通过挂载目录访问指定的代码仓库中的数据。使用gitRepo存储卷的Pod资源在创建时，会首先创建一个空目录（emptyDir）并克隆（clone）一份指定的Git仓库中的数据至该目录，而后再创建容器并挂载该存储卷。定义gitRepo类型的存储卷时，其可嵌套使用字段有如下3个。 repository &lt;string&gt;：Git仓库的URL，必选字段。 directory &lt;string&gt;：目标目录名称，但名称中不能包含“..”字符；“.”表示将仓库中的数据直接克隆至存储卷映射的目录中，其他字符则表示将数据克隆至存储卷上以用户指定的字符串为名称的子目录中。 revision &lt;string&gt;：特定revision的提交哈希码。 注意:使用gitRepo存储卷的Pod资源运行的工作节点上必须安装有Git程序，否则克隆仓库的操作将无法完成。 下面的配置清单示例（volumes-gitrepo-demo.yaml）中的Pod资源在创建时，会先创建一个空目录，将指定的Git仓库https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git中的数据克隆一份直接保存在此目录中，而后将此目录创建为存储卷html，再由容器nginx将此存储卷挂载到/usr/share/nginx/html目录上。 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: volumes-gitrepo-demospec: containers: - name: nginx image: nginx:alpine volumeMounts: - name: html mountPath: /usr/share/nginx/html volumes: - name: html gitRepo: repository: https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git directory: . revision: &quot;master&quot; 访问此Pod资源中的nginx服务，即可看到它来自Git仓库中的页面资源。不过，gitRepo存储卷在其创建完成后不会再与指定的仓库执行同步操作，这意味着在Pod资源运行期间，如果仓库中的数据发生了变化，gitRepo存储卷不会同步到这些内容。当然，此时可以为Pod资源创建一个Sidecar容器来执行此类的同步操作，尤其是数据来源于私有仓库时，通过Sidecar容器完成认证等必要步骤后再进行克隆操作就更为必要。gitrRepo存储卷构建于emptyDir之上，其生命周期与Pod资源一样，故使用中不应在此类存储卷中保存由容器生成的重要数据。另外，gitRepo存储插件即将废弃，建议在初始化容器或Sidecar容器中运行git命令来完成相应的功能。 hostPath存储卷hostPath存储卷插件是将工作节点上某文件系统的目录或文件关联到Pod上的一种存储卷类型，其数据具有同工作节点生命周期一样的持久性。hostPath存储卷使用的是工作节点本地的存储空间，所以仅适用于特定情况下的存储卷使用需求，例如将工作节点上的文件系统关联为Pod的存储卷，从而让容器访问节点文件系统上的数据，或者排布分布式存储系统的存储设备等。hostPath存储卷在运行有管理任务的系统级Pod资源，以及Pod资源需要访问节点上的文件时尤为有用。配置hostPath存储卷的嵌套字段有两个：一个用于指定工作节点上的目录路径的必选字段path；另一个用于指定节点之上存储类型的type。hostPath支持使用的节点存储类型有如下几种。 DirectoryOrCreate：指定的路径不存在时，自动将其创建为0755权限的空目录，属主和属组均为kubelet。 Directory：事先必须存在的目录路径。 FileOrCreate：指定的路径不存在时，自动将其创建为0644权限的空文件，属主和属组均为kubelet。 File：事先必须存在的文件路径。 Socket：事先必须存在的Socket文件路径。 CharDevice：事先必须存在的字符设备文件路径。 BlockDevice：事先必须存在的块设备文件路径。 “”：空字符串，默认配置，在关联hostPath存储卷之前不进行任何检查。 这类Pod对象通常受控于DaemonSet类型的Pod控制器，它运行在集群中的每个工作节点上，负责收集工作节点上系统级的相关数据，因此使用hostPath存储卷也理所应当。然而，基于同一个模板创建Pod对象仍可能会因节点上文件的不同而存在着不同的行为，而且在节点上创建的文件或目录默认仅root用户可写，若期望容器内的进程拥有写权限，则需要将该容器运行于特权模式，不过这存在潜在的安全风险。下面是定义在配置清单volumes-hostpath-demo.yaml中的Pod对象，容器中的filebeat进程负责收集工作节点及容器相关的日志信息并发往Redis服务器，它使用了3个hostPath类型的存储卷，第一个指向了宿主机的日志文件目录/var/logs，后面两个则与宿主机上的Docker运行时环境有关。 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata: name: vol-hostpath-podspec: containers: - name: filebeat image: ikubernetes/filebeat:5.6.7-alpine env: - name: REDIS_HOST value: redis.ilinux.io:6379 - name: LOG_LEVEL value: info volumeMounts: - name: varlog mountPath: /var/log - name: socket mountPath: /var/run/docker.sock - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: socket hostPath: path: /var/run/docker.sock 上面配置清单中Pod对象的正确运行要依赖于REDIS_HOST和LOG_LEVEL环境变量，它们分别用于定义日志缓冲队列服务和日志级别。如果有可用的Redis服务器，我们就可通过环境变量REDIS_HOST将其对应的主机名或IP地址传递给Pod对象，待Pod对象准备好之后即可通过Redis服务器查看到由该Pod发送的日志信息。测试时，我们仅需要给REDIS_HOST环境变量传递一个任意值（例如清单中的redis.ilinux.io）便可直接创建Pod对象，只不过该Pod中容器的日志会报出无法解析指定主机名的错误，但这并不影响存储卷的配置和使用。对于由Deployment或StatefulSet等一类控制器管控的、使用了hostPath存储卷的Pod对象来说，需要注意在基于资源可用状态的调度器调度Pod对象时，并不支持参考目标节点之上hostPath类型的存储卷，在Pod对象被重新调度至其他节点时，容器进程此前创建的文件或目录则大多不会存在。一个常用的解决办法是通过在Pod对象上使用nodeSelector或者nodeAffinity赋予该Pod对象指定要绑定到的具体节点来影响调度器的决策，但即便如此，管理员仍然不得不手动管理涉及的多个节点之上的目录，低效且易错。因此，hostPath存储卷虽然能持久保存数据，但对于由调度器按需调度的应用来说并不适用。 网络存储卷5.4.1 NFS存储卷Kubernetes的NFS存储卷用于关联某事先存在的NFS服务器上导出的存储空间到Pod对象中以供容器使用，该类型的存储卷在Pod对象终止后仅是被卸载而非被删除。而且，NFS是文件系统级共享服务，它支持同时存在的多路挂载请求，可由多个Pod对象同时关联使用。定义NFS存储卷时支持嵌套使用以下几个字段。 server &lt;string&gt;：NFS服务器的IP地址或主机名，必选字段。 path &lt;string&gt;：NFS服务器导出（共享）的文件系统路径，必选字段。 readOnly &lt;boolean&gt;：是否以只读方式挂载，默认为false。 Redis基于内存存储运行，数据持久化存储的需求通过周期性地将数据同步到主机磁盘之上完成，因此将Redis抽象为Pod对象部署运行于Kubernetes系统之上时，需要考虑节点级或网络级的持久化存储卷的支持，本示例就是以NFS存储卷为例，为Redis进程提供跨Pod对象生命周期的数据持久化功能。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: volumes-nfs-demo labels: app: redisspec: containers: - name: redis image: redis:alpine ports: - containerPort: 6379 name: redisport securityContext: runAsUser: 999 volumeMounts: - mountPath: /data name: redisdata volumes: - name: redisdata nfs: # NFS存储卷插件 server: nfs.ilinux.io path: /data/redis readOnly: false 上面的示例定义在名为volumes-nfs-demo.yaml资源清单文件中，容器镜像文件redis:alpine默认会以redis用户（UID是999）运行redis-server进程，并将数据持久保存在容器文件系统上的/data目录中，因而需要确保UID为999的用户有权限读写该目录。与此对应，NFS服务器上用于该Pod对象的存储卷的导出目录（本示例中为/data/redis目录）也需要确保让UID为999的用户拥有读写权限，因而需要在nfs.ilinux.io服务器上创建该用户，将该用户设置为/data/redis目录的属主，或通过facl设置该用户拥有读写权限。以Ubuntu Server18.04为例，在一个专用的主机（nfs.ilinux.io）上以root用户设定所需的NFS服务器的步骤如下。 1）安装NFS Server程序包，Ubuntu 18.04上的程序包名为nfs-kernel-server。 1~# apt -y install nfs-kernel-server 2）设定基础环境，包括用户、数据目录及相应授权。 123~# mkdir /data/redis~# useradd -u 999 redis~# chown redis /data/redis 3）编辑/etc/exports配置文件，填入类似如下内容： 1/data/redis 172.29.0.0/16(rw,no_root_squash) 10.244.0.0/16(rw,no_root_squash) 4）启动NFS服务器： 1~# systemctl start nfs-server 5）在各工作节点安装NFS服务客户端程序包，Ubuntu 18.04上的程序包名为nfs-common。 1~# apt install -y nfs-common 待上述步骤执行完成后，切换回Kubernetes集群可运行kubectl命令的主机之上，运行命令创建配置清单中的Pod对象： 12~$ kubectl apply -f volumes-nfs-demo.yaml pod/volumes-nfs-demo created 资源创建完成后，可通过其命令客户端redis-cli创建测试数据，并手动触发其与存储系统同步，下面加粗部分的字体为要执行的Redis命令。 12345678~$ kubectl exec -it volumes-nfs-demo -- redis-cli127.0.0.1:6379&gt; set mykey &quot;hello ilinux.io&quot;OK127.0.0.1:6379&gt; get mykey&quot;hello ilinux.io&quot;127.0.0.1:6379&gt; BGSAVEBackground saving started127.0.0.1:6379&gt; exit 为了测试其数据持久化效果，下面先删除此前创建的Pod对象vol-nfs-pod，而后待重建该Pod对象后检测数据是否依然能够访问。 1234~$ kubectl delete pods/volumes-nfs-demopod &quot;volumes-nfs-demo&quot; deleted~$ kubectl apply -f volumes-nfs-demo.yaml pod/volumes-nfs-demo created 待其重建完成后，通过再次创建的Pod资源的详细描述信息可以观察到它挂载使用NFS存储卷的相关状态，也可通过下面的命令来检查redis-server中是否还保存有此前存储的数据。 1234~$ kubectl exec -it volumes-nfs-demo -- redis-cli127.0.0.1:6379&gt; get mykey&quot;hello ilinux.io&quot;127.0.0.1:6379&gt; 上面的命令结果显示出此前创建的键mykey及其数据在Pod对象删除并重建后依然存在，这表明删除Pod对象后，其关联的外部存储设备及数据并不会被一同删除，因而才具有了跨Pod生命周期的数据持久性。若需要在删除Pod后清除具有持久存储功能的存储设备上的数据，则需要用户或管理员通过存储系统的管理接口手动进行。 RBD存储卷Ceph是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和对象存储3种存储接口。它是个高度可配置的系统，并提供了一个命令行界面用于监视和控制其存储集群。Kubernetes支持通过RBD卷插件和CephFS卷插件，基于Ceph存储系统为Pod提供存储卷。要配置Pod对象使用RBD存储卷，需要事先满足以下前提条件。▪存在某可用的Ceph RBD存储集群，否则需要创建一个。▪在Ceph RBD集群中创建一个能满足Pod资源数据存储需要的存储映像。▪在Kubernetes集群内的各节点上安装Ceph客户端程序包（ceph-common）。定义RBD类型的存储卷时需要指定要连接的目标服务器和认证信息等配置，它们依赖如下几个可用的嵌套字段。▪monitors &lt;[]string&gt;：Ceph存储监视器，逗号分隔的字符串列表；必选字段。▪image ：rados image（映像）的名称，必选字段。▪pool ：Ceph存储池名称，默认为rbd。▪user ：Ceph用户名，默认为admin。▪keyring ：用户认证到Ceph集群时使用的keyring文件路径，默认为/etc/ceph/keyring。▪secretRef ：用户认证到Ceph集群时使用的保存有相应认证信息的Secret资源对象，该字段会覆盖由keyring字段提供的密钥信息。▪readOnly ：是否以只读方式访问。▪fsType：要挂载的存储卷的文件系统类型，至少应该是节点操作系统支持的文件系统，例如Ext4、xfs、NTFS等，默认为Ext4。下面提供的RBD存储卷插件使用示例定义在volumes-rbd-demo.yaml配置清单文件中，它使用kube用户认证到Ceph集群中，并关联RDB存储池kube中的存储映像redis-img1为Pod对象volumes-rbd-demo的存储卷，由容器进程挂载至/data目录进行数据存取。 123456789101112131415161718192021222324252627apiVersion: v1kind: Podmetadata: name: volumes-rbd-demospec: containers: - name: redis image: redis:alpine ports: - containerPort: 6379 name: redisport volumeMounts: - mountPath: /data name: redis-rbd-vol volumes: - name: redis-rbd-vol rbd: monitors: - &#x27;172.29.200.1:6789&#x27; - &#x27;172.29.200.2:6789&#x27; - &#x27;172.29.200.3:6789&#x27; pool: kube image: redis-img1 fsType: xfs readOnly: false user: kube keyring: /etc/ceph/ceph.client.kube.keyring RBD存储卷插件依赖Ceph存储集群作为存储系统，这里假设其监视器（MON）的地址为172.29.200.1、172.29.200.2和172.29.200.3，集群上的存储池kube中需要有事先创建好的存储映像redis-img1。客户端访问集群时要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了用户的keyring文件。该示例实现的逻辑架构如图5-5所示。 为了完成示例中定义的资源的测试，需要事先完成如下几个步骤。1）在Ceph集群上的kube存储池中创建用作Pod存储卷的RBD映像文件，并设置映像特性。 12~# rbd create --pool kube --size 1G redis-img1~# rbd feature disable -p kube redis-img1 object-map fast-diff deep-flatten 2）在Ceph集群上创建存储卷客户端账号并进行合理授权。 123~# ceph auth get-or-create client.kube mon &#x27;allow r&#x27; \\ osd &#x27;allow class-read object_prefix rbd_children, allow rwx pool=kube&#x27; \\ -o /etc/ceph/ceph.client.kube.keyring 3）在Kubernetes集群的各工作节点上执行如下命令安装Ceph客户端库。 1234~# wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -~# echo deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \\ | tee /etc/apt/sources.list.d/ceph.list ~# apt update &amp;&amp; apt install ceph-common 4）在Ceph集群某节点上执行如下命令，以复制Ceph集群的配置文件及客户端认证使用的keyring文件到Kubernetes集群的各工作节点之上。 12~# for kubehost in k8s-node01 k8s-node02 k8s-node03; do \\ scp -p /etc/ceph/&#123;ceph.conf,ceph.client.kube.keyring&#125; $&#123;kubehost&#125;:/etc/ceph/; done 待完成如上必要的准备步骤后，便可执行如下命令将前面定义在volumes-rbd-demo.yaml中的Pod资源创建在Kubernetes集群上进行测试。 12~$ kubectl apply -f volumes-rbd-demo.yaml pod/volumes-rbd-demo created 随后从集群上的Pod对象volumes-rbd-demo的详细描述中获取存储的相关状态信息，确保其创建操作得以成功执行。下面是相关的存储卷信息示例。 1234567891011Volumes: redis-rbd-vol: Type: RBD (a Rados Block Device mount on the host that shares a pod&#x27;s lifetime) CephMonitors: [172.29.200.1:6789 172.29.200.2:6789 172.29.200.3:6789] RBDImage: redis-img1 FSType: xfs RBDPool: kube RadosUser: kube Keyring: /etc/ceph/ceph.client.kube.keyring SecretRef: nil ReadOnly: false 删除Pod对象仅会解除它对RBD映像的引用而非级联删除它，因而RBD映像及数据将依然存在，除非管理员手动进行删除。我们可使用类似前一节测试Redis数据持久性的方式来测试本示例中的容器数据的持久能力，这里不再给出具体步骤。另外，实践中，应该把认证到Ceph集群上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用keyring字段引用相应用户的keyring文件。 CephFS存储卷CephFS（Ceph文件系统）是在分布式对象存储RADOS之上构建的POSIX兼容的文件系统，它致力于为各种应用程序提供多用途、高可用和高性能的文件存储。CephFS将文件元数据和文件数据分别存储在各自专用的RADOS存储池中，其中MDS通过元数据子树分区等支持高吞吐量的工作负载，而数据则由客户端直接相关的存储池直接进行读写操作，其扩展能跟随底层RADOS存储的大小进行线性扩展。Kubernetes的CephFS存储卷插件以CephFS为存储方案为Pod提供存储卷，因而可受益于CephFS的存储扩展和性能优势。CephFS存储卷插件嵌套定义于Pod资源的spec.volumes.cephfs字段中，它支持通过如下字段的定义接入到存储预配服务中。 monitors &lt;[]string&gt;：Ceph存储监视器，为逗号分隔的字符串列表；必选字段。 user &lt;string&gt;：Ceph集群用户名，默认为admin。 secretFile &lt;string&gt;：用户认证到Ceph集群时使用的Base64格式的密钥文件（非keyring文件），默认为/etc/ceph/user.secret。 secretRef &lt;Object&gt;：用户认证到Ceph集群过程中加载其密钥时使用的Kubernetes Secret资源对象。 path &lt;string&gt;：挂载的文件系统路径，默认为CephFS文件系统的根（/），可以使用CephFS文件系统上的子路径，例如/kube/namespaces/default/redis1等。 readOnly &lt;boolean&gt;：是否挂载为只读模式，默认为false。 下面提供的CephFS存储卷插件使用示例定义在volumes-cephfs-demo.yaml配置清单文件中，它使用fsclient用户认证到Ceph集群中，并关联CephFS上的子路径/kube/namespaces/default/redis1，作为Pod对象volumes-cephfs-demo的存储卷，并由容器进程挂载至/data目录进行数据存取。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: volumes-cephfs-demospec: containers: - name: redis image: redis:alpine volumeMounts: - mountPath: &quot;/data&quot; name: redis-cephfs-vol volumes: - name: redis-cephfs-vol cephfs: monitors: - 172.29.200.1:6789 - 172.29.200.2:6789 - 172.29.200.3:6789 path: /kube/namespaces/default/redis1 user: fsclient secretFile: &quot;/etc/ceph/fsclient.key&quot; readOnly: false Kubernetes集群上需要启用了CephFS，并提供了满足条件的用户账号及授权才能使用CephFS存储卷插件。客户端访问集群时需要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了保存在/etc/ceph/fsclient.key文件中的CephFS专用用户认证信息。要完成示例清单中定义的资源的测试，需要事先完成如下几个步骤。 1）将授权访问CephFS的用户fsclient的Secret文件fsclient.key复制到Kubernetes集群的各工作节点，以便kubelet可加载并使用它。在生成fsclient.key的Ceph节点上执行如下命令以复制必要的文件。 12~# for kubehost in k8s-node01 k8s-node02 k8s-node03; do \\ scp -p /etc/ceph/fsclient.key /etc/ceph/ceph.conf $&#123;kubehost&#125;:/etc/ceph/; done 2）在Kubernetes集群的各工作节点上执行如下命令，以安装Ceph客户端库。 1234~# wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -~# echo deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \\ | tee /etc/apt/sources.list.d/ceph.list ~# apt update &amp;&amp; apt install ceph-common 3）在Kubernetes的某工作节点上手动挂载CephFS，以创建由Pod对象使用的数据目录。 12~# mount -t ceph ceph01:6789:/ /mnt -o name=fsclient,secretfile=/etc/ceph/fsclient.key~# mkdir -p /mnt/kube/namespaces/default/redis1 上述准备步骤执行完成后即可运行如下命令创建清单volumes-cephfs-demo.yaml中定义的Pod资源，并进行测试： 12~$ kubectl apply -f volumes-cephfs-demo.yaml pod/volumes-cephfs-demo created 随后通过Pod对象volumes-cephfs-demo的详细描述了解其创建及运行状态，若一切无误，则相应的存储卷会显示出类似如下的描述信息： 123456789Volumes: redis-cephfs-vol: Type: CephFS (a CephFS mount on the host that shares a pod&#x27;s lifetime) Monitors: [172.29.200.1:6789 172.29.200.2:6789 172.29.200.3:6789] Path: /kube/namespaces/default/redis1 User: fsclient SecretFile: /etc/ceph/fsclient.key SecretRef: nil ReadOnly: false 删除Pod对象仅会卸载其挂载的CephFS文件系统（或子目录），因而文件系统（或目录）及相关数据将依然存在，除非管理员手动进行删除。我们可使用类似5.4.1节中测试Redis数据持久性的方式来测试本示例中的容器数据的持久性，这里不再给出具体步骤。另外在实践中，应该把认证到CephFS文件系统上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用secretFile字段引用相应用户密钥信息文件。 GlusterFS存储卷GlusterFS（Gluster File System）是一个开源的分布式文件系统，是水平扩展存储解决方案Gluster的核心，它具有强大的横向扩展能力，通过扩展能够支持PB级的存储容量和数千个客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据，它基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能，是另一种流行的分布式存储解决方案。Kubernetes的GlusterFS存储卷插件依赖于GlusterFS存储集群作为存储方案。要配置Pod资源使用GlusterFS存储卷，需要事先满足以下前提条件： 1）存在某可用的GlusterFS存储集群，否则要创建一个。2）在GlusterFS集群中创建一个能满足Pod资源数据存储需要的卷。3）在Kubernetes集群内的各节点上安装GlusterFS客户端程序包（glusterfs和glusterfs-fuse）。GlusterFS存储卷嵌套定义在Pod资源的spec.volumes.glusterfs字段中，它常用的配置字段有如下几个。 endpoints &lt;string&gt;：Endpoints资源的名称，此资源需要事先存在，用于提供Gluster集群的部分节点信息作为其访问入口；必选字段。 path &lt;string&gt;：用到的GlusterFS集群的卷路径，例如kube-redis；必选字段。 readOnly &lt;boolean&gt;：是否为只读卷。 下面提供的GlusterFS存储卷插件使用示例定义在volumes-glusterfs-demo.yaml配置清单文件中，它通过glusterfs-endpoints资源中定义的GlusterFS集群节点信息接入集群，并以kube-redis卷作为Pod资源的存储卷。glusterfs-endpoints资源需要在Kubernetes集群中事先创建，而kube-redis则需要先于Gluster集群创建。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: volumes-glusterfs-demo labels: app: redisspec: containers: - name: redis image: redis:alpine ports: - containerPort: 6379 name: redisport volumeMounts: - mountPath: /data name: redisdata volumes: - name: redisdata glusterfs: endpoints: glusterfs-endpoints path: kube-redis readOnly: false 用于访问Gluster集群的相关节点信息要事先保存在某特定的Endpoint资源中，例如上面示例中调用的glusterfs-endpoints。此类的Endpoint资源依赖用户根据实际需求手动创建，例如，下面保存在glusterfs-endpoints.yaml文件中的资源示例定义了3个接入相关的Gluster存储集群的节点：gfs01.ilinux.io、gfs02.ilinux.io和gfs03.ilinux.io，其中的端口信息仅为满足Endpoint资源的必选字段要求，因此其值可以随意填写。 1234567891011121314151617181920apiVersion: v1kind: Endpointsmetadata: name: glusterfs-endpointssubsets: - addresses: - ip: gfs01.ilinux.io ports: - port: 24007 name: glusterd - addresses: - ip: gfs02.ilinux.io ports: - port: 24007 name: glusterd - addresses: - ip: gfs03.ilinux.io ports: - port: 24007 name: glusterd 准备好必要的存储供给条件后，先创建Endpoint资源glusterfs-endpoints，之后创建Pod资源vol-glusterfs-pod，即可测试其数据持久存储的效果 持久存储卷通过5.4节网络存储卷及使用示例可知，用户必须要清晰了解用到的网络存储系统的访问细节才能完成存储卷相关的配置任务，例如RBD存储卷插件配置中的监视器（monitor）、存储池（pool）、存储映像（image）和密钥环（keyring）等来自于Ceph存储系统中的概念，这就要求用户对该类存储系统有着一定的了解才能够顺利使用。这与Kubernetes向用户和开发隐藏底层架构的目标有所背离，最好对存储资源的使用也能像计算资源一样，用户和开发人员既无须了解Pod资源究竟运行在哪个节点，也不用了解存储系统是什么设备、位于何处以及如何访问。PV（PersistentVolume）与PVC（PersistentVolumeClaim）就是在用户与存储服务之间添加的一个中间层，管理员事先根据PV支持的存储卷插件及适配的存储方案（目标存储系统）细节定义好可以支撑存储卷的底层存储空间，而后由用户通过PVC声明要使用的存储特性来绑定符合条件的最佳PV定义存储卷，从而实现存储系统的使用与管理职能的解耦，大大简化了用户使用存储的方式。PV和PVC的生命周期由Controller Manager中专用的PV控制器（PV Controller）独立管理，这种机制的存储卷不再依附并受限于Pod对象的生命周期，从而实现了用户和集群管理员的职责相分离，也充分体现出Kubernetes把简单留给用户，把复杂留给自己的管理理念。 PV与PVC基础PV是由集群管理员于全局级别配置的预挂载存储空间，它通过支持的存储卷插件及给定的配置参数关联至某个存储系统上可用数据存储的一段空间，这段存储空间可能是Ceph存储系统上的一个存储映像、一个文件系统（CephFS）或其子目录，也可能是NFS存储系统上的一个导出目录等。PV将存储系统之上的存储空间抽象为Kubernetes系统全局级别的API资源，由集群管理员负责管理和维护。将PV提供的存储空间用于Pod对象的存储卷时，用户需要事先使用PVC在名称空间级别声明所需要的存储空间大小及访问模式并提交给Kubernetes API Server，接下来由PV控制器负责查找与之匹配的PV资源并完成绑定。随后，用户在Pod资源中使用persistentVolumeClaim类型的存储卷插件指明要使用的PVC对象的名称即可使用其绑定到的PV所指向的存储空间，如图5-6所示。 由此可见，尽管PVC及PV将存储资源管理与使用的职责分离至用户和集群管理员两类不同的人群之上，简化了用户对存储资源的使用机制，但也对二者之间的协同能力提出了要求。管理员需要精心预测和规划集群用户的存储使用需求，提前创建出多种规格的PV，以便于在用户声明PVC后能够由PV控制器在集群中找寻到合适的甚至是最佳匹配的PV进行绑定。不难揣测，这种通过管理员手动创建PV来满足PVC需求的静态预配（static provisioning）存在着不少的问题。第一，集群管理员难以预测出用户的真实需求，很容易导致某些类型的PVC无法匹配到PV而被挂起，直到管理员参与到问题的解决过程中。第二，那些能够匹配到PV的PVC也很有可能存在资源利用率不佳的状况，例如一个声明使用5G存储空间的PVC绑定到一个20GB的PV之上。更好的解决方案是一种称为动态预配、按需创建PV的机制。集群管理员要做的仅是事先借助存储类（StorageClass）的API资源创建出一到多个“PV模板”，并在模板中定义好基于某个存储系统创建PV所依赖的存储组件（例如Ceph RBD存储映像或CephfFS文件系统等）时需要用到的配置参数。创建PVC时，用户需要为其指定要使用PV模板（StorageClass资源），而后PV控制器会自动连接相应存储类上定义的目标存储系统的管理接口，请求创建匹配该PVC需求的存储组件，并将该存储组件创建为Kubernetes集群上可由该PVC绑定的PV资源。需要说明的是，静态预配的PV可能属于某存储类，也可能没有存储类，这取决于管理员的设定。但动态PV预配依赖存储类的辅助，PVC必须向一个事先存在的存储类发起动态分配PV的请求，没有指定存储类的PVC不支持使用动态预配PV的方式。PV和PVC是一对一的关系：一个PVC仅能绑定一个PV，而一个PV在某一时刻也仅可被一个PVC所绑定。为了能够让用户更精细地表达存储需求，PV资源对象的定义支持存储容量、存储类、卷模型和访问模式等属性维度的约束。相应地，PVC资源能够从访问模式、数据源、存储资源容量需求和限制、标签选择器、存储类名称、卷模型和卷名称等多个不同的维度向PV资源发起匹配请求并完成筛选。 PV的生命周期从较为高级的实现上来讲，Kubernetes系统与存储相关的组件主要有存储卷插件、存储卷管理器、PV/PVC控制器和AD控制器（Attach/Detach Controller）这4种，如图5-7所示。 存储卷插件：Kubernetes存储卷功能的基础设施，是存储任务相关操作的执行方；它是存储相关的扩展接口，用于对接各类存储设备。 存储卷管理器：kubelet内置管理器组件之一，用于在当前节点上执行存储设备的挂载（mount）、卸载（unmount）和格式化（format）等操作；另外，存储卷管理器也可执行节点级别设备的附加（attach）及拆除（detach）操作。 PV控制器：负责PV及PVC的绑定和生命周期管理，并根据需求进行存储卷的预配和删除操作； AD控制器：专用于存储设备的附加和拆除操作的组件，能够将存储设备关联（attach）至目标节点或从目标节点之上剥离（detach）。这4个组件中，存储卷插件是其他3个组件的基础库，换句话说，PV控制器、AD控制器和存储卷管理器均构建于存储卷插件之上，以提供不同维度管理功能的接口，具体的实现逻辑均由存储卷插件完成。除了创建、删除PV对象，以及完成PV和PVC的状态迁移等生命周期管理之外，PV控制器还要负责绑定PVC与PV对象，而且PVC只能在绑定到PV之后方可由Pod作为存储卷使用。创建后未能正确关联到存储设备的PV将处于Pending状态，直到成功关联后转为Available状态。而后一旦该PV被某个PVC请求并成功绑定，其状态也就顺应转为Bound，直到相应的PVC删除后而自动解除绑定，PV才会再次发生状态转换，此时的状态为（Released），随后PV的去向将由其“回收策略”（reclaim policy）所决定，具体如下。 1）Retain（保留）：删除PVC后将保留其绑定的PV及存储的数据，但会把该PV置为Released状态，它不可再被其他PVC所绑定，且需要由管理员手动进行后续的回收操作：首先删除PV，接着手动清理其关联的外部存储组件上的数据，最后手动删除该存储组件或者基于该组件重新创建PV。2）Delete（删除）：对于支持该回收策略的卷插件，删除一个PVC将同时删除其绑定的PV资源以及该PV关联的外部存储组件；动态的PV回收策略继承自StorageClass资源，默认为Delete。多数情况下，管理员都需要根据用户的期望修改此默认策略，以免导致数据非计划内的删除。3）Recycle（回收）：对于支持该回收策略的卷插件，删除PVC时，其绑定的PV所关联的外部存储组件上的数据会被清空，随后，该PV将转为Available状态，可再次接受其他PVC的绑定请求。不过，该策略已被废弃。 相应地，创建后的PVC也将处于Pending状态，仅在遇到条件匹配、状态为Available的PV，且PVC请求绑定成功才会转为Bound状态。PV和PVC的状态迁移如图5-8所示。总结起来，PV和PVC的生命周期存在以几个关键阶段。 1）存储预配（provision）：存储预配是指为PVC准备PV的途径，Kubernetes支持静态和动态两种PV预配方式，前者是指由管理员以手动方式创建PV的操作，而后者则是由PVC基于StorageClass定义的模板，按需请求创建PV的机制。2）存储绑定：用户基于一系列存储需求和访问模式定义好PVC后，PV控制器即会为其查找匹配的PV，完成关联后它们二者同时转为已绑定状态，而且动态预配的PV与PVC之间存在强关联关系。无法找到可满足条件的PV的PVC将一直处于Pending状态，直到有符合条件的PV出现并完成绑定为止。3）存储使用：Pod资源基于persistenVolumeClaim存储卷插件的定义，可将选定的PVC关联为存储卷并用于内部容器应用的数据存取。4）存储回收：存储卷的使用目标完成之后，删除PVC对象可使得此前绑定的PV资源进入Released状态，并由PV控制器根据PV回收策略对PV作出相应的处置。目前，可用的回收策略有Retaine、Delete和Recycle这3种。如前所述，处于绑定状态的PVC删除后，相应的PV将转为Released状态，之后的处理机制依赖于其回收策略。而处于绑定状态的PV将会导致相应的PVC转为Lost状态，而无法再由Pod正常使用，除非PVC再绑定至其他Available状态的PV之上，但应用是否能正常运行，则取决于对此前数据的依赖度。另一方面，为了避免使用中的存储卷被移除而导致数据丢失，Kubernetes自1.9版引入了“PVC保护机制”，其目的在于，用户删除了仍被某Pod对象使用中的PVC时，Kubernetes不会立即移除该PVC，而是会推迟到它不再被任何Pod对象使用后方才真正执行删除操作。处于保护阶段的PVC资源的status字段值为Termination，并且其Finalizers字段值中包含有kubernetes.io/pvc-protection。 静态PV资源PersistentVolume是隶属于Kubernetes核心API群组中的标准资源类型，它的目标在于通过存储卷插件机制，将支持的外部存储系统上的存储组件定义为可被PVC声明所绑定的资源对象。但PV资源隶属于Kubernetes集群级别，因而它只能由集群管理员进行创建。这种由管理员手动定义和创建的PV被人们习惯地称为静态PV资源。PV支持的存储卷插件类型是Pod对象支持的存储卷插件类型的一个子集，它仅涵盖Pod支持的网络存储卷类别中的所有存储插件以及local卷插件。除了存储卷插件之外，PersistentVolume资源规范Spec字段主要支持嵌套以下几个通用字段，它们用于定义PV的容量、访问模式和回收策略等属性。 capacity &lt;map[string]string&gt;：指定PV的容量；目前，Capacity仅支持存储容量设定，将来应该还可以指定IOPS和吞吐量（throughput）。 accessModes &lt;[]string&gt;：指定当前PV支持的访问模式；存储系统支持的存取能力大体可分为ReadWriteOnce（单路读写）、ReadOnlyMany（多路只读）和ReadWrite-Many（多路读写）3种类型，某个特定的存储系统可能会支持其中的部分或全部的能力。 persistentVolumeReclaimPolicy &lt;string&gt;：PV空间被释放时的处理机制；可用类型仅为Retain（默认）、Recycle或Delete。目前，仅NFS和hostPath支持Recycle策略，也仅有部分存储系统支持Delete策略。 volumeMode &lt;string&gt;：该PV的卷模型，用于指定此存储卷被格式化为文件系统使用还是直接使用裸格式的块设备；默认值为Filesystem，仅块设备接口的存储系统支持该功能。 storageClassName &lt;string&gt;：当前PV所属的StorageClass资源的名称，指定的存储类需要事先存在；默认为空值，即不属于任何存储类。 mountOptions &lt;string&gt;：挂载选项组成的列表，例如ro、soft和hard等。 nodeAffinity &lt;Object&gt;：节点亲和性，用于限制能够访问该PV的节点，进而会影响与该PV关联的PVC的Pod的调度结果。 PV的访问模式用于反映它关联的存储系统所支持的某个或全部存取能力，例如NFS存储系统支持以上3种存取能力，于是NFS PV可以仅支持ReadWriteOnce访问模式。需要注意的是，PV在某个特定时刻仅可基于一种模式进行存取，哪怕它同时支持多种模式。 NFS PV示例下面的配置示例来自于pv-nfs-demo.yaml资源清单，它定义了一个使用NFS存储系统的PV资源，它将空间大小限制为5GB，并支持多路的读写操作。 1234567891011121314151617apiVersion: v1kind: PersistentVolumemetadata: name: pv-nfs-demospec: capacity: storage: 5Gi volumeMode: Filesystem accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain mountOptions: - hard - nfsvers=4.1 nfs: path: &quot;/data/redis002&quot; server: nfs.ilinux.io 在NFS服务器nfs.ilinux.io上导出/data/redis002目录后，便可使用如下命令创建该PV资源。 12~$ kubectl apply -f pv-nfs-demo.yamlpersistentvolume/pv-nfs-demo created 若能够正确关联到指定的后端存储，该PV对象的状态将显示为Available，否则其状态为Pending，直至能够正确完成存储资源关联或者被删除。我们同样可使用describe命令来获取PV资源的详细描述信息。 1234567891011~$ kubectl describe pv/pv-nfs-demoName: pv-nfs-demo…… Status: Available…… Source: Type: NFS (an NFS mount that lasts the lifetime of a pod) Server: nfs.ilinux.io Path: /data/redis002 ReadOnly: falseEvents: &lt;none&gt; 描述信息中的Available表明该PV已经可以接受PVC的绑定请求，并在绑定完成后转变其状态至Bound。2. RBD PV示例下面是另一个PV资源的配置清单（pv-rbd-demo.yaml），它使用了RBD存储后端，空间大小等同于指定的RBD存储映像的大小（这里为2GB），并限定支持的访问模式为RWO，回收策略为Retain。除此之外，该PV资源还拥有一个名为usedof的资源标签，该标签可被PVC的标签选择器作为筛选PV资源的标准之一。 1234567891011121314151617181920212223apiVersion: v1kind: PersistentVolumemetadata: name: pv-rbd-demo labels: usedof: redisdataspec: capacity: storage: 2Gi accessModes: - ReadWriteOnce rbd: monitors: - ceph01.ilinux.io - ceph02.ilinux.io - ceph03.ilinux.io pool: kube image: pv-test user: kube keyring: /etc/ceph/ceph.client.kube.keyring fsType: xfs readOnly: false persistentVolumeReclaimPolicy: Retain 将RBD卷插件内嵌字段相关属性值设定为Ceph存储系统的实际的环境，包括监视器地址、存储池、存储映像、用户名和认证信息（keyring或secretRef）等。测试时，请事先部署好Ceph集群，参考5.4.2节中设定专用用户账号和Kubernetes集群工作节点的方式，准备好基础环境，并在Ceph集群的管理节点运行如下命令创建用到的存储映像： 12~$ rbd create pv-test --size 2G --pool kube~$ rbd feature disable -p kube pv-test object-map fast-diff deep-flatten 待所有准备工作就绪后，即可运行如下命令创建示例清单中定义的PV资源pv-rbd-demo： 12$ kubectl apply -f pv-rbd-demo.yaml persistentvolume/pv-rbd-demo created 我们同样可以使用describe命令了解pv-rbd-demo的详细描述，若处于Pending状态则需要详细检查存储卷插件的定义是否能吻合存储系统的真实环境。 PVC资源PersistentVolumeClaim也是Kubernetes系统上标准的API资源类型之一，它位于核心API群组，属于名称空间级别。用户提交新建的PVC资源最初处于Pending状态，由PV控制器找寻最佳匹配的PV并完成二者绑定后，两者都将转入Bound状态，随后Pod对象便可基于persistentVolumeClaim存储卷插件配置使用该PVC对应的持久存储卷。定义PVC时，用户可通过访问模式（accessModes）、数据源（dataSource）、存储资源空间需求和限制（resources）、存储类、标签选择器、卷模型和卷名称等匹配标准来筛选集群上的PV资源，其中，resources和accessModes是最重要的筛选标准。PVC的Spec字段的可嵌套字段有如下几个。 accessModes &lt;[]string&gt;：PVC的访问模式；它同样支持RWO、RWX和ROX这3种模式。 dataSrouces &lt;Object&gt;：用于从指定的数据源恢复该PVC卷，它目前支持的数据源包括一个现存的卷快照对象（snapshot.storage.k8s.io/VolumeSnapshot）、一个既有的PVC对象（PersistentVolumeClaim）或一个既有的用于数据转存的自定义资源对象（resource/object）。 resources &lt;Object&gt;：声明使用的存储空间的最小值和最大值；目前，PVC的资源限定仅支持空间大小一个维度。 selector &lt;Object&gt;：筛选PV时额外使用的标签选择器（matchLabels）或匹配条件表达式（matchExpressions）。 storageClassName &lt;string&gt;：该PVC资源隶属的存储类资源名称；指定了存储类资源的PVC仅能在同一个存储类下筛选PV资源，否则就只能从所有不具有存储类的PV中进行筛选。 volumeMode &lt;string&gt;：卷模型，用于指定此卷可被用作文件系统还是裸格式的块设备；默认值为Filesystem。 volumeName &lt;string&gt;：直接指定要绑定的PV资源的名称。下面通过匹配前一节中创建的PV资源的两个具体示例来说明PVC资源的配置方法，两个PV资源目前的状态如下所示，它仅截取了命令结果中的一部分。 1234~$kubectl get pvNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM pv-nfs-demo 5Gi RWX Retain Available pv-rbd-demo 2Gi RWO Retain Available NFS PVC示例下面的配置清单（pvc-demo-0001.yaml）定义了一个名为pvc-nfs-demo的PVC资源示例，它仅定义了期望的存储空间范围、访问模式和卷模式以筛选集群上的PV资源。 12345678910111213apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-demo-0001 namespace: defaultspec: accessModes: [&quot;ReadWriteMany&quot;] volumeMode: Filesystem resources: requests: storage: 3Gi limits: storage: 10Gi 显然，此前创建的两个PV资源中，pv-nfs-demo能够完全满足该PVC的筛选条件，因而创建示例清单中的资源后，它能够迅速绑定至PV之上，如下面的创建和资源查看命令结果所示。 12345~$ kubectl apply -f pvc-demo-0001.yamlpersistentvolumeclaim/pvc-demo-0001 created~$ kubectl get pvc pvc-nfs-0001NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-demo-0001 Bound pv-nfs-demo 5Gi RWX 3s 被PVC资源pvc-demo-0001绑定的PV资源pv-nfs-demo的状态也将从Available转为Bound，如下面的命令结果所示。 12~$ kubectl get pv/pv-nfs-demo -o jsonpath=&#123;.status.phase&#125;Bound 集群上的PV资源数量很多时，用户可通过指定多维度的过滤条件来缩小PV资源的筛选范围，以获取到最佳匹配。2. RBD PVC示例下面这个定义在pvc-demo-0002.yaml中的配置清单定义了一个PVC资源，除了期望的访问模式、卷模型和存储空间容量边界之外，它使用了标签选择器来匹配PV资源的标签。 12345678910111213141516apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-demo-0002 namespace: defaultspec: accessModes: [&quot;ReadWriteOnce&quot;] volumeMode: Filesystem resources: requests: storage: 2Gi limits: storage: 5Gi selector: matchLabels: usedof: &quot;redisdata&quot; 配置清单中的资源PVC/pvc-demo-0002特地为绑定此前创建的资源PV/pv-rbd-demo而创建，其筛选条件可由该PV完全满足，因而创建配置清单中的PVC/pvc-demo-0002资源后会即刻绑定于PV/pv-rbd-demo之上，如下面命令的结果所示。 12345~$ kubectl apply -f pvc-demo-0002.yamlpersistentvolumeclaim/pvc-demo-0002 created~$ kubectl get pvc/pvc-demo-0002NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGEpvc-demo-0002 Bound pv-rbd-demo 2Gi RWO 10s 删除一个PVC将导致其绑定的PV资源转入Released状态，并由相应的回收策略完成资源回收。反过来，直接删除一个仍由某PVC绑定的PV资源，会由PVC保护机制延迟该删除操作至相关的PVC资源被删除。 在Pod中使用PVC需要特别说明的是，PVC资源隶属名称空间级别，它仅可被同一名称空间中的Pod对象通过persistentVolumeClaim插件所引用并作为存储卷使用，该存储卷插件可嵌套使用如下两个字段。 claimName：要调用的PVC存储卷的名称，PVC卷要与Pod在同一名称空间中。 readOnly：是否强制将存储卷挂载为只读模式，默认为false。下面的配置清单（volumes-pvc-demo.yaml）定义了一个Pod资源，该配置清单将5.5.2节中直接使用RBD存储的Pod资源改为了调用指定的PVC存储卷。 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: volumes-pvc-demo namespace: defaultspec: containers: - name: redis image: redis:alpine imagePullPolicy: IfNotPresent ports: - containerPort: 6379 name: redisport volumeMounts: - mountPath: /data name: redis-rbd-vol volumes: - name: redis-rbd-vol persistentVolumeClaim: claimName: pvc-demo-0002 存储类存储类也是Kubernetes系统上的API资源类型之一，它位于storage.k8s.io群组中。存储类通常由集群管理员为管理PV资源之便而按需创建的存储资源类别（逻辑组），例如可将存储系统按照其性能高低或者综合服务质量级别分类（见图5-9）、依照备份策略分类，甚至直接按管理员自定义的标准分类等。存储类也是PVC筛选PV时的过滤条件之一，这意味着PVC仅能在其隶属的存储类之下找寻匹配的PV资源。不过，Kubernetes系统自身无法理解“类别”到底意味着什么，它仅仅把存储类中的信息当作PV资源的特性描述使用。 存储类的最重要功能之一便是对PV资源动态预配机制的支持，它可被视作动态PV资源的创建模板，能够让集群管理员从维护PVC和PV资源之间的耦合关系的束缚中解脱出来。需要用到具有持久化功能的存储卷资源时，用户只需要向满足其存储特性要求的存储类声明一个PVC资源，存储类将会根据该声明创建恰好匹配其需求的PV对象。 StorageClass资源StorageClass资源的期望状态直接与apiVersion、kind和metadata定义在同一级别而无须嵌套在spec字段中，它支持使用的字段包括如下几个。 allowVolumeExpansion &lt;boolean&gt;：是否支持存储卷空间扩展功能。 allowedTopologies &lt;[]Object&gt;：定义可以动态配置存储卷的节点拓扑，仅启用了卷调度功能的服务器才会用到该字段；每个卷插件都有自己支持的拓扑规范，空的拓扑选择器表示无拓扑限制。 provisioner &lt;string&gt;：必选字段，用于指定存储服务方（provisioner，或称为预配器），存储类要基于该字段值来判定要使用的存储插件，以便适配到目标存储系统；Kubernetes内置支持许多的provisioner，它们的名字都以kubernetes.io/为前缀，例如kubernetes.io/glusterfs等。 parameters &lt;map[string]string&gt;：定义连接至指定的provisioner类别下的某特定存储时需要使用的各相关参数；不同provisioner的可用参数各不相同。 reclaimPolicy &lt;string&gt;：由当前存储类动态创建的PV资源的默认回收策略，可用值为Delete（默认）和Retain两个；但那些静态PV的回收策略则取决于它们自身的定义。 volumeBindingMode &lt;string&gt;：定义如何为PVC完成预配和绑定，默认值为Volume-BindingImmediate；该字段仅在启用了存储卷调度功能时才能生效。 mountOptions &lt;[]string&gt;：由当前类动态创建的PV资源的默认挂载选项列表。 下面是一个定义在storageclass-rbd-demo.yaml配置文件中的StorageClass资源清单，它定义了一个以Ceph存储系统的RBD接口为后端存储的StorageClass资源fast-rbd，因此，其存储预配标识为kubernetes.io/rbd。 123456789101112131415161718apiVersion: storage.k8s.io/v1kind: StorageClassmetadata: name: fast-rbdprovisioner: kubernetes.io/rbdparameters: monitors: ceph01.ilinux.io:6789,ceph02.ilinux.io:6789 adminId: admin adminSecretName: ceph-admin-secret adminSecretNamespace: kube-system pool: kube userId: kube userSecretName: ceph-kube-secret userSecretNamespace: kube-system fsType: ext4 imageFormat: &quot;2&quot; imageFeatures: &quot;layering&quot;reclaimPolicy: Retain 不同的provisioner的parameters字段中可嵌套使用的字段各有不同，上面示例中Ceph RBD存储服务可使用的各字段及意义如下。 monitors &lt;string&gt;：Ceph存储系统的监视器访问接口，多个套接字间以逗号分隔。 adminId &lt;string&gt;：有权限在指定的存储池中创建image的管理员用户名，默认为admin。 adminSecretName &lt;string&gt;：存储有管理员账号认证密钥的Secret资源名称。 adminSecretNamespace &lt;string&gt;：管理员账号相关的Secret资源所在的名称空间。 pool &lt;string&gt;：Ceph存储系统的RBD存储池名称，默认为rbd。 userId &lt;string&gt;：用于映射RBD镜像的Ceph用户账号，默认同adminId字段。 userSecretName &lt;string&gt;：存储有用户账号认证密钥的Secret资源名称。 userSecretNamespace &lt;string&gt;：用户账号相关的Secret资源所在的名称空间。 fsType &lt;string&gt;：存储映像格式化的文件系统类型，默认为ext4。 imageFormat &lt;string&gt;：存储映像的格式，其可用值仅有“1”和“2”，默认值为“2”。 imageFeatures &lt;string&gt;：“2”格式的存储映像支持的特性，目前仅支持layering，默认为空值，并且不支持任何功能。 存储类接入其他存储系统时使用的参数请参考https://kubernetes.io/docs/concepts/storage/storage-classes/。与Pod或PV资源上的RBD卷插件配置格式不同的是，StorageClass上的RBD供给者参数不支持使用keyring直接认证到Ceph，它仅能引用Secret资源中存储的认证密钥完成认证操作。因而，我们需要先将Ceph用户admin和kube的认证密钥分别创建为Secret资源对象。 1）在Ceph管理节点上分别获取admin和kube的认证密钥，不同Ceph集群上的输出结果应该有所不同： 1234~$ ceph auth get-key client.admin AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==~$ ceph auth get-key client.kubeAQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA= 2）在Kubernetes集群管理客户端上使用kubectl命令分别将二者创建为Secret资源，在具体测试操作中，需要将其中的密钥分别替换为前一步中的命令输出结果： 123456~$ kubectl create secret generic ceph-admin-secret --type=&quot;kubernetes.io/rbd&quot; \\ --from-literal=key=&#x27;AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==&#x27; \\ --namespace=kube-system~$ kubectl create secret generic ceph-kube-secret --type=&quot;kubernetes.io/rbd&quot; \\ --from-literal=key=&#x27;AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA==&#x27; \\ --namespace=kube-system 示例中使用的账号及存储池的管理方式请参考5.4节和5.5节给出的步骤。待相关Secret资源准备完成后，将示例清单中的StorageClass资源创建在集群上，即可由PVC或PV资源将其作为存储类 12~$ kubectl apply -f storageclass-rbd-demo.yaml storageclass.storage.k8s.io/fast-rbd created 我们还可以使用kubectl get sc/NAME命令打印存储类的相关信息，或者使用kubectl describe sc NAME命令获取详细描述来进一步了解其状态。2. PV动态预配动态PV预配功能的使用有两个前提条件：支持动态PV创建功能的卷插件，以及一个使用了对应于该存储卷插件的后端存储系统的StorageClass资源。不过，Kubernetes并非内置支持所有的存储卷插件的PV动态预配功能，具体信息如图5-10所示。 RBD存储卷插件，结合5.5.4节中定义关联至Ceph RBD存储系统接口的存储类资源fast-rbd就能实现PV的动态预配功能，用户于该存储类中创建PVC资源后，运行于kube-controller-manager守护进程中的PV控制器会根据fast-rbd存储类的定义接入Ceph存储系统创建出相应的存储映像，并在自动创建一个关联至该存储映像的PV资源后，将其绑定至PVC资源。动态PV预配的过程中，PVC控制器会调用相关存储系统的管理接口API或专用的客户端工具来完成后端存储系统上的存储组件管理。以Ceph RBD为例，PV控制器会以存储类参数adminId中指定的用户身份调用rbd命令创建存储映像。然而，以kubeadm部署且运行为静态Pod资源的kube-controller-manager容器并未自行附带此类工具，如ceph-common程序包。常见的解决方案有3种：在Kubernetes系统上部署kubernetes-incubator/external-storage中的rbd-provisioner，从而以外置的方式提供相关工具程序，或基于CSI卷插件使用ceph-csi项目来支持更加丰富的卷功能，或定制kube-controller-manager的容器镜像，为其安装ceph-common程序包。本节将给出第三种方式的实现过程。提示若以二进制程序包部署Kubernetes集群，则直接在Master节点安装ceph-common就能解决问题。首先，我们使用如下的Dockerfile文件，并基于现有kube-controller-manager镜像文件为其额外安装ceph-common程序包，随后重新打包为容器镜像。 123456789101112131415ARG KUBE_VERSION=&quot;v1.19.0&quot;FROM registry.aliyuncs.com/google_containers/kube-controller-manager:$&#123;KUBE_VERSION&#125;RUN apt update &amp;&amp; apt install -y wget gnupg lsb-releaseARG CEPH_VERSION=&quot;octopus&quot;RUN wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add - &amp;&amp; \\ echo deb https://mirrors.aliyun.com/ceph/debian-$&#123;CEPH_VERSION&#125;/ $(lsb_ release -sc) main &gt; /etc/apt/sources.list.d/ceph.list &amp;&amp; \\ apt update &amp;&amp; \\ apt install -y ceph-common ceph-fuseRUN rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/* 将上面的内容保存于某专用目录下（例如kube-controller-manager）的名为Dockerfile的文件中，而后使用如下命令将其打包为镜像即可。其中，构建时参数KUBE_VERSION和CEPH_VERSION可分别修改为适用的版本。 1234~$ cd kube-controller-manager~$ docker image build . --build-args KUBE_VERSION= &quot;v1.19.0&quot; \\ --build-args CEPH_VERSION=“octopus”\\ -t ikubernetes/kube-controller-manager:v1.19.0 而后，将该镜像分发至各Master节点，并分别修改它们的/etc/kubernetes/manifests/kube-controller-manager.yaml配置清单中的容器镜像为定制的镜像ikubernetes/kube-controller-manager:v1.19.0，待Controller Manager相关的Pod自动重启后即可进行动态PV的创建测试。下面是定义于pvc-dyn-rbd-demo.yaml配置清单中的PVC资源，它向存储类fast-rbd声明了需要的存储空间及访问模式。 1234567891011121314apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-dyn-rbd-demo namespace: defaultspec: accessModes: [&quot;ReadWriteOnce&quot;] volumeMode: Filesystem resources: requests: storage: 3Gi limits: storage: 10Gi storageClassName: fast-rbd 将示例清单中的PVC资源创建至Kubernetes集群之上，便会触发PV控制器在指定的存储类中自动创建匹配的PV资源。 12~$ kubectl apply -f pvc-dyn-rbd-demo.yamlpersistentvolumeclaim/pvc-dyn-rbd-demo created 下面的命令显示出该PVC资源已经绑定到了一个名为pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce的PV之上。 12~$ kubectl get pvc/pvc-dyn-rbd-demo -o jsonpath=&#123;.spec.volumeName&#125;pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce 如下命令输出的该PV的详细描述之中，Annotations中的kubernetes.io/createdby: rbd-dynamic-provisioner表示它是由rbd-dynamic-provisioner动态创建，而Source段中的信息更能印证这种结论。 1234567891011121314151617181920212223242526272829~$ kubectl describe pv pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ceName: pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ceLabels: &lt;none&gt;Annotations: kubernetes.io/createdby: rbd-dynamic-provisioner pv.kubernetes.io/bound-by-controller: yes pv.kubernetes.io/provisioned-by: kubernetes.io/rbdFinalizers: [kubernetes.io/pv-protection]StorageClass: fast-rbdStatus: BoundClaim: default/pvc-sc-rbd-demoReclaim Policy: Delete # 回收策略Access Modes: RWO # 访问模式VolumeMode: Filesystem # 卷模式Capacity: 3Gi # 卷空间容量Node Affinity: &lt;none&gt;Message: Source: # 数据源标识 Type: RBD (a Rados Block Device mount on the host that shares a pod&#x27;s lifetime) CephMonitors: [ceph01.ilinux.io:6789 ceph02.ilinux.io:6789 ceph03.ilinux. io:6789] RBDImage: kubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e FSType: ext4 RBDPool: kube RadosUser: kube Keyring: /etc/ceph/keyring SecretRef: &amp;SecretReference&#123;Name:ceph-kube-secret,Namespace:kube-system,&#125; ReadOnly: falseEvents: &lt;none&gt; 上面命令结果中显示出，该PV的容量、访问模式和卷模式均符合PVC所声明的要求，并且能够通过下面的命令验证相关的存储映像已经存在于Ceph存储集群之上： 12~$ rbd ls -p kubekubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e 另外，该PV继承自存储类fast-rbd中的回收策略为Delete，这也是存储类默认使用的回收策略，因此，删除其绑定的PVC对象也将删除该PV对象。对于多数持久存储场景而言，这可能是存在着一定风险的策略，建议定义存储类时手动修改该策略。感兴趣的读者可自行测试这种级联删除的效果。 容器存储接口CSI存储卷管理器通过调用存储卷插件实现当前节点上存储卷相关的附加、分离、挂载/卸载等操作，对于未被Kubernetes内置（In-Tree）的卷插件所支持的存储系统或服务来说，扩展定义新的卷插件是解决问题的唯一途径。但将存储供应商提供的第三方存储代码打包到Kubernetes的核心代码可能会导致可靠性及安全性方面的问题，因而这就需要一种简单、便捷的、外置于Kubernetes代码树（Out-Of-Tree）的扩展方式，FlexVolume和CSI（容器存储接口）就是这样的存储卷插件，换句话说，它们自身是内置的存储卷插件，但实现的却是第三方存储卷的扩展接口。 CSI基础FlexVolume是Kubernetes自v1.8版本进入GA（高可用）阶段的一种存储插件扩展方式，它要求将外部插件的二进制文件部署在预先配置的路径中（例如/usr/libexec/kubernetes/kubelet-plugins/volume/exec/），并设定系统环境满足其正常运行所需要的全部依赖关系。事实上，一个FlexVolume类型的插件就是一款可被kubelet驱动的可执行文件，它实现了特定存储的挂载、卸载等存储插件接口，而对该类插件的调用相当于请求运行该程序文件，并要求返回JSON格式的响应内容。第三方需要提供的CSI组件主要是两个CSI存储卷驱动程序，一个是节点插件（Identity+Node），用于同kubelet交互实现存储卷的挂载和卸载等功能，另一个是自定义控制器（Identity+Controller），负责处理来自API Server的存储卷管理请求，例如创建和删除等，它的功能类似于控制器管理器中的PV控制器，如图5-11中实线的圆角方框所示。 kubelet对存储卷的挂载和卸载操作将通过UNIX Socket调用在同一主机上运行的外部CSI卷驱动程序完成。初始化外部CSI卷驱动程序时，kubelet必须调用CSI方法NodeGetInfo才能将Kubernetes的节点名称映射为CSI的节点标识（NodeID）。于是，为了降低部署外部容器化的CSI卷驱动程序时的复杂度，Kubernetes团队提供了一个以Sidecar容器运行的应用——Kubernetes CSI Helper，以辅助自动完成UNIX Sock套接字注册及NodeID的初始化，如图5-11中的node-driver-registrar容器所示。不受Kubernetes信任的第三方卷驱动程序运行为独立的容器，它无法直接同控制器管理器通信，而是要借助于Kubernetes API Server进行；换句话说，CSI存储卷驱动需要注册监视（watch）API Server上的特定资源并针对存储卷管理器面向其存储卷的请求执行预配、删除、附加和分离等操作。同样为了降低外部容器化CSI卷驱动及控制器程序部署的复杂度，Kubernetes团队提供了一到多个以Sidecar容器运行的代理应用Kubernetes to CSI来负责监视Kubernetes API，并触发针对CSI卷驱动程序容器的相应操作，如图5-11中的external-attacher和external-privisioner等，它们各自的简要功能如下所示。 external-privisioner：CSI存储卷的创建和删除。 external-attacher：CSI存储卷的附加和分离。 external-resizer：CSI存储卷的容量调整（扩缩容）。 external-snapshotter：CSI存储卷的快照管理（创建和删除等）。 尽管Kubernetes并未指定CSI卷驱动程序的打包标准，但它提供了以下建议，以简化容器化CSI卷驱动程序的部署。 1）创建一个独立CSI卷驱动容器镜像，由其实现存储卷插件的标准行为，并在运行时通过UNIX Socket公开其API。2）将控制器级别的各辅助容器（external-privisioner和external-attacher等）以Sidecar的形式同带有自定义控制器功能的CSI卷驱动程序容器运行在同一个Pod中，而后借助StatefulSet或Deployment控制器资源确保各辅助容器可正常运行相应数目的实例副本，将负责各容器间通信的UNIX Socket存储到共享的emptyDir存储卷上。3）将节点上需要的辅助容器node-driver-registrar以Sidecar的形式与运行CSI卷驱动程序的容器运行在同一Pod中，而后借助DaemonSet控制器资源确保辅助容器可在每个节点上运行一个实例。下一节将以Longhorn存储系统为例简单说明CSI卷插件解决方案的部署及简单使用方式。 Longhorn存储系统 Longhorn是由Rancher实验室创建的一款云原生的、轻量级、可靠且易用的开源分布式块存储系统，后来由CNCF孵化。它借助CSI存储卷插件以外置的存储解决方案形式运行。Longhorn遵循微服务的原则，利用容器将小型独立组件构建为分布式块存储，并使用编排工具来协调这些组件，从而形成弹性分布式系统。部署到Kubernetes集群上之后，Longhorn会自动将集群中所有节点上可用的本地存储（默认为/var/lib/longhorn/目录所在的设备）聚集为存储集群，而后利用这些存储管理分布式、带有复制功能的块存储，且支持快照及数据备份操作。 面向现代云环境设计的存储系统的控制器随着待编排存储卷数量的急速增加也变得高度复杂。为了摆脱这种困境，Longhorn充分利用了近年来关于如何编排大量容器的关键技术，采用微服务的设计模式，将大型复杂的存储控制器切分为每个存储卷一个专用的、小型存储控制器，而后借助现代编排工具来管理这些控制器，从而将每个CSI卷构建为一个独立的微服务。如图5-12所示的存储架构中，3个Pod分别使用了一个Longhorn存储卷，每个卷有一个专用的控制器（Engine）资源和两个副本（Replica）资源，它们都是为了便于描述其应用而由Longhorn引入的自定义资源类型。 Engine容器仅负责单个存储卷的管理，其生命周期与存储卷相同，因而它并非真正的CSI插件级别的卷控制器或节点插件。Longhorn上负责处理来自Kubernetes CSI卷插件的API调用，以及完成存储卷管理的组件是Longhorn Manager（node-driver-registrar），它是一个容器化应用且受DaemonSet控制器资源编排，在Kubernetes集群的每个节点上运行一个副本。Longhorn Manager持续监视Kubernetes API上与Longhorn存储卷相关的资源变动，一旦发现新的资源创建，它负责在该卷附加的节点（即Pod被Kubernetes调度器绑定的目标节点）上创建一个Engine资源对象，并在副本相关的每个目标节点上相应创建一个Replica资源对象。Kubernetes集群内部通过CSI插件接口调用Longhorn插件以管理相关类型的存储卷，而Longhorn存储插件则基于Longhorn API与Longhorn Manager进行通信，卷管理之外的其他功能则要依赖Longhorn UI完成，例如快照、备份、节点和磁盘的管理等。另外，Longhorn的块设备存储卷的实现建立在iSCSI协议之上，因而需要调用Longhorn存储卷的Pod所在节点必须部署了相关的程序包，例如open-iscsi或iscsiadm等。目前版本（v1.0.1）的Longhorn要求运行于v.1.13或更高版本的Docker环境下，以及v.1.4或更高版本的Kubernetes之上，并且要求各节点部署了open-iscsi、curl、findmnt、grep、awk、blkid和lsblk等程序包。基础环境准备完成后，我们使用类似如下的命令即能完成Longhorn应用的部署。 12~$ kubectl apply -f \\ https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml 该部署清单会在默认的longhorn-system名称空间下部署csi-attacher、csi-provisioner、csi-resizer、engine-image-ei、longhorn-csi-plugin和longhorn-manager等应用相关的Pod对象，待这些Pod对象成功转为Running状态之后即可测试使用Longhorn CSI插件。该部署清单还会默认创建如下面资源清单中定义的名为longhorn的StorageClass资源，它以部署好的Longhorn为后端存储系统，支持存储卷动态预配机制。我们也能够以类似的方式定义基于该存储系统的、使用了不同配置的其他StorageClass资源，例如仅有一个副本以用于测试场景或对数据可靠性要求并非特别高的应用等。 12345678910kind: StorageClass # 资源类型apiVersion: storage.k8s.io/v1 # API群组及版本metadata: name: longhornprovisioner: driver.longhorn.io # 存储供给驱动allowVolumeExpansion: true # 是否支持存储卷弹性扩缩容parameters: numberOfReplicas: &quot;3&quot; # 副本数量 staleReplicaTimeout: &quot;2880&quot; # 过期副本超时时长 fromBackup: &quot;&quot; 随后，我们随时可以按需创建基于该存储类的PVC资源来使用Longhorn存储系统上的持久存储卷提供的存储空间。下面的示例资源清单（pvc-dyn-longhorn-demo.yaml）便定义了一个基于Longhorn存储类的PVC，它请求使用2GB的空间。 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc-dyn-longhorn-demo namespace: defaultspec: accessModes: [&quot;ReadWriteOnce&quot;] volumeMode: Filesystem resources: requests: storage: 2Gi storageClassName: longhorn 如前所述，Longhorn存储设备支持动态预配，于是以默认创建的存储类Longhorn为模板的PVC在无满足其请求条件的PV时，可由控制器自动创建出适配的PV卷来。下面两条命令及结果也反映了这种预配机制。 12345~$ kubectl apply -f pvc-dyn-longhorn-demo.yaml persistentvolumeclaim/pvc-dyn-longhorn-demo created~$ kubectl get pvc/pvc-dyn-longhorn-demoNAME STATUS VOLUME CAPACITY…pvc-dyn-longhorn-demo Bound pvc-c67415ae-560b-49c7-8515-3467f4160794 2Gi… 对于每个存储卷，Longhorn存储系统都会使用自定义的Volumes类型资源对象维持及跟踪其运行状态，每个Volumes资源都会有一个Engines资源对象作为其存储控制器，如下面的两个命令及结果所示。 123456~$ kubectl get volumes -n longhorn-systemNAME AGEpvc-c67415ae-560b-49c7-8515-3467f4160794 90s~$ kubectl get engines -n longhorn-systemNAME AGEpvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 2m10s Engines资源对象的详细描述或资源规范中的spec和status字段记录有当前资源的详细信息，包括关联的副本、purge状态、恢复状态和快照信息等，为了节约篇幅，下面的命令仅给出了部分运行结果。 1234567891011121314151617~$ kubectl describe engines pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \\ -n longhorn-system……Spec: Backup Volume: Desire State: stopped Disable Frontend: false Engine Image: longhornio/longhorn-engine:v1.0.1 Frontend: blockdev Log Requested: false Node ID: # 绑定的节点，它必须与调用了该存储卷的Pod运行于同一节点 Replica Address Map: # 关联的存储卷副本 pvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3: 10.244.3.58:10000 pvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050: 10.244.2.53:10000 pvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db: 10.244.1.61:10000 Volume Name: pvc-c67415ae-560b-49c7-8515-3467f4160794 Volume Size: 2147483648 Replicas也是Longhorn提供的一个独立资源类型，每个资源对象对应着一个存储卷副本，如下面的命令结果所示。 12345~$ kubectl get replicas -n longhorn-systemNAME AGEpvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3 2m36spvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050 2m36spvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db 2m36s 基于Longhorn存储卷的PVC被Pod引用后，Pod所在的节点便是该存储卷Engine对象运行所在的节点，Engine的状态也才会由Stopped转为Running。示例清单volumes-pvc-longhorn-demo.yaml定义了一个调用pvc/pvc-dyn-longhorn-demo资源的Pod资源，因而该Pod所在的节点便是该PVC后端PV相关的Engine绑定的节点，如下面3个命令及其结果所示。 1234567~$ kubectl apply -f volumes-pvc-longhorn-demo.yaml pod/volumes-pvc-longhorn-demo created~$ kubectl get pods/volumes-pvc-longhorn-demo -o jsonpath=&#x27;&#123;.spec.nodeName&#125;&#x27;k8s-node03.ilinux.io~$ kubectl get engines/pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \\ -n longhorn-system -o jsonpath=&#x27;&#123;.spec.nodeID&#125;&#x27;k8s-node03.ilinux.io 由以上Longhorn存储系统的部署及测试结果可知，该存储系统不依赖于任何外部存储设备，仅基于Kubernetes集群工作节点本地的存储即能正常提供存储卷服务，且支持动态预配等功能。但应用于生产环境时，还是有许多步骤需要优化，例如将数据存储与操作系统等分离到不同的磁盘设备，是否可以考虑关闭底层的RAID设备等，具体请参考Longhorn文档中的最佳实践。为了便于通过Kubernetes集群外部的浏览器访问该用户接口，我们需要把相关的Service对象的类型修改为NodePort。 1234~$ kubectl patch svc/longhorn-frontend -p &#x27;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&#x27; -n longhorn-systemservice/longhorn-frontend patched~$ kubectl get svc/longhorn-frontend -n longhorn-system -o jsonpath=&#x27;&#123;.spec.ports[0].nodePort&#125;&#x27;30180 随后，我们经由任意一个节点的IP地址节点端口（例如上面命令中自动分配而来的30180）即可访问该UI，如图5-13所示。节点、存储卷、备份和系统设置导航标签各自给出了相关功能的配置入口，感兴趣的读者可自行探索其使用细节。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"}]},{"title":"Service和服务发现","slug":"Service和服务发现","date":"2022-02-09T11:48:21.000Z","updated":"2022-02-09T11:49:31.197Z","comments":true,"path":"2022/02/09/Service和服务发现/","link":"","permalink":"https://marmotad.github.io/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/","excerpt":"","text":"标签与标签选择器它是附加在Kubernetes任何资源对象之上的键值型数据，常用于标签选择器的匹配度检查，从而完成资源筛选。Kubernetes系统的部分基础功能的实现也要依赖标签和标签选择器，例如Service筛选并关联后端Pod对象，由ReplicaSet、StatefulSet和DaemonSet等控制器过滤并关联后端Pod对象等，从而提升用户的资源管理效率。 资源标签标签可在资源创建时直接指定，也可随时按需添加在活动对象上。一个对象可拥有不止一个标签，而同一个标签也可添加至多个对象之上。下面是较为常用的标签。 版本标签：”release” : “stable”，”release” : “canary”，”release” : “beta”。 环境标签：”environment” : “dev”，”environment” : “qa”，”environment” : “prod”。 应用标签：”app” : “ui”，”app” : “as”，”app” : “pc”，”app” : “sc”。 架构层级标签：”tier” : “frontend”，”tier” : “backend”，”tier” : “cache”。 分区标签：”partition” : “customerA”，”partition” : “customerB”。 品控级别标签：”track” : “daily”，”track” : “weekly”。 标签中的键名称通常由“键前缀”和“键名”组成，其格式形如KEY_PREFIX/KEY_NAME，键前缀为可选部分。键名至多能使用63个字符，支持字母、数字、连接号（-）、下划线（）、点号（.）等字符，且只能以字母或数字开头。而键前缀必须为DNS子域名格式，且不能超过253个字符。省略键前缀时，键将被视为用户的私有数据。由Kubernetes系统组件或第三方组件自动为用户资源添加的键必须使用键前缀，kubernetes.io/和k8s.io/前缀预留给了Kubernetes的核心组件使用，例如Node对象上常用的kubernetes.io/os、kubernetes.io/arch和kubernetes.io/hostname等。标签的键值必须不能多于63个字符，键值要么为空，要么以字母或数字开头及结尾，且中间只能使用字母、数字、连接号（-）、下划线（）或点号（.）等字符。 创建资源时定义标签创建资源时，可直接在其metadata中嵌套使用labels字段定义要附加的标签项。例如在下面的Namespace资源配置清单文件中，示例ns-with-labels.yaml中使用了两个标签，env=dev和app=eshop。 12345678910apiVersion: v1kind: Namespacemetadata: name: eshop labels: app: eshop env: devspec: finalizers: - kubernetes 可在kubectl get namespaces命令中使用–show-labels选项，以额外显示对象的标签信息。 12345~$ kubectl apply -f ns-with-labels.yaml namespace/eshop created~$ kubectl get namespaces eshop --show-labelsNAME STATUS AGE LABELSdemoapp Active 11s app=eshop,env=dev kubectl get命令上使用-L key1,key2,…选项可指定有特定键的标签信息。例如，仅显示eshop名称空间上的env和app标签： 123~$ kubectl get namespaces eshop -L env,appNAME STATUS AGE ENV APPeshop Active 89s dev eshop kubectl label命令可直接管理活动对象的标签，以按需进行添加或修改等操作。例如为eshop名称空间添加release=beta标签： 12~$ kubectl label namespaces/eshop release=betanamespace/eshop labeled 已经附带了指定键名的标签，使用kubectl label为其设定新的键值时需同时使用–overwrite命令，强制覆盖原有键值。例如，将eshop名称空间的release标签值修改为canary： 12~$ kubectl label namespaces/eshop release=canary --overwritenamespace/eshop labeled 删除活动对象上的标签时同样要使用kubectl label命令，但仅需要指定标签名称并紧跟一个减号“–”，例如，下面的命令首先删除eshop名称空间中的env标签，而后显示其现有的所有标签： 12345~$ kubectl label namespaces/eshop env-namespace/eshop labeled~$ kubectl get namespaces eshop --show-labels NAME STATUS AGE LABELSeshop Active 6m46s app=eshop,release=beta 标签选择器标签选择器用于表达标签的查询条件或选择标准，目前Kubernetes API支持两个选择器：基于等值关系（equality-based）的标签选项器与基于集合关系（set-based）的标签选择器。在指定多个选择器时需要以逗号分隔，各选择器之间遵循逻辑“与”，即必须要满足所有条件，而且空值的选择器将不选择任何对象。 基于等值关系的标签选择器 可用操作符有=、==和!=，其中前两个意义相同，都表示“等值”关系，最后一个表示“不等”。例如env=dev和env!=prod都是基于等值关系的选择器。 基于集合的标签选择器 根据标签名的一组值进行筛选，它支持in、notin和exists这3种操作符，例如tier in (frontend,backend)表示所有包含tier标签且其值为frontend或backend的资源对象。 kubectl get命令的“-l”选项能够指定使用标签选择器筛选目标资源，例如，如下命令显示标签release的值不等于beta，且标签app的值等于eshop的所有名称空间： 123~$ kubectl get namespaces -l &#x27;release!=beta,app=eshop&#x27; -L app,releaseNAME STATUS AGE APP RELEASEeshop Active 60m eshop canary 基于集合关系的标签选择器用于基于一组值进行过滤，它支持in、notin和exists 3种操作符，各操作符的使用格式及意义如下。 KEY in (VALUE1,VALUE2,…)：指定键名的值存在于给定的列表中即满足条件。 KEY notin (VALUE1,VALUE2,…)：指定键名的值不存在于给定列表中即满足条件。 KEY：所有存在此键名标签的资源。 !KEY：所有不存在此键名标签的资源。 例如，下面的命令可以过滤出标签键名release的值为beta或canary的所有Namespace对象： 123~$ kubectl get namespaces -l &#x27;release in (beta,canary)&#x27; -L releaseNAME STATUS AGE RELEASEeshop Active 63m canary 再如，下面的命令可以列出集群中拥有node-role.kubernetes.io标签的各Node对象： 12345~$ kubectl get nodes -l &#x27;node-role.kubernetes.io/master&#x27; -L kubernetes.io/hostnameNAME STATUS ROLES AGE VERSION HOSTNAMEk8s-master01.ilinux.io Ready master 25d v1.17.3 k8s-master01.ilinux.iok8s-master02.ilinux.io Ready master 25d v1.17.3 k8s-master02.ilinux.iok8s-master03.ilinux.io Ready master 25d v1.17.3 k8s-master03.ilinux.io 此外，Kubernetes的诸多资源对象必须以标签选择器的方式关联到Pod资源对象，例如Service资源在spec字段中嵌套使用selector字段定义标签选择器，而Deployment与StatefulSet等资源在selector字段中通过matchLabels和matchExpressions构造复杂的标签选择机制。 matchLabels：直接给定键值对指定标签选择器。 matchExpressions：基于表达式指定的标签选择器列表，每个选择器形如{key: KEY_NAME, operator: OPERATOR, values: [VALUE1,VALUE2,…]}，选择器列表间为“逻辑与”关系；使用In或NotIn操作符时，其values必须为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空。 下面的资源清单片段是一个示例，它同时定义了两类标签选择器。 123456selector: matchLabels: component: redis matchExpressions: - &#123;key: tier, operator: In, values: [cache]&#125; - &#123;key: environment, operator: Exists, values:&#125; Service与服务发现Service对象的IP地址都仅在Kubernetes集群内可达，它们无法接入集群外部的访问流量。在解决此类问题时，除了可以在单一节点上做端口（hostPort）暴露及让Pod资源共享使用工作节点的网络名称空间（hostNetwork）之外，更推荐用户使用NodePort或LoadBalancer类型的Service资源，或者是有七层负载均衡能力的Ingress资源。 Service资源及其实现模型Service是Kubernetes的核心资源类型之一。它事实上是一种抽象：通过规则定义出由多个Pod对象组合而成的逻辑集合，以及访问这组Pod的策略。Service关联Pod资源的规则要借助标签选择器完成。 Service资源概述Service资源基于标签选择器把筛选出的一组Pod对象定义成一个逻辑组合，并通过自己的IP地址和端口将请求分发给该组内的Pod对象。 Service对象的IP地址（可称为ClusterIP或ServiceIP）是虚拟IP地址，由Kubernetes系统在Service对象创建时在专用网络（Service Network）地址中自动分配或由用户手动指定，并且在Service对象的生命周期中保持不变。Service基于端口过滤到达其IP地址的客户端请求，并根据定义将请求转发至其后端的Pod对象的相应端口之上，因此这种代理机制也称为“端口代理”或四层代理，工作于TCP/IP协议栈的传输层。Service对象会通过API Server持续监视（watch）标签选择器匹配到的后端Pod对象，并实时跟踪这些Pod对象的变动情况，例如IP地址变动以及Pod对象的增加或删除等。不过，Service并不直接连接至Pod对象，它们之间还有一个中间层——Endpoints资源对象，该资源对象是一个由IP地址和端口组成的列表，这些IP地址和端口则来自由Service的标签选择器匹配到的Pod对象。这也是很多场景中会使用“Service的后端端点”这一术语的原因。默认情况下，创建Service资源对象时，其关联的Endpoints对象会被自动创建。 kube-proxy代理模型每个工作节点的kube-proxy组件通过API Server持续监控着各Service及其关联的Pod对象，并将Service对象的创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。客户端、Service及Pod对象的关系如图7-3所示。 Service对象的ClusterIP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现Kubernetes集群网络内部通信，且仅能够以规则中定义的转发服务的请求作为目标地址予以响应，这也是它之所以被称作虚拟IP的原因之一。kube-proxy把请求代理至相应端点的方式有3种：userspace、iptables和ipvs。 userspace代理模型此处的userspace是指Linux操作系统的用户空间。在这种模型中，kube-proxy负责跟踪API Server上Service和Endpoints对象的变动（创建或移除），并据此调整Service资源的定义。对于每个Service对象，它会随机打开一个本地端口（运行于用户空间的kube-proxy进程负责监听），任何到达此代理端口的连接请求都将被代理至当前Service资源后端的各Pod对象，至于哪个Pod对象会被选中则取决于当前Service资源的调度方式，默认调度算法是轮询（round-robin）。userspace代理模型工作逻辑如图7-4所示。另外，此类Service对象还会创建iptables规则以捕获任何到达ClusterIP和端口的流量。在Kubernetes 1.1版本之前，userspace是默认的代理模型。 在这种代理模型中，请求流量到达内核空间后经由套接字送往用户空间中的kube-proxy进程，而后由该进程送回内核空间，发往调度分配的目标后端Pod对象。因请求报文在内核空间和用户空间来回转发，所以必然导致模型效率不高。 iptables代理模型创建Service对象的操作会触发集群中的每个kube-proxy并将其转换为定义在所属节点上的iptables规则，用于转发工作接口接收到的、与此Service资源ClusterIP和端口相关的流量。客户端发来请求将直接由相关的iptables规则进行目标地址转换（DNAT）后根据算法调度并转发至集群内的Pod对象之上，而无须再经由kube-proxy进程进行处理，因而称为iptables代理模型，如图7-5所示。对于每个Endpoints对象，Service资源会为其创建iptables规则并指向其iptables地址和端口，而流量转发到多个Endpoint对象之上的默认调度机制是随机算法。iptables代理模型由Kubernetes v1.1版本引入，并于v1.2版本成为默认的类型。 在iptables代理模型中，Service的服务发现和负载均衡功能都使用iptables规则实现，而无须将流量在用户空间和内核空间来回切换，因此更为高效和可靠，但是性能一般，而且受规模影响较大，仅适用于少量Service规模的集群。 ipvs代理模型Kubernetes自v1.9版本起引入ipvs代理模型，且自v1.11版本起成为默认设置。在此种模型中，kube-proxy跟踪API Server上Service和Endpoints对象的变动，并据此来调用netlink接口创建或变更ipvs（NAT）规则，如图7-6所示。它与iptables规则的不同之处仅在于客户端请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。 ipvs代理模型中Service的服务发现和负载均衡功能均基于内核中的ipvs规则实现。类似于iptables，ipvs也构建于内核中的netfilter之上，但它使用hash表作为底层数据结构且工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性，适用于存在大量Service资源且对性能要求较高的场景。ipvs代理模型支持rr、lc、dh、sh、sed和nq等多种调度算法。 Service资源类型无论哪一种代理模型，Service资源都可统一根据其工作逻辑分为ClusterIP、NodePort、LoadBalancer和ExternalName这4种类型。 （1）ClusterIP通过集群内部IP地址暴露服务，ClusterIP地址仅在集群内部可达，因而无法被集群外部的客户端访问。此为默认的Service类型。 （2）NodePortNodePort类型是对ClusterIP类型Service资源的扩展，它支持通过特定的节点端口接入集群外部的请求流量，并分发给后端的Server Pod处理和响应。因此，这种类型的Service既可以被集群内部客户端通过ClusterIP直接访问，也可以通过套接字&lt;NodeIP&gt;: &lt;NodePort&gt;与集群外部客户端进行通信，如图7-7所示。显然，若集群外部的请求报文首先到的节点并非Service调度的目标Server Pod所在的节点，该请求必然因需要额外的转发过程（跃点）和更多的处理步骤而产生更多延迟 （3）LoadBalancer这种类型的Service依赖于部署在IaaS云计算服务之上并且能够调用其API接口创建软件负载均衡器的Kubernetes集群环境。LoadBalancer Service构建在NodePort类型的基础上，通过云服务商提供的软负载均衡器将服务暴露到集群外部，因此它也会具有NodePort和ClusterIP。简言之，创建LoadBalancer类型的Service对象时会在集群上创建一个NodePort类型的Service，并额外触发Kubernetes调用底层的IaaS服务的API创建一个软件负载均衡器，而集群外部的请求流量会先路由至该负载均衡器，并由该负载均衡器调度至各节点上该Service对象的NodePort，如图7-8所示。该Service类型的优势在于，它能够把来自集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而不是让客户端自行决定连接哪个节点，也避免了因客户端指定的节点故障而导致的服务不可用。 （4）ExternalName通过将Service映射至由externalName字段的内容指定的主机名来暴露服务，此主机名需要被DNS服务解析至CNAME类型的记录中。换言之，此种类型不是定义由Kubernetes集群提供的服务，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部服务的一种实现方式，如图7-9所示。因此，这种类型的Service没有ClusterIP和NodePort，没有标签选择器用于选择Pod资源，也不会有Endpoints存在。 总体来说，若需要将Service资源发布至集群外部，应该将其配置为NodePort或Load-Balancer类型，而若要把外部的服务发布于集群内部供Pod对象使用，则需要定义一个ExternalName类型的Service资源，只是这种类型的实现要依赖于v1.7及更高版本的Kubernetes。 应用Service资源Service是Kubernetes核心API群组（core）中的标准资源类型之一，其管理操作的基本逻辑类似于Namespace和ConfigMap等资源，支持基于命令行和配置清单的管理方式。Service资源配置规范中常用的字段及意义如下所示。 12345678910111213141516171819apiVersion: v1kind: Servicemetadata: name: … namespace: …spec: type &lt;string&gt; # Service类型，默认为ClusterIP selector &lt;map[string]string&gt; # 等值类型的标签选择器，内含“与”逻辑 ports： # Service的端口对象列表 - name &lt;string&gt; # 端口名称 protocol &lt;string&gt; # 协议，目前仅支持TCP、UDP和SCTP，默认为TCP port &lt;integer&gt; # Service的端口号，被映射进Pod上的应用程序监听的端口； 而且如果后端Pod有多个端口，并且每个端口都想通过SErvice暴露的话，每个都要单独定义。 最终接收请求的是PodIP和containerPort； targetPort &lt;string&gt; # 后端目标进程的端口号或名称，名称需由Pod规范定义 nodePort &lt;integer&gt; # 节点端口号，仅适用于NodePort和LoadBalancer类型 clusterIP &lt;string&gt; # Service的集群IP，建议由系统自动分配,也支持由用户手动分配 externalTrafficPolicy &lt;string&gt; # 外部流量策略处理方式，Local表示由当前节点处理， # Cluster表示向集群范围内调度 loadBalancerIP &lt;string&gt; # 外部负载均衡器使用的IP地址，仅适用于LoadBlancer externalName &lt;string&gt; # 外部服务名称，该名称将作为Service的DNS CNAME值 应用ClusterIP Service资源创建Service对象的常用方法有两种：一是利用此前曾使用过的kubectl create service命令创建，另一个则是利用资源配置清单创建。Service资源对象的期望状态定义在spec字段中，较为常用的内嵌字段为selector和ports，用于定义标签选择器和服务端口。下面的配置清单是定义在services-clusterip-demo.yaml中的一个Service资源示例： 12345678910111213kind: ServiceapiVersion: v1metadata: name: demoapp-svc namespace: defaultspec: selector: app: demoapp ports: - name: http # 端口名称标识 protocol: TCP # 协议，支持TCP、UDP和SCTP port: 80 # Service自身的端口号 targetPort: 80 # 目标端口号，即Endpoint上定义的端口号 Service资源的spec.selector仅支持以映射（字典）格式定义的等值类型的标签选择器，例如上面示例中的app: demoapp。定义服务端口的字段spec.ports的值则是一个对象列表，它主要定义Service对象自身的端口与目标后端端口的映射关系。我们可以将示例中的Service对象创建于集群中，通过其详细描述了解其特性，如下面的命令及结果所示。 1234567891011121314~$ kubectl apply -f services-clusterip-demo.yaml service/demoapp-svc created~ $ kubectl describe services/demoapp-svcName: demoapp-svcNamespace: defaultLabels: &lt;none&gt;Annotations: Selector: app=demoappType: ClusterIPIP: 10.97.72.1Port: http 80/TCPTargetPort: 80/TCPEndpoints: &lt;none&gt;Session Affinity: NoneEvents: &lt;none&gt; 上面命令中的结果显示，demoapp-svc默认设定为ClusterIP类型，并得到一个自动分配的IP地址10.97.72.1。创建Service对象的同时会创建一个与之同名且拥有相同标签选择器的Endpoint对象，若该标签选择器无法匹配到任何Pod对象的标签，则Endpoint对象无任何可用端点数据，于是Service对象的Endpoints字段值便成了。我们知道，Service对象自身只是iptables或ipvs规则，它并不能处理客户端的服务请求，而是需要把请求报文通过目标地址转换（DNAT）后转发至后端某个Server Pod，这意味着没有可用的后端端点的Service对象是无法响应客户端任何服务请求的，如下面从集群节点上发起的请求命令结果所示。 12mageedu@k8s-master01:~$ curl 10.97.72.1curl: (7) Failed to connect to 10.97.72.1 port 80: Connection refused 下面使用命令式命令手动创建一个与该Service对象具有相同标签选择器的Deployment对象demoapp，它默认会自动创建一个拥有标签app: demoapp的Pod对象。 12345~$ kubectl create deploy demoapp --image=ikubernetes/demoapp:v1.0deployment.apps/demoapp created~$ kubectl get pods -l app=demoapp NAME READY STATUS RESTARTS AGEdemoapp-6c5d545684-g85gl 1/1 Running 0 8s Service对象demoapp-svc通过API Server获知这种匹配变动后，会立即创建一个以该Pod对象的IP和端口为列表项的名为demoapp-svc的Endpoints对象，而该Service对象详细描述信息中的Endpoint字段便以此列表项为值，如下面的命令结果所示。 12345~$ kubectl get endpoints/demoapp-svcNAME ENDPOINTS AGEdemoapp-svc 10.244.2.7:80 42s~$ kubectl describe services/demoapp-svc | grep &quot;^Endpoints&quot;Endpoints: 10.244.2.7:80 扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息 1234567~$ kubectl scale deployments/demoapp --replicas=3deployment.apps/demoapp scaled~$ kubectl get endpoints/demoapp-svcNAME ENDPOINTS AGEdemoapp-svc 10.244.1.11:80,10.244.2.7:80,10.244.3.9:80 96s~$ kubectl describe services/demoapp-svc | grep &quot;^Endpoints&quot;Endpoints: 10.244.1.11:80,10.244.2.7:80,10.244.3.9:80 扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息 1234567~$ kubectl scale deployments/demoapp --replicas=3deployment.apps/demoapp scaled~$ kubectl get endpoints/demoapp-svcNAME ENDPOINTS AGEdemoapp-svc 10.244.1.11:80,10.244.2.7:80,10.244.3.9:80 96s~$ kubectl describe services/demoapp-svc | grep &quot;^Endpoints&quot;Endpoints: 10.244.1.11:80,10.244.2.7:80,10.244.3.9:80 接下来可于集群中的某节点上再次向服务对象demoapp-svc发起访问请求以进行测试，多次的访问请求还可评估负载均衡算法的调度效果，如下面的命令及结果所示。 12345mageedu@k8s-master01:~$ while true; do curl -s 10.97.72.1/hostname; sleep .2; doneServerName: demoapp-6c5d545684-89w4fServerName: demoapp-6c5d545684-zlm2wServerName: demoapp-6c5d545684-g85glServerName: demoapp-6c5d545684-g85gl kubeadm部署的Kubernetes集群的Service代理模型默认为iptables，它使用随机调度算法，因此Service会把客户端请求随机调度至其关联的某个后端Pod对象。请求取样次数越多，其调度效果也越接近算法的目标效果。 应用NodePort Service资源部署Kubernetes集群系统时会预留一个端口范围，专用于分配给需要用到NodePort的Service对象，该端口范围默认为30000～32767。与Cluster类型的Service资源的一个显著不同之处在于，NodePort类型的Service资源需要显式定义.spec.type字段值为NodePort，必要时还可以手动指定具体的节点端口号。例如下面的配置清单（services-nodeport-demo.yaml）中定义的Service资源对象demoapp-nodeport-svc，它使用了NodePort类型，且人为指定了32223这个节点端口。 1234567891011121314kind: ServiceapiVersion: v1metadata: name: demoapp-nodeport-svcspec: type: NodePort selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 nodePort: 32223 实践中，并不鼓励用户自定义节点端口，除非能事先确定它不会与某个现存的Service资源产生冲突。无论如何，只要没有特别需要，留给系统自动配置总是较好的选择。将配置清单中定义的Service对象demoapp-nodeport-svc创建于集群之上，以便通过详细描述了解其状态细节。 12345678910111213141516~$ kubectl apply -f services-nodeport-demo.yaml service/demoapp-nodeport-svc created~$ kubectl describe services demoapp-nodeport-svc Name: demoapp-nodeport-svcNamespace: defaultLabels: &lt;none&gt;Annotations: Selector: app=demoappType: NodePortIP: 10.97.227.67Port: http 80/TCPTargetPort: 80/TCPNodePort: http 32223/TCPEndpoints: 10.244.1.11:80,10.244.2.7:80,10.244.3.9:80Session Affinity: NoneExternal Traffic Policy: ClusterEvents: &lt;none&gt; 命令结果显示，该Service对象用于调度集群外部流量时使用默认的Cluster策略，该策略优先考虑负载均衡效果，哪怕目标Pod对象位于另外的节点之上而带来额外的网络跃点，因而针对该NodePort的请求将会被分散调度至该Serivce对象关联的所有端点之上。可以在集群外的某节点上对任一工作节点的NodePort端口发起HTTP请求以进行测试。以节点k8s-node03.ilinux.io为例，我们以如下命令向它的IP地址172.29.9.13的32223端口发起多次请求。 1234~$ while true; do curl -s 172.29.9.13:32223; sleep 1; done…… ClientIP: 10.244.3.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-zlm2w, ServerIP: 10.244.1.11!…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-g85gl, ServerIP: 10.244.2.7! 上面命令的结果显示出外部客户端的请求被调度至该Service对象的每一个后端Pod之上，而这些Pod对象可能会分散于集群中的不同节点。命令结果还显示，请求报文的客户端IP地址是最先接收到请求报文的节点上用于集群内部通信的IP地址，而非外部客户端地址，这也能够在Pod对象的应用访问日志中得到进一步验证，如下所示。 12~$ kubectl logs demoapp-6c5d545684-g85gl | tail -n 110.244.3.0 - - [31/Aug/2020 02:30:00] &quot;GET / HTTP/1.1&quot; 200 - NodePort类型的Service对象会对请求报文同时进行源地址转换（SNAT）和目标地址转换（DNAT）操作。另一个外部流量策略Local则仅会将流量调度至请求的目标节点本地运行的Pod对象之上，以减少网络跃点，降低网络延迟，但当请求报文指向的节点本地不存在目标Service相关的Pod对象时将直接丢弃该报文。下面先把demoapp-nodeport-svc的外部流量策略修改为Local，而后再进行访问测试。简单起见，这里使用kubectl patch命令来修改Service对象的流量策略。 12~$ kubectl patch services/demoapp-nodeport-svc -p &#x27;&#123;&quot;spec&quot;: &#123;&quot;externalTrafficPolicy&quot;: &quot;Local&quot;&#125;&#125;&#x27; service/demoapp-nodeport-svc patched -p选项中指定的补丁是一个JSON格式的配置清单片段，它引用了spec.externalTrafficPolicy字段，并为其赋一个新的值。配置完成后，我们再次发起测试请求时会看到，请求都被调度给了目标节点本地运行的Pod对象。另外，Local策略下无须在集群中转发流量至其他节点，也就不用再对请求报文进行源地址转换，Server Pod所看到的客户端IP就是外部客户端的真实地址。 1234~$ while true; do curl -s 172.29.9.13:32223; sleep 1; done …… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9! NodePort类型的Service资源同样会被配置ClusterIP，以确保集群内的客户端对该服务的访问请求可在集群范围的通信中完成。 应用LoadBalancer Service资源NodePort类型的Service资源虽然能够在集群外部访问，但外部客户端必须事先得知NodePort和集群中至少一个节点IP地址，一旦被选定的节点发生故障，客户端还得自行选择请求访问其他的节点，因而一个有着固定IP地址的固定接入端点将是更好的选择。此外，集群节点很可能是某IaaS云环境中仅具有私有IP地址的虚拟主机，这类地址对互联网客户端不可达，为此类节点接入流量也要依赖于集群外部具有公网IP地址的负载均衡器，由其负责接入并调度外部客户端的服务请求至集群节点相应的NodePort之上。IaaS云计算环境通常提供了LBaaS（Load Balancer as a Service）服务，它允许租户动态地在自己的网络创建一个负载均衡设备。部署在此类环境之上的Kubernetes集群可借助于CCM（Cloud Controller Manager）在创建LoadBalancer类型的Service资源时调用IaaS的相应API，按需创建出一个软件负载均衡器。但CCM不会为那些非LoadBalancer类型的Service对象创建负载均衡器，而且当用户将LoadBalancer类型的Service调整为其他类型时也将删除此前创建的负载均衡器。kubeadm在部署Kubernetes集群时并不会默认部署CCM，有需要的用户需要自行部署。对于没有此类API可用的Kubernetes集群，管理员也可以为NodePort类型的Service手动部署一个外部的负载均衡器（推荐使用HA配置模型），并配置将请求流量调度至各节点的NodePort之上，这种方式的缺点是管理员需要手动维护从外部负载均衡器到内部服务的映射关系。从实现方式上来说，LoadBalancer类型的Service就是在NodePort类型的基础上请求外部管理系统的API，并在Kubernetes集群外部额外创建一个负载均衡器，将流量调度至该NodePort Service之上。Kubernetes以异步方式请求创建负载均衡器，并将有关配置保存在Service对象的.status.loadBalancer字段中。下面是定义在services-loadbalancer-demo.yam配置清单中的LoadBalancer类型Service资源，在最简单的配置模型中，用户仅需要修改NodePort Service服务定义中type字段的值为LoadBalancer即可。 12345678910111213kind: ServiceapiVersion: v1metadata: name: demoapp-loadbalancer-svcspec: type: LoadBalancer selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 Service对象的loadBalancerIP负责承接外部发来的流量，该IP地址通常由云服务商系统动态配置，或者借助.spec.loadBalancerIP字段显式指定，但有些云服务商不支持用户设定该IP地址，这种情况下，即便提供了也会被忽略。外部负载均衡器的流量会直接调度至Service后端的Pod对象之上，而如何调度流量则取决于云服务商，有些环境可能还需要为Service资源的配置定义添加注解，必要时请自行参考云服务商文档说明。另外，LoadBalancer Service还支持使用.spec. loadBalancerSourceRanges字段指定负载均衡器允许的客户端来源的地址范围。 外部IP若集群中部分或全部节点除了有用于集群通信的节点IP地址之外，还有可用于外部通信的IP地址，如图7-10中的EIP-1和EIP-2，那么我们还可以在Service资源上启用spec.externalIPs字段来基于这些外部IP地址向外发布服务。所有路由到指定的外部IP（externalIP）地址某端口的请求流量都可由该Service代理到后端Pod对象之上，如图7-10所示。从这个角度来说，请求流量到达外部IP与节点IP并没有本质区别，但外部IP却可能仅存在于一部分的集群节点之上，而且它不受Kubernetes集群管理，需要管理员手动介入其配置和回收等操作任务中。 外部IP地址可结合ClusterIP、NodePort或LoadBalancer任一类型的Service资源使用，而到达外部IP的请求流量会直接由相关联的Service调度转发至相应的后端Pod对象进行处理。假设示例Kubernetes集群中的k8s-node01节点上拥有一个可被路由到的IP地址172.29.9.26，我们期望能够将demoapp的服务通过该外部IP地址发布到集群外部，则可以使用下列配置清单（services-externalip-demo.yaml）中的Service资源实现。 12345678910111213141516kind: ServiceapiVersion: v1metadata: name: demoapp-externalip-svc namespace: defaultspec: type: ClusterIP selector: app: demoapp ports: - name: http protocol: TCP port: 80 targetPort: 80 externalIPs: - 172.29.9.26 节点k8s-node01故障也必然导致该外部IP上公开的服务不再可达，除非该IP地址可以浮动到其他节点上。如今，大多数云服务商都支持浮动IP的功能，该IP地址可绑定在某个主机，并在其故障时通过某种触发机制自动迁移至其他主机。在不具有浮动IP功能的环境中进行测试之前，需要先在k8s-node01上（或根据规划的其他的节点上）手动配置172.29.9.26这个外部IP地址。而且，在模拟节点故障并手动将外部IP地址配置在其他节点进行浮动IP测试时，还需要清理之前的ARP地址缓存。 Service与Endpoint资源端点是指通过LAN或WAN连接的能够用于网络通信的硬件设备，它在广义上可以指代任何与网络连接的设备。在Kubernetes语境中，端点通常代表Pod或节点上能够建立网络通信的套接字，并由专用的资源类型Endpoint进行定义和跟踪。 Endpoint与容器探针Service对象借助于Endpoint资源来跟踪其关联的后端端点，但Endpoint是“二等公民”，Service对象可根据标签选择器直接创建同名的Endpoint对象，不过用户几乎很少有直接使用该类型资源的需求。 123456789101112131415161718192021222324252627282930313233343536373839kind: ServiceapiVersion: v1metadata: name: services-readiness-demo namespace: defaultspec: selector: app: demoapp-with-readiness ports: - name: http protocol: TCP port: 80 targetPort: 80---apiVersion: apps/v1kind: Deployment # 定义Deployment对象，它使用Pod模板创建Pod对象metadata: name: demoapp2spec: replicas: 2 # 该Deployment对象要求满足的Pod对象数量 selector: # Deployment对象的标签选择器，用于筛选Pod对象并完成计数 matchLabels: app: demoapp-with-readiness template: # 由Deployment对象使用的Pod模板，用于创建足额的Pod对象 metadata: creationTimestamp: null labels: app: demoapp-with-readiness spec: containers: - image: ikubernetes/demoapp:v1.0 name: demoapp imagePullPolicy: IfNotPresent readinessProbe: httpGet: # 定义探针类型和探测方式 path: &#x27;/readyz&#x27; port: 80 initialDelaySeconds: 15 # 初次检测延迟时长 periodSeconds: 10 # 检测周期 Endpoint对象会根据就绪状态把同名Service对象标签选择器筛选出的后端端点的IP地址分别保存在subsets.addresses字段和subsets.notReadyAddresses字段中，通过API Server持续、动态跟踪每个端点的状态变动，并即时反映到端点IP所属的字段。仅那些位于subsets.addresses字段的端点地址可由相关的Service用作后端端点。此外，相关Service对象标签选择器筛选出的Pod对象数量的变动也将会导致Endpoint对象上的端点数量变动。上面配置清单中定义Endpoint对象services-readiness-demo会筛选出Deployment对象demoapp2创建的两个Pod对象，将它们的IP地址和服务端口创建为端点对象。但延迟15秒启动的容器探针会导致这两个Pod对象至少要在15秒以后才能转为“就绪”状态，这意味着在上面配置清单中的Service资源创建后至少15秒之内无可用后端端点，例如下面的资源创建和Endpoint资源监视命令结果中，在20秒之后，Endpoint资源services-readiness-demo才得到第一个可用的后端端点IP。 12345678~$ kubectl apply -f services-readiness-demo.yaml service/services-readiness-demo createddeployment.apps/demoapp2 created~$ kubectl get endpoints/services-readiness-demo -w NAME ENDPOINTS AGEservices-readiness-demo 6sservices-readiness-demo 10.244.1.15:80 20sservices-readiness-demo 10.244.1.15:80,10.244.2.9:80 31s 因任何原因导致的后端端点就绪状态检测失败，都会触发Endpoint对象将该端点的IP地址从subsets.addresses字段移至subsets.notReadyAddresses字段。例如，我们使用如下命令人为地将地址10.244.2.9的Pod对象中的容器就绪状态检测设置为失败，以进行验证。 1~$ curl -s -X POST -d &#x27;readyz=FAIL&#x27; 10.244.2.9/readyz 等待至少3个检测周期共30秒之后，获取Endpoint对象services-readiness-demo的资源清单的命令将返回类似如下信息。 12345678910111213141516171819202122232425~$ kubectl get endpoints/services-readiness-demo -o yaml……subsets:- addresses: - ip: 10.244.1.15 nodeName: k8s-node01.ilinux.io targetRef: kind: Pod name: demoapp2-85595465d-dhbzs namespace: default resourceVersion: &quot;321388&quot; uid: 8d2a3bb6-c628-4558-917a-f8f6df9b8573 notReadyAddresses: - ip: 10.244.2.9 nodeName: k8s-node02.ilinux.io targetRef: kind: Pod name: demoapp2-85595465d-z7w5h namespace: default resourceVersion: &quot;323328&quot; uid: 380050ae-4e32-4724-af22-e079ab2ec02e ports: - name: http port: 80 protocol: TCP 该故障端点重新转回就绪状态后，Endpoints对象会将其移回subsets.addresses字段中。这种处理机制确保了Service对象不会将客户端请求流量调度给那些处于运行状态但服务未就绪（notReady）的端点。 自定义Endpoint资源除了借助Service对象的标签选择器自动关联后端端点外，Kubernetes也支持自定义Endpoint对象，用户可通过配置清单创建具有固定数量端点的Endpoint对象，而调用这类Endpoint对象的同名Service对象无须再使用标签选择器。Endpoint资源的API规范如下。 123456789101112131415161718192021222324apiVersion: v1kind: Endpointmetadata: # 对象元数据 name: namespace:subsets: # 端点对象的列表- addresses: # 处于“就绪”状态的端点地址对象列表 - hostname &lt;string&gt; # 端点主机名 ip &lt;string&gt; # 端点的IP地址，必选字段 nodeName &lt;string&gt; # 节点主机名 targetRef： # 提供了该端点的对象引用 apiVersion &lt;string&gt; # 被引用对象所属的API群组及版本 kind &lt;string&gt; # 被引用对象的资源类型，多为Pod name &lt;string&gt; # 对象名称 namespace &lt;string&gt; # 对象所属的名称空间 fieldPath &lt;string&gt; # 被引用的对象的字段，在未引用整个对象时使用，通常仅引用 # 指定Pod对象中的单容器，例如spec.containers[1] uid &lt;string&gt; # 对象的标识符 notReadyAddresses: # 处于“未就绪”状态的端点地址对象列表，格式与address相同 ports: # 端口对象列表 - name &lt;string&gt; # 端口名称 port &lt;integer&gt; # 端口号，必选字段 protocol &lt;string&gt; # 协议类型，仅支持UDP、TCP和SCTP，默认为TCP appProtocol &lt;string&gt; # 应用层协议 自定义Endpoint常将那些不是由编排程序编排的应用定义为Kubernetes系统的Service对象，从而让客户端像访问集群上的Pod应用一样请求外部服务。例如，假设要把Kubernetes集群外部一个可经由172.29.9.51:3306或172.29.9.52:3306任一端点访问的MySQL数据库服务引入集群中，便可使用如下清单中的配置完成。 1234567891011121314151617181920212223242526apiVersion: v1kind: Endpointsmetadata: name: mysql-external namespace: defaultsubsets:- addresses: - ip: 172.29.9.51 - ip: 172.29.9.52 ports: - name: mysql port: 3306 protocol: TCP---apiVersion: v1kind: Servicemetadata: name: mysql-external namespace: defaultspec: type: ClusterIP ports: - name: mysql port: 3306 targetPort: 3306 protocol: TCP 显然，非经Kubernetes管理的端点，其就绪状态难以由Endpoint通过注册监视特定的API资源对象进行跟踪，因而用户需要手动维护这种调用关系的正确性。Endpoint资源提供了在Kubernetes集群上跟踪端点的简单途径，但对于有着大量端点的Service来说，将所有的网络端点信息都存储在单个Endpoint资源中，会对Kubernetes控制平面组件产生较大的负面影响，且每次端点资源变动也会导致大量的网络流量。EndpointSlice（端点切片）通过将一个服务相关的所有端点按固定大小（默认为100个）切割为多个分片，提供了一种更具伸缩性和可扩展性的端点替代方案。EndpointSlice由引用的端点资源组成，类似于Endpoint，它可由用户手动创建，也可由EndpointSlice控制器根据用户在创建Service资源时指定的标签选择器筛选集群上的Pod对象自动创建。单个EndpointSlice资源默认不能超过100个端点，小于该数量时，EndpointSlice与Endpoint存在1:1的映射关系且性能相同。EndpointSlice控制器会尽可能地填满每一个EndpointSlice资源，但不会主动进行重新平衡，新增的端点会尝试添加到现有的EndpointSlice资源上，若超出现有任何EndpointSlice对象的可用的空余空间，则将创建新的EndpointSlice，而非分散填充。 123~$ kubectl get endpointslice -n kube-systemNAME ADDRESSTYPE PORTS ENDPOINTS AGEkube-dns-mbdj5 IPv4 53,9153,53 10.244.0.6,10.244.0.7 13d EndpointSlice资源根据其关联的Service与端口划分成组，每个组隶属于同一个Service。更具体的使用方式请参考Kubernetes的相关文档。 深入理解Service资源本质上，Service对象代表着由kube-proxy借助于自身的程序逻辑（userspace）、iptables或ipvs，甚至是某种形式的组合所构建出的流量代理和调度转发机制，每个Service对象的创建、更新与删除都会经由kube-proxy反映为程序配置、iptables规则或ipvs规则的相应操作。 iptables代理模型由集群中每个节点上的kube-proxy进程将Service定义、转换且配置于节点内核上的iptables规则。每个Service的定义主要由Service流量匹配规则、流量调度规则和以每个后端Endpoint为单位的DNAT规则组成，这些规则负责完成Service资源的核心功能。此外，iptables代理模型还会额外在filter表和mangle表上使用一些辅助类的规则。 ClusterIP ServiceClusterIP类型Service资源的请求流量是指以某个特定Service对象的ClusterIP（或称为Service_IP）为目标地址，同时以Service_Port为目标端口的报文，它们可能源自Kubernetes集群中某个特定节点上的Pod、独立容器（非托管至Kubernetes集群）或进程，也可能源自节点之外。通常，源自独立容器或节点外部的请求报文的源IP地址为Pod网络（例如Flannel默认的10.244.0.0/16）之外的IP地址。Cluster类型Service对象的相关规则主要位于KUBE-SERVICES、KUBE-MARQ-MASK和KUBE-POSTROUTING这3个自定义链，以及那些以KUBE-SVC或KUBE-SEP为前缀的各个自定义链上，用于实现Service流量筛选、分发和目标地址转换（端点地址），以及为非源自Pod网络的请求报文进行源地址转换。各相关的规则链及调用关系如图7-11所示。 ▪KUBE-SERVICES：包含所有ClusterIP类型Service的流量匹配规则，由PREROUTING和OUTPUT两个内置链直接调用。每个Service对象包含两条规则定义，对于所有发往该Service（目标IP为Service_IP且目标端口为Service_Port）的请求报文：前一条规则用于为非源自Pod网络（! -s 10.244.0.0/16）中的请求报文打上特有的防火墙标记，而打标签的操作则要借助KUBE-MARQ-MASK自定义链中的规则，后一条规则负责将所有报文转至专用的以KUBE-SVC为名称前缀的自定义链，后缀是Service信息的HASH值。▪KUBE-MARQ-MASK：专用目的自定义链，所有转至该自定义链的报文都将被打上特有的防火墙标记（0x4000），以便于将特定类型的报文定义为单独的分类，进而在将该类报文转发到目标端点之前由POSTROUTING规则链进行源地址转换。▪KUBE-SVC-：定义一个服务的流量调度规则，它通过随机调度算法将请求分发给该Service的所有后端端点，每个后端端点定义在以KUBE-SEP为前缀名称的自定义链上，后缀是端点信息的hash值。▪KUBE-SEP-：定义一个端点相关的流量处理规则。它通常包含两条规则：前一条用于为那些源自该端点自身（-s ep_ip）的流量请求调用自定义链KUBE-MARQ-MASK，打上特有的防火墙标记；后一条负责对发往该端点的所有流量进行目标IP地址和端口转换，新目标为该端点的IP和端口（-j DNAT –to-destination ep_ip:ep_port）。▪KUBE-POSTROUTING：专用的自定义链，由内置链POSTROUTING无条件调用，负责对带特有防火墙标记0x4000的请求报文进行源地址转换或地址伪装（MASQUERADE），新的源地址为报文离开协议栈时流经接口的主IP地址。我们可通过实际存在的Service对象来验证这些设定，以7.2.1节创建的demoapp-svc为例，在集群中的任何一个工作节点上使用iptables -t nat -vnL或iptables -t nat -S命令打印与它相关的iptables规则。下面的命令打印了该Service对象用于流量匹配的相关规则，它定义在KUBE-SERVICES自定义链上。 123root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;default/demoapp-svc&quot;-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ-A KUBE-SERVICES -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-ZAGXFVDPX7HH4UMW 第一条规则用于将那些发往demoapp-svc的、来自10.244.0.0/16网络之外的请求报文交由自定义链KUBE-MARK-MASQ上的规则添加专用标记0x4000，该条规则如下所示。 12root@k8s-node01:~# iptables -t nat -S KUBE-MARK-MASQ | grep &quot;^-A&quot;-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000 添加标记的处理并不会短路iptables规则链KUBE-SERVICES对流量的处理，因此所有发往demoapp-svc的流量还会继续由后一条规则指向的、以KUBE-SVC为名称前缀的自定义链KUBE-SVC-ZAGXFVDPX7HH4UMW中的规则处理。该自定义链专用于为demoapp-svc中的所有可用端点定义流量调度规则，它包含如下3条规则： 1234root@k8s-node01:~# iptables -t nat -S KUBE-SVC-ZAGXFVDPX7HH4UMW | grep &quot;^-A&quot; -A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-HDIVJIPCJU2JBJVX-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ZAFCYSF77K72PY72-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-SEP-FUO5ALUGHUE426HZ 注意所有以KUBE-SEP和KUBE-SVC为前缀的自定义链的名称在重新创建Service或重启Kubernetes集群后都有可能发生改变，但它们的引用关系不变。 这3条规则的处理目标分别为3个以KUBE-SEP为名称前缀的自定义链，每个链上定义了一个端点的流量处理规则，因而意味着该Service对象共有3个Endpoint对象，所有流量将在这3个Endpoint之间随机（–mode random）分配。到达KUBE-SVC-ZAGXFVDPX7HH4UMW的流量将由这3条规则以“短路”方式进行匹配检查和处理，任何一条规则处理后都不会再匹配后续的其他规则。第一条规则将处理大约1/3（–probability 0.33333333349）的流量，余下的所有流量（即由第一条规则处理后余下的2/3）将由第二条规则处理一半（–probability 0.50000000000），再余下的所有流量都将由第三条规则处理，因此3个Endpoint将各自得到大约1/3的流量。每个Endpoint专用的自定义链以KUBE-SEP为名称前缀，它包含某单点端点相关的流量处理规则。以专用IP地址为10.244.1.11的Endpoint对象为例，它对应于自定义链KUBE-SEP-HZPGLN57HG6GZW4O，该链下包含两个iptables规则，如下面的命令结果所示： 123root@k8s-node01:~# iptables -t nat -S KUBE-SEP-HDIVJIPCJU2JBJVX | grep &quot;^-A&quot; -A KUBE-SEP-HDIVJIPCJU2JBJVX -s 10.244.1.11/32 -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-MARK-MASQ-A KUBE-SEP-HDIVJIPCJU2JBJVX -p tcp -m comment --comment &quot;default/demoapp-svc:http&quot; -m tcp -j DNAT --to-destination 10.244.1.11:80 Pod对象也可能会向自己所属的Service对象发起访问请求，而且该请求经由OUTPUT链到达KUBE-SERVICES链后存在被调度回当前Pod对象的可能性。第一条规则就是为该类报文添加专有的流量标记。第二条规则将接收到的所有流量进行目标地址转换（DNAT），新的目标为10.244.1.11:80，它对应Kubernetes集群上由Service对象demoapp-svc匹配到的一个特定Pod对象。不难猜测，特定节点（例如前面示例中的k8s-node01）接收到的请求报文的源地址为Pod网络中的IP地地址的，必然源自该节点或节点上的Pod对象。它们的IP地址位于该节点的PodCIDR之中，这些流量离开节点之前无须进行源地址转换，因而目标端点直接响应给客户端IP就能够正确到达请求方。而请求报文的源地址并非为Pod网络中的IP地址的，例如请求方为该节点上的某独立容器，则Service必须在其离开本节点之前，将请求报文的源地址转换为该节点上报文离开时要经由接口的IP地址（例如cni0上的10.244.1.0），以确保响应报文可正确回送至该节点，并由该节点响应给相应的客户端，由内置链POSTROUTING所调用的自定义链KUBE-POSTROUTING上的规则便用于实现此类功能。 12root@k8s-node01:~# iptables -t nat -S KUBE-POSTROUTING | grep &quot;^-A&quot;-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE 由此可见，对于集群内部的后端端点来说，它们收到的请求报文的源地址，要么是Pod的IP地址，要么是节点IP地址，因而直接发送响应报文给请求方即可。但那些本身并非源自Pod或节点的请求的响应报文，还需要由节点自动执行一次目标地址转换，以便把报文送达真正的请求方。注意kube-proxy也支持在iptables代理模型上使用masquerade all，从而对通过ClusterIP地址访问的所有请求进行源地址转换，但在大多数场景中，这都不是必要的选择。2. NodePort Service相较于ClusterIP类型来说，所有发往NodePort类型的Service对象的请求流量的目标IP和端口分别是节点IP和NodePort，这类报文无法由KUBE-SERVICES自定义链上那些基于Service IP和Service Port定义的流量匹配规则所匹配，但会由该自定义链上的最后一条规则转给KUBE-NODEPORTS自定义链。 12root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | tail -n 1-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS KUBE-NODEPORTS链以类似ClusterIP Service拦截规则的方式定义了NodePort Service对象的拦截规则，其中每个Service对象包含两条规则定义。对于所有发往该Service（目标IP为该NodeIP，目标端口为NodePort）的请求报文：前一条规则为发往该Service对象的所有请求报文，基于KUBE-MARQ-MASK自定义链中的规则打上特有的防火墙标记；后一条规则负责将这些报文转至专用的、以KUBE-SVC为前缀的自定义链。以前面创建的demoapp-nodeport-svc为例，它拥有以下两条iptables规则。 123root@k8s-node01:~# iptables -t nat -S KUBE-NODEPORTS | grep &quot;default/demoapp-nodeport-svc&quot;-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-MARK-MASQ-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-SVC-HCTPASJ7WVWOBYLM 我们已经知道，Service对象的专用自定义链定义了一组调度规则，以调度发往该Service对象匹配的所有后端端点的相关流量，而其中的每一个后端端点又有自己专用的自定义链，用于对请求报文进行目标地址转换。另外，NodePort类型的Service为所有从NodePort进入的请求报文都打了特有防火墙标记，因此这些请求报文会按照POSTROUTING和KUBE-POSTROUTING链上的规则将源地址转换为该报文离开节点时所经由的接口的IP地址。这些处理步骤与ClusterIP类型的Service对象几乎完全相同。完整的处理流程如图7-12所示。 对于集群内部的后端端点来说，它们收到的请求报文的源地址都是节点IP地址。以Flannel插件环境中10.244.1.0/24这个Pod CIDR为例，该IP地址可能是flannel.1接口上的10.244.1.0/24，也可能是cni0上的10.244.1.1/24。于是，后端端点会把报文响应给请求报文进入时的节点，再由该节点将目标地址转换为客户端IP后发送。但是，对于将外部流量策略定义为Local的NodePort Service对象来说，由于流量报文不会在集群内跨节点转发，也就没有必要对请求报文进行SNAT操作，所以后端端点可以看到真实的客户端IP。它的具体处理流程如图7-13所示。 1）KUBE-SERVICES链把目标地址指向当前节点的报文，并转给KUBE-NODEPORTS处理。2）对于一个Local策略的NodePort Service来说，KUBE-NODEPORTS会定义两条规则：前一条负则将源地址位于127.0.0.0/8网络的请求报文借助KUBE-MARK-MASQ打上0x4000防火墙标记；后一条则将报文转给该Service专用的KUBE-XLB-自定义链。3）KUBE-XLB-自定义链将源自Pod网络（10.244.0.0/16）的请求报文以类似ClusterIP Service使用的方式进行处理，只转换请求报文目标地址；将源自当前节点所处的本地网络中的请求报文，按照常规的NodePort Service使用的方式进行处理，并同时转换源地址和目标地址；而将其他类型的请求报文直接转交给指定的本地后端端点处理，这也体现了本地流量策略的真正意义。显然，若某节点自身未运行NodePort Service后端Pod，则本地策略类型的请求将得到失败的响应结果。提示未配置外部IP地址的LoadBalancer类型的Service对象的工作方式与NodePort类型几乎完全相同，这里不再专门描述。3. External IP在iptables中，外部IP表现为一种专有的Service访问入口。在KUBE-SERVICES自定义链上，每个外部IP都有3条相关的iptables规则：第1条用于为发往该外部IP的服务端口的请求流量，借助KUBE-MARK-MASQ自定义链打上特有的防火墙标记0x4000；第2条将这些请求流量中从非物理接口进入且源地址类型不是本地地址的流量，交由相应Service的专用自定义链进行流量分发；第3条用于将这些流量中目标地址类型是本地地址的请求报文，也交由相应Service的专用自定义链进行流量分发。具体的处理过程如图7-14所示。 以前面定义的default/demoapp-externalip-svc中使用的外部IP 172.29.9.26为例，下面的命令可以在KUBE-SERVICES获取到相应的专用规则。 1234root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;172.29.9.26&quot; -A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m addrtype --dst-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56 由此可见，尽管外部IP需要结合ClusterIP、NodePort或LoadBalancer中任一类型的Service对象使用，但到达外部IP的服务请求流量却有着专用的拦截规则，请求报文也是交由相应Service的专用自定义链直接进行向后分发。 ipvs代理模型由前一节的介绍可知，单个Service对象的iptables数量与后端端点的数量正相关，对于拥有较多Service对象和大规模Pod对象的Kubernetes集群，每个节点的内核上将充斥着大量的iptables规则。Service对象的变动会导致所有节点刷新netfilter上的iptables规则，而且每次的Service请求也都将经历多次的规则匹配检测和处理过程，这会占用节点上相当比例的系统资源。因此，iptables代理模型不适用于Service和Pod数量较多的集群。ipvs代理模型通过将流量匹配和分发功能配置为少量ipvs规则，有效降低了对系统资源的占用，从而能够承载更大规模的Kubernetes集群。 调整kube-proxy代理模型kube-proxy使用的代理模型定义在配置文件中，kubeadm部署的Kubernetes集群以DaemonSet控制器编排kube-proxy在每个节点上运行一个实例，配置文件则以kube-system名称空间中名为kube-proxy的ConfigMap对象的形式提供，默认使用iptables代理模型。在测试集群环境中，可直接使用kubectl edit configmaps/kube-proxy -n kube-system命令编辑该ConfigMap对象，将代理模型修改为ipvs，配置要点如下所示。 123456789101112131415161718192021222324data: config.conf: |- apiVersion: kubeproxy.config.k8s.io/v1alpha1 bindAddress: 0.0.0.0 …… iptables: # iptables配置细节 masqueradeAll: false # 是否将通过ClusterIP访问的流量全部进行SNAT masqueradeBit: null minSyncPeriod: 0s syncPeriod: 0s ipvs: # ipvs配置细节 excludeCIDRs: null minSyncPeriod: 0s scheduler: &quot;&quot; # 调度算法，默认为rr strictARP: false syncPeriod: 0s tcpFinTimeout: 0s tcpTimeout: 0s udpTimeout: 0s kind: KubeProxyConfiguration metricsBindAddress: &quot;&quot; mode: &quot;ipvs&quot; # 代理模型，空值代表是iptables nodePortAddresses: null …… 配置完成后，以灰度模式手动逐个或分批次删除kube-system名称空间中kube-proxy旧版本的Pod实例，全部更新完成后便切换到了ipvs代理模型。或者，在测试环境中，可以直接使用如下命令一次性完成所有实例的强制更新。 1~$ kubectl delete pods -l k8s-app=kube-proxy -n kube-system 提示用于生产环境时，建议在部署Kubernetes集群时直接选定要使用的代理模型，或在集群部署完成后立即调整代理模型，而后再部署其他应用。 ipvs代理模型下的Service资源相较于iptables代理模型的复杂表示逻辑，ipvs的代理逻辑也较为简单，它仅有两个关键配置要素。首先，kube-proxy会在每个节点上创建一个名为kube-ipvs0的虚拟网络接口，并将集群上所有Service对象的ClusterIP和ExternalIP配置到该接口，使相应IP地址的流量都可被当前节点捕获。其次，kube-proxy会为每个Service生成相关的ipvs虚拟服务器（Virtual Server）定义，该虚拟服务器的真实服务器（Real Server）是由相应Service对象的后端端点组成，到达虚拟服务器VIP（虚拟IP地址）上的服务端口的请求流量由默认或指定的调度算法分发至相关的各真实服务器。但kube-proxy对ClusterIP和NodePort类型Service对象的虚拟服务定义方式略有不同。对于每个ClusterIP类型的Service，kube-proxy仅针对Service_IP生成单个虚拟服务，协议和端口遵循Service的定义。以前面创建的demoapp-svc为例，它的ClusterIP是10.97.72.1，它的虚拟服务定义如下，这些可以通过ipvsadm -Ln命令在集群中的任意一个节点上获取。 12345root@k8s-node01:~# ipvsadm -Ln | grep -A 3 &quot;10.97.72.1&quot;TCP 10.97.72.1:80 rr -&gt; 10.244.1.11:80 Masq 1 0 0 -&gt; 10.244.2.7:80 Masq 1 0 0 -&gt; 10.244.3.9:80 Masq 1 0 0 而对于NodePort类型Service，kube-proxy会针对kube-ipvs0上的Service_IP:Service_Port，以及当前节点上的所有活动接口的主IP地址的NodePort各定义一个虚拟服务，下面的命令用于获取前面创建的NodePort类型Service对象的demoapp-nodeport-svc的相关虚拟服务的定义。 1234567root@k8s-node01:~# ipvsadm -Ln | grep -E &quot;31398|10.97.56.1&quot;TCP 172.29.9.11:31398 rr # 节点IPTCP 10.97.56.1:80 rr # ClusterIPTCP 10.244.1.0:31398 rr # flannel.1接口IPTCP 10.244.1.1:31398 rr # cni0接口IPTCP 127.0.0.1:31398 rr # lo接口IPTCP 172.17.0.1:31398 rr # docker0接口IP LoadBalancer类型Service的配置方式与NodePort类型相似，这里不再单独说明。另外，对于每个ExternalI，kube-proxy也会根据每个ExternalIP:Service_Port的组合生成一个虚拟服务，下面的命令及结果显示出前面创建的外部IP地址172.29.9.26相关的虚拟服务。 12root@k8s-node01:~# ipvsadm -Ln | grep &quot;172.29.9.26&quot;TCP 172.29.9.26:80 rr 上述每种Service类型对应的所有虚拟服务内部同样都使用NAT模式进行请求代理，除了更加多样的调度算法选择外，它的转发性能并没有显著提升，不过因为避免了使用大量的iptables规则，所以系统资源开销显著降低。ipvs仅实现了代理和调度机制，Service资源中的报文过滤和源地址转换等功能，依旧要由iptables完成，但相应的规则数量较少且较为固定。 Kubernetes服务发现Kubernetes系统上的Service为Pod中的服务类应用提供了一个固定的访问入口，但Pod客户端中的应用还需要借助服务发现机制获取特定服务的IP和端口。 服务发现概述服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），由服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费方周期性地从注册中心获取服务提供者的最新位置信息，从而“发现”要访问的目标服务资源。复杂的服务发现机制还会让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。 客户端发现：由客户端到服务注册中心发现其依赖的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。 服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。服务注册中心是服务发现得以落地的核心组件。 在传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统，它们可提供基本的数据存储功能，但距离实现完整的服务发现机制还有大量的二次开发任务需要完成。而且，它们更注重数据一致性而不得不弱化可用性（分布式系统的CAP理论），这背离了微服务发现场景中更注重服务可用性的需求。Netflix的Eureka是专用于服务发现的分布式系统，遵从“存在少量的错误数据，总比完全不可用要好”的设计原则，服务发现和可用性是其核心目标，能够在多种故障期间保持服务发现和服务注册的功能。另一个同级别的实现是Consul，它于服务发现的基础功能之外还提供了多数据中心的部署等一众出色的特性。尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目结合古老的DNS技术和时髦的Go语言、Raft算法，并构建于etcd存储系统之上，为Kubernetes系统实现了一种独特且实用的服务发现机制。Kubernetes在v1.3版本引入的KubeDNS由kubedns、dnsmasq和sidecar这3个部分组合而成。第一个部分包含kubedns和skydns两个组件，前者负责将Service和Endpoint转换为SkyDNS可以理解的格式；第二部分用于增强解析功能；第三部分为前两者添加健康状态检查机制，因而我们可以把KubeDNS视为SkyDNS的增强版。而另一个基于DNS较新的服务发现项目是由CNCF孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链实现所有功能，也允许用户自行开发和添加必要的插件，但所有功能运行在单个容器之中。另外，CoreDNS使用Caddy作为底层的Web Server，可以支持以UDP、TLS、gRPC和HTTPS等方式对外提供DNS服务。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。 基于环境变量的服务发现创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的Link兼容的变量。 （1）Kubernetes Service环境变量Kubernetes为每个Service资源生成包括以下形式的环境变量在内的一系列环境变量，在同一名称空间中创建的Pod对象都会自动拥有这些变量： {SVCNAME}SERVICE_HOST {SVCNAME}_SERVICE_PORT注意如果SVCNAME中使用了连接线，Kubernetes会在定义环境变量时将其转换为下划线。 （2）Docker Link形式的环境变量Docker使用–link选项实现容器连接时所设置的环境变量形式，具体使用方式请参考Docker的相关文档。在创建Pod对象时，Kubernetes也会把与此形式兼容的一系列环境变量注入Pod对象中。例如，在Service资源demoapp-svc创建后创建的Pod对象中查看可用的环境变量，其中以DEMOAPP_SVC_SERVICE开头的为Kubernetes Service环境变量，名称中不包含SERVICE字符串的环境变量为Docker Link形式的环境变量。下面的命令创建了一个临时Pod对象，并在其命令行列出与demoapp-svc的相关环境变量。 12345678910~$ kubectl run client-pod --image=ikubernetes/admin-toolbox:v1.0 -it --command -- /bin/sh[root@client-pod /]# printenv | grep DEMOAPP_SVCDEMOAPP_SVC_SERVICE_PORT_HTTP=80DEMOAPP_SVC_SERVICE_HOST=10.97.72.1DEMOAPP_SVC_SERVICE_PORT=80DEMOAPP_SVC_PORT=tcp://10.97.72.1:80DEMOAPP_SVC_PORT_80_TCP_ADDR=10.97.72.1DEMOAPP_SVC_PORT_80_TCP_PORT=80DEMOAPP_SVC_PORT_80_TCP_PROTO=tcpDEMOAPP_SVC_PORT_80_TCP=tcp://10.97.72.1:80 基于环境变量的服务发现功能简单、易用，但存在一定局限，例如只有那些与新建Pod对象在同一名称空间中且事先存在的Service对象的信息才会以环境变量形式注入，而那些不在同一名称空间，或者在Pod资源创建之后才创建的Service对象的相关环境变量则不会被添加。 基于DNS的服务发现名称解析和服务发现是Kubernetes系统许多功能得以实现的基础服务，ClusterDNS通常是集群安装完成后应该立即部署的附加组件。Kubernetes集群上的每个Service资源对象在创建时都会被自动指派一个遵循&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的名称，并由ClusterDNS为该名称自动生成资源记录，service、ns和zone分别代表服务的名称、名称空间的名称和集群的域名。例如demoapp-svc的DNS名称为demoapp-svc.default.svc.cluster.local.，其中cluster.local.是未明确指定域名后缀的集群默认使用的域名。无论使用kubeDNS还是CoreDNS，它们提供的基于DNS的服务发现解决方案都会负责为该DNS名称解析相应的资源记录类型以实现服务发现。以拥有ClusterIP的多种Service资源类型（ClusterIP、NodePort和LoadBalancer）为例，每个Service对象都会具有以下3个类型的DNS资源记录。 1）根据ClusterIP的地址类型，为IPv4生成A记录，为IPv6生成AAAA记录。 &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;cluster-ip&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;cluster-ip&gt; 2）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。 _&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.&lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. 3）对于每个给定的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4）都要生成PTR记录，它们各自的格式如下所示： &lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.例如， 前面在default名称空间中创建的Service对象demoapp-svc的地址为10.97.72.1，且为TCP协议的80端口取名http，对于默认的cluster.local域名来说，它会拥有如下3个DNS资源记录。 A记录：demoapp-svc.default.svc.cluster.local. 30 IN A 10.97.72.1 SRV记录：_http._tcp.demoapp-svc.default.svc.cluster.local. 30 IN SRV 0 100 80 demoapp- svc.default.svc.cluster.local. PTR记录：1.72.97.10.in-addr.arpa. 30 IN PTR demoapp-svc.default.svc.cluster.local。 kubelet会为创建的每一个容器在/etc/resolv.conf配置文件中生成DNS查询客户端依赖的必要配置，相关的配置信息源自kubelet的配置参数。各容器的DNS服务器由clusterDNS参数的值设定，它的取值为kube-system名称空间中的Service对象kube-dns的ClusterIP，默认为10.96.0.10，而DNS搜索域的值由clusterDomain参数的值设定，若部署Kubernetes集群时未特别指定，其值将为cluster.local、svc.cluster.local和NAMESPACENAME.svc.cluster.local。下面的示例取自集群上一个随机选择的Pod中的容器。 123nameserver 10.96.0.10search default.svc.cluster.local svc.cluster.local cluster.localoptions ndots:5 上述search参数中指定的DNS各搜索域，是以次序指定的几个域名后缀，它们各自的域名如下所示。 &lt;ns&gt;.svc.&lt;zone&gt;：附带有特定名称空间的域名，例如default.svc.cluster.local。 svc. &lt;zone&gt;：附带了Kubernetes标识Service专用子域svc的域名，例如svc.cluster.local。 &lt;zone&gt;：集群本地域名，例如cluster.local。 各容器能够直接向集群上的ClusterDNS发起服务名称和端口名称解析请求完成服务发现，各名称也支持短格式，由搜索域自动补全相关的后缀。我们可以在Kubernetes集群上通过任意一个有nslookup等DNS测试工具的容器进行测试。下面基于此前创建专用于测试的客户端Pod对象client-pod的交互式接口完成后续测试操作。 12~$ kubectl exec -it client-pod -- /bin/sh[root@client-pod /]# 接下来便可以进行名称解析测试。例如，下面的命令用于请求同一名称空间（default）中的服务名称demoapp-svc的解析结果，并获得了正确的返回值。 123456[root@client-pod /]# nslookup -query=A demoapp-svcServer: 10.96.0.10Address: 10.96.0.10#53Name: demoapp-svc.default.svc.cluster.localAddress: 10.97.72.1 ClusterDNS解析demoapp-svc服务名称的搜索次序依次是default.svc.cluster.local、svc.cluster.local和cluster.local，因此基于DNS的服务发现不受Service资源所在名称空间和创建时间的限制。上面的解析结果也正是默认的default名称空间中创建的demoapp-svc服务的IP地址。SRV记录中的端口名称的格式_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，同样可使用短格式名称。下面的命令用于请求解析demoapp-svc上的http端口，它返回的结果为80。 12345[root@client-pod ~]# nslookup -query=SRV _http._tcp.demoapp-svcServer: 10.96.0.10Address: 10.96.0.10#53_http._tcp.demoapp-svc.default.svc.cluster.local service = 0 100 80 demoapp-svc.default.svc.cluster.local. 请求解析其他名称空间中的Service对象名称时需要明确指定服务名称和名称空间，下面以kube-dns.kube-system为例进行解析请求。 123456[root@client-pod /]# nslookup -query=A kube-dns.kube-systemServer: 10.96.0.10Address: 10.96.0.10#53Name: kube-dns.kube-system.svc.cluster.localAddress: 10.96.0.10 端口名称解析时同样需要指定Service名称及其所在的名称空间，下面的命令用于请求解析kube-dns.kube-system上的metrics端口，它返回了9153的端口号。 12345[root@client-pod /]# nslookup -query=SRV _metrics._tcp.kube-dns.kube-systemServer: 10.96.0.10Address: 10.96.0.10#53_metrics._tcp.kube-dns.kube-system.svc.cluster.local service = 0 100 9153 kube-dns.kube-system.svc.cluster.local. 为了减少搜索次数，无论是否处于同一名称空间，客户端都可以直接使用FQDN格式的名称解析Service名称和端口名称，这也是在某应用的配置文件中引用其他服务时建议遵循的方式。 Pod的DNS解析策略与配置Kubernetes还支持在单个Pod资源规范上自定义DNS解析策略和配置，它们分别使用spec.dnsPolicy和spec.dnsConfig进行定义，并组合生效。目前，Kubernetes支持如下DNS解析策略，它们定义在spec.dnsPolicy字段上。 Default：从运行所在的节点继承DNS名称解析相关的配置。 ClusterFirst：在集群DNS服务器上解析集群域内的名称，其他域名的解析则交由从节点继承而来的上游名称服务器。 ClusterFirstWithHostNet：专用于在设置了hostNetwork的Pod对象上使用的ClusterFirst策略，任何配置了hostNetwork的Pod对象都应该显式使用该策略。 None：用于忽略Kubernetes集群的默认设定，而仅使用由dnsConfig自定义的配置。Pod资源的自定义DNS配置要通过嵌套在spec.dnsConfig字段中的如下几个字段进行，它们的最终生效结果要结合dnsPolicy的定义生成。 nameservers &lt;[]string&gt;：DNS名称服务器列表，它附加在由dnsPolicy生成的DNS名称服务器之后。 searches &lt;[]string&gt;：DNS名称解析时的搜索域，它附加在dnsPolicy生成的搜索域之后。 options &lt;[]Object&gt;：DNS解析选项列表，它将会同dnsPolicy生成的解析选项合并成最终生效的定义。下面配置清单示例（pod-with-dnspolicy.yaml）中定义的Pod资源完全使用自定义的配置，它通过将dnsPolicy设置为None而拒绝从节点继承DNS配置信息，并在dnsConfig中自定义了要使用的DNS服务、搜索域和DNS选项。 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: pod-with-dnspolicy namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent dnsPolicy: None dnsConfig: nameservers: - 10.96.0.10 - 223.5.5.5 - 223.6.6.6 searches: - svc.cluster.local - cluster.local - ilinux.io options: - name: ndots value: &quot;5&quot; 将上述配置清单中定义的Pod资源创建到集群之上，它最终会生成类似如下内容的/etc/resolv.conf配置文件。 12345nameserver 10.96.0.10nameserver 223.5.5.5nameserver 223.6.6.6search svc.cluster.local cluster.local ilinux.iooptions ndots:5 上面配置中的搜索域要求，即便是客户端与目标服务位于同一名称空间，也要求在短格式的服务名称上显式指定其所处的名称空间。感兴趣的读者可自行测试其效果。 配置CoreDNSCoreDNS是高度模块化的DNS服务器，几乎全部功能均由可插拔的插件实现。CoreDNS调用的插件及相关的配置定义在称为Corefile的配置文件中。CoreDNS主要用于定义各服务器监听地址和端口、授权解析的区域以及加载的插件等，配置格式如下： 123ZONE:[PORT] &#123; [PLUGIN]...&#125; 参数说明如下。 ZONE：定义该服务器授权解析的区域，它监听由PORT指定的端口。 PLUGIN：定义要加载的插件，每个插件可能存在一系列属性，而每个属性还可能存在可配置的参数。由kubeadm在部署Kubernetes集群时自动部署的CoreDNS的Corefile存储为kube-system名称空间中名为coredns的ConfigMap对象，定义了一个监听53号端口授权解析根区域的服务器，详细的配置信息及各插件的简单说明如下所示。 12345678910111213141516171819202122232425apiVersion: v1kind: ConfigMapmetadata: name: coredns namespace: kube-systemdata: Corefile: | .:53 &#123; errors # 将错误日志发往标准输出stdout health &#123; lameduck 5s &#125; # 通过http://localhost:8080/health报告健康状态 ready # 待所有插件就绪后通过8181端口响应“200 OK”以报告就绪状态 kubernetes cluster.local in-addr.arpa ip6.arpa &#123; pods insecure fallthrough in-addr.arpa ip6.arpa ttl 30 &#125; # Kubernetes系统的本地区域及专用的名称解析配置 prometheus :9153 # 通过http://localhost:9153/metrics输出指标数据 forward . /etc/resolv.conf # 非Kubernetes本地域名的解析转发逻辑 cache 30 # 缓存时长 loop # 探测转发循环并终止其过程 reload # Corefile内容改变时自动重载配置信息 loadbalance # A、AAAA或MX记录的负载均衡器，使用round-robin算法 &#125; 在该配置文件中，专用于Kubernetes系统上的名称解析服务由名为kubernetes的插件进行定义，该插件负责处理指定的权威区域中的所有查询，例如上面示例中的正向解析区域cluster.local，以及反向解析区域in-addr.arpa和ip6.arpa。该插件支持多个配置参数，例如endpoint、tls、kubeconfig、namespaces、labels、pods、ttl和fallthrough等，上面示例中用到的3个参数的功能如下。 1）pods POD-MODE：设置用于处理基于Pod IP地址的A记录的工作模式，以便在直接同Pod建立SSL通信时验证证书信息；默认值为disabled，表示不处理Pod请求，总是响应NXDOMAIN；在其他可用值中，insecure表示直接响应A记录而无须向Kubernetes进行校验，目标在于兼容kube-dns；而verified表示仅在指定的名称空间中存在一个与A记录中的IP地址相匹配的Pod对象时才会将结果响应给客户端。 2）fallthrough [ZONES…]：常规情况下，该插件的权威区域解析结果为NXDOMAIN时即为最终结果，而该参数允许将该响应的请求继续转给后续的其他插件处理；省略指定目标区域时表示生效于所有区域，否则，将仅生效于指定的区域。 3）ttl：自定义响应结果的可缓存时长，默认为5秒，可用值范围为[0,3600]。那些非由kubernetes插件所负责解析的本地匹配的名称，将由forward插件定义的方式转发给其他DNS服务器进行解析，示例中的配置表示将根区域的解析请求转发给主机配置文件/etc/resolv.conf中指定的DNS服务器进行。若要将请求直接转发给指定的DNS服务器，则将该文件路径替换为目标DNS服务器的IP地址即可，多个IP地址之间以空白字符分隔。例如，下面的配置示例表示将除了ilinux.io区域之外的其他请求转给223.5.5.5或223.6.6.6进行解析。 12345. &#123; forward . 223.5.5.5 223.6.6.6 &#123; except ilinux.io &#125;&#125; CoreDNS的各插件与相关的配置属性、参数及详细使用方式请参考官方文档中的介绍：https://coredns.io/plugins/。 Headless Service资源解析常规的ClusterIP、NodePort和LoadBalancer类型的Service对象可通过不同的入口来接收和分发客户端请求，且它们都拥有集群IP地址（ClusterIP）。然而，个别场景也可能不必或无须使用Service对象的负载均衡功能以及集群IP地址，而是借助ClusterDNS服务来代替实现这部分功能。Kubernetes把这类不具有ClusterIP的Service资源形象地称为Headless Service，该Service的请求流量无须kube-proxy处理，也不会有负载均衡和路由相关的iptables或ipvs规则。至于ClusterDNS如何自动配置Headless Service，则取决于Service标签选择器的定义。 有标签选择器：由端点控制器自动创建与Service同名的Endpoint资源，而ClusterDNS则将Service名称的A记录直接解析为后端各端点的IP而非ClusterIP。 无标签选择器：ClusterDNS的配置分为两种情形，为ExternalName类型的服务（配置了spec.externalName字段）创建CNAME记录，而为与该Service同名的Endpoint对象上的每个端点创建一个A记录。显然，ClusterDNS对待无标签选择器的第二种情形的Headless Service与对待有标签选择器的Headless Service的方式相同，区别仅在于相应的Endpoint资源是否由端点控制器基于标签选择器自动创建。通常，我们把无标签选择器的第一种情形（使用CNAME记录）的Headless Service当作一种独立的Service类型使用，即ExternalName Service，而将那些把Service名称使用A记录解析为端点IP地址的类型统一称为Headless Service。 ExternalName ServiceExternalName Service是一种特殊类型的Service资源，它不需要使用标签选择器关联任何Pod对象，也无须定义任何端口或Endpoints，但必须要使用spec.externalName属性定义一个CNAME记录，用于返回真正提供服务的服务名称的别名。ClusterDNS会为这种类型的Service资源自动生成&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN CNAME &lt;extname&gt;.格式的DNS资源记录。下面配置清单示例（externalname-redis-svc.yaml）中定义了一个名为externalname-redis-svc的Service资源，它使用DNS CNAME记录指向集群外部的redis.ik8s.io这一FQDN。 1234567891011121314kind: ServiceapiVersion: v1metadata: name: externalname-redis-svc namespace: defaultspec: type: ExternalName externalName: redis.ik8s.io ports: - protocol: TCP port: 6379 targetPort: 6379 nodePort: 0 selector: &#123;&#125; 待Service资源externalname-redis-svc创建完成后，各Pod对象即可通过短格式或FQDN格式的Service名称访问相应的服务。ClusterDNS会把该名称以CNAME格式解析为.spec.externalName字段中的名称，而后通过DNS服务将其解析为相应主机的IP地址。我们可通过此前Pod对象client-pod对该名称进行解析测试。 12~$ kubectl exec -it client -- /bin/sh [root@client-pod /]# 未指定解析类型的，nslookup命令会对解析得到的CNAME结果自动进行更进一步的解析。例如下面命令中，请求解析externalname-redis-svc.default.svc.cluster.local名称得到CNAME格式的结果redis.ik8s.io将被进一步解析为A记录格式的结果。 1234567[root@client-pod /]# nslookup externalname-redis-svcServer: 10.96.0.10Address: 10.96.0.10#53externalname-redis-svc.default.svc.cluster.local canonical name = redis.ik8s.io.Name: redis.ik8s.ioAddress: 1.2.3.4 ExternalName用于通过DNS别名将外部服务发布到Kubernetes集群上，这类的DNS别名同本地服务的DNS名称具有相同的形式。因而Pod对象可像发现和访问集群内部服务一样来访问这些发布到集群之上的外部服务，这样隐藏了服务的位置信息，使得各工作负载能够以相同的方式调用本地和外部服务。等到了能够或者需要把该外部服务引入到Kubernetes集群上之时，管理员只需要修改相应ExternalName Service对象的类型为集群本地服务即可。 Headless Service除了为每个Service资源对象在创建时自动指派一个遵循&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的DNS名称，ClusterDNS还会为Headless Service中的每个端点指派一个遵循&lt;hostname&gt;. &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的DNS名称，因此，每个Headless Service资源对象的名称都会由ClusterDNS自动生成以下几种类型的资源记录。 1）根据端点IP地址的类型，在Service名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。 &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt; 2）根据端点IP地址的类型，在端点自身的hostname名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。 &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt; &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt; 3）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。 _&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. 4）对于每个给定的每个端点的主机名称的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4），都要生成PTR记录，它们各自的格式如下所示。 &lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.定义Service资源时，只需要将其ClusterIP字段的值显式设置为None即可将其定义为Headless类型。下面是一个Headless Service资源配置示例，它拥有标签选择器，因而能够自动创建同名的Endpoint资源。 123456789101112kind: ServiceapiVersion: v1metadata: name: demoapp-headless-svcspec: clusterIP: None selector: app: demoapp ports: - port: 80 targetPort: 80 name: http 将上面定义的Headless Service资源创建到集群上，我们从其资源详细描述中可以看出，demoapp-headless-svc没有ClusterIP，但因标签选择器能够匹配到Pod资源，因此它拥有端点记录。 12345678910111213~$ kubectl apply -f demoapp-headless-svc.yaml service/demoapp-headless-svc created~$ kubectl describe svc demoapp-headless-svcName: demoapp-headless-svcNamespace: defaultLabels: &lt;none&gt;Annotations: Selector: app=demoappType: ClusterIPIP: NonePort: http 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.16:80,10.244.2.10:80,10.244.3.11:80…… 根据Headless Service的工作特性可知，它记录在ClusterDNS的A记录的相关解析结果是后端端点的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源。下面依然通过Pod对象client-pod的交互式接口进行测试： 1234567891011~$ kubectl exec -it client-pod -- /bin/sh[root@client-pod /]# nslookup -query=A demoapp-headless-svcServer: 10.96.0.10Address: 10.96.0.10#53Name: demoapp-headless-svc.default.svc.cluster.localAddress: 10.244.3.11Name: demoapp-headless-svc.default.svc.cluster.localAddress: 10.244.1.16Name: demoapp-headless-svc.default.svc.cluster.localAddress: 10.244.2.10 其解析结果正是Headless Service通过标签选择器关联到的所有Pod资源的IP地址。于是，客户端向此Service对象发起的请求将直接接入Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源是由DNS服务器接收到查询请求时以轮询方式返回的IP地址。另一方面，每个IP地址的反向解析记录（PTR）对应的FQDN名称是相应端点所在主机的主机名称。对于Kubernetes上的容器来说，其所在主机的主机名是指Pod对象上的主机名称，它由Pod资源的spec.hostname字段和spec.subdomain组合定义，格式为&lt;hostname&gt;.subdomain&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，其中的&lt;subdomain&gt;可省略。若此两者都未定义，则&lt;hostname&gt;值取自IP地址，IP地址a.b.c.d对应的主机名为a-b-c-d，如下面命令的解析结果所示。 1234[root@client-pod /]# nslookup -query=PTR 10.244.3.11Server: 10.96.0.10Address: 10.96.0.10#5311.3.244.10.in-addr.arpa name = 10-244-3-11.demoapp-headless-svc.default.svc.cluster.local. StatefulSet控制器对象是Headless Service资源的一个典型应用场景，相关话题将会在第8章中详细描 本章小结本章重点讲解了Kubernetes的Service资源基础概念、类型、实现机制及其发布方式等话题，并介绍了服务发现及Headless Service。 Service资源通过标签选择器为一组任务负载创建一个统一的访问入口，它把客户端请求代理调度至后端各端点。 Service支持userspace、iptables和ipvs代理模型，iptables模式更为成熟稳定，而ipvs则在有大规模Service的场景中有着更好的性能表现。 ClusterIP是最基础的Service类型，它仅适用于集群内通信，NodePort和LoadBalancer能够将服务发布到集群外部；外部IP能够与这3种类型的Service组合使用，从而开放特定的IP接入外部流量。 Endpoint和EndpointSlice用于跟踪端点资源，并将端点信息提供给Service等。 Headless Service是没有ClusterIP的Service资源类型，它要么结合externalName以CNAME资源记录的形式映射至其他服务，要么以A记录或AAAA记录的形式解析至端点IP地址。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"}]},{"title":"Kubernetes基础","slug":"Kubernetes基础","date":"2022-02-09T09:56:37.000Z","updated":"2022-02-09T10:16:52.414Z","comments":true,"path":"2022/02/09/Kubernetes基础/","link":"","permalink":"https://marmotad.github.io/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Kubernetes集群架构Kubernetes属于典型的Server-Client形式的二层架构，在程序级别，Master主要由API Server（kube-apiserver）、Controller-Manager（kube-controller-manager）和Scheduler（kube-scheduler）这3个组件，以及一个用于集群状态存储的etcd存储服务组成，它们构成整个集群的控制平面；而每个Node节点则主要包含kubelet、kube-proxy及容器运行时（Docker是最为常用的实现）3个组件，它们承载运行各类应用容器。 Kubernetes系统组件Master组件Master 它维护有Kubernetes的所有对象记录，负责持续管理对象状态并响应集群中各种资源对象的管理操作，以及确保各资源对象的实际状态与所需状态相匹配。控制平面的各组件支持以单副本形式运行于单一主机，也能够将每个组件以多副本方式同时运行于多个主机上，提高服务可用级别。控制平面各组件及其主要功能如下。 API ServerAPI Server是Kubernetes控制平面的前端，支持不同类型应用的生命周期编排，包括部署、缩放和滚动更新等。它还是整个集群的网关接口，由kube-apiserver守护程序运行为服务，通过HTTP/HTTPS协议将RESTful API公开给用户，是发往集群的所有REST操作命令的接入点，用于接收、校验以及响应所有的REST请求，并将结果状态持久存储于集群状态存储系统（etcd）中。 集群状态存储(ETCD)Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中。 控制器管理器(kube-controller-manager)控制器负责实现用户通过API Server提交的终态声明，驱动API对象的当前状态逼近或等同于期望状态。Kubernetes提供了驱动Node、Pod、Server、Endpoint、ServiceAccount和Token等API对象的控制器。 调度器(kube-scheduler)Kubernetes系统上的调度是指为API Server接收到的每一个Pod创建请求，并在集群中为其匹配出一个最佳工作节点。kube-scheduler是默认调度器程序。 Node组件Node组件是集群的“体力”输出者，每个Node会定期向Master报告自身的状态变动，并接受Master的管理。 kubeletkubelet是运行于每个Node之上的“节点代理”服务，负责接收并执行Master发来的指令，以及管理当前Node上Pod对象的容器等任务。kubelet会持续监视当前节点上各Pod的健康状态，包括基于用户自定义的探针进行存活状态探测，并在任何Pod出现问题时将其重建为新实例。它还内置了一个HTTP服务器，监听TCP协议的10248和10250端口：10248端口通过/healthz响应对kubelet程序自身的健康状态进行检测；10250端口用于暴露kubelet API，以验证、接收并响应API Server的通信请求。 容器运行时环境Pod是一组容器组成的集合并包含这些容器的管理机制。kubelet通过CRI（容器运行时接口）可支持多种类型的OCI容器运行时，例如docker、containerd、CRI-O、runC、fraki和Kata Containers等。 kube-proxykube-proxy，它把API Server上的Service资源对象转换为当前节点上的iptables或（与）ipvs规则，这些规则能够将那些发往该Service对象ClusterIP的流量分发至它后端的Pod端点之上。kube-proxy是Kubernetes的核心网络组件，它本质上更像是Pod的代理及负载均衡器，负责确保集群中Node、Service和Pod对象之间的有效通信。 核心附件附件（add-ons）用于扩展Kubernetes的基本功能，它们通常运行于Kubernetes集群自身之上，可根据重要程度将其划分为必要和可选两个类别。网络插件是必要附件，管理员需要从众多解决方案中根据需要及项目特性选择，常用的有Flannel、Calico、Canal、Cilium和Weave Net等。KubeDNS通常也是必要附件之一，而Web UI（Dashboard）、容器资源监控系统、集群日志系统和Ingress Controller等是常用附件。 CoreDNSKubernetes使用定制的DNS应用程序实现名称解析和服务发现功能，它自1.11版本起默认使用CoreDNS——一种灵活、可扩展的DNS服务器；之前的版本中用到的是kube-dns项目，SkyDNS则是更早一代的解决方案。 Dashboard基于Web的用户接口，用于可视化Kubernetes集群。 容器资源监控系统Kubernetes常用的指标监控附件有Metrics-Server、kube-state-metrics和Prometheus等。 集群日志系统Kubernetes常用的集中式日志系统是由ElasticSearch、Fluentd和Kibana（称之为EFK）组合提供的整体解决方案。 Ingress Controller Pod与Service Pod本质上是共享Network、IPC和UTS名称空间以及存储资源的容器集合。 同一Pod内部的容器，它们共享网络协议栈、网络设备、路由、IP地址和端口等网络资源，可以基于本地回环接口lo互相通信。每个Pod上还可附加一组“存储卷”（volume）资源，它们同样可由内部所有容器使用而实现数据共享。持久类型的存储卷还能够确保在容器终止后被重启，甚至容器被删除后数据也不会丢失。同时，这些以Pod形式运行于Kubernetes之上的应用通常以服务类程序居多，其客户端可能来自集群之外，例如现实中的用户，也可能是当前集群中其他Pod中的应用，如图1-10所示。Kubernetes集群的网络模型要求其各Pod对象的IP地址位于同一网络平面内（同一IP网段），各Pod间可使用真实IP地址直接进行通信而无须NAT功能介入，无论它们运行于集群内的哪个工作节点之上，这些Pod对象就像是运行于同一局域网中的多个主机上。 Service是由基于匹配规则在集群中挑选出的一组Pod对象的集合、访问这组Pod集合的固定IP地址，以及对请求进行调度的方法等功能所构成的一种API资源类型，是Pod资源的代理和负载均衡器。Service匹配Pod对象的规则可用“标签选择器”进行体现，并根据标签来过滤符合条件的资源对象，如图1-11所示。标签是附加在Kubernetes API资源对象之上的具有辨识性的分类标识符，使用键值型数据表达，通常仅对用户具有特定意义。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上。 每个节点上运行的kube-proxy组件负责管理各Pod与Service之间的网络连接，它并非Kubernetes内置的代理服务器，而是一个基于出站流量的负载均衡器组件。针对每个Service，kube-proxy都会在当前节点上转换并添加相应iptables DNAT规则或ipvs规则，从而将目标地址为某Service对象的ClusterIP的流量调度至该Service根据标签选择器匹配出的Pod对象之上。CoreDNS附件会为集群中的每个Service对象（包括DNS服务自身)生成唯一的DNS名称标识，以及相应的DNS资源记录，服务的DNS名称遵循标准的svc.namespace.svc.cluster-domain格式。例如CoreDNS自身的服务名称为kube-dns.kube-system.svc.cluster.local.，则它的ClusterIP通常是10.96.0.10。除非出于管理目的有意调整，Service资源的名称和ClusterIP在其整个生命周期内都不会发生变动。kubelet会在创建Pod容器时，自动在/etc/resolv.conf文件中配置Pod容器使用集群上CoreDNS服务的ClusterIP作为DNS服务器，因而各Pod可针对任何Service的名称直接请求相应的服务。换句话说，Pod可通过kube-dns.kube-system.svc.cluster.local.来访问集群DNS服务。Ingress资源是Kubernetes将集群外部HTTP/HTTPS流量引入到集群内部专用的资源类型，它仅用于控制流量的规则和配置的集合，其自身并不能进行“流量穿透”，要通过Ingress控制器发挥作用；目前，此类的常用项目有Nginx、Traefik、Envoy、Gloo、kong及HAProxy等。 应用部署、运行与管理应用容器与Pod资源 同一Pod中，这些容器共享PID、IPC、Network和UTS名称空间的容器彼此间可通过IPC通信，共享使用主机名和网络接口、IP地址、端口和路由等各种网络资源，因而各容器进程能够通过lo网络接口通信且不能使用相同的网络套接字地址。一个Pod内通常仅应该运行具有强耦合关系的容器，否则除了pause以外，只应该存在单个容器，或者只存在单个主容器和一个以上的辅助类容器（例如服务网格中的Sidecar容器等）。 容器设计模式单容器模式单容器模式是指将应用程序封装为应用容器运行。该模式需要遵循简单和单一原则，每个容器仅承载一种工作负载。 单节点多容器模式单节点多容器模式的常见实现有Sidecar（边车）、适配器（Adapter）、大使（Ambassador）、初始化（Initializer）容器模式等。 (1) Sidecar模式Sidecar模式是多容器系统设计的最常用模式，它由一个主应用程序（通常是Web应用程序）以及一个辅助容器（Sidecar容器）组成，该辅助容器用于为主容器提供辅助服务以增强主容器的功能，是主应用程序是必不可少的一部分，但却不一定非得存在于应用程序本身内部。 sidecar的优势 辅助应用的运行时环境和编程语言与主应用程序无关，因而无须为每种编程语言分别开发一个辅助工具； 二者可基于IPC、lo接口或共享存储进行数据交换，不存在明显的通信延迟； 容器镜像是发布的基本单位，将主应用与辅助应用划分为两个容器使得其可由不同团队开发和维护，从而变得方便及高效，单独测试及集成测试也变得可能； 容器限制了故障边界，使得系统整体可以优雅降级，例如Sidecar容器异常时，主容器仍可继续提供服务； 容器是部署的基本单元，每个功能模块均可独立部署及回滚。事实上，这些优势对于其他模型来说同样存在。 (2) 大使模式大使模式本质上是一类代理程序，它代表主容器发送网络请求至外部环境中，因此可以将其视作与客户端（主容器应用）位于同一位置的“外交官”。 大使模式的最佳用例之一是提供对数据库的访问。实践中，开发环境、测试环境和生产环境中的主应用程序可能需要分别连接到不同的数据库服务。更好的方案是让应用程序始终通过localhost连接至大使容器，而如何正确连接到目标数据的责任则由大使容器完成。 (3) 适配器模式适配器模式（见图4-4）用于为主应用程序提供一致的接口，实现了模块重用，支持标准化和规范化主容器应用程序的输出以便于外部服务进行聚合。大使模式为内部容器提供了简化统一的外部服务视图，适配器模式则刚好反过来，它通过标准化容器的输出和接口，为外界展示了一个简化的应用视图。 (4) 初始化容器模式初始化容器模式（见图4-5）负责以不同于主容器的生命周期来完成那些必要的初始化任务， 初始化容器将Pod内部的容器分成了两组：初始化容器和应用程序容器（主容器和Sidecar容器等），初始化容器可以不止一个，但它们需要以特定的顺序串行运行，并需要在启动应用程序容器之前成功终止。不过，多个应用程序容器一般需要并行启动和运行。就Kubernetes来说，除了初始化容器之外，还有一些其他可用的初始化技术，例如admission controllers、admission webhooks和PodPresets等。 多节点模式(1) 领导者选举模式领导者选举模式示意图。 (2) 工作队列模式分布式应用程序的各组件间存在大量的事件传递需求，当某应用组件需要将信息广播至大量订阅者时，可能需要与多个独立开发的，可能使用了不同平台、编程语言和通信协议的应用程序或服务通信，并且无须订阅者实时响应地通信，它具有解耦子系统、提高伸缩能力和可靠性、支持延迟事件处理、简化异构组件间的集成等优势。图4-7为工作队列模式示意图。 (3) 分散/聚集分散/聚集模式与工作队列模式非常相似，它同样将大型任务拆分为较小的任务，区别是容器会立即将响应返回给用户，一个很好的例子是MapReduce算法。该模式需要两类组件：一个称为“根”节点或“父”节点的组件，将来自客户端的请求切分成多个小任务并分散到多个节点并行计算；另一类称为“计算”节点或“叶子”节点，每个节点负责运行一部分任务分片并返回结果数据，“根”节点收集这些结果数据并聚合为有意义的数据返回给客户端。开发这类分布式系统需要请求扇出、结果聚合以及与客户端交互等大量的模板代码，但大部分都比较通用。因而要实现该模式，我们只需要分别将两类组件各自构建为容器即可。 Pod的生命周期Kubernetes为Pod资源严格定义了5种相位，并将特定Pod对象的当前相位存储在其内部的子对象PodStatus的phase字段上，因而它总是应该处于其生命进程中以下几个相位之一。 Pending：API Server创建了Pod资源对象并已存入etcd中，但它尚未被调度完成，或仍处于从仓库中下载容器镜像的过程中。 Running：Pod已经被调度至某节点，所有容器都已经被kubelet创建完成，且至少有一个容器处于启动、重启或运行过程中。 Succeeded：Pod中的所有容器都已经成功终止且不会再重启。 Failed：所有容器都已经终止，但至少有一个容器终止失败，即容器以非0状态码退出或已经被系统终止。 Unknown：API Server无法正常获取到Pod对象的状态信息，通常是由于其无法与所在工作节点的kubelet通信所致。 阶段仅是对Pod对象生命周期运行阶段的概括性描述，而非Pod或内部容器状态的综合汇总，因此Pod对象的status字段中的状态值未必一定是可用的相位，它也有可能是Pod的某个错误状态，例如CrashLoopBackOff或Error等。Pod资源的核心职责是运行和维护称为主容器的应用程序容器，在其整个生命周期之中的多种可选行为也是围绕更好地实现该功能而进行，如图4-8所示。其中，初始化容器（init container）是常用的Pod环境初始化方式，健康状态检测（startupProbe、livenessProbe和readinessProbe）为编排工具提供了监测容器运行状态的编程接口，而事件钩子（preStop和postStart）则赋予了应用容器读取来自编排工具上自定义事件的机制。 若用户给出了上述全部定义，则一个Pod对象生命周期的运行步骤如下。 1）在启动包括初始化容器在内的任何容器之前先创建pause基础容器，它初始化Pod环境并为后续加入的容器提供共享的名称空间。2）按顺序以串行方式运行用户定义的各个初始化容器进行Pod环境初始化；任何一个初始化容器运行失败都将导致Pod创建失败，并按其restartPolicy的策略进行处理，默认为重启。3）待所有初始化容器成功完成后，启动应用程序容器，多容器Pod环境中，此步骤会并行启动所有应用容器，例如主容器和Sidecar容器，它们各自按其定义展开其生命周期；本步骤及后面的几个步骤都将以主容器为例进行说明；容器启动的那一刻会同时运行主容器上定义的PostStart钩子事件，该步骤失败将导致相关容器被重启。4）运行容器启动健康状态监测（startupProbe），判定容器是否启动成功；该步骤失败，同样参照restartPolicy定义的策略进行处理；未定义时，默认状态为Success。5）容器启动成功后，定期进行存活状态监测（liveness）和就绪状态监测（readiness）；存活状态监测失败将导致容器重启，而就绪状态监测失败会使得该容器从其所属的Service对象的可用端点列表中移除。6）终止Pod对象时，会先运行preStop钩子事件，并在宽限期（terminationGrace-PeriodSeconds）结束后终止主容器，宽限期默认为30秒。 在Pod中运行应用Pod资源中可同时存在初始化容器、应用容器和临时容器3种类型的容器，不过创建并运行一个具体的Pod对象时，仅有应用容器是必选项，并且可以仅为其定义单个容器。 使用单容器Pod资源一个Pod对象的核心职责在于以主容器形式运行单个应用，因而定义API资源的关键配置就在于定义该容器，它以对象形式定义在Pod对象的spec.containers字段中，基本格式如下： 12345678910apiVersion: v1kind: Podmetadata: name: … # Pod的标识名，在名称空间中必须唯一 namespace: … # 该Pod所属的名称空间，省略时使用默认名称空间，例如defaultspec: containers: # 定义容器，它是一个列表对象，可包括多个容器的定义，至少得有一个 - name: … # 容器名称，必选字段，在当前Pod中必须唯一 image: … # 创建容器时使用的镜像 imagePullPolicy: … # 容器镜像下载策略，可选字段 image虽为可选字段，这只是为方便更高级别的管理类资源（例如Deployment等）能覆盖它以实现某种高级管理功能而设置，对于非控制器管理的自主式Pod来说并不能省略该字段。 12345678910apiVersion: v1kind: Podmetadata: name: pod-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent 把上面的内容保存于配置文件pod-demo.yaml中，随后即可使用kubectl apply或kubectl create命令进行资源对象创建 12~$ kubectl apply -f pod-demo.yamlpod/pod-demo created 该Pod对象由调度器绑定至特定工作节点后，由相应的kubelet负责创建和维护，实时状态也将同步给API Server并由其存储至etcd中。Pod创建并尝试启动的过程中，可能会经历Pending、ContainerCreating、Running等多种不同的状态，若Pod可正常启动，则kubectl get pods/POD命令输出字段中的状态（STATUS）则显示为Running 123~$ kubectl get pods/pod-demo -n defaultNAME READY STATUS RESTARTS AGEpod-demo 1/1 Running 0 5m 随后即可对Pod中运行着的主容器的服务发起访问请求。镜像demoapp默认运行了一个Web服务程序，该服务监听TCP协议的80端口，镜像可通过“/”、/hostname、/user-agent、/livez、/readyz和/configs等路径服务于客户端的请求。例如，下面的命令先获取到Pod的IP地址，而后对其支持的Web资源路径/和/user-agent分别发出了一个访问请求： 12345~$ demoIP=$(kubectl get pods/pod-demo -o jsonpath=&#123;.status.podIP&#125;)~ $ curl -s http://$demoIPiKubernetes demoapp v1.0 ! ClientIP: 10.244.0.0, ServerName: pod-demo, ServerIP: 10.244.2.3!~$ curl -s http://$demoIP/user-agentUser-Agent: curl/7.58.0 容器的imagePullPolicy字段用于为其指定镜像获取策略，可用值包括如下几个。 Always：每次启动Pod时都要从指定的仓库下载镜像。 IfNotPresent：仅本地镜像缺失时方才从目标仓库wp下载镜像。 Never：禁止从仓库下载镜像，仅使用本地镜像。 对于标签为latest的镜像文件，其默认的镜像获取策略为Always，其他标签的镜像，默认策略则为IfNotPresent。需要注意的是，从私有仓库中下载镜像时通常需要事先到Registry服务器认证后才能进行。认证过程要么需要在相关节点上交互式执行docker login命令，要么将认证信息定义为专有的Secret资源，并配置Pod通过imagePullSecretes字段调用此认证信息完成。删除Pod对象则使用kubectl delete命令。 命令式命令：kubectl delete pods/NAME。 命令式对象配置：kubectl delete -f FILENAME。 若删除后Pod一直处于Terminating状态，则可再一次执行删除命令，并同时使用–force和–grace-period=0选项进行强制删除。 获取Pod与容器状态详情kubectl有多个子命令，用于从不同角度显示对象的状态信息，这些信息有助于用户了解对象的运行状态、属性详情等。 kubectl describe：显示资源的详情，包括运行状态、事件等信息，但不同的资源类型输出内容不尽相同。 kubectl logs：查看Pod对象中容器输出到控制台的日志信息；当Pod中运行有多个容器时，需要使用选项-c指定容器名称。 kubectl exec：在Pod对象某容器内运行指定的程序，其功能类似于docker exec命令，可用于了解容器各方面的相关信息或执行必需的设定操作等，具体功能取决于容器内可用的程序。 打印Pod对象的状态kubectl describe pods/NAME -n NAMESPACE命令可打印Pod对象的详细描述信息，包括events和controllers等关系的子对象等，Priority、Status、Containers和Events等字段通常是重点关注的目标字段。另外，也可以通过kubectl get pods/POD -o yaml|json命令的status字段来了解Pod的状态详情，它保存有Pod对象的当前状态。如下命令显示了pod-demo的状态信息，结果输出做了尽可能的省略。 12345678910111213141516171819202122232425262728~$ kubectl get pods/pod-demo -o yamlstatus: conditions: - lastProbeTime: null lastTransitionTime: &quot;2020-08-16T03:36:48Z&quot; message: &#x27;containers with unready status: [demo]&#x27; reason: ContainersNotReady status: &quot;False&quot; type: ContainersReady …… containerStatuses: # 容器级别的状态信息 - containerID: docker://…… image: ikubernetes/demoapp:v1.0 imageID: docker-pullable://ikubernetes/demoapp@sha256:…… lastState: &#123;&#125; # 前一次的状态 name: demo ready: true # 是否已经就绪 restartCount: 0 # 重启次数 started: true state: # 当前状态 running: startedAt: &quot;2020-08-16T03:36:48Z&quot; # 启动时间 hostIP: 172.29.9.12 # 节点IP phase: Running # Pod当前的相位 podIP: 10.244.2.3 # Pod的主IP地址 podIPs: # Pod上的所有IP地址 - ip: 10.244.2.3 qosClass: BestEffort # QoS类别 上面的命令结果中，conditions字段是一个称为PodConditions的数组，它记录了Pod所处的“境况”或者“条件”，其中的每个数组元素都可能由如下6个字段组成。 lastProbeTime：上次进行Pod探测时的时间戳。 lastTransitionTime：Pod上次发生状态转换的时间戳。 message：上次状态转换相关的易读格式信息。 reason：上次状态转换原因，用驼峰格式的单个单词表示。 status：是否为状态信息，可取值有True、False和Unknown。 type：境况的类型或名称，有4个固定值；PodScheduled表示已经与节点绑定；Ready表示已经就绪，可服务客户端请求；Initialized表示所有的初始化容器都已经成功启动；ContainersReady则表示所有容器均已就绪。 另外，containerStatuses字段描述了Pod中各容器的相关状态信息，包括容器ID、镜像和镜像ID、上一次的状态、名称、启动与否、就绪与否、重启次数和状态等。 查看容器日志kubectl logs POD [-c CONTAINER]命令可直接获取并打印控制台日志，不过，若Pod对象中仅运行有一个容器，则可以省略-c选项及容器名称。例如，下面的命令打印了pod-demo中唯一的主容器的控制台日志： 1234~$ kubectl logs pod-demo* Running on http://0.0.0.0:80/ (Press CTRL+C to quit)172.29.9.1 - - [16/Aug/2020 03:54:42] &quot;GET / HTTP/1.1&quot; 200 -172.29.9.11 - - [16/Aug/2020 03:54:50] &quot;GET / HTTP/1.1&quot; 200 - 在容器中额外运行其他程序kubectl exec可以让用户在Pod的某容器中运行用户所需要的任何存在于容器中的程序。在kubectl logs获取的信息不够全面时，此命令可以通过在Pod中运行其他指定的命令（前提是容器中存在此程序）来辅助用户获取更多信息。一个更便捷的使用接口是直接交互式运行容器中的某shell程序。例如，直接查看Pod中的容器运行的进程： 1234~$ kubectl exec pod-demo -- ps auxPID USER TIME COMMAND 1 root 0:01 python3 /usr/local/bin/demo.py 8 root 0:00 ps aux 注意:如果Pod中运行多个容器，需要使用-c 选项指定运行程序的容器名称。 有时候需要打开容器的交互式shell接口以方便多次执行命令，为kubectl exec命令额外使用-it选项，并指定运行镜像中可用的shell程序就能进入交互式接口 1234567~$ kubectl -it exec pod-demo /bin/sh[root@pod-demo /]# hostnamepod-demo[root@pod-demo /]# netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 自定义容器应用与参数容器镜像启动容器时运行的默认应用程序由其Dockerfile文件中的ENTRYPOINT指令进行定义，传递给程序的参数则通过CMD指令设定，ETRYPOINT指令不存在时，CMD可同时指定程序及其参数。例如，要了解镜像ikubernetes/demoapp:v1.0中定义的ENTRYPOINT和CMD，可以在任何存在此镜像的节点上执行类似如下命令来获取： 1234~# docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Entrypoint&#125;&#125;[/bin/sh -c python3 /usr/local/bin/demo.py]~# docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Cmd&#125;&#125;[] Pod配置中，spec.containers[].command字段可在容器上指定非镜像默认运行的应用程序，且可同时使用spec.containers[].args字段进行参数传递，它们将覆盖镜像中默认定义的参数。若定义了args字段，该字段值将作为参数传递给镜像中默认指定运行的应用程序；而仅定义了command字段时，其值将覆盖镜像中定义的程序及参数。下面的资源配置清单保存在pod-demo-with-cmd-and-args.yaml文件中，它把镜像ikubernetes/demoapp:v1.0的默认应用程序修改为/bin/sh -c，参数定义为python3 /usr/local/bin/demo.py -p 8080，其中的-p选项可修改服务监听的端口为指定的自定义端口 123456789101112apiVersion: v1kind: Podmetadata: name: pod-demo-with-cmd-and-args namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;python3 /usr/local/bin/demo.py -p 8080&#x27;] 下面将上述清单中定义的Pod对象创建到集群上，验证其监听的端口是否从默认的80变为了指定的8080： 123456~$ kubectl create -f pod-demo-with-cmd-and-args.yaml pod/pod-demo-with-cmd-and-args created~$ kubectl exec pod-demo-with-cmd-and-args -- netstat -tnl Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 容器环境变量容器环境变量需要应用程序支持通过环境变量进行配置，否则用户要在制作Docker镜像时通过entrypoint脚本完成环境变量到程序配置文件的同步。向Pod对象中容器环境变量传递数据的方法有两种：env和envFrom，这里重点介绍第一种方式，第二种方式将在介绍ConfigMap和Secret资源时进行说明。通过环境变量的配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构成的列表。每个环境变量通常由name和value字段构成。 name ：环境变量的名称，必选字段。 value ：传递给环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。 示例中使用镜像demoapp中的应用服务器支持通过HOST与PORT环境变量分别获取监听的地址和端口，它们的默认值分别为0.0.0.0和80，下面的配置保存在清单文件pod-using-env.yaml中，它分别为HOST和PORT两个环境变量传递了一个不同的值，以改变容器监听的地址和端口。 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-using-env namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: HOST value: &quot;127.0.0.1&quot; - name: PORT value: &quot;8080&quot; 下面将清单文件中定义的Pod对象创建至集群中，并查看应用程序监听的地址和端口来验证配置结果： 123456~$ kubectl apply -f pod-using-env.yamlpod/pod-using-env created~$ kubectl exec pod-using-env -- netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN Pod的创建与删除过程Pod资源对象的创建过程。 1）用户通过kubectl或其他API客户端提交Pod Spec给API Server。2）API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。3）Scheduler（调度器）通过其watcher监测到API Server创建了新的Pod对象，于是为该Pod对象挑选一个工作节点并将结果信息更新至API Server。4）调度结果信息由API Server更新至etcd存储系统，并同步给Scheduler。5）相应节点的kubelet监测到由调度器绑定于本节点的Pod后会读取其配置信息，并由本地容器运行时创建相应的容器启动Pod对象后将结果回存至API Server。6）API Server将kubelet发来的Pod状态信息存入etcd系统，并将确认信息发送至相应的kubelet。另一方面，Pod可能曾用于处理生产数据或向用户提供服务等，Kubernetes可删除宽限期确保终止操作能够以平滑方式优雅完成，从而用户也可以在正常提交删除操作后获知何时开始终止并最终完成。删除时，用户提交请求后系统即会进行强制删除操作的宽限期倒计时，并将TERM信号发送给Pod对象中每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，Pod对象也随即由API Server删除。如果在等待进程终止的过程中kubelet或容器管理器发生了重启，则终止操作会重新获得一个满额的删除宽限期并重新执行删除操作。 Pod的终止过程 如图4-10所示，一个典型的Pod对象终止流程如下。 1）用户发送删除Pod对象的命令。2）API服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认为30秒），Pod被视为dead。3）将Pod标记为Terminating状态。4）（与第3步同时运行）kubelet在监控到Pod对象转为Terminating状态的同时启动Pod关闭过程。5）（与第3步同时运行）端点控制器监控到Pod对象的关闭行为时将其从所有匹配到此端点的Service资源的端点列表中移除。6）如果当前Pod对象定义了preStop钩子句柄，在其标记为terminating后即会以同步方式启动执行；如若宽限期结束后，preStop仍未执行完，则重新执行第2步并额外获取一个时长为2秒的小宽限期。7）Pod对象中的容器进程收到TERM信号。8）宽限期结束后，若存在任何一个仍在运行的进程，Pod对象即会收到SIGKILL信号。9）Kubelet请求API Server将此Pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。默认情况下，所有删除操作的宽限期都是30秒，不过kubectl delete命令可以使用–grace-period=选项自定义其时长，使用0值则表示直接强制删除指定的资源，不过此时需要同时为命令使用–force选项。 暴露容器服务不考虑通过Service资源进行服务暴露的情况下，服务于集群外部的客户端的常用方式有两种：一种是在其运行的节点上进行端口映射，由节点IP和选定的协议端口向Pod内的应用容器进行DNAT转发；另一种是让Pod共享其所在的工作节点的网络名称空间，应用进程将直接监听工作节点IP地址和协议端口。 其他容器端口映射其他Kubernetes系统的网络模型中，各Pod的IP地址处于同一网络平面，无论是否为容器指定了要暴露的端口都不会影响集群中其他节点之上的Pod客户端对其进行访问，这意味着，任何在非本地回环接口lo上监听的端口都可直接通过Pod网络被请求。从这个角度来说，容器端口只是信息性数据，它仅为集群用户提供了一个快速了解相关Pod对象的可访问端口的途径，但显式指定容器的服务端口可额外为其赋予一个名称以方便按名称调用。定义容器端口的ports字段的值是一个列表，由一到多个端口对象组成，它的常用嵌套字段有如下几个。 containerPort ：必选字段，指定在Pod对象的IP地址上暴露的容器端口，有效范围为(0,65536)；使用时，需要指定为容器应用程序需要监听的端口。 name ：当前端口的名称标识，必须符合IANA_SVC_NAME规范且在当前Pod内要具有唯一性；此端口名可被Service资源按名调用。 protocol ：端口相关的协议，其值仅支持TCP、SCTP或UDP三者之一，默认为TCP。 需要借助于Pod所在节点将容器服务暴露至集群外部时，还需要使用hostIP与hostPort两个字段来指定占用的工作节点地址和端口。如图4-11所示的Pod A与Pod C可分别通过各自所在节点上指定的hostIP和hostPort服务于客户端请求。 hostPort ：主机端口，它将接收到的请求通过NAT机制转发至由container-Port字段指定的容器端口。 hostIP ：主机端口要绑定的主机IP，默认为主机之上所有可用的IP地址；该字段通常使用默认值。 下面的资源配置清单示例（pod-using-hostport.yaml）中定义的demo容器指定了要暴露容器上TCP协议的80端口，并将之命名为http，该容器可通过工作节点的10080端口接入集群外部客户端的请求。 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-using-hostport namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP hostPort: 10080 在集群中创建配置清单中定义的Pod对象后，需获取其被调度至的目标节点，例如下面第二个命令结果中的k8s-node02.ilinux.io/172.29.9.12，而后从集群外部向该节点的10080端口发起Web请求进行访问测试： 123456~$ kubectl apply -f pod-using-hostport.yaml pod/pod-using-hostport~$ kubectl describe pods/ pod-using-hostport | grep &quot;^Node:&quot;Node: k8s-node02.ilinux.io/172.29.9.12~$ curl 172.29.9.12:10080iKubernetes demoapp v1.0 !! ClientIP: 172.29.0.1, ServerName: pod-using-hostport, ServerIP: 10.244.2.9! 注意，hostPort与NodePort类型的Service对象暴露端口的方式不同，NodePort是通过所有节点暴露容器服务，而hostPort则能经由Pod对象所在节点的IP地址进行。 配置Pod使用节点网络同一个Pod对象的各容器运行于一个独立、隔离的Network、UTS和IPC名称空间中，共享同一个网络协议栈及相关的网络设备，但也有些特殊的Pod对象需要运行于所在节点的名称空间中，执行系统级的管理任务（例如查看和操作节点的网络资源甚至是网络设备等），或借助节点网络资源向集群外客户端提供服务等，如图4-12中的右图所示。 由kubeadm部署的Kubernetes集群中的kube-apiserver、kube-controller-manager、kube-scheduler，以及kube-proxy和kube-flannel等通常都是第二种类型的Pod对象。网络名称空间是Pod级别的属性，用户配置的Pod对象，仅需要设置其spec.hostNetwork的属性为true即可创建共享节点网络名称空间的Pod对象，如下面保存在pod-using-hostnetwork.yaml文件中的配置清单所示。 1234567891011apiVersion: v1kind: Podmetadata: name: pod-using-hostnetwork namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent hostNetwork: true 将上面配置清单中定义的pod-using-hostnetwork创建于集群上，并查看主机名称或网络接口的相关属性信息以验证它是否能共享使用工作节点的网络名称空间。 1234~$ kubectl apply -f pod-using-hostnetwork.yaml pod/pod-using-hostnetwork created~$ kubectl exec -it pod-using-hostnetwork -- hostnamek8s-node01.ilinux.io 上面第二个命令的结果显示出的主机名称，表示该Pod已然共享了其所在节点的UTS名称空间，以及Network和IPC名称空间。这意味着，Pod对象中运行容器化应用可在其所在的工作节点的IP地址之上监听，这可以通过直接向k8s-node01.ilinux.io节点发起请求来验证。 12~$ curl k8s-node01.ilinux.ioiKubernetes demoapp v1.0 !! ClientIP: 172.29.9.1, ServerName: k8s-node01.ilinux.io, ServerIP: 172.29.9.11! 容器安全上下文Kubernetes为安全运行Pod及容器运行设计了安全上下文机制，该机制允许用户和管理员定义Pod或容器的特权与访问控制，以配置容器与主机以及主机之上的其他容器间的隔离级别。安全上下文就是一组用来决定容器是如何创建和运行的约束条件，这些条件代表创建和运行容器时使用的运行时参数。需要提升容器权限时，用户通常只应授予容器执行其工作所需的访问权限，以“最小权限法则”来抑制容器对基础架构及其他容器产生的负面影响。Kubernetes支持用户在Pod及容器级别配置安全上下文，并允许管理员通过Pod安全策略在集群全局级别限制用户在创建和运行Pod时可设定的安全上下文。本节仅描述Pod和容器级别的配置，Pod安全策略的话题将在第9章展开。Pod和容器的安全上下文设置包括以下几个方面。 自主访问控制（DAC）：传统UNIX的访问控制机制，它允许对象（OS级别，例如文件等）的所有者基于UID和GID设定对象的访问权限。 Linux功能：Linux为突破系统上传统的两级用户（root和普通用户）授权模型，而将内核管理权限打散成多个不同维度或级别的权限子集，每个子集称为一种“功能”或“能力”，例如CAP_NET_ADMIN、CAP_SYS_TIME、CAP_SYS_PTRACE和CAP_SYS_ADMIN等，从而允许进程仅具有一部分内核管理功能就能完成必要的管理任务。 seccomp：全称为secure computing mode，是Linux内核的安全模型，用于为默认可发起的任何系统调用进程施加控制机制，人为地禁止它能够发起的系统调用，有效降低了程序被劫持时的危害级别。 AppArmor：全称为Application Armor，意为“应用盔甲”，是Linux内核的一个安全模块，通过加载到内核的配置文件来定义对程序的约束与控制。 SELinux：全称为Security-Enhanced Linux，意为安全加强的Linux，是Linux内核的一个安全模块，提供了包括强制访问控制在内的访问控制安全策略机制。 Privileged模式：即特权模式容器，该模式下容器中的root用户拥有所有的内核功能，即具有真正的管理员权限，它能看到主机上的所有设备，能够挂载文件系统，甚至可以在容器中运行容器；容器默认运行于非特权（unprivileged）模式。 AllowPrivilegeEscalation：控制是否允许特权升级，即进程是否能够获取比父进程更多的特权；运行于特权模式或具有CAP_SYS_ADMIN能力的容器默认允许特权升级。这些安全上下文相关的特性多数嵌套定义在Pod或容器的securityContext字段中，而且有些特性对应的嵌套字段还不止一个。而seccomp和AppArmor的安全上下文则需要以资源注解的方式进行定义，而且仅能由管理员在集群级别进行Pod安全策略配置。 配置格式速览安全上下文可分别设置Pod级别和容器级别。但有些参数并不适合通用设定，例如特权模式、特权升级、只读根文件系统和内核能力等，它们只可用于容器之上。但也有参数仅可用于Pod级别进行通用设定，例如设置内核参数的sysctl和设置存储卷新件文件默认属组的fsgroup等。下面以Pod资源的配置格式给出了这些配置选项，以便于读者快速预览和了解安全上下文的用法。 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: &#123;…&#125;spec: securityContext: # Pod级别的安全上下文，对内部所有容器均有效 runAsUser &lt;integer&gt; # 以指定的用户身份运行容器进程，默认由镜像中的USER指定 runAsGroup &lt;integer&gt; # 以指定的用户组运行容器进程，默认使用的组随容器运行时设定 supplementalGroups &lt;[]integer&gt; # 为容器中1号进程的用户添加的附加组 fsGroup &lt;integer&gt; # 为容器中的1号进程附加一个专用组，其功能类似于sgid runAsNonRoot &lt;boolean&gt; # 是否以非root身份运行 seLinuxOptions &lt;Object&gt; # SELinux的相关配置 sysctls &lt;[]Object&gt; # 应用到当前Pod名称空间级别的sysctl参数设置列表 windowsOptions &lt;Object&gt; # Windows容器专用的设置 containers: - name: … image: … securityContext: # 容器级别的安全上下文，仅在当前容器生效 runAsUser &lt;integer&gt; # 以指定的用户身份运行容器进程 runAsGroup &lt;integer&gt; # 以指定的用户组运行容器进程 runAsNonRoot &lt;boolean&gt; # 是否以非root身份运行 allowPrivilegeEscalation &lt;boolean&gt; # 是否允许特权升级 capabilities &lt;Object&gt; # 为当前容器添加（add）或删除（drop）内核能力 add &lt;[]string&gt; # 添加由列表定义的各内核能力 drop &lt;[]string&gt; # 移除由列表定义的各内核能力 privileged &lt;boolean&gt; # 是否运行为特权容器 procMount &lt;string&gt; # 设置容器的procMount类型，默认为DefaultProcMount； readOnlyRootFilesystem &lt;boolean&gt; # 是否将根文件系统设置为只读模式 seLinuxOptions &lt;Object&gt; # SELinux的相关配置 windowsOptions &lt;Object&gt; # Windows容器专用的设置 Kubernetes默认以非特权模式创建并运行容器，同时禁用了其他与管理功能相关的内核能力，但未额外设定其他上下文参数。 管理容器进程的运行身份制作Docker镜像时，Dockerfile支持以USER指令明确指定运行应用进程时的用户身份。对于未通过USER指令显式定义运行身份的镜像，创建和启动容器时，其进程的默认用户身份为容器中的root用户和root组，该用户有着其他一些附加的系统用户组，例如sys、daemon、wheel和bin等。然而，有些应用程序的进程需要以特定的专用用户身份运行，或者以指定的用户身份运行时才能获得更好的安全特性，这种需求可以在Pod或容器级别的安全上下文中使用runAsUser得以解决，必要时可同时使用runAsGroup设置进程的组身份。下面的资源清单（securitycontext-runasuer-demo.yaml）配置以1001这个UID和GID的身份来运行容器中的demoapp应用，考虑到非特权用户默认无法使用1024以下的端口号，文件中通过环境变量改变了应用监听的端口。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: securitycontext-runasuser-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: PORT value: &quot;8080&quot; securityContext: runAsUser: 1001 runAsGroup: 1001 下面的命令先将配置清单中定义的Pod对象securitycontext-runasuser-demo创建到集群上，随后的两条命令验证了容器用户身份确为配置中预设的UID和GID。 1234567~$ kubectl apply -f securitycontext-runasuser-demo.yaml pod/securitycontext-runasuser-demo created~$ kubectl exec securitycontext-runasuser-demo -- id uid=1001 gid=1001$ kubectl exec securitycontext-runasuser-demo -- ps auxPID USER TIME COMMAND 1 1001 0:00 python3 /usr/local/bin/demo.py 还可在上面的配置清单中的安全上下文定义中，同时使用supplement-Groups选项定义主进程用户的其他附加用户组，这对于有着复杂权限模型的应用是一个非常有用的选项。另外，若运行容器时使用的镜像文件中已经使用USER指令指定了非root用户的运行身份，我们也可以在安全上下文中使用runAsNonRoot参数定义容器必须使用指定的非root用户身份运行，而无须使用runAsUser参数额外指定用户。 管理容器的内核功能传统UNIX仅实现了特权和非特权两类进程，前者是指以0号UID身份运行的进程，而后者则是从属非0号UID用户的进程。Linux内核从2.2版开始将附加于超级用户的权限分割为多个独立单元，这些单元是线程级别的，它们可配置在每个线程之上，为其赋予特定的管理能力。Linux内核常用的功能包括但不限于如下这些。 CAP_CHOWN：改变文件的UID和GID。 CAP_MKNOD：借助系统调用mknod()创建设备文件。 CAP_NET_ADMIN：网络管理相关的操作，可用于管理网络接口、netfilter上的iptables规则、路由表、透明代理、TOS、清空驱动统计数据、设置混杂模式和启用多播功能等。 CAP_NET_BIND_SERVICE：绑定小于1024的特权端口，但该功能在重新映射用户后可能会失效。 AP_NET_RAW：使用RAW或PACKET类型的套接字，并可绑定任何地址进行透明代理。 CAP_SYS_ADMIN：支持内核上的很大一部分管理功能。 CAP_SYS_BOOT：重启系统。 CAP_SYS_CHROOT：使用chroot()进行根文件系统切换，并能够调用setns()修改Mount名称空间。 CAP_SYS_MODULE：装载内核模块。 CAP_SYS_TIME：设定系统时钟和硬件时钟。 CAP_SYSLOG：调用syslog()执行日志相关的特权操作等。 系统管理员可以通过get命令获取程序文件上的内核功能，并可使用setcap命令为程序文件设定内核功能或取消（-r选项）其已有的内核功能。而为Kubernetes上运行的进程设定内核功能则需要在Pod内容器上的安全上下文中嵌套capabilities字段，添加和移除内核能力还需要分别在下一级嵌套中使用add或drop字段。这两个字段可接受以内核能力名称为列表项，但引用各内核能力名称时需移除CAP_前缀，例如可使用NET_ADMIN和NET_BIND_SERVICE这样的功能名称。下面的配置清单（securitycontext-capabilities-demo.yaml）中定义的Pod对象的demo容器，在安全上下文中启用了内核功能NET_ADMIN，并禁用了CHOWN。demo容器的镜像未定义USER指令，它将默认以root用户的身份运行容器应用。 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: securitycontext-capabilities-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;,&quot;-c&quot;] args: [&quot;/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80 &amp;&amp; /usr/bin/python3 /usr/local/bin/demo.py&quot;] securityContext: capabilities: add: [&#x27;NET_ADMIN&#x27;] drop: [&#x27;CHOWN&#x27;] 容器中的root用户将默认映射为系统上的普通用户，它实际上并不具有管理网络接口、iptables规则和路由表等相关的权限，但内核功能NET_ADMIN可以为其开放此类权限。但容器中的root用户默认就具有修改容器文件系统上的文件从属关系的能力，而禁用CHOWN功能则关闭了这种操作权限。下面创建该Pod对象并运行在集群上，来验证清单中的配置。 12~ $ kubectl apply -f securitycontext-capabilities-demo.yaml pod/securitycontext-capabilities-demo created 而后，检查Pod网络名称空间中netfilter之上的规则，清单中的iptables命令添加的规则位于NAT表的PREROUTING链上。下面的命令结果表示iptables命令已然生成的规则，NET_ADMIN功能启用成功。 1234$ kubectl exec securitycontext-capabilities-demo -- iptables -t nat -nL PREROUTING Chain PREROUTING (policy ACCEPT)target prot opt source destination REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 redir ports 80 接着，下面用于检查demo容器中的root用户是否能够修改容器文件系统上文件的属主和属组的命令结果表示，其CHOWN功能已然成功关闭。 123$ kubectl exec securitycontext-capabilities-demo -- chown 200.200 /etc/hostschown: /etc/hosts: Operation not permittedcommand terminated with exit code 1 内核的各项功能均可按其原本的意义在容器的安全上下文中按需打开或关闭，但SYS_ADMIN功能拥有内核中的许多管理权限，实在太过强大，出于安全方面的考虑，用户应该基于最小权限法则组合使用内核功能完成容器运行。 特权模式容器相较于内核功能，SYS_ADMIN赋予了进程很大一部分的系统级管理功能，特权（privileged）容器几乎将宿主机内核的完整权限全部开放给了容器进程，它提供的是远超SYS_ADMIN的授权，包括写操作到/proc和/sys目录以及管理硬件设备等，因而仅应该用到基础架构类的系统级管理容器之上。例如，使用kubeadm部署的集群中，kube-proxy中的容器就运行于特权模式。下面的第一个命令从kube-system名称空间中取出一个kube-proxy相关的Pod对象名称，第二个命令则用于打印该Pod对象的配置清单，限于篇幅，这里仅列出了其中一部分内容： 123456789101112131415~$ pod-name=$(kubectl get pods -l k8s-app=kube-proxy -n kube-system \\ -o jsonpath=&#123;.items[0].metadata.name&#125;)~$ kubectl get pods $pod-name -n kube-system -o yaml#从命令结果中截取的启动容器应用的命令及传递的参数containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=$(NODE_NAME) image: …… imagePullPolicy: IfNotPresent name: kube-proxy resources: &#123;&#125; securityContext: privileged: true 上面保留的命令结果的最后两行是定义特权容器的格式，唯一用到的privileged字段只能嵌套在容器的安全上下文中，它使用布尔型值，true表示启用特权容器机制。 在Pod上使用sysctlLinux系统上的sysctl接口允许在运行时修改内核参数，管理员可通过/proc/sys/下的虚拟文件系统接口来修改或查询这些与内核、网络、虚拟内存或设备等各子系统相关的参数。Kubernetes也允许在Pod上独立安全地设置支持名称空间级别的内核参数，它们默认处于启用状态，而节点级别内核参数则被认为是不安全的，它们默认处于禁用状态。截至目前，仅kernel.shm_rmid_forced、net.ipv4.ip_local_port_range和net.ipv4.tcp_syncookies这3个内核参数被Kubernetes视为安全参数，它们可在Pod安全上下文的sysctl参数内嵌套使用，而余下的绝大多数的内核参数都是非安全参数，需要管理员手动在每个节点上通过kubelet选项逐个启用后才能配置到Pod上。例如，在各工作节点上编辑/etc/default/kubelet文件，添加如下内容以允许在Pod上使用指定的两个非安全的内核参数，并重启kubelet服务使之生效。 1KUBELET_EXTRA_ARGS=&#x27;--allowed-unsafe-sysctls=net.core.somaxconn,net.ipv4.ip_unprivileged_port_start&#x27; net.core.somaxconn参数定义了系统级别入站连接队列最大长度，默认值是128；而net.ipv4.ip_unprivileged_port_start参数定义的是非特权用户可以使用的内核端口起始值，默认为1024，它限制了非特权用户所能够使用的端口范围。下面配置清单（securitycontext-sysctls-demo.yaml）中定义的Pod对象在安全上下文中通过sysctls字段嵌套使用了一个安全的内核参数kernel.shm_rmid_forced，以及一个已经启用的非安全内核参数net.ipv4.ip_unprivileged_port_start，它将该非安全内核参数的值设置为0来允许非特权用户使用11024以内端口的权限。 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: securitycontext-sysctls-demo namespace: defaultspec: securityContext: sysctls: - name: kernel.shm_rmid_forced value: &quot;0&quot; - name: net.ipv4.ip_unprivileged_port_start value: &quot;0&quot; containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent securityContext: runAsUser: 1001 runAsGroup: 1001 尽管上面配置清单设定了以非特权用户1001的身份运行容器应用，但受上面内核参数的影响，非管理员用户也具有了监听80端口的权限，因而不会遇到无法监听特权端口的情形。下面将配置清单中定义的资源创建在集群之上，来验证设定的结果。 12~$ kubectl apply -f securitycontext-sysctls-demo.yaml pod/securitycontext-sysctls-demo created 下面的命令结果显示，以普通用户身份运行的demo容器成功监听了TCP协议的80端口。 1234~ $ kubectl exec securitycontext-sysctls-demo -- netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1/python3 在Pod对象之上启用非安全内核参数，其配置结果可能会存在无法预料的结果，在正式使用之前一定要经过充分测试。例如，在某一Pod之上同时配置启用前面示例的两个非安全内核参数可能存在生效结果异常的情况。本节中介绍了设置Pod与容器安全上下文配置方法及几种常用使用方式。从示例中我们可以看出，设置特权容器和添加内核功能等，以及在Pod上共享宿主机的Network和PID名称空间等，对于多项目或多团队共享的Kubernetes集群存在着不小的安全隐患，这就要求管理员应该在集群级别使用Pod安全策略（PodSecurityPolicy），来精心管控这些与安全相关配置的运用能力。 容器应用的管理接口健康状态监测接口监测容器自身运行的API包括分别用于健康状态检测、指标、分布式跟踪和日志等实现类型，如图4-13所示。即便没有完全实现，至少容器化应用也应该提供用于健康状态检测（liveness和readiness）的API，以便编排系统能更准确地判定应用程序的运行状态。 Kubelet仅能在控制循环中根据容器主进程的运行状态来判断其健康与否，主进程以非0状态码退出代表处于不健康状态，其他均为正常状态。然而，有些异常场景中，仍处于运行状态的进程内部的业务处理机制可能已然处于僵死状态或陷入死循环等，无法正常处理业务请求，对于这种状态的判断便要依赖应用自身专用于健康状态监测的接口。存活状态（liveness）检测用于定期检测容器是否正常运行，就绪状态（readiness）检测用于定期检测容器是否可以接收流量，它们能够通过减少运维问题和提高服务质量来使服务更健壮和更具弹性。Kubernetes在Pod内部的容器资源上提供了livenessProbe和readinessProbe两个字段，分别让用户自定义容器应用的存活状态和就绪状态检测。 存活状态检测：用于判定容器是否处于“运行”状态；若此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为Success。 就绪状态检测：用于判断容器是否准备就绪并可对外提供服务；未通过该检测时，端点控制器（例如Service对象）会将其IP地址从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中；未定义就绪状态检测的容器的默认状态为Success。容器探测是Pod对象生命周期中的一项重要日常任务，它由kubelet周期性执行。kubelet可在活动容器上分别执行由用户定义的启动状态检测（startupProbe）、存活状态检测（livenessProbe）和就绪状态检测（readinessProbe），定义在容器上的存活状态和就绪状态操作称为检测探针，它要通过容器的句柄（handler）进行定义。Kubernetes定义了用于容器探测的3种句柄。 ExecAction：通过在容器中执行一个命令并根据其返回的状态码进行的诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。 TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常状态，否则为不健康状态。 HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起http GET请求进行诊断，响应码为2xx或3xx即为成功，否则为失败。上面的每种探测方式都可能存在3种返回结果：Success（成功）、Failure（失败）或Unknown（未知），仅第一种结果表示成功通过检测。另外，Kubernetes自v1.16版本起还支持启动状态（startup）检测。将传统模式开发的大型应用程序迁移至容器编排平台运行时，可能需要相当长的时间进行启动后的初始化，但其初始过程是否正确完成的检测机制和探测参数都可能有别于存活状态检测，例如需要更长的间隔周期和更高的错误阈值等。该类检测的结果处理机制与存活状态检测相同，检测失败时kubelet将杀死容器并根据其restartPolicy决定是否将其重启，而未定义时的默认状态为Success。需要注意的是，一旦定义了启动检测探针，则必须等启动检测成功完成之后，存活探针和就绪探针才可启动。 容器存活状态检测存活性探测是隶属于容器级别的配置，kubelet可基于它判定何时需要重启容器。目前，Kubernetes在容器上支持的存活探针有3种类型：ExecAction、TCPSocketAction和HTTPGetAction。 1. 存活探针配置格式Pod配置格式中，spec.containers.livenessProbe字段用于定义此类检测，配置格式如下所示。但一个容器之上仅能定义一种类型的探针，即exec、httpGet和tcpSocket三者互斥，它们不可在一个容器同时使用。 12345678910111213spec: containers: - name: … image: … livenessProbe: exec &lt;Object&gt; # 命令式探针 httpGet &lt;Object&gt; # http GET类型的探针 tcpSocket &lt;Object&gt; # tcp Socket类型的探针 initialDelaySeconds &lt;integer&gt; # 发起初次探测请求的延后时长 periodSeconds &lt;integer&gt; # 请求周期 timeoutSeconds &lt;integer&gt; # 超时时长 successThreshold &lt;integer&gt; # 成功阈值 failureThreshold &lt;integer&gt; # 失败阈值 探针之外的其他字段用于定义探测操作的行为方式，用户没有明确定义这些属性字段时，它们会使用各自的默认值: initialDelaySeconds ：首次发出存活探测请求的延迟时长，即容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，即容器启动后便立刻进行探测；该参数值应该大于容器的最大初始化时长，以避免程序永远无法启动。 timeoutSeconds ：存活探测的超时时长，显示为timeout属性，默认为1秒，最小值也为1秒；应该为此参数设置一个合理值，以避免因应用负载较大时的响应延迟导致Pod被重启。 periodSeconds ：存活探测的频度，显示为period属性，默认为10秒，最小值为1秒；需要注意的是，过高的频率会给Pod对象带来较大的额外开销，而过低的频率又会使得对错误反应不及时。 successThreshold ：处于失败状态时，探测操作至少连续多少次的成功才被认为通过检测，显示为#success属性，仅可取值为1。 failureThreshold：处于成功状态时，探测操作至少连续多少次的失败才被视为检测不通过，显示为#failure属性，默认值为3，最小值为1；尽量设置宽容一些的失败计数，能有效避免一些场景中的服务级联失败。使用kubectl describe命令查看配置了存活性探测的Pod对象的详细信息时，其相关容器中会输出类似如下一行内容，它给出了探测方式及其额外的配置属性delay、timeout、period、success和failure及其各自的相关属性值。 1Liveness: …… delay=0s timeout=1s period=10s #success=1 #failure=3 exec探针exec类型的探针通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，命令状态返回值为0表示“成功”通过检测，其余值均为“失败”状态。spec.containers.livenessProbe.exec字段只有一个可用属性command，用于指定要执行的命令。demoapp应用程序通过/livez输出内置的存活状态检测接口，服务正常时，它以200响应码返回OK，否则为5xx响应码，我们可基于exec探针使用HTTP客户端向该path发起请求，并根据命令的结果状态来判定容器健康与否。系统刚启动时，对该路径的请求将会延迟大约不到5秒的时长，且默认响应值为OK。它还支持由用户按需向该路径发起POST请求，并向参数livez传值来自定义其响应内容。下面是定义在资源清单文件liveness-exec-demo.yaml中的示例。 123456789101112131415apiVersion: v1kind: Podmetadata: name: liveness-exec-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: exec: command: [&#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;[ &quot;$(curl -s 127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;] initialDelaySeconds: 5 periodSeconds: 5 该配置清单中定义的Pod对象为demo容器定义了exec探针，它通过在容器本地执行测试命令来比较curl -s 127.0.0.1/livez的返回值是否为OK以判定容器的存活状态。命令成功执行则表示容器正常运行，否则3次检测失败之后则将其判定为检测失败。首次检测在容器启动5秒之后进行，请求间隔也是5秒。 12~$ kubectl apply -f liveness-exec-demo.yaml pod/liveness-exec-demo created 创建完成后，Pod中的容器demo会正常运行，存活检测探针也不会遇到检测错误而导致容器重启。若要测试存活状态检测的效果，可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。 1~$ kubectl exec liveness-exec-demo -- curl -s -X POST -d &#x27;livez=FAIL&#x27; 127.0.0.1/livez 而后经过1个检测周期，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令结果中的事件可知，容器因健康状态检测失败而被重启。 12345~$ kubectl describe pods/liveness-exec-demo……Events: Warning Unhealthy 17s (x3 over 27s) kubelet, k8s-node03.ilinux.io Liveness probe failed:Normal Killing 17s kubelet, k8s-node03.ilinux.io Container demo failed liveness probe, will be restarted 另外，下面输出信息中的Containers一段中还清晰显示了容器健康状态检测及状态变化的相关信息：容器当前处于Running状态，但前一次是为Terminated，原因是退出码为137的错误信息，它表示进程是被外部信号所终止。137事实上由两部分数字之和生成：128+signum，其中signum是导致进程终止的信号的数字标识，9表示SIGKILL，这意味着进程是被强行终止的。 12345678910111213Containers: demo: …… State: Running Started: Thu, 29 Aug 2020 14:30:02 +0800 Last State: Terminated Reason: Error Exit Code: 137 Started: Thu, 29 Aug 2020 14:22:20 +0800 Finished: Thu, 29 Aug 2020 14:30:02 +0800 Ready: True Restart Count: 1…… 待容器重启完成后，/livez的响应内容会重置镜像中默认定义的OK，因而其存活状态检测不会再遇到错误，这模拟了一种典型的通过“重启”应用而解决问题的场景。需要特别说明的是，exec指定的命令运行在容器中，会消耗容器的可用计算资源配额，另外考虑到探测操作的效率等因素，探测操作的命令应该尽可能简单和轻量。 HTTP探针HTTP探针是基于HTTP协议的探测（HTTPGetAction），通过向目标容器发起一个GET请求，并根据其响应码进行结果判定，2xx或3xx类的响应码表示检测通过。HTTP探针可用配置字段有如下几个。▪host ：请求的主机地址，默认为Pod IP；也可以在httpHeaders使用“Host:”来定义。▪port ：请求的端口，必选字段。▪httpHeaders &lt;[]Object&gt;：自定义的请求报文头部。▪path ：请求的HTTP资源路径，即URL path。▪scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。下面是一个定义在资源清单文件liveness-httpget-demo.yaml中的示例，它使用HTTP探针直接对/livez发起访问请求，并根据其响应码来判定检测结果。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: liveness-httpget-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 scheme: HTTP initialDelaySeconds: 5 上面清单文件中定义的httpGet测试中，请求的资源路径为/livez，地址默认为Pod IP，端口使用了容器中定义的端口名称http，这也是明确为容器指明要暴露的端口的用途之一。下面测试其效果，首先创建此Pod对象： 12~ $ kubectl apply -f liveness-httpget-demo.yamlpod/liveness-httpget-demo created 首次检测为延迟5秒，这刚好超过了demoapp的/livez接口默认会延迟响应的时长。镜像中定义的默认响应是以200状态码响应、以OK为响应结果，存活状态检测会成功完成。为了测试存活状态检测的效果，同样可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。~$ kubectl exec liveness-httpget-demo – curl -s -X POST -d ‘livez=FAIL’ 127.0.0.1/livez而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。 1234~ $ kubectl describe pods/liveness-httpget-demo……Warning Unhealthy 7s (x3 over 27s) kubelet, k8s-node01.ilinux.io Liveness probe failed: HTTP probe failed with statuscode: 520 Normal Killing 7s kubelet, k8s-node01.ilinux.io Container demo failed liveness probe, will be restarted 一般来说，HTTP探针应该针对专用的URL路径进行。这种检测方式仅对分层架构中的当前一层有效，例如，它能检测应用程序工作正常与否的状态，但重启操作却无法解决其后端服务（例如数据库或缓存服务）导致的故障。此时，容器可能会被反复重启，直到后端服务恢复正常。其他两种检测方式也存在类似的问题。 TCP探针TCP探针是基于TCP协议进行存活性探测（TCPSocketAction），通过向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。相比较来说，它比基于HTTP协议的探测要更高效、更节约资源，但精准度略低，毕竟连接建立成功未必意味着页面资源可用。spec.containers.livenessProbe.tcpSocket字段用于定义此类检测，它主要有以下两个可用字段：1）host ：请求连接的目标IP地址，默认为Pod自身的IP；2）port ：请求连接的目标端口，必选字段，可以名称调用容器上显式定义的端口。下面是一个定义在资源清单文件liveness-tcpsocket-demo.yaml中的示例，它向Pod对象的TCP协议的80端口发起连接请求，并根据连接建立的状态判定测试结果。为了能在容器中通过iptables阻止接收对80端口的请求以验证TCP检测失败，下面的配置还在容器上启用了特殊的内核权限NET_ADMIN。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: liveness-tcpsocket-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 securityContext: capabilities: add: - NET_ADMIN livenessProbe: tcpSocket: port: http periodSeconds: 5 initialDelaySeconds: 20 按照配置，将该清单中的Pod对象创建在集群之上，20秒之后即会进行首次的tcpSocket检测。 12~$ kubectl apply -f liveness-tcpsocket-demo.yaml pod/liveness-tcpsocket-demo created 容器应用demoapp启动后即监听于TCP协议的80端口，tcpSocket检测也就可以成功执行。为了测试效果，可使用下面的命令在Pod的Network名称空间中设置iptables规则以阻止对80端口的请求： 1~$ kubectl exec liveness-tcpsocket-demo -- iptables -A INPUT -p tcp --dport 80 -j REJECT 而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。 123456~ $ kubectl describe pods/liveness-httpget-demo……Events:……Warning Unhealthy 3s (x3 over 23s) kubelet, k8s-node03.ilinux.io Liveness probe failed: dial tcp 10.244.3.19:80: i/o timeout Normal Killing 3s kubelet, k8s-node03.ilinux.io Container demo failed liveness probe, will be restarted 不过，重启容器并不会导致Pod资源的重建操作，网络名称空间的设定附加在pause容器之上，因而添加的iptables规则在应用重启后依然存在，它是一个无法通过重启而解决的问题。若需要手消除该问题，删除添加至Pod中的iptables规则即可。 Pod的重启策略Pod对象的应用容器因程序崩溃、启动状态检测失败、存活状态检测失败或容器申请超出限制的资源等原因都可能导致其被终止，此时是否应该重启则取决于Pod上的restartPolicy（重启策略）字段的定义，该字段支持以下取值。1）Always：无论因何原因、以何种方式终止，kubelet都将重启该Pod，此为默认设定。2）OnFailure：仅在Pod对象以非0方式退出时才将其重启。3）Never：不再重启该Pod。需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一个节点上重新启动Pod对象的相关容器。首次需要重启的容器，其重启操作会立即进行，而再次重启操作将由kubelet延迟一段时间后进行，反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么被终止，直到节点故障、被删除或被驱逐。 容器就绪状态检测Pod对象启动后，应用程序通常需要一段时间完成其初始化过程，例如加载配置或数据、缓存初始化等，甚至有些程序需要运行某类预热过程等，因此通常应该避免在Pod对象启动后立即让其处理客户端请求，而是需要等待容器初始化工作执行完成并转为“就绪”状态，尤其是存在其他提供相同服务的Pod对象的场景更是如此。与存活探针不同的是，就绪状态检测是用来判断容器应用就绪与否的周期性（默认周期为10秒钟）操作，它用于检测容器是否已经初始化完成并可服务客户端请求。与存活探针触发的操作不同，检测失败时，就绪探针不会杀死或重启容器来确保其健康状态，而仅仅是通知其尚未就绪，并触发依赖其就绪状态的其他操作（例如从Service对象中移除此Pod对象），以确保不会有客户端请求接入此Pod对象。就绪探针也支持Exec、HTTP GET和TCP Socket这3种探测方式，且它们各自的定义机制与存活探针相同。因而，将容器定义中的livenessProbe字段名替换为readinessProbe，并略做适应性修改即可定义出就绪性检测的配置来，甚至有些场景中的就绪探针与存活探针的配置可以完全相同。demoapp应用程序通过/readyz暴露了专用于就绪状态检测的接口，它于程序启动约15秒后能够以200状态码响应、以OK为响应结果，也支持用户使用POST请求方法通过readyz参数传递自定义的响应内容，不过所有非OK的响应内容都被响应以5xx的状态码。一个简单的示例如下面的配置清单（readiness-httpget-demo.yaml）所示。 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: readiness-httpget-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 2 periodSeconds: 5 failureThreshold: 3 restartPolicy: Always 下面来测试该Pod就绪探针的作用。按照配置，将Pod对象创建在集群上约15秒后启动首次探测，在该探测结果成功返回之前，Pod将一直处于未就绪状态： 12~ $ kubectl apply -f readiness-httpget-demo.yamlpod/readiness-httpget-demo created 接着运行kubectl get -w命令监视其资源变动信息，由如下命令结果可知，尽管Pod对象处于Running状态，但直到就绪检测命令执行成功后Pod资源才转为“就绪”。 1234~$ kubectl get pods/readiness-httpget-demo -wNAME READY STATUS RESTARTS AGEreadiness-httpget-demo 0/1 Running 0 10sreadiness-httpget-demo 1/1 Running 0 20s Pod运行过程中的某一时刻，无论因何原因导致的就绪状态检测的连续失败都会使得该Pod从就绪状态转变为“未就绪”，并且会从各个通过标签选择器关联至该Pod对象的Service后端端点列表中删除。为了测试就绪状态检测效果，下面修改/readyz响应以非OK内容。 1~$ kubectl exec readiness-httpget-demo -- curl -s -XPOST -d &#x27;readyz=FAIL&#x27; 127.0.0.1/readyz 而后在至少1个检测周期之后，通过该Pod的描述信息可以看到就绪检测失败相关的事件描述，命令及结果如下所示： 123~$ kubectl describe pods/readiness-httpget-demo……Warning Unhealthy 4s (x11 over 54s) kubelet, k8s-node03.ilinux.io Readiness probe failed: HTTP probe failed with statuscode: 521 未定义就绪性检测的Pod对象在进入Running状态后将立即“就绪”，这在容器需要时间进行初始化的场景中可能会导致客户请求失败。因此，生产实践中，必须为关键性Pod资源中的容器定义就绪探针。 容器生命周期容器生命周期接口工作示意图: 容器需要处理来自平台的最重要事件是SIGTERM信号，任何需要“干净”关闭进程的应用程序都需要捕捉该信号进行必要处理，例如释放文件锁、关闭数据库连接和网络连接等，而后尽快终止进程，以避免宽限期过后强制关闭信号SIGKILL的介入。SIGKILL信号是由底层操作系统接收的，而非应用进程，一旦检测到该信号，内核将停止为相应进程提供内核资源，并终止进程正在使用的所有CPU线程，类似于直接切断了进程的电源。但是，容器应用很可能是功能复杂的分布式应用程序的一个组件，仅依赖信号终止进程很可能不足以完成所有的必要操作。因此，容器还需要支持postStart和preStop事件，前者常用于为程序启动前进行预热，后者则一般在“干净”地关闭应用之前释放占用的资源。生命周期钩子函数lifecycle hook是编程语言（例如Angular）中常用的生命周期管理组件，它实现了程序运行周期中的关键时刻的可见性，并赋予用户为此采取某种行动的能力。类似地，容器生命周期钩子使它能够感知自身生命周期管理中的事件，并在相应时刻到来时运行由用户指定的处理程序代码。Kubernetes同样为容器提供了postStart和preStop两种生命周期钩子。 postStart：在容器创建完成后立即运行的钩子句柄（handler），该钩子定义的事件执行完成后容器才能真正完成启动过程，如图4-15中的左图所示；不过Kubernetes无法确保它一定会在容器的主应用程序（由ENTRYPOINT定义）之前运行。 preStop：在容器终止操作执行之前立即运行的钩子句柄，它以同步方式调用，因此在其完成之前会阻塞删除容器的操作；这意味着该钩子定义的事件成功执行并退出，容器终止操作才能真正完成，如图4-15中的右图所示。 钩子句柄的实现方式类似于容器探针句柄的类型，同样有exec、httpGet和tcpSocket这3种，它们各自的配置格式和工作逻辑也完全相同，exec在容器中执行用户定义的一个或多个命令，httpGet在容器中向指定的本地URL发起HTTP连接请求，而tcpSocket则试图与指定的端口建立TCP连接。postStart和preStop句柄定义在容器的lifecycle字段中，其内部一次仅支持嵌套使用一种句柄类型。下面的配置清单（lifecycle-demo.yaml）示例中同时使用了postStart和preStop钩子处理相应的事件。 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: lifecycle-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent securityContext: capabilities: add: - NET_ADMIN livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 scheme: HTTP initialDelaySeconds: 5 lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-ports 80&#x27;] preStop: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;while killall python3; do sleep 1; done&#x27;] restartPolicy: Always 示例中的demo容器通过postStart执行iptables命令设置端口重定向规则，将发往该Pod IP的8080端口的所有请求重定向至80端口，从而让容器应用能够同时从8080端口接收请求。demo容器又借助preStop执行killall命令，它假设该命令能够更优雅地终止基于Python3运行的容器应用demoapp。将清单中的Pod对象创建于集群中便可展开后续的测试： 12~$ kubectl apply -f lifecycle-demo.yaml pod/lifecycle-demo created 而后可获取容器内网络名称空间中PREROUTING链上的iptables规则，验证postStart钩子事件的执行结果： 1234~$ kubectl exec lifecycle-demo -- iptables -t nat -nL PREROUTING Chain PREROUTING (policy ACCEPT)target prot opt source destination REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 redir ports 80 上面的配置清单中有意同时添加了httpGet类型的存活探针，我们可以人为地将探针检测结果置为失败状态，以促使kubelet重启demo容器验证preStop钩子事件的执行。不过，该示例中给出的操作是终止容器应用，那么容器成功重启即验证了相应脚本的运行完成。 多容器Pod容器设计模式中的单节点多容器模型中，初始化容器和Sidecar容器是目前使用较多的模式，尤其是服务网格的发展极大促进了Sidecar容器的应用。 初始化容器初始化代码要首先运行，且只能运行一次，它们常用于验证前提条件、基于默认值或传入的参数初始化对象实例的字段等。Pod中的初始化容器（Init Container）功能与此类似，它们为那些有先决条件的应用容器完成必要的初始设置，例如设置特殊权限、生成必要的iptables规则、设置数据库模式，以及获取最新的必要数据等。有很多场景都需要在应用容器启动之前进行部分初始化操作，如等待其他关联组件服务可用、基于环境变量或配置模板为应用程序生成配置文件、从配置中心获取配置等。初始化容器的典型应用需求有如下几种。 用于运行需要管理权限的工具程序，例如iptables命令等，出于安全等方面的原因，应用容器不适合拥有运行这类程序的权限。 提供主容器镜像中不具备的工具程序或自定义代码。 为容器镜像的构建和部署人员提供了分离、独立工作的途径，部署人员使用专用的初始化容器完成特殊的部署逻辑，从而使得他们不必协同起来制作单个镜像文件。 初始化容器和应用容器处于不同的文件系统视图中，因此可分别安全地使用敏感数据，例如Secrets资源等。 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得以满足。 Pod对象中的所有初始化容器必须按定义的顺序串行运行，直到它们全部成功结束才能启动应用容器，因而初始化容器通常很小，以便它们能够以轻量的方式快速运行。某初始化容器运行失败将会导致整个Pod重新启动（重启策略为Never时例外），初始化容器也必将再次运行，因此需要确保所有初始化容器的操作具有幂等性，以避免无法预知的副作用。Pod资源的spec.initContainers字段以列表形式定义可用的初始化容器，其嵌套可用字段类似于spec.containers。下面的资源清单（init-container-demo.yaml）在Pod对象上定义了一个名为iptables-init的初始化容器示例。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: init-container-demo namespace: defaultspec: initContainers: # 定义初始化容器 - name: iptables-init image: ikubernetes/admin-box:latest imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80&#x27;] securityContext: capabilities: add: - NET_ADMIN containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 示例中，应用容器demo默认监听TCP协议的80端口，但我们又期望该Pod能够在TCP协议的8080端口通过端口重定向方式为客户端提供服务，因此需要在其网络名称空间中添加一条相应的iptables规则。但是，添加该规则的iptables命令依赖于内核中的网络管理权限，出于安全原因，我们并不期望应用容器拥有该权限，因而使用了拥有网络管理权限的初始化容器来完成此功能。下面先把配置清单中定义的资源创建于集群之上： 12~$ kubectl apply -f init-container-demo.yaml pod/init-container-demo created 随后，在Pod对象init-container-demo的描述信息中的初始化容器信息段可以看到如下内容，它表明初始化容器启动后大约1秒内执行完成返回0状态码并成功退出。 123456789101112Command: /bin/sh -cArgs: iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80State: Terminated Reason: Completed Exit Code: 0 Started: Sun, 30 Aug 2020 11:44:28 +0800 Finished: Sun, 30 Aug 2020 11:44:29 +0800Ready: TrueRestart Count: 0 这表明，向Pod网络名称空间中添加iptables规则的操作已经完成，我们可通过应用容器来请求查看这些规则，但因缺少网络管理权限，该查看请求会被拒绝： 1234~$ kubectl exec init-container-demo -- iptables -t nat -vnLiptables v1.8.3 (legacy): can&#x27;t initialize iptables table `nat&#x27;: Permission denied (you must be root)Perhaps iptables or your kernel needs to be upgraded.command terminated with exit code 3 另一方面，应用容器中的服务却可以正常通过Pod IP的8080端口接收并响应，如下面的命令及执行结果所示： 123~$ podIP=$(kubectl get pods/init-container-demo -o jsonpath=&#123;.status.podIP&#125;)~$ curl http://$&#123;podIP&#125;:8080iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: init-container-demo, … 由此可见，初始化容器及容器的postStop钩子都能完成特定的初始化操作，但postStop必须在应用容器内部完成，它依赖的条件（例如管理权限）也必须为应用容器所有，这无疑会为应用容器引入安全等方面的风险。另外，考虑到应用容器镜像内部未必存在执行初始化操作的命令或程序库，使用初始化容器也就成了不二之选。 Sidecar容器Sidecar容器是Pod中与主容器松散耦合的实用程序容器，遵循容器设计模式，并以单独容器进程运行，负责运行应用的非核心功能，以扩展、增强主容器。Sidecar模式最著名的用例是充当服务网格中的微服务的代理应用（例如Istio中的数据控制平面Envoy），其他典型使用场景包括日志传送器、监视代理和数据加载器等。下面的配置清单（sidecar-container-demo.yaml）中定义了两个容器：一个是运行demoapp的主容器demo，一个运行envoy代理的Sidecar容器proxy。 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: sidecar-container-demo namespace: defaultspec: containers: - name: proxy image: envoyproxy/envoy-alpine:v1.13.1 command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;] lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;wget -O /etc/envoy/envoy.yaml https:// raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/ master/chapter4/envoy.yaml&#x27;] - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: HOST value: &quot;127.0.0.1&quot; - name: PORT value: &quot;8080&quot; Envoy程序是服务网格领域著名的数据平面实现，它在Istio服务网格中以Sidecar的模式同每一个微服务应用程序单独组成一个Pod，负责代理该微服务应用的所有通信事件，并为其提供限流、熔断、超时、重试等多种高级功能。这里我们将demoapp视作一个微服务应用，配置Envoy为其代理并调度入站（Ingress）流量，因而在示例中demo容器基于环境变量被配置为监听127.0.0.1地址上一个特定的8080端口，而proxy容器将监听Pod所有IP地址上的80端口，以接收客户端请求。proxy容器上的postStart事件用于为Envoy代理下载一个适用的配置文件，以便将proxy接收到的所有请求均代理至demo容器。下面说明整个测试过程。先将配置清单中定义的对象创建到集群之上。 12~$ kubectl apply -f sidecar-container-demo.yaml pod/sidecar-container-demo created 随后，等待Pod中的两个容器成功启动且都转为就绪状态，可通过各Pod内端口监听的状态来确认服务已然正常运行。下面命令的结果表示，Envoy已经正常运行并监听了TCP协议的80端口和9901端口（Envoy的内置管理接口）。 123456$ kubectl exec sidecar-container-demo -c proxy -- netstat -tnlp Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:9901 0.0.0.0:* LISTEN 1/envoytcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1/envoytcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN - 接下来，我们向Pod的80端口发起HTTP请求，若它能以demoapp的页面响应，则表示代理已然成功运行，甚至可以根据响应头部来判断其是否有代理服务Envoy发来的代理响应，如下面的命令及结果所示。 12345678910~$ podIP=$(kubectl get pods/sidecar-container-demo -o jsonpath=&#123;.status.podIP&#125;)$ curl http://$podIPiKubernetes demoapp v1.0 !! ClientIP: 127.0.0.1, ServerName: sidecar-container-demo, ……~$ curl -I http://$podIPHTTP/1.1 200 OKcontent-type: text/html; charset=utf-8content-length: 108server: envoydate: Sun, 22 May 2020 06:43:04 GMTx-envoy-upstream-service-time: 3 虽然Sidecar容器可以称得上是Pod中的常规容器，但直到v1.18版本，Kubernetes才将其添加作为内置功能。在此之前，Pod中的各应用程序彼此间没有区别，用户无从预测和控制容器的启动及关闭顺序，但多数场景都要求Sidecar容器必须要先于普通应用容器启动以做一些准备工作，例如分发证书、创建存储卷或获取一些数据等，且它们需要晚于其他应用容器终止。Kubernetes从v1.18版本开始支持用户在生命周期字段中将容器标记为Sidecar，这类容器全部转为就绪状态后，普通应用容器方可启动。因而，这个新特性根据生命周期将Pod的容器重新划分成了初始化容器、Sidecar容器和应用容器3类。所有的Sidecar容器都是应用容器，唯一不同之处是，需要手动为Sidecar容器在lifecycle字段中嵌套定义type类型的值为Sidecar。配置格式如下所示： 12345678910spec: containers: - name: proxy image: envoyproxy/envoy-alpine:v1.13.1 lifecycle: type: Sidecar …… - name: demo image: ikubernetes/demoapp:v1.0 …… 另外，可能也有一些场景需要Sidecar容器启动晚于普通应用容器，这种特殊的应用需求，目前可通过OpernKruise项目中的SidecarSet提供的PostSidecar模型来解决。将来，该项目或许支持以DAG的方式来灵活编排容器的启动顺序。 资源需求与资源限制资源需求与限制在Kubernetes上，可由容器或Pod请求与消费的“资源”主要是指CPU和内存（RAM），它可统称为计算资源，另一种资源是事关可用存储卷空间的存储资源。相比较而言，CPU属于可压缩型资源，即资源额度可按需弹性变化，而内存（当前）则是不可压缩型资源，CPU和内存资源的配置主要在Pod对象中的容器上进行，并且每个资源存在如图4-16所示的需求和限制两种类型。 资源需求：定义需要系统预留给该容器使用的资源最小可用值，容器运行时可能用不到这些额度的资源，但用到时必须确保有相应数量的资源可用。 资源限制：定义该容器可以申请使用的资源最大可用值，超出该额度的资源使用请求将被拒绝；显然，该限制需要大于等于requests的值，但系统在某项资源紧张时，会从容器回收超出request值的那部分。 在Kubernetes系统上，1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1 core）相当于1000个微核心（millicores，以下简称为m），因此500m相当于是0.5个核心，即1/2个核心。内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。 容器资源需求下面的配置清单示例（resource-requests-demo.yaml）中的自主式Pod要求为stress容器确保128MiB的内存及1/5个CPU核心（200m）资源可用。Pod运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时stress容器也会尽可能多地占用CPU资源，另外再启动一个专用的CPU压力测试进程（-c 1）。stress-ng是一个多功能系统压力测试具，master/worker模型，master为主进程，负载生成和控制子进程，worker是负责执行各类特定测试的子进程，例如测试CPU的子进程，以及测试RAM的子进程等。 12345678910111213apiVersion: v1kind: Podmetadata: name: stress-podspec: containers: - name: stress image: ikubernetes/stress-ng command: [&quot;/usr/bin/stress-ng&quot;, &quot;-m 1&quot;, &quot;-c 1&quot;, &quot;-metrics-brief&quot;] resources: requests: memory: &quot;128Mi&quot; cpu: &quot;200m&quot; 上面的配置清单中，stress容器请求使用的CPU资源大小为200m，这意味着一个CPU核心足以确保其以期望的最快方式运行。另外，配置清单中期望使用的内存大小为128MiB，不过其运行时未必真的会用到这么多。考虑到内存为非压缩型资源，当超出时存在因OOM被杀死的可能性，于是请求值是其理想中使用的内存空间上限。接下来创建并运行此Pod对象以对其资源限制效果进行检查。因为显示结果涉及资源占用比例等，因此同样的测试配置对不同的系统环境来说，其结果也会有所不同，作者为测试资源需求和资源限制功能而使用的系统环境中，每个节点的可用CPU核心数为8，物理内存空间为16GB。 1~$ kubectl create -f resource-requests-demo.yaml 而后在Pod资源的容器内运行top命令，观察CPU及内存资源占用状态，如下所示。其中{stress-ng-vm}是执行内存压测的子进程，它默认使用256MB的内存空间，{stress-ng-cpu}是执行CPU压测的专用子进程。 123456789~$ kubectl exec stress-pod -- topMem: 2884676K used, 13531796K free, 27700K shrd, 2108K buff, 1701456K cachedCPU: 25% usr 0% sys 0% nic 74% idle 0% io 0% irq 0% sirqLoad average: 0.57 0.60 0.71 3/435 15PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND9 8 root R 262m 2% 6 13% &#123;stress-ng-vm&#125; /usr/bin/stress-ng7 1 root R 6888 0% 3 13% &#123;stress-ng-cpu&#125; /usr/bin/stress-ng1 0 root S 6244 0% 1 0% /usr/bin/stress-ng -c 1 -m 1 --met…… top命令的输出结果显示，每个测试进程的CPU占用率为13%（实际12.5%），{stress-ng-vm}的内存占用量为262MB（VSZ），此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用范围内尽量多地占用相关的资源。两个测试线程分布于两个CPU核心，以满载的方式运行，系统共有8个核心，因此其使用率为25%（2/8）。另外，节点上的内存资源充裕，所以，尽管容器的内存用量远超128MB，但它依然可以运行。一旦资源紧张时，节点仅保证该容器有1/5个CPU核心（其需求中的定义）可用。在有着8个核心的节点上来说，它的占用率为2.5%，于是每个进程占比为1.25%，多占用的资源会被压缩。内存为非可压缩型资源，该Pod对象在内存资源紧张时可能会因OOM被杀死。对于压缩型的资源CPU来说，若未定义容器的资源请求用量，以确保其最小可用资源量，该Pod占用的CPU资源可能会被其他Pod对象压缩至极低的水平，甚至到该Pod对象无法被调度运行的境地。而对于非压缩型内存资源来说，资源紧缺情形下可能导致相关的容器进程被杀死。因此，在Kubernetes系统上运行关键型业务相关的Pod时，必须要使用requests属性为容器明确定义资源需求。当然，我们也可以为Pod对象定义较高的优先级来改变这种局面。集群中的每个节点都拥有定量的CPU和内存资源，调度器将Pod绑定至节点时，仅计算资源余量可满足该Pod对象需求量的节点才能作为该Pod运行的可用目标节点。也就是说，Kubernetes的调度器会根据容器的requests属性定义的资源需求量来判定哪些节点可接收并运行相关的Pod对象，而对于一个节点的资源来说，每运行一个Pod对象，该Pod对象上所有容器requests属性定义的请求量都要给予预留，直到节点资源被绑定的所有Pod对象瓜分完毕为止。 容器资源限制一旦定义资源限制，分配资源时，可压缩型资源CPU的控制阀可自由调节，容器进程也就无法获得超出其CPU配额的可用值。但是，若进程申请使用超出limits属性定义的内存资源时，该进程将可能被杀死。不过，该进程随后仍可能会被其控制进程重启，例如，当Pod对象的重启策略为Always或OnFailure时，或者容器进程存在有监视和管理功能的父进程等。下面的配置清单文件（resource-limits-demo.yaml）中定义使用simmemleak镜像运行一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而被杀死。 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: memleak-pod labels: app: memleakspec: containers: - name: simmemleak image: ikubernetes/simmemleak imagePullPolicy: IfNotPresent resources: requests: memory: &quot;64Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;1&quot; 下面将配置清单中定义的Pod对象创建到集群中，测试资源限制的实施效果。 12~$ kubectl apply -f resource-limits-demo.yamlpod/memleak-pod created Pod资源的默认重启策略为Always，于是在simmemleak容器因内存资源达到硬限制而被终止后会立即重启，因此用户很难观察到其因OOM而被杀死的相关信息。不过，多次因内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制（退避算法），即每次重启的时间间隔会不断地拉长，因而用户看到Pod对象的相关状态通常为CrashLoopBackOff。 123~$ kubectl get pods -l app=memleak NAME READY STATUS RESTARTS AGEmemleak-pod 0/1 CrashLoopBackOff 1 24s Pod对象的重启策略在4.5.3节介绍过，这里不再赘述。我们可通过Pod对象的详细描述了解其相关状态，例如下面的命令及部分结果所示。 1234567891011~]$ kubectl describe pods memleak-podName: memleak-pod……Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Mon, 31 Aug 2020 12:42:50 +0800 Finished: Mon, 31 Aug 2020 12:42:50 +0800 Ready: False Restart Count: 3…… 上面的命令结果中，OOMKilled表示容器因内存耗尽而被终止，因此为limits属性中的memory设置一个合理值至关重要。与资源需求不同的是，资源限制并不影响Pod对象的调度结果，即一个节点上的所有Pod对象的资源限制数量之和可以大于节点拥有的资源量，即支持资源的过载使用（overcommitted）。不过，这么一来，一旦内存资源耗尽，几乎必然地会有容器因OOMKilled而终止。另外需要说明的是，Kubernetes仅会确保Pod对象获得它们请求的CPU时间额度，它们能否取得额外（throttled）的CPU时间，则取决于其他正在运行作业的CPU资源占用情况。例如对于总数为1000m的CPU资源来说，容器A请求使用200m，容器B请求使用500m，在不超出它们各自最大限额的前下，则余下的300m在双方都需要时会以2 : 5（200m : 500m）的方式进行配置。 容器可见资源在容器中运行top等命令观察资源可用量信息时，容器可用资源受限于requests和limits属性中的定义，但容器中可见的资源量依然是节点级别的可用总量。例如，为前面定义的stress-pod添加如下limits属性定义。 123limits: memory: &quot;512Mi&quot; cpu: &quot;400m&quot; 重新创建stress-pod对象，并在其容器内分别列出容器可见的内存和CPU资源总量，命令及结果如下所示。 1234~$ kubectl exec stress-pod -- cat /proc/meminfo | grep ^MemTotalMemTotal: 16416472 kB$ kubectl exec stress-pod -- cat /proc/cpuinfo | grep -c ^processor8 命令结果中显示其可用内存资源总量为16416472 kB（16GB），CPU核心数为8个，这是节点级的资源数量，而非由容器的limits属性所定义的512MiB和400m。较为典型的是在Pod中运行Java应用程序时，若未使用-Xmx选项指定JVM的堆内存可用总量，则会默认设置为主机内存总量的一个空间比例（例如30%），这会导致容器中的应用程序申请内存资源时很快达到上限，而转为OOMKilled状态。另外，即便使用了-Xmx选项设置其堆内存上限，但该设置对非堆内存的可用空间不产生任何限制作用，仍然存在达到容器内存资源上限的可能性。另一个典型代表是在Pod中运行Nginx应用时，其配置参数worker_processes的值设置为auto，则会创建与可见CPU核心数量等同的worker进程数，若容器的CPU可用资源量远小于节点所需资源量时，这种设置在较大的访问负荷下会产生严重的资源竞争，并且会带来更多的内存资源消耗。一种较为妥当的解决方案是使用Downward API将limits定义的资源量暴露给容器，这将在后面的章节中予以介绍。 Pod服务质量类别前面曾提到，Kubernetes允许节点的Pod对象过载使用资源，这意味着节点无法同时满足绑定其上的所有Pod对象以资源满载的方式运行。因而在内存资源紧缺的情况下，应该以何种次序终止哪些Pod对象就变成了问题。事实上，Kubernetes无法自行对此做出决策，它需要借助于Pod对象的服务质量和优先级等完成判定。根据Pod对象的requests和limits属性，Kubernetes把Pod对象归类到BestEffort、Burstable和Guaranteed这3个服务质量类别（Quality of Service，QoS）类别下。 Guaranteed：Pod对象为其每个容器都设置了CPU资源需求和资源限制，且二者具有相同值；同时为每个容器都设置了内存资需求和内存限制，且二者具有相同值。这类Pod对象具有最高级别服务质量。 Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别的设定要求，这类Pod对象具有中等级别服务质量。 BestEffort：不为任何一个容器设置requests或limits属性，这类Pod对象可获得的服务质量为最低级别。一旦内存资源紧缺，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够做到尽可能多地占用资源。若此时系统上已然不存任何BestEffort类别的容器，则接下来将轮到Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod对象存在。 每个运行状态的容器都有其OOM评分，评分越高越优先被杀死。OOM评分主要根据两个维度进行计算：由服务质量类别继承而来的默认分值，以及容器的可用内存资源比例，而同等类别的Pod对象的默认分值相同。下面的代码片段取自pkg/kubelet/qos/policy.go源码文件，它们定义的是各种类别的Pod对象的OOM调节（Adjust）分值，即默认分值。其中，Guaranteed类别Pod资源的Adjust分值为–998，而BestEffort类别的默认分值为1000，Burstable类别的Pod资源的Adjust分值经由相应的算法计算得出。 12345678const ( PodInfraOOMAdj int = -998 KubeletOOMScoreAdj int = -999 DockerOOMScoreAdj int = -999 KubeProxyOOMScoreAdj int = -999 guaranteedOOMScoreAdj int = -998 besteffortOOMScoreAdj int = 1000) 因此，同等级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将先被杀死。例如，图4-17中的同属于Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例为95%，要大于Pod B的80% 需要特别说明的是，OOM是内存耗尽时的处理机制，与可压缩型资源CPU无关，因此CPU资源的需求无法得到保证时，Pod对象仅仅是暂时获取不到相应的资源来运行而已。 综合应用案例下面的配置清单（all-in-one.yaml）中定义的Pod对象all-in-one将前面的用到的大多数配置整合在一起：它有一个初始化容器和两个应用容器，其中sidecar-proxy为Sidecar容器，负责为主容器demo代理服务客户端请求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: v1kind: Podmetadata: name: all-in-one namespace: defaultspec: initContainers: - name: iptables-init image: ikubernetes/admin-box:latest imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80&#x27;] securityContext: capabilities: add: - NET_ADMIN containers: - name: sidecar-proxy image: envoyproxy/envoy-alpine:v1.13.1 command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;] lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;wget -O /etc/envoy/envoy.yaml https:// raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_ Practical_2rd/master/chapter4/envoy.yaml&#x27;] livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: PORT value: &#x27;8080&#x27; livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 8080 initialDelaySeconds: 5 readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 8080 initialDelaySeconds: 15 securityContext: runAsUser: 1001 runAsGroup: 1001 resources: requests: cpu: 0.5 memory: &quot;64Mi&quot; limits: cpu: 2 memory: &quot;1024Mi&quot; securityContext: supplementalGroups: [1002, 1003] fsGroup: 2000 配置清单的Pod对象的各容器中，主容器demo在Pod的IP地址上监听TCP协议的8080端口，以接收并响应HTTP请求；Sidecar容器sidecar-proxy监听TCP协议的80端口，接收HTTP请求并将其代理至demo容器的8080端口；初始化容器在Pod的Network名称空间中添加了一条iptables重定向规则，该规则负责把所有发往Pod IP上8080端口的请求重定向至80端口，因而demo容器仅能从127.0.0.1的8080端口接收到请求。读者朋友可将清单中的Pod对象创建到集群上，并逐一测试其各项配置的效果。","categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"}]},{"title":"git及CI/CD","slug":"git使用","date":"2022-02-09T06:41:10.000Z","updated":"2022-02-11T01:08:34.928Z","comments":true,"path":"2022/02/09/git使用/","link":"","permalink":"https://marmotad.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/","excerpt":"","text":"CI/CD的功能CI/CD 是一种通过在应用开发阶段引入自动化来频频繁向客户交付应用的方法。CI/CD 的核心概念是持续集成、持续交付和持续部署。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“集成地狱”）。 具体而言，CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）。这些关联的事务通常被统称为“CI/CD 管道”，由开发和运维团队以敏捷方式协同支持。 CI 是什么？CI 和 CD 有什么区别？CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。 CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。 持续交付通常是指开发人员对应用的更改会自动进行错误测试并上传到存储库（如 GitHub 或容器注册表），然后由运维团队将其部署到实时生产环境中。这旨在解决开发和运维团队之间可见性及沟通较差的问题。因此，持续交付的目的就是确保尽可能减少部署新代码时所需的工作量。 持续部署（另一种“CD”）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用。它主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。 CI/CD 既可能仅指持续集成和持续交付构成的关联环节，也可以指持续集成、持续交付和持续部署这三项构成的关联环节。更为复杂的是，有时“持续交付”也包含了持续部署流程。 归根结底，我们没必要纠结于这些语义，您只需记得 CI/CD 其实就是一个流程（通常形象地表述为管道），用于实现应用开发中的高度持续自动化和持续监控。因案例而异，该术语的具体含义取决于 CI/CD 管道的自动化程度。许多企业最开始先添加 CI，然后逐步实现交付和部署的自动化（例如作为云原生应用的一部分）。 CI 持续集成（Continuous Integration）现代应用开发的目标是让多位开发人员同时处理同一应用的不同功能。但是，如果企业安排在一天内将所有分支源代码合并在一起（称为“合并日”），最终可能造成工作繁琐、耗时，而且需要手动完成。这是因为当一位独立工作的开发人员对应用进行更改时，有可能会与其他开发人员同时进行的更改发生冲突。如果每个开发人员都自定义自己的本地集成开发环境（IDE），而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。 持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。 进一步了解技术细节 CD 持续交付（Continuous Delivery）完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。 在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。 CD 持续部署（Continuous Deployment）对于一个成熟的 CI/CD 管道来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。由于在生产之前的管道阶段没有手动门控，因此持续部署在很大程度上都得依赖精心设计的测试自动化。 实际上，持续部署意味着开发人员对应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这更加便于持续接收和整合用户反馈。总而言之，所有这些 CI/CD 的关联步骤都有助于降低应用的部署风险，因此更便于以小件的方式（而非一次性）发布对应用的更改。不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期投资还是会很大。 版本控制系统什么是版本控制系统版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术 版本控制系统解决的问题1、追溯文件历史变更 2、多人团队协同开发 3、代码集中管理 常见的版本控制系统（集中式VS分布式）Subversion集中式版本控制系统 Subversion的特点概括起来主要由以下几条： 每个版本库有唯一的URL（官方地址），每个用户都从这个地址获取代码和数据； 获取代码的更新，也只能连接到这个唯一的版本库，同步以取得最新数据； 提交必须有网络连接（非本地版本库）； 提交需要授权，如果没有写权限，提交会失败； 提交并非每次都能够成功。如果有其他人先于你提交，会提示“改动基于过时的版本，先更新再提交”… 诸如此类； 冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。 好处：每个人都可以一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限。 缺点：中央服务器的单点故障。 若是宕机一小时，那么在这一小时内，谁都无法提交更新、还原、对比等，也就无法协同工作。如果中央服务器的磁盘发生故障，并且没做过备份或者备份得不够及时的话，还会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，被客户端提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人提取出来。 Subversion原理上只关心文件内容的具体差异。每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容。 Git属于分布式的版本控制系统Git记录版本历史只关心文件数据的整体是否发生变化。Git 不保存文件内容前后变化的差异数据。 实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一连接。 在分布式版本控制系统中，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程。 另外，因为Git在本地磁盘上就保存着所有有关当前项目的历史更新，并且Git中的绝大多数操作都只需要访问本地文件和资源，不用连网，所以处理起来速度飞快。用SVN的话，没有网络或者断开VPN你就无法做任何事情。但用Git的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程的镜像仓库。换作其他版本控制系统，这么做几乎不可能，抑或是非常麻烦。 Git具有以下特点： Git中每个克隆(clone)的版本库都是平等的。你可以从任何一个版本库的克隆来创建属于你自己的版本库，同时你的版本库也可以作为源提供给他人，只要你愿意。 Git的每一次提取操作，实际上都是一次对代码仓库的完整备份。 提交完全在本地完成，无须别人给你授权，你的版本库你作主，并且提交总是会成功。 甚至基于旧版本的改动也可以成功提交，提交会基于旧的版本创建一个新的分支。 Git的提交不会被打断，直到你的工作完全满意了，PUSH给他人或者他人PULL你的版本库，合并会发生在PULL和PUSH过程中，不能自动解决的冲突会提示您手工完成。 冲突解决不再像是SVN一样的提交竞赛，而是在需要的时候才进行合并和冲突解决。 Git 也可以模拟集中式的工作模式 Git版本库统一放在服务器中 可以为 Git 版本库进行授权：谁能创建版本库，谁能向版本库PUSH，谁能够读取（克隆）版本库 团队的成员先将服务器的版本库克隆到本地；并经常的从服务器的版本库拉（PULL）最新的更新； 团队的成员将自己的改动推（PUSH）到服务器的版本库中，当其他人和版本库同步（PULL）时，会自动获取改变 Git 的集中式工作模式非常灵活 你完全可以在脱离Git服务器所在网络的情况下，如移动办公／出差时，照常使用代码库 你只需要在能够接入Git服务器所在网络时，PULL和PUSH即可完成和服务器同步以及提交 Git提供 rebase 命令，可以让你的改动看起来是基于最新的代码实现的改动 Git 有更多的工作模式可以选择，远非 Subversion可比 git基本使用配置git 通常只需要配置你是谁，邮箱是什么。就可以知道是谁提交了什么内容 1234[root@localhost ~]# git config --global user.name &quot;fanyang&quot;[root@localhost ~]# git config --global user.email &quot;fanyang@163.com&quot;[root@localhost ~]# git config --global color.ui true[root@localhost ~]# cat .gitconfig git如何提交目录文件到本地仓库1、首先创建git仓库，这个目录里的所有文件都可以被git管理起来，每个文件的修改、删除、GIt都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原” 1234567# 创建git工作目录[root@localhost ~]# mkdir /git[root@localhost ~]# cd /git# 初始化该目录为git仓库[root@localhost git]# git init 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 创建新的文件[root@localhost git]# touch file&#123;1..3&#125;[root@localhost git]# lsfile1 file2 file3# 查看git状态[root@localhost git]# git statusOn branch masterNo commits yetUntracked files:# 有三个未提交的文件 (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) file1 file2 file3nothing added to commit but untracked files present (use &quot;git add&quot; to track)# 添加本地所有文件到本地git缓存[root@localhost git]# git add .# 查看git状态[root@localhost git]# git statusOn branch masterNo commits yetChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)# 有三个新的文件 new file: file1 new file: file2 new file: file3# 添加git描述[root@localhost git]# git commit -m &quot;新增file&#123;1..3&#125;到git仓库&quot;[master (root-commit) 8acf856] 新增file&#123;1..3&#125;到git仓库 3 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1 create mode 100644 file2 create mode 100644 file3# 修改file1[root@localhost git]# echo 1 &gt;file1 [root@localhost git]# git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: file1no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;修改file1&quot;[master 53 30aef] 修改file1 1 file changed, 1 insertion(+) 文件改名后重新提交到本地git仓库1234567[root@localhost git]# git mv file1 file [root@localhost git]# git status On branch masterChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) renamed: file1 -&gt; file 对比文件差异对比本地文件和暂存区文件的差异12345678910[root@localhost git]# git diff file[root@localhost git]# echo test &gt;&gt; file[root@localhost git]# git diff filediff --git a/file b/fileindex d00491f..c0f2f8d 100644--- a/file+++ b/file@@ -1 +1,2 @@ 1+test 对比暂存区文件和本地git仓库文件差异1234567891011121314151617181920212223[root@localhost git]# git add .[root@localhost git]# git diff file[root@localhost git]# git diff file --cache filefatal: option &#x27;--cache&#x27; must come before non-option arguments[root@localhost git]# git diff file --cached filefatal: option &#x27;--cached&#x27; must come before non-option arguments[root@localhost git]# git diff --cached filediff --git a/file b/filenew file mode 100644index 0000000..c0f2f8d--- /dev/null+++ b/file@@ -0,0 +1,2 @@+1+test[root@localhost git]# [root@localhost git]# git commit -m &quot;修改file文件&quot;[master 8b1ecec] 修改file文件 2 files changed, 2 insertions(+), 1 deletion(-) create mode 100644 file delete mode 100644 file1[root@localhost git]# git diff --cached file 文件回滚操作导致文件被清空（本地目录与暂存区间的撤销）123456789101112131415# 使用以前提交到暂存区的内容覆盖本地目录[root@localhost git]# echo &gt; file[root@localhost git]# git status On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: fileno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git checkout file[root@localhost git]# cat file1test 本地文件误操作提交至暂存区本地仓库覆盖暂存区—–&gt; 暂存区覆盖本地目录 12345678910111213141516171819202122232425262728[root@localhost git]# echo ddd &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# git status On branch masterChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: file[root@localhost git]# git reset HEAD fileUnstaged changes after reset:M file[root@localhost git]# git status On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: fileno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git checkout file[root@localhost git]# git status On branch masternothing to commit, working tree clean[root@localhost git]# cat file1test 多次提交到本地仓库后回滚123456789101112131415161718192021222324252627[root@localhost git]# echo test &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# lsfile file2 file3[root@localhost git]# git commit -m &quot;test&quot;[master c691c60] test 1 file changed, 1 insertion(+)[root@localhost git]# echo test1 &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;test1&quot;[master 3595d2b] test1 1 file changed, 1 insertion(+)[root@localhost git]# git log --oneline 3595d2b (HEAD -&gt; master) test1c691c60 test8b1ecec 修改file文件5330aef 修改file18acf856 新增file&#123;1..3&#125;到git仓库[root@localhost git]# git reset --hard c691c60HEAD is now at c691c60 test[root@localhost git]# git status On branch masternothing to commit, working tree clean[root@localhost git]# cat file1testtest git 回退后，恢复回退前版本123456789101112131415[root@localhost git]# git reflog c691c60 (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to c691c603595d2b HEAD@&#123;1&#125;: commit: test1c691c60 (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: test8b1ecec HEAD@&#123;3&#125;: commit: 修改file文件5330aef HEAD@&#123;4&#125;: commit: 修改file18acf856 HEAD@&#123;5&#125;: commit (initial): 新增file&#123;1..3&#125;到git仓库[root@localhost git]# git rerebase reflog remote repack replace request-pull reset revert [root@localhost git]# git reset --hard c691c60[root@localhost git]# git log --oneline c691c60 (HEAD -&gt; master) test8b1ecec 修改file文件5330aef 修改file18acf856 新增file&#123;1..3&#125;到git仓库 git 分支管理查看、创建、切换分支12345678910111213# 查看分支[root@localhost git]# git branch * master# 创建分支[root@localhost git]# git branch dev[root@localhost git]# git branch dev* master# 切换分支[root@localhost git]# git checkout devSwitched to branch &#x27;dev&#x27;[root@localhost git]# git branch * dev 合并分支master合并到dev—-&gt;测试合并后的dev分支—–&gt;dev分支合并到master 12[root@localhost git]# git merge master[root@localhost git]# git merge dev 删除分支1234[root@localhost git]# git branch dev -dDeleted branch dev (was 06b4943).[root@localhost git]# git branch * masterl,; 标签创建标签123456789# 对当前分支当前的版本打标签[root@localhost git]# git tag -a &quot;v1.0&quot; -m &quot;第一个版本&quot;# 查看当前分支有哪些标签[root@localhost git]# git tag# 查看标签内容[root@localhost git]# git show v1.0 # 对指定的id打标签[root@localhost git]# git tag -a &quot;v1.0&quot; c691c60 -m &quot;未发布的版本&quot;fatal: tag &#x27;v1.0&#x27; already exists 删除标签12[root@localhost git]# git tag -d v0.9 Deleted tag &#x27;v0.9&#x27; (was 8d74209) git 操作远程仓库关联远程仓库1234[root@localhost git]# git remote add origin git@172.18.128.4:root/git-test.git[root@localhost git]# git remote -vorigin git@172.18.128.4:root/git-test.git (fetch)origin git@172.18.128.4:root/git-test.git (push) 将本地仓库内容推送到远程仓库123[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;修改file文件&quot;[root@localhost git]# git push -u origin master 删除远程仓库12[root@localhost git]# git remote remove origin # origin ：用户名称 新用户加入需要做的123456789101112131415161718192021222324252627282930313233343536373839[root@localhost ~]# mkdir /test[root@localhost ~]# cd /test[root@localhost test]# git init Initialized empty Git repository in /test/.git/[root@localhost test]# git status # On branch master## Initial commit#nothing to commit (create/copy files and use &quot;git add&quot; to track)[root@localhost git-test]# git config --global user.name &quot;Your Name&quot;[root@localhost git-test]# git config --global user.email you@example.co[root@localhost test]# git remote add origin git@172.18.128.4:root/git-test.git[root@localhost test]# git remote -vorigin git@172.18.128.4:root/git-test.git (fetch)origin git@172.18.128.4:root/git-test.git (push)[root@localhost test]# git clone git@172.18.128.4:root/git-test.gitCloning into &#x27;git-test&#x27;...[root@localhost test]# lsgit-test[root@localhost test]# cd git-test/[root@localhost git-test]# lsfile file2 file3 file5 file7 file8[root@localhost git-test]# touch file9[root@localhost git-test]# git add .[root@localhost git-test]# git commit -m &quot;new file&quot;# On branch master# Your branch is ahead of &#x27;origin/master&#x27; by 1 commit.# (use &quot;git push&quot; to publish your local commits)#nothing to commit, working directory clean[root@localhost git-test]# git push origin master Counting objects: 3, done.Compressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 233 bytes | 0 bytes/s, done.Total 2 (delta 1), reused 0 (delta 0)To git@172.18.128.4:root/git-test.git 06b4943..b3fdc9f master -&gt; master 同步远程仓库中否代码1[root@localhost git-test]# git pull origin master","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/categories/CI-CD/"}],"tags":[{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/tags/CI-CD/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-02-09T03:10:39.808Z","updated":"2022-02-09T03:08:03.358Z","comments":true,"path":"2022/02/09/hello-world/","link":"","permalink":"https://marmotad.github.io/2022/02/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"","slug":"Kubernetes-magedu","date":"2022-01-16T02:41:45.961Z","updated":"2022-02-09T12:48:39.562Z","comments":true,"path":"2022/01/16/Kubernetes-magedu/","link":"","permalink":"https://marmotad.github.io/2022/01/16/Kubernetes-magedu/","excerpt":"","text":"","categories":[],"tags":[]}],"categories":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/categories/Kubernetes/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/categories/CI-CD/"}],"tags":[{"name":"Kubernetes","slug":"Kubernetes","permalink":"https://marmotad.github.io/tags/Kubernetes/"},{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/tags/CI-CD/"}]}