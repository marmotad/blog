{"meta":{"title":"marmotad","subtitle":"blog","description":"myBlog","author":"John Doe","url":"https://marmotad.github.io","root":"/blog/"},"pages":[],"posts":[{"title":"Kubernetes基础","slug":"Kubernetes基础","date":"2022-02-09T09:56:37.000Z","updated":"2022-02-09T10:05:45.404Z","comments":true,"path":"2022/02/09/Kubernetes基础/","link":"","permalink":"https://marmotad.github.io/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/","excerpt":"","text":"Kubernetes集群架构Kubernetes属于典型的Server-Client形式的二层架构，在程序级别，Master主要由API Server（kube-apiserver）、Controller-Manager（kube-controller-manager）和Scheduler（kube-scheduler）这3个组件，以及一个用于集群状态存储的etcd存储服务组成，它们构成整个集群的控制平面；而每个Node节点则主要包含kubelet、kube-proxy及容器运行时（Docker是最为常用的实现）3个组件，它们承载运行各类应用容器。 Kubernetes系统组件Master组件Master 它维护有Kubernetes的所有对象记录，负责持续管理对象状态并响应集群中各种资源对象的管理操作，以及确保各资源对象的实际状态与所需状态相匹配。控制平面的各组件支持以单副本形式运行于单一主机，也能够将每个组件以多副本方式同时运行于多个主机上，提高服务可用级别。控制平面各组件及其主要功能如下。 API ServerAPI Server是Kubernetes控制平面的前端，支持不同类型应用的生命周期编排，包括部署、缩放和滚动更新等。它还是整个集群的网关接口，由kube-apiserver守护程序运行为服务，通过HTTP/HTTPS协议将RESTful API公开给用户，是发往集群的所有REST操作命令的接入点，用于接收、校验以及响应所有的REST请求，并将结果状态持久存储于集群状态存储系统（etcd）中。 集群状态存储(ETCD)Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中。 控制器管理器(kube-controller-manager)控制器负责实现用户通过API Server提交的终态声明，驱动API对象的当前状态逼近或等同于期望状态。Kubernetes提供了驱动Node、Pod、Server、Endpoint、ServiceAccount和Token等API对象的控制器。 调度器(kube-scheduler)Kubernetes系统上的调度是指为API Server接收到的每一个Pod创建请求，并在集群中为其匹配出一个最佳工作节点。kube-scheduler是默认调度器程序。 Node组件Node组件是集群的“体力”输出者，每个Node会定期向Master报告自身的状态变动，并接受Master的管理。 kubeletkubelet是运行于每个Node之上的“节点代理”服务，负责接收并执行Master发来的指令，以及管理当前Node上Pod对象的容器等任务。kubelet会持续监视当前节点上各Pod的健康状态，包括基于用户自定义的探针进行存活状态探测，并在任何Pod出现问题时将其重建为新实例。它还内置了一个HTTP服务器，监听TCP协议的10248和10250端口：10248端口通过/healthz响应对kubelet程序自身的健康状态进行检测；10250端口用于暴露kubelet API，以验证、接收并响应API Server的通信请求。 容器运行时环境Pod是一组容器组成的集合并包含这些容器的管理机制。kubelet通过CRI（容器运行时接口）可支持多种类型的OCI容器运行时，例如docker、containerd、CRI-O、runC、fraki和Kata Containers等。 kube-proxykube-proxy，它把API Server上的Service资源对象转换为当前节点上的iptables或（与）ipvs规则，这些规则能够将那些发往该Service对象ClusterIP的流量分发至它后端的Pod端点之上。kube-proxy是Kubernetes的核心网络组件，它本质上更像是Pod的代理及负载均衡器，负责确保集群中Node、Service和Pod对象之间的有效通信。 核心附件附件（add-ons）用于扩展Kubernetes的基本功能，它们通常运行于Kubernetes集群自身之上，可根据重要程度将其划分为必要和可选两个类别。网络插件是必要附件，管理员需要从众多解决方案中根据需要及项目特性选择，常用的有Flannel、Calico、Canal、Cilium和Weave Net等。KubeDNS通常也是必要附件之一，而Web UI（Dashboard）、容器资源监控系统、集群日志系统和Ingress Controller等是常用附件。 CoreDNSKubernetes使用定制的DNS应用程序实现名称解析和服务发现功能，它自1.11版本起默认使用CoreDNS——一种灵活、可扩展的DNS服务器；之前的版本中用到的是kube-dns项目，SkyDNS则是更早一代的解决方案。 Dashboard基于Web的用户接口，用于可视化Kubernetes集群。 容器资源监控系统Kubernetes常用的指标监控附件有Metrics-Server、kube-state-metrics和Prometheus等。 集群日志系统Kubernetes常用的集中式日志系统是由ElasticSearch、Fluentd和Kibana（称之为EFK）组合提供的整体解决方案。 Ingress Controller Pod与Service Pod本质上是共享Network、IPC和UTS名称空间以及存储资源的容器集合。 同一Pod内部的容器，它们共享网络协议栈、网络设备、路由、IP地址和端口等网络资源，可以基于本地回环接口lo互相通信。每个Pod上还可附加一组“存储卷”（volume）资源，它们同样可由内部所有容器使用而实现数据共享。持久类型的存储卷还能够确保在容器终止后被重启，甚至容器被删除后数据也不会丢失。同时，这些以Pod形式运行于Kubernetes之上的应用通常以服务类程序居多，其客户端可能来自集群之外，例如现实中的用户，也可能是当前集群中其他Pod中的应用，如图1-10所示。Kubernetes集群的网络模型要求其各Pod对象的IP地址位于同一网络平面内（同一IP网段），各Pod间可使用真实IP地址直接进行通信而无须NAT功能介入，无论它们运行于集群内的哪个工作节点之上，这些Pod对象就像是运行于同一局域网中的多个主机上。 Service是由基于匹配规则在集群中挑选出的一组Pod对象的集合、访问这组Pod集合的固定IP地址，以及对请求进行调度的方法等功能所构成的一种API资源类型，是Pod资源的代理和负载均衡器。Service匹配Pod对象的规则可用“标签选择器”进行体现，并根据标签来过滤符合条件的资源对象，如图1-11所示。标签是附加在Kubernetes API资源对象之上的具有辨识性的分类标识符，使用键值型数据表达，通常仅对用户具有特定意义。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上。 每个节点上运行的kube-proxy组件负责管理各Pod与Service之间的网络连接，它并非Kubernetes内置的代理服务器，而是一个基于出站流量的负载均衡器组件。针对每个Service，kube-proxy都会在当前节点上转换并添加相应iptables DNAT规则或ipvs规则，从而将目标地址为某Service对象的ClusterIP的流量调度至该Service根据标签选择器匹配出的Pod对象之上。CoreDNS附件会为集群中的每个Service对象（包括DNS服务自身)生成唯一的DNS名称标识，以及相应的DNS资源记录，服务的DNS名称遵循标准的svc.namespace.svc.cluster-domain格式。例如CoreDNS自身的服务名称为kube-dns.kube-system.svc.cluster.local.，则它的ClusterIP通常是10.96.0.10。除非出于管理目的有意调整，Service资源的名称和ClusterIP在其整个生命周期内都不会发生变动。kubelet会在创建Pod容器时，自动在/etc/resolv.conf文件中配置Pod容器使用集群上CoreDNS服务的ClusterIP作为DNS服务器，因而各Pod可针对任何Service的名称直接请求相应的服务。换句话说，Pod可通过kube-dns.kube-system.svc.cluster.local.来访问集群DNS服务。Ingress资源是Kubernetes将集群外部HTTP/HTTPS流量引入到集群内部专用的资源类型，它仅用于控制流量的规则和配置的集合，其自身并不能进行“流量穿透”，要通过Ingress控制器发挥作用；目前，此类的常用项目有Nginx、Traefik、Envoy、Gloo、kong及HAProxy等。 应用部署、运行与管理应用容器与Pod资源 同一Pod中，这些容器共享PID、IPC、Network和UTS名称空间的容器彼此间可通过IPC通信，共享使用主机名和网络接口、IP地址、端口和路由等各种网络资源，因而各容器进程能够通过lo网络接口通信且不能使用相同的网络套接字地址。一个Pod内通常仅应该运行具有强耦合关系的容器，否则除了pause以外，只应该存在单个容器，或者只存在单个主容器和一个以上的辅助类容器（例如服务网格中的Sidecar容器等）。 容器设计模式单容器模式单容器模式是指将应用程序封装为应用容器运行。该模式需要遵循简单和单一原则，每个容器仅承载一种工作负载。 单节点多容器模式单节点多容器模式的常见实现有Sidecar（边车）、适配器（Adapter）、大使（Ambassador）、初始化（Initializer）容器模式等。 (1) Sidecar模式Sidecar模式是多容器系统设计的最常用模式，它由一个主应用程序（通常是Web应用程序）以及一个辅助容器（Sidecar容器）组成，该辅助容器用于为主容器提供辅助服务以增强主容器的功能，是主应用程序是必不可少的一部分，但却不一定非得存在于应用程序本身内部。 sidecar的优势 辅助应用的运行时环境和编程语言与主应用程序无关，因而无须为每种编程语言分别开发一个辅助工具； 二者可基于IPC、lo接口或共享存储进行数据交换，不存在明显的通信延迟； 容器镜像是发布的基本单位，将主应用与辅助应用划分为两个容器使得其可由不同团队开发和维护，从而变得方便及高效，单独测试及集成测试也变得可能； 容器限制了故障边界，使得系统整体可以优雅降级，例如Sidecar容器异常时，主容器仍可继续提供服务； 容器是部署的基本单元，每个功能模块均可独立部署及回滚。事实上，这些优势对于其他模型来说同样存在。 (2) 大使模式大使模式本质上是一类代理程序，它代表主容器发送网络请求至外部环境中，因此可以将其视作与客户端（主容器应用）位于同一位置的“外交官”。 大使模式的最佳用例之一是提供对数据库的访问。实践中，开发环境、测试环境和生产环境中的主应用程序可能需要分别连接到不同的数据库服务。更好的方案是让应用程序始终通过localhost连接至大使容器，而如何正确连接到目标数据的责任则由大使容器完成。 (3) 适配器模式适配器模式（见图4-4）用于为主应用程序提供一致的接口，实现了模块重用，支持标准化和规范化主容器应用程序的输出以便于外部服务进行聚合。大使模式为内部容器提供了简化统一的外部服务视图，适配器模式则刚好反过来，它通过标准化容器的输出和接口，为外界展示了一个简化的应用视图。 (4) 初始化容器模式初始化容器模式（见图4-5）负责以不同于主容器的生命周期来完成那些必要的初始化任务， 初始化容器将Pod内部的容器分成了两组：初始化容器和应用程序容器（主容器和Sidecar容器等），初始化容器可以不止一个，但它们需要以特定的顺序串行运行，并需要在启动应用程序容器之前成功终止。不过，多个应用程序容器一般需要并行启动和运行。就Kubernetes来说，除了初始化容器之外，还有一些其他可用的初始化技术，例如admission controllers、admission webhooks和PodPresets等。 多节点模式(1) 领导者选举模式领导者选举模式示意图。 (2) 工作队列模式分布式应用程序的各组件间存在大量的事件传递需求，当某应用组件需要将信息广播至大量订阅者时，可能需要与多个独立开发的，可能使用了不同平台、编程语言和通信协议的应用程序或服务通信，并且无须订阅者实时响应地通信，它具有解耦子系统、提高伸缩能力和可靠性、支持延迟事件处理、简化异构组件间的集成等优势。图4-7为工作队列模式示意图。 (3) 分散/聚集分散/聚集模式与工作队列模式非常相似，它同样将大型任务拆分为较小的任务，区别是容器会立即将响应返回给用户，一个很好的例子是MapReduce算法。该模式需要两类组件：一个称为“根”节点或“父”节点的组件，将来自客户端的请求切分成多个小任务并分散到多个节点并行计算；另一类称为“计算”节点或“叶子”节点，每个节点负责运行一部分任务分片并返回结果数据，“根”节点收集这些结果数据并聚合为有意义的数据返回给客户端。开发这类分布式系统需要请求扇出、结果聚合以及与客户端交互等大量的模板代码，但大部分都比较通用。因而要实现该模式，我们只需要分别将两类组件各自构建为容器即可。 Pod的生命周期Kubernetes为Pod资源严格定义了5种相位，并将特定Pod对象的当前相位存储在其内部的子对象PodStatus的phase字段上，因而它总是应该处于其生命进程中以下几个相位之一。 Pending：API Server创建了Pod资源对象并已存入etcd中，但它尚未被调度完成，或仍处于从仓库中下载容器镜像的过程中。 Running：Pod已经被调度至某节点，所有容器都已经被kubelet创建完成，且至少有一个容器处于启动、重启或运行过程中。 Succeeded：Pod中的所有容器都已经成功终止且不会再重启。 Failed：所有容器都已经终止，但至少有一个容器终止失败，即容器以非0状态码退出或已经被系统终止。 Unknown：API Server无法正常获取到Pod对象的状态信息，通常是由于其无法与所在工作节点的kubelet通信所致。 阶段仅是对Pod对象生命周期运行阶段的概括性描述，而非Pod或内部容器状态的综合汇总，因此Pod对象的status字段中的状态值未必一定是可用的相位，它也有可能是Pod的某个错误状态，例如CrashLoopBackOff或Error等。Pod资源的核心职责是运行和维护称为主容器的应用程序容器，在其整个生命周期之中的多种可选行为也是围绕更好地实现该功能而进行，如图4-8所示。其中，初始化容器（init container）是常用的Pod环境初始化方式，健康状态检测（startupProbe、livenessProbe和readinessProbe）为编排工具提供了监测容器运行状态的编程接口，而事件钩子（preStop和postStart）则赋予了应用容器读取来自编排工具上自定义事件的机制。 若用户给出了上述全部定义，则一个Pod对象生命周期的运行步骤如下。 1）在启动包括初始化容器在内的任何容器之前先创建pause基础容器，它初始化Pod环境并为后续加入的容器提供共享的名称空间。2）按顺序以串行方式运行用户定义的各个初始化容器进行Pod环境初始化；任何一个初始化容器运行失败都将导致Pod创建失败，并按其restartPolicy的策略进行处理，默认为重启。3）待所有初始化容器成功完成后，启动应用程序容器，多容器Pod环境中，此步骤会并行启动所有应用容器，例如主容器和Sidecar容器，它们各自按其定义展开其生命周期；本步骤及后面的几个步骤都将以主容器为例进行说明；容器启动的那一刻会同时运行主容器上定义的PostStart钩子事件，该步骤失败将导致相关容器被重启。4）运行容器启动健康状态监测（startupProbe），判定容器是否启动成功；该步骤失败，同样参照restartPolicy定义的策略进行处理；未定义时，默认状态为Success。5）容器启动成功后，定期进行存活状态监测（liveness）和就绪状态监测（readiness）；存活状态监测失败将导致容器重启，而就绪状态监测失败会使得该容器从其所属的Service对象的可用端点列表中移除。6）终止Pod对象时，会先运行preStop钩子事件，并在宽限期（terminationGrace-PeriodSeconds）结束后终止主容器，宽限期默认为30秒。 在Pod中运行应用Pod资源中可同时存在初始化容器、应用容器和临时容器3种类型的容器，不过创建并运行一个具体的Pod对象时，仅有应用容器是必选项，并且可以仅为其定义单个容器。 使用单容器Pod资源一个Pod对象的核心职责在于以主容器形式运行单个应用，因而定义API资源的关键配置就在于定义该容器，它以对象形式定义在Pod对象的spec.containers字段中，基本格式如下： 12345678910apiVersion: v1kind: Podmetadata: name: … # Pod的标识名，在名称空间中必须唯一 namespace: … # 该Pod所属的名称空间，省略时使用默认名称空间，例如defaultspec: containers: # 定义容器，它是一个列表对象，可包括多个容器的定义，至少得有一个 - name: … # 容器名称，必选字段，在当前Pod中必须唯一 image: … # 创建容器时使用的镜像 imagePullPolicy: … # 容器镜像下载策略，可选字段 image虽为可选字段，这只是为方便更高级别的管理类资源（例如Deployment等）能覆盖它以实现某种高级管理功能而设置，对于非控制器管理的自主式Pod来说并不能省略该字段。 12345678910apiVersion: v1kind: Podmetadata: name: pod-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent 把上面的内容保存于配置文件pod-demo.yaml中，随后即可使用kubectl apply或kubectl create命令进行资源对象创建 12~$ kubectl apply -f pod-demo.yamlpod/pod-demo created 该Pod对象由调度器绑定至特定工作节点后，由相应的kubelet负责创建和维护，实时状态也将同步给API Server并由其存储至etcd中。Pod创建并尝试启动的过程中，可能会经历Pending、ContainerCreating、Running等多种不同的状态，若Pod可正常启动，则kubectl get pods/POD命令输出字段中的状态（STATUS）则显示为Running 123~$ kubectl get pods/pod-demo -n defaultNAME READY STATUS RESTARTS AGEpod-demo 1/1 Running 0 5m 随后即可对Pod中运行着的主容器的服务发起访问请求。镜像demoapp默认运行了一个Web服务程序，该服务监听TCP协议的80端口，镜像可通过“/”、/hostname、/user-agent、/livez、/readyz和/configs等路径服务于客户端的请求。例如，下面的命令先获取到Pod的IP地址，而后对其支持的Web资源路径/和/user-agent分别发出了一个访问请求： 12345~$ demoIP=$(kubectl get pods/pod-demo -o jsonpath=&#123;.status.podIP&#125;)~ $ curl -s http://$demoIPiKubernetes demoapp v1.0 ! ClientIP: 10.244.0.0, ServerName: pod-demo, ServerIP: 10.244.2.3!~$ curl -s http://$demoIP/user-agentUser-Agent: curl/7.58.0 容器的imagePullPolicy字段用于为其指定镜像获取策略，可用值包括如下几个。 Always：每次启动Pod时都要从指定的仓库下载镜像。 IfNotPresent：仅本地镜像缺失时方才从目标仓库wp下载镜像。 Never：禁止从仓库下载镜像，仅使用本地镜像。 对于标签为latest的镜像文件，其默认的镜像获取策略为Always，其他标签的镜像，默认策略则为IfNotPresent。需要注意的是，从私有仓库中下载镜像时通常需要事先到Registry服务器认证后才能进行。认证过程要么需要在相关节点上交互式执行docker login命令，要么将认证信息定义为专有的Secret资源，并配置Pod通过imagePullSecretes字段调用此认证信息完成。删除Pod对象则使用kubectl delete命令。 命令式命令：kubectl delete pods/NAME。 命令式对象配置：kubectl delete -f FILENAME。 若删除后Pod一直处于Terminating状态，则可再一次执行删除命令，并同时使用–force和–grace-period=0选项进行强制删除。 获取Pod与容器状态详情kubectl有多个子命令，用于从不同角度显示对象的状态信息，这些信息有助于用户了解对象的运行状态、属性详情等。 kubectl describe：显示资源的详情，包括运行状态、事件等信息，但不同的资源类型输出内容不尽相同。 kubectl logs：查看Pod对象中容器输出到控制台的日志信息；当Pod中运行有多个容器时，需要使用选项-c指定容器名称。 kubectl exec：在Pod对象某容器内运行指定的程序，其功能类似于docker exec命令，可用于了解容器各方面的相关信息或执行必需的设定操作等，具体功能取决于容器内可用的程序。 打印Pod对象的状态kubectl describe pods/NAME -n NAMESPACE命令可打印Pod对象的详细描述信息，包括events和controllers等关系的子对象等，Priority、Status、Containers和Events等字段通常是重点关注的目标字段。另外，也可以通过kubectl get pods/POD -o yaml|json命令的status字段来了解Pod的状态详情，它保存有Pod对象的当前状态。如下命令显示了pod-demo的状态信息，结果输出做了尽可能的省略。 12345678910111213141516171819202122232425262728~$ kubectl get pods/pod-demo -o yamlstatus: conditions: - lastProbeTime: null lastTransitionTime: &quot;2020-08-16T03:36:48Z&quot; message: &#x27;containers with unready status: [demo]&#x27; reason: ContainersNotReady status: &quot;False&quot; type: ContainersReady …… containerStatuses: # 容器级别的状态信息 - containerID: docker://…… image: ikubernetes/demoapp:v1.0 imageID: docker-pullable://ikubernetes/demoapp@sha256:…… lastState: &#123;&#125; # 前一次的状态 name: demo ready: true # 是否已经就绪 restartCount: 0 # 重启次数 started: true state: # 当前状态 running: startedAt: &quot;2020-08-16T03:36:48Z&quot; # 启动时间 hostIP: 172.29.9.12 # 节点IP phase: Running # Pod当前的相位 podIP: 10.244.2.3 # Pod的主IP地址 podIPs: # Pod上的所有IP地址 - ip: 10.244.2.3 qosClass: BestEffort # QoS类别 上面的命令结果中，conditions字段是一个称为PodConditions的数组，它记录了Pod所处的“境况”或者“条件”，其中的每个数组元素都可能由如下6个字段组成。 lastProbeTime：上次进行Pod探测时的时间戳。 lastTransitionTime：Pod上次发生状态转换的时间戳。 message：上次状态转换相关的易读格式信息。 reason：上次状态转换原因，用驼峰格式的单个单词表示。 status：是否为状态信息，可取值有True、False和Unknown。 type：境况的类型或名称，有4个固定值；PodScheduled表示已经与节点绑定；Ready表示已经就绪，可服务客户端请求；Initialized表示所有的初始化容器都已经成功启动；ContainersReady则表示所有容器均已就绪。 另外，containerStatuses字段描述了Pod中各容器的相关状态信息，包括容器ID、镜像和镜像ID、上一次的状态、名称、启动与否、就绪与否、重启次数和状态等。 查看容器日志kubectl logs POD [-c CONTAINER]命令可直接获取并打印控制台日志，不过，若Pod对象中仅运行有一个容器，则可以省略-c选项及容器名称。例如，下面的命令打印了pod-demo中唯一的主容器的控制台日志： 1234~$ kubectl logs pod-demo* Running on http://0.0.0.0:80/ (Press CTRL+C to quit)172.29.9.1 - - [16/Aug/2020 03:54:42] &quot;GET / HTTP/1.1&quot; 200 -172.29.9.11 - - [16/Aug/2020 03:54:50] &quot;GET / HTTP/1.1&quot; 200 - 在容器中额外运行其他程序kubectl exec可以让用户在Pod的某容器中运行用户所需要的任何存在于容器中的程序。在kubectl logs获取的信息不够全面时，此命令可以通过在Pod中运行其他指定的命令（前提是容器中存在此程序）来辅助用户获取更多信息。一个更便捷的使用接口是直接交互式运行容器中的某shell程序。例如，直接查看Pod中的容器运行的进程： 1234~$ kubectl exec pod-demo -- ps auxPID USER TIME COMMAND 1 root 0:01 python3 /usr/local/bin/demo.py 8 root 0:00 ps aux 注意:如果Pod中运行多个容器，需要使用-c 选项指定运行程序的容器名称。 有时候需要打开容器的交互式shell接口以方便多次执行命令，为kubectl exec命令额外使用-it选项，并指定运行镜像中可用的shell程序就能进入交互式接口 1234567~$ kubectl -it exec pod-demo /bin/sh[root@pod-demo /]# hostnamepod-demo[root@pod-demo /]# netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 自定义容器应用与参数容器镜像启动容器时运行的默认应用程序由其Dockerfile文件中的ENTRYPOINT指令进行定义，传递给程序的参数则通过CMD指令设定，ETRYPOINT指令不存在时，CMD可同时指定程序及其参数。例如，要了解镜像ikubernetes/demoapp:v1.0中定义的ENTRYPOINT和CMD，可以在任何存在此镜像的节点上执行类似如下命令来获取： 1234~# docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Entrypoint&#125;&#125;[/bin/sh -c python3 /usr/local/bin/demo.py]~# docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Cmd&#125;&#125;[] Pod配置中，spec.containers[].command字段可在容器上指定非镜像默认运行的应用程序，且可同时使用spec.containers[].args字段进行参数传递，它们将覆盖镜像中默认定义的参数。若定义了args字段，该字段值将作为参数传递给镜像中默认指定运行的应用程序；而仅定义了command字段时，其值将覆盖镜像中定义的程序及参数。下面的资源配置清单保存在pod-demo-with-cmd-and-args.yaml文件中，它把镜像ikubernetes/demoapp:v1.0的默认应用程序修改为/bin/sh -c，参数定义为python3 /usr/local/bin/demo.py -p 8080，其中的-p选项可修改服务监听的端口为指定的自定义端口 123456789101112apiVersion: v1kind: Podmetadata: name: pod-demo-with-cmd-and-args namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;python3 /usr/local/bin/demo.py -p 8080&#x27;] 下面将上述清单中定义的Pod对象创建到集群上，验证其监听的端口是否从默认的80变为了指定的8080： 123456~$ kubectl create -f pod-demo-with-cmd-and-args.yaml pod/pod-demo-with-cmd-and-args created~$ kubectl exec pod-demo-with-cmd-and-args -- netstat -tnl Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN 容器环境变量容器环境变量需要应用程序支持通过环境变量进行配置，否则用户要在制作Docker镜像时通过entrypoint脚本完成环境变量到程序配置文件的同步。向Pod对象中容器环境变量传递数据的方法有两种：env和envFrom，这里重点介绍第一种方式，第二种方式将在介绍ConfigMap和Secret资源时进行说明。通过环境变量的配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构成的列表。每个环境变量通常由name和value字段构成。 name ：环境变量的名称，必选字段。 value ：传递给环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。 示例中使用镜像demoapp中的应用服务器支持通过HOST与PORT环境变量分别获取监听的地址和端口，它们的默认值分别为0.0.0.0和80，下面的配置保存在清单文件pod-using-env.yaml中，它分别为HOST和PORT两个环境变量传递了一个不同的值，以改变容器监听的地址和端口。 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-using-env namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: HOST value: &quot;127.0.0.1&quot; - name: PORT value: &quot;8080&quot; 下面将清单文件中定义的Pod对象创建至集群中，并查看应用程序监听的地址和端口来验证配置结果： 123456~$ kubectl apply -f pod-using-env.yamlpod/pod-using-env created~$ kubectl exec pod-using-env -- netstat -tnlActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN Pod的创建与删除过程Pod资源对象的创建过程。 1）用户通过kubectl或其他API客户端提交Pod Spec给API Server。2）API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。3）Scheduler（调度器）通过其watcher监测到API Server创建了新的Pod对象，于是为该Pod对象挑选一个工作节点并将结果信息更新至API Server。4）调度结果信息由API Server更新至etcd存储系统，并同步给Scheduler。5）相应节点的kubelet监测到由调度器绑定于本节点的Pod后会读取其配置信息，并由本地容器运行时创建相应的容器启动Pod对象后将结果回存至API Server。6）API Server将kubelet发来的Pod状态信息存入etcd系统，并将确认信息发送至相应的kubelet。另一方面，Pod可能曾用于处理生产数据或向用户提供服务等，Kubernetes可删除宽限期确保终止操作能够以平滑方式优雅完成，从而用户也可以在正常提交删除操作后获知何时开始终止并最终完成。删除时，用户提交请求后系统即会进行强制删除操作的宽限期倒计时，并将TERM信号发送给Pod对象中每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，Pod对象也随即由API Server删除。如果在等待进程终止的过程中kubelet或容器管理器发生了重启，则终止操作会重新获得一个满额的删除宽限期并重新执行删除操作。 Pod的终止过程 如图4-10所示，一个典型的Pod对象终止流程如下。 1）用户发送删除Pod对象的命令。2）API服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认为30秒），Pod被视为dead。3）将Pod标记为Terminating状态。4）（与第3步同时运行）kubelet在监控到Pod对象转为Terminating状态的同时启动Pod关闭过程。5）（与第3步同时运行）端点控制器监控到Pod对象的关闭行为时将其从所有匹配到此端点的Service资源的端点列表中移除。6）如果当前Pod对象定义了preStop钩子句柄，在其标记为terminating后即会以同步方式启动执行；如若宽限期结束后，preStop仍未执行完，则重新执行第2步并额外获取一个时长为2秒的小宽限期。7）Pod对象中的容器进程收到TERM信号。8）宽限期结束后，若存在任何一个仍在运行的进程，Pod对象即会收到SIGKILL信号。9）Kubelet请求API Server将此Pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。默认情况下，所有删除操作的宽限期都是30秒，不过kubectl delete命令可以使用–grace-period=选项自定义其时长，使用0值则表示直接强制删除指定的资源，不过此时需要同时为命令使用–force选项。 暴露容器服务不考虑通过Service资源进行服务暴露的情况下，服务于集群外部的客户端的常用方式有两种：一种是在其运行的节点上进行端口映射，由节点IP和选定的协议端口向Pod内的应用容器进行DNAT转发；另一种是让Pod共享其所在的工作节点的网络名称空间，应用进程将直接监听工作节点IP地址和协议端口。 其他容器端口映射其他Kubernetes系统的网络模型中，各Pod的IP地址处于同一网络平面，无论是否为容器指定了要暴露的端口都不会影响集群中其他节点之上的Pod客户端对其进行访问，这意味着，任何在非本地回环接口lo上监听的端口都可直接通过Pod网络被请求。从这个角度来说，容器端口只是信息性数据，它仅为集群用户提供了一个快速了解相关Pod对象的可访问端口的途径，但显式指定容器的服务端口可额外为其赋予一个名称以方便按名称调用。定义容器端口的ports字段的值是一个列表，由一到多个端口对象组成，它的常用嵌套字段有如下几个。 containerPort ：必选字段，指定在Pod对象的IP地址上暴露的容器端口，有效范围为(0,65536)；使用时，需要指定为容器应用程序需要监听的端口。 name ：当前端口的名称标识，必须符合IANA_SVC_NAME规范且在当前Pod内要具有唯一性；此端口名可被Service资源按名调用。 protocol ：端口相关的协议，其值仅支持TCP、SCTP或UDP三者之一，默认为TCP。 需要借助于Pod所在节点将容器服务暴露至集群外部时，还需要使用hostIP与hostPort两个字段来指定占用的工作节点地址和端口。如图4-11所示的Pod A与Pod C可分别通过各自所在节点上指定的hostIP和hostPort服务于客户端请求。 hostPort ：主机端口，它将接收到的请求通过NAT机制转发至由container-Port字段指定的容器端口。 hostIP ：主机端口要绑定的主机IP，默认为主机之上所有可用的IP地址；该字段通常使用默认值。 下面的资源配置清单示例（pod-using-hostport.yaml）中定义的demo容器指定了要暴露容器上TCP协议的80端口，并将之命名为http，该容器可通过工作节点的10080端口接入集群外部客户端的请求。 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-using-hostport namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 protocol: TCP hostPort: 10080 在集群中创建配置清单中定义的Pod对象后，需获取其被调度至的目标节点，例如下面第二个命令结果中的k8s-node02.ilinux.io/172.29.9.12，而后从集群外部向该节点的10080端口发起Web请求进行访问测试： 123456~$ kubectl apply -f pod-using-hostport.yaml pod/pod-using-hostport~$ kubectl describe pods/ pod-using-hostport | grep &quot;^Node:&quot;Node: k8s-node02.ilinux.io/172.29.9.12~$ curl 172.29.9.12:10080iKubernetes demoapp v1.0 !! ClientIP: 172.29.0.1, ServerName: pod-using-hostport, ServerIP: 10.244.2.9! 注意，hostPort与NodePort类型的Service对象暴露端口的方式不同，NodePort是通过所有节点暴露容器服务，而hostPort则能经由Pod对象所在节点的IP地址进行。 配置Pod使用节点网络同一个Pod对象的各容器运行于一个独立、隔离的Network、UTS和IPC名称空间中，共享同一个网络协议栈及相关的网络设备，但也有些特殊的Pod对象需要运行于所在节点的名称空间中，执行系统级的管理任务（例如查看和操作节点的网络资源甚至是网络设备等），或借助节点网络资源向集群外客户端提供服务等，如图4-12中的右图所示。 由kubeadm部署的Kubernetes集群中的kube-apiserver、kube-controller-manager、kube-scheduler，以及kube-proxy和kube-flannel等通常都是第二种类型的Pod对象。网络名称空间是Pod级别的属性，用户配置的Pod对象，仅需要设置其spec.hostNetwork的属性为true即可创建共享节点网络名称空间的Pod对象，如下面保存在pod-using-hostnetwork.yaml文件中的配置清单所示。 1234567891011apiVersion: v1kind: Podmetadata: name: pod-using-hostnetwork namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent hostNetwork: true 将上面配置清单中定义的pod-using-hostnetwork创建于集群上，并查看主机名称或网络接口的相关属性信息以验证它是否能共享使用工作节点的网络名称空间。 1234~$ kubectl apply -f pod-using-hostnetwork.yaml pod/pod-using-hostnetwork created~$ kubectl exec -it pod-using-hostnetwork -- hostnamek8s-node01.ilinux.io 上面第二个命令的结果显示出的主机名称，表示该Pod已然共享了其所在节点的UTS名称空间，以及Network和IPC名称空间。这意味着，Pod对象中运行容器化应用可在其所在的工作节点的IP地址之上监听，这可以通过直接向k8s-node01.ilinux.io节点发起请求来验证。 12~$ curl k8s-node01.ilinux.ioiKubernetes demoapp v1.0 !! ClientIP: 172.29.9.1, ServerName: k8s-node01.ilinux.io, ServerIP: 172.29.9.11! 容器安全上下文Kubernetes为安全运行Pod及容器运行设计了安全上下文机制，该机制允许用户和管理员定义Pod或容器的特权与访问控制，以配置容器与主机以及主机之上的其他容器间的隔离级别。安全上下文就是一组用来决定容器是如何创建和运行的约束条件，这些条件代表创建和运行容器时使用的运行时参数。需要提升容器权限时，用户通常只应授予容器执行其工作所需的访问权限，以“最小权限法则”来抑制容器对基础架构及其他容器产生的负面影响。Kubernetes支持用户在Pod及容器级别配置安全上下文，并允许管理员通过Pod安全策略在集群全局级别限制用户在创建和运行Pod时可设定的安全上下文。本节仅描述Pod和容器级别的配置，Pod安全策略的话题将在第9章展开。Pod和容器的安全上下文设置包括以下几个方面。 自主访问控制（DAC）：传统UNIX的访问控制机制，它允许对象（OS级别，例如文件等）的所有者基于UID和GID设定对象的访问权限。 Linux功能：Linux为突破系统上传统的两级用户（root和普通用户）授权模型，而将内核管理权限打散成多个不同维度或级别的权限子集，每个子集称为一种“功能”或“能力”，例如CAP_NET_ADMIN、CAP_SYS_TIME、CAP_SYS_PTRACE和CAP_SYS_ADMIN等，从而允许进程仅具有一部分内核管理功能就能完成必要的管理任务。 seccomp：全称为secure computing mode，是Linux内核的安全模型，用于为默认可发起的任何系统调用进程施加控制机制，人为地禁止它能够发起的系统调用，有效降低了程序被劫持时的危害级别。 AppArmor：全称为Application Armor，意为“应用盔甲”，是Linux内核的一个安全模块，通过加载到内核的配置文件来定义对程序的约束与控制。 SELinux：全称为Security-Enhanced Linux，意为安全加强的Linux，是Linux内核的一个安全模块，提供了包括强制访问控制在内的访问控制安全策略机制。 Privileged模式：即特权模式容器，该模式下容器中的root用户拥有所有的内核功能，即具有真正的管理员权限，它能看到主机上的所有设备，能够挂载文件系统，甚至可以在容器中运行容器；容器默认运行于非特权（unprivileged）模式。 AllowPrivilegeEscalation：控制是否允许特权升级，即进程是否能够获取比父进程更多的特权；运行于特权模式或具有CAP_SYS_ADMIN能力的容器默认允许特权升级。这些安全上下文相关的特性多数嵌套定义在Pod或容器的securityContext字段中，而且有些特性对应的嵌套字段还不止一个。而seccomp和AppArmor的安全上下文则需要以资源注解的方式进行定义，而且仅能由管理员在集群级别进行Pod安全策略配置。 配置格式速览安全上下文可分别设置Pod级别和容器级别。但有些参数并不适合通用设定，例如特权模式、特权升级、只读根文件系统和内核能力等，它们只可用于容器之上。但也有参数仅可用于Pod级别进行通用设定，例如设置内核参数的sysctl和设置存储卷新件文件默认属组的fsgroup等。下面以Pod资源的配置格式给出了这些配置选项，以便于读者快速预览和了解安全上下文的用法。 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: &#123;…&#125;spec: securityContext: # Pod级别的安全上下文，对内部所有容器均有效 runAsUser &lt;integer&gt; # 以指定的用户身份运行容器进程，默认由镜像中的USER指定 runAsGroup &lt;integer&gt; # 以指定的用户组运行容器进程，默认使用的组随容器运行时设定 supplementalGroups &lt;[]integer&gt; # 为容器中1号进程的用户添加的附加组 fsGroup &lt;integer&gt; # 为容器中的1号进程附加一个专用组，其功能类似于sgid runAsNonRoot &lt;boolean&gt; # 是否以非root身份运行 seLinuxOptions &lt;Object&gt; # SELinux的相关配置 sysctls &lt;[]Object&gt; # 应用到当前Pod名称空间级别的sysctl参数设置列表 windowsOptions &lt;Object&gt; # Windows容器专用的设置 containers: - name: … image: … securityContext: # 容器级别的安全上下文，仅在当前容器生效 runAsUser &lt;integer&gt; # 以指定的用户身份运行容器进程 runAsGroup &lt;integer&gt; # 以指定的用户组运行容器进程 runAsNonRoot &lt;boolean&gt; # 是否以非root身份运行 allowPrivilegeEscalation &lt;boolean&gt; # 是否允许特权升级 capabilities &lt;Object&gt; # 为当前容器添加（add）或删除（drop）内核能力 add &lt;[]string&gt; # 添加由列表定义的各内核能力 drop &lt;[]string&gt; # 移除由列表定义的各内核能力 privileged &lt;boolean&gt; # 是否运行为特权容器 procMount &lt;string&gt; # 设置容器的procMount类型，默认为DefaultProcMount； readOnlyRootFilesystem &lt;boolean&gt; # 是否将根文件系统设置为只读模式 seLinuxOptions &lt;Object&gt; # SELinux的相关配置 windowsOptions &lt;Object&gt; # Windows容器专用的设置 Kubernetes默认以非特权模式创建并运行容器，同时禁用了其他与管理功能相关的内核能力，但未额外设定其他上下文参数。 管理容器进程的运行身份制作Docker镜像时，Dockerfile支持以USER指令明确指定运行应用进程时的用户身份。对于未通过USER指令显式定义运行身份的镜像，创建和启动容器时，其进程的默认用户身份为容器中的root用户和root组，该用户有着其他一些附加的系统用户组，例如sys、daemon、wheel和bin等。然而，有些应用程序的进程需要以特定的专用用户身份运行，或者以指定的用户身份运行时才能获得更好的安全特性，这种需求可以在Pod或容器级别的安全上下文中使用runAsUser得以解决，必要时可同时使用runAsGroup设置进程的组身份。下面的资源清单（securitycontext-runasuer-demo.yaml）配置以1001这个UID和GID的身份来运行容器中的demoapp应用，考虑到非特权用户默认无法使用1024以下的端口号，文件中通过环境变量改变了应用监听的端口。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: securitycontext-runasuser-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: PORT value: &quot;8080&quot; securityContext: runAsUser: 1001 runAsGroup: 1001 下面的命令先将配置清单中定义的Pod对象securitycontext-runasuser-demo创建到集群上，随后的两条命令验证了容器用户身份确为配置中预设的UID和GID。 1234567~$ kubectl apply -f securitycontext-runasuser-demo.yaml pod/securitycontext-runasuser-demo created~$ kubectl exec securitycontext-runasuser-demo -- id uid=1001 gid=1001$ kubectl exec securitycontext-runasuser-demo -- ps auxPID USER TIME COMMAND 1 1001 0:00 python3 /usr/local/bin/demo.py 还可在上面的配置清单中的安全上下文定义中，同时使用supplement-Groups选项定义主进程用户的其他附加用户组，这对于有着复杂权限模型的应用是一个非常有用的选项。另外，若运行容器时使用的镜像文件中已经使用USER指令指定了非root用户的运行身份，我们也可以在安全上下文中使用runAsNonRoot参数定义容器必须使用指定的非root用户身份运行，而无须使用runAsUser参数额外指定用户。 管理容器的内核功能传统UNIX仅实现了特权和非特权两类进程，前者是指以0号UID身份运行的进程，而后者则是从属非0号UID用户的进程。Linux内核从2.2版开始将附加于超级用户的权限分割为多个独立单元，这些单元是线程级别的，它们可配置在每个线程之上，为其赋予特定的管理能力。Linux内核常用的功能包括但不限于如下这些。 CAP_CHOWN：改变文件的UID和GID。 CAP_MKNOD：借助系统调用mknod()创建设备文件。 CAP_NET_ADMIN：网络管理相关的操作，可用于管理网络接口、netfilter上的iptables规则、路由表、透明代理、TOS、清空驱动统计数据、设置混杂模式和启用多播功能等。 CAP_NET_BIND_SERVICE：绑定小于1024的特权端口，但该功能在重新映射用户后可能会失效。 AP_NET_RAW：使用RAW或PACKET类型的套接字，并可绑定任何地址进行透明代理。 CAP_SYS_ADMIN：支持内核上的很大一部分管理功能。 CAP_SYS_BOOT：重启系统。 CAP_SYS_CHROOT：使用chroot()进行根文件系统切换，并能够调用setns()修改Mount名称空间。 CAP_SYS_MODULE：装载内核模块。 CAP_SYS_TIME：设定系统时钟和硬件时钟。 CAP_SYSLOG：调用syslog()执行日志相关的特权操作等。 系统管理员可以通过get命令获取程序文件上的内核功能，并可使用setcap命令为程序文件设定内核功能或取消（-r选项）其已有的内核功能。而为Kubernetes上运行的进程设定内核功能则需要在Pod内容器上的安全上下文中嵌套capabilities字段，添加和移除内核能力还需要分别在下一级嵌套中使用add或drop字段。这两个字段可接受以内核能力名称为列表项，但引用各内核能力名称时需移除CAP_前缀，例如可使用NET_ADMIN和NET_BIND_SERVICE这样的功能名称。下面的配置清单（securitycontext-capabilities-demo.yaml）中定义的Pod对象的demo容器，在安全上下文中启用了内核功能NET_ADMIN，并禁用了CHOWN。demo容器的镜像未定义USER指令，它将默认以root用户的身份运行容器应用。 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: securitycontext-capabilities-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent command: [&quot;/bin/sh&quot;,&quot;-c&quot;] args: [&quot;/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80 &amp;&amp; /usr/bin/python3 /usr/local/bin/demo.py&quot;] securityContext: capabilities: add: [&#x27;NET_ADMIN&#x27;] drop: [&#x27;CHOWN&#x27;] 容器中的root用户将默认映射为系统上的普通用户，它实际上并不具有管理网络接口、iptables规则和路由表等相关的权限，但内核功能NET_ADMIN可以为其开放此类权限。但容器中的root用户默认就具有修改容器文件系统上的文件从属关系的能力，而禁用CHOWN功能则关闭了这种操作权限。下面创建该Pod对象并运行在集群上，来验证清单中的配置。 12~ $ kubectl apply -f securitycontext-capabilities-demo.yaml pod/securitycontext-capabilities-demo created 而后，检查Pod网络名称空间中netfilter之上的规则，清单中的iptables命令添加的规则位于NAT表的PREROUTING链上。下面的命令结果表示iptables命令已然生成的规则，NET_ADMIN功能启用成功。 1234$ kubectl exec securitycontext-capabilities-demo -- iptables -t nat -nL PREROUTING Chain PREROUTING (policy ACCEPT)target prot opt source destination REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 redir ports 80 接着，下面用于检查demo容器中的root用户是否能够修改容器文件系统上文件的属主和属组的命令结果表示，其CHOWN功能已然成功关闭。 123$ kubectl exec securitycontext-capabilities-demo -- chown 200.200 /etc/hostschown: /etc/hosts: Operation not permittedcommand terminated with exit code 1 内核的各项功能均可按其原本的意义在容器的安全上下文中按需打开或关闭，但SYS_ADMIN功能拥有内核中的许多管理权限，实在太过强大，出于安全方面的考虑，用户应该基于最小权限法则组合使用内核功能完成容器运行。 特权模式容器相较于内核功能，SYS_ADMIN赋予了进程很大一部分的系统级管理功能，特权（privileged）容器几乎将宿主机内核的完整权限全部开放给了容器进程，它提供的是远超SYS_ADMIN的授权，包括写操作到/proc和/sys目录以及管理硬件设备等，因而仅应该用到基础架构类的系统级管理容器之上。例如，使用kubeadm部署的集群中，kube-proxy中的容器就运行于特权模式。下面的第一个命令从kube-system名称空间中取出一个kube-proxy相关的Pod对象名称，第二个命令则用于打印该Pod对象的配置清单，限于篇幅，这里仅列出了其中一部分内容： 123456789101112131415~$ pod-name=$(kubectl get pods -l k8s-app=kube-proxy -n kube-system \\ -o jsonpath=&#123;.items[0].metadata.name&#125;)~$ kubectl get pods $pod-name -n kube-system -o yaml#从命令结果中截取的启动容器应用的命令及传递的参数containers: - command: - /usr/local/bin/kube-proxy - --config=/var/lib/kube-proxy/config.conf - --hostname-override=$(NODE_NAME) image: …… imagePullPolicy: IfNotPresent name: kube-proxy resources: &#123;&#125; securityContext: privileged: true 上面保留的命令结果的最后两行是定义特权容器的格式，唯一用到的privileged字段只能嵌套在容器的安全上下文中，它使用布尔型值，true表示启用特权容器机制。 在Pod上使用sysctlLinux系统上的sysctl接口允许在运行时修改内核参数，管理员可通过/proc/sys/下的虚拟文件系统接口来修改或查询这些与内核、网络、虚拟内存或设备等各子系统相关的参数。Kubernetes也允许在Pod上独立安全地设置支持名称空间级别的内核参数，它们默认处于启用状态，而节点级别内核参数则被认为是不安全的，它们默认处于禁用状态。截至目前，仅kernel.shm_rmid_forced、net.ipv4.ip_local_port_range和net.ipv4.tcp_syncookies这3个内核参数被Kubernetes视为安全参数，它们可在Pod安全上下文的sysctl参数内嵌套使用，而余下的绝大多数的内核参数都是非安全参数，需要管理员手动在每个节点上通过kubelet选项逐个启用后才能配置到Pod上。例如，在各工作节点上编辑/etc/default/kubelet文件，添加如下内容以允许在Pod上使用指定的两个非安全的内核参数，并重启kubelet服务使之生效。 1KUBELET_EXTRA_ARGS=&#x27;--allowed-unsafe-sysctls=net.core.somaxconn,net.ipv4.ip_unprivileged_port_start&#x27; net.core.somaxconn参数定义了系统级别入站连接队列最大长度，默认值是128；而net.ipv4.ip_unprivileged_port_start参数定义的是非特权用户可以使用的内核端口起始值，默认为1024，它限制了非特权用户所能够使用的端口范围。下面配置清单（securitycontext-sysctls-demo.yaml）中定义的Pod对象在安全上下文中通过sysctls字段嵌套使用了一个安全的内核参数kernel.shm_rmid_forced，以及一个已经启用的非安全内核参数net.ipv4.ip_unprivileged_port_start，它将该非安全内核参数的值设置为0来允许非特权用户使用11024以内端口的权限。 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: securitycontext-sysctls-demo namespace: defaultspec: securityContext: sysctls: - name: kernel.shm_rmid_forced value: &quot;0&quot; - name: net.ipv4.ip_unprivileged_port_start value: &quot;0&quot; containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent securityContext: runAsUser: 1001 runAsGroup: 1001 尽管上面配置清单设定了以非特权用户1001的身份运行容器应用，但受上面内核参数的影响，非管理员用户也具有了监听80端口的权限，因而不会遇到无法监听特权端口的情形。下面将配置清单中定义的资源创建在集群之上，来验证设定的结果。 12~$ kubectl apply -f securitycontext-sysctls-demo.yaml pod/securitycontext-sysctls-demo created 下面的命令结果显示，以普通用户身份运行的demo容器成功监听了TCP协议的80端口。 1234~ $ kubectl exec securitycontext-sysctls-demo -- netstat -tnlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1/python3 在Pod对象之上启用非安全内核参数，其配置结果可能会存在无法预料的结果，在正式使用之前一定要经过充分测试。例如，在某一Pod之上同时配置启用前面示例的两个非安全内核参数可能存在生效结果异常的情况。本节中介绍了设置Pod与容器安全上下文配置方法及几种常用使用方式。从示例中我们可以看出，设置特权容器和添加内核功能等，以及在Pod上共享宿主机的Network和PID名称空间等，对于多项目或多团队共享的Kubernetes集群存在着不小的安全隐患，这就要求管理员应该在集群级别使用Pod安全策略（PodSecurityPolicy），来精心管控这些与安全相关配置的运用能力。 容器应用的管理接口健康状态监测接口监测容器自身运行的API包括分别用于健康状态检测、指标、分布式跟踪和日志等实现类型，如图4-13所示。即便没有完全实现，至少容器化应用也应该提供用于健康状态检测（liveness和readiness）的API，以便编排系统能更准确地判定应用程序的运行状态。 Kubelet仅能在控制循环中根据容器主进程的运行状态来判断其健康与否，主进程以非0状态码退出代表处于不健康状态，其他均为正常状态。然而，有些异常场景中，仍处于运行状态的进程内部的业务处理机制可能已然处于僵死状态或陷入死循环等，无法正常处理业务请求，对于这种状态的判断便要依赖应用自身专用于健康状态监测的接口。存活状态（liveness）检测用于定期检测容器是否正常运行，就绪状态（readiness）检测用于定期检测容器是否可以接收流量，它们能够通过减少运维问题和提高服务质量来使服务更健壮和更具弹性。Kubernetes在Pod内部的容器资源上提供了livenessProbe和readinessProbe两个字段，分别让用户自定义容器应用的存活状态和就绪状态检测。 存活状态检测：用于判定容器是否处于“运行”状态；若此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为Success。 就绪状态检测：用于判断容器是否准备就绪并可对外提供服务；未通过该检测时，端点控制器（例如Service对象）会将其IP地址从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中；未定义就绪状态检测的容器的默认状态为Success。容器探测是Pod对象生命周期中的一项重要日常任务，它由kubelet周期性执行。kubelet可在活动容器上分别执行由用户定义的启动状态检测（startupProbe）、存活状态检测（livenessProbe）和就绪状态检测（readinessProbe），定义在容器上的存活状态和就绪状态操作称为检测探针，它要通过容器的句柄（handler）进行定义。Kubernetes定义了用于容器探测的3种句柄。 ExecAction：通过在容器中执行一个命令并根据其返回的状态码进行的诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。 TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常状态，否则为不健康状态。 HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起http GET请求进行诊断，响应码为2xx或3xx即为成功，否则为失败。上面的每种探测方式都可能存在3种返回结果：Success（成功）、Failure（失败）或Unknown（未知），仅第一种结果表示成功通过检测。另外，Kubernetes自v1.16版本起还支持启动状态（startup）检测。将传统模式开发的大型应用程序迁移至容器编排平台运行时，可能需要相当长的时间进行启动后的初始化，但其初始过程是否正确完成的检测机制和探测参数都可能有别于存活状态检测，例如需要更长的间隔周期和更高的错误阈值等。该类检测的结果处理机制与存活状态检测相同，检测失败时kubelet将杀死容器并根据其restartPolicy决定是否将其重启，而未定义时的默认状态为Success。需要注意的是，一旦定义了启动检测探针，则必须等启动检测成功完成之后，存活探针和就绪探针才可启动。 容器存活状态检测存活性探测是隶属于容器级别的配置，kubelet可基于它判定何时需要重启容器。目前，Kubernetes在容器上支持的存活探针有3种类型：ExecAction、TCPSocketAction和HTTPGetAction。 1. 存活探针配置格式Pod配置格式中，spec.containers.livenessProbe字段用于定义此类检测，配置格式如下所示。但一个容器之上仅能定义一种类型的探针，即exec、httpGet和tcpSocket三者互斥，它们不可在一个容器同时使用。 12345678910111213spec: containers: - name: … image: … livenessProbe: exec &lt;Object&gt; # 命令式探针 httpGet &lt;Object&gt; # http GET类型的探针 tcpSocket &lt;Object&gt; # tcp Socket类型的探针 initialDelaySeconds &lt;integer&gt; # 发起初次探测请求的延后时长 periodSeconds &lt;integer&gt; # 请求周期 timeoutSeconds &lt;integer&gt; # 超时时长 successThreshold &lt;integer&gt; # 成功阈值 failureThreshold &lt;integer&gt; # 失败阈值 探针之外的其他字段用于定义探测操作的行为方式，用户没有明确定义这些属性字段时，它们会使用各自的默认值: initialDelaySeconds ：首次发出存活探测请求的延迟时长，即容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，即容器启动后便立刻进行探测；该参数值应该大于容器的最大初始化时长，以避免程序永远无法启动。 timeoutSeconds ：存活探测的超时时长，显示为timeout属性，默认为1秒，最小值也为1秒；应该为此参数设置一个合理值，以避免因应用负载较大时的响应延迟导致Pod被重启。 periodSeconds ：存活探测的频度，显示为period属性，默认为10秒，最小值为1秒；需要注意的是，过高的频率会给Pod对象带来较大的额外开销，而过低的频率又会使得对错误反应不及时。 successThreshold ：处于失败状态时，探测操作至少连续多少次的成功才被认为通过检测，显示为#success属性，仅可取值为1。 failureThreshold：处于成功状态时，探测操作至少连续多少次的失败才被视为检测不通过，显示为#failure属性，默认值为3，最小值为1；尽量设置宽容一些的失败计数，能有效避免一些场景中的服务级联失败。使用kubectl describe命令查看配置了存活性探测的Pod对象的详细信息时，其相关容器中会输出类似如下一行内容，它给出了探测方式及其额外的配置属性delay、timeout、period、success和failure及其各自的相关属性值。 1Liveness: …… delay=0s timeout=1s period=10s #success=1 #failure=3 exec探针exec类型的探针通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，命令状态返回值为0表示“成功”通过检测，其余值均为“失败”状态。spec.containers.livenessProbe.exec字段只有一个可用属性command，用于指定要执行的命令。demoapp应用程序通过/livez输出内置的存活状态检测接口，服务正常时，它以200响应码返回OK，否则为5xx响应码，我们可基于exec探针使用HTTP客户端向该path发起请求，并根据命令的结果状态来判定容器健康与否。系统刚启动时，对该路径的请求将会延迟大约不到5秒的时长，且默认响应值为OK。它还支持由用户按需向该路径发起POST请求，并向参数livez传值来自定义其响应内容。下面是定义在资源清单文件liveness-exec-demo.yaml中的示例。 123456789101112131415apiVersion: v1kind: Podmetadata: name: liveness-exec-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: exec: command: [&#x27;/bin/sh&#x27;, &#x27;-c&#x27;, &#x27;[ &quot;$(curl -s 127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;] initialDelaySeconds: 5 periodSeconds: 5 该配置清单中定义的Pod对象为demo容器定义了exec探针，它通过在容器本地执行测试命令来比较curl -s 127.0.0.1/livez的返回值是否为OK以判定容器的存活状态。命令成功执行则表示容器正常运行，否则3次检测失败之后则将其判定为检测失败。首次检测在容器启动5秒之后进行，请求间隔也是5秒。 12~$ kubectl apply -f liveness-exec-demo.yaml pod/liveness-exec-demo created 创建完成后，Pod中的容器demo会正常运行，存活检测探针也不会遇到检测错误而导致容器重启。若要测试存活状态检测的效果，可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。 1~$ kubectl exec liveness-exec-demo -- curl -s -X POST -d &#x27;livez=FAIL&#x27; 127.0.0.1/livez 而后经过1个检测周期，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令结果中的事件可知，容器因健康状态检测失败而被重启。 12345~$ kubectl describe pods/liveness-exec-demo……Events: Warning Unhealthy 17s (x3 over 27s) kubelet, k8s-node03.ilinux.io Liveness probe failed:Normal Killing 17s kubelet, k8s-node03.ilinux.io Container demo failed liveness probe, will be restarted 另外，下面输出信息中的Containers一段中还清晰显示了容器健康状态检测及状态变化的相关信息：容器当前处于Running状态，但前一次是为Terminated，原因是退出码为137的错误信息，它表示进程是被外部信号所终止。137事实上由两部分数字之和生成：128+signum，其中signum是导致进程终止的信号的数字标识，9表示SIGKILL，这意味着进程是被强行终止的。 12345678910111213Containers: demo: …… State: Running Started: Thu, 29 Aug 2020 14:30:02 +0800 Last State: Terminated Reason: Error Exit Code: 137 Started: Thu, 29 Aug 2020 14:22:20 +0800 Finished: Thu, 29 Aug 2020 14:30:02 +0800 Ready: True Restart Count: 1…… 待容器重启完成后，/livez的响应内容会重置镜像中默认定义的OK，因而其存活状态检测不会再遇到错误，这模拟了一种典型的通过“重启”应用而解决问题的场景。需要特别说明的是，exec指定的命令运行在容器中，会消耗容器的可用计算资源配额，另外考虑到探测操作的效率等因素，探测操作的命令应该尽可能简单和轻量。 HTTP探针HTTP探针是基于HTTP协议的探测（HTTPGetAction），通过向目标容器发起一个GET请求，并根据其响应码进行结果判定，2xx或3xx类的响应码表示检测通过。HTTP探针可用配置字段有如下几个。▪host ：请求的主机地址，默认为Pod IP；也可以在httpHeaders使用“Host:”来定义。▪port ：请求的端口，必选字段。▪httpHeaders &lt;[]Object&gt;：自定义的请求报文头部。▪path ：请求的HTTP资源路径，即URL path。▪scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。下面是一个定义在资源清单文件liveness-httpget-demo.yaml中的示例，它使用HTTP探针直接对/livez发起访问请求，并根据其响应码来判定检测结果。 12345678910111213141516apiVersion: v1kind: Podmetadata: name: liveness-httpget-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 scheme: HTTP initialDelaySeconds: 5 上面清单文件中定义的httpGet测试中，请求的资源路径为/livez，地址默认为Pod IP，端口使用了容器中定义的端口名称http，这也是明确为容器指明要暴露的端口的用途之一。下面测试其效果，首先创建此Pod对象： 12~ $ kubectl apply -f liveness-httpget-demo.yamlpod/liveness-httpget-demo created 首次检测为延迟5秒，这刚好超过了demoapp的/livez接口默认会延迟响应的时长。镜像中定义的默认响应是以200状态码响应、以OK为响应结果，存活状态检测会成功完成。为了测试存活状态检测的效果，同样可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。~$ kubectl exec liveness-httpget-demo – curl -s -X POST -d ‘livez=FAIL’ 127.0.0.1/livez而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。 1234~ $ kubectl describe pods/liveness-httpget-demo……Warning Unhealthy 7s (x3 over 27s) kubelet, k8s-node01.ilinux.io Liveness probe failed: HTTP probe failed with statuscode: 520 Normal Killing 7s kubelet, k8s-node01.ilinux.io Container demo failed liveness probe, will be restarted 一般来说，HTTP探针应该针对专用的URL路径进行。这种检测方式仅对分层架构中的当前一层有效，例如，它能检测应用程序工作正常与否的状态，但重启操作却无法解决其后端服务（例如数据库或缓存服务）导致的故障。此时，容器可能会被反复重启，直到后端服务恢复正常。其他两种检测方式也存在类似的问题。 TCP探针TCP探针是基于TCP协议进行存活性探测（TCPSocketAction），通过向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。相比较来说，它比基于HTTP协议的探测要更高效、更节约资源，但精准度略低，毕竟连接建立成功未必意味着页面资源可用。spec.containers.livenessProbe.tcpSocket字段用于定义此类检测，它主要有以下两个可用字段：1）host ：请求连接的目标IP地址，默认为Pod自身的IP；2）port ：请求连接的目标端口，必选字段，可以名称调用容器上显式定义的端口。下面是一个定义在资源清单文件liveness-tcpsocket-demo.yaml中的示例，它向Pod对象的TCP协议的80端口发起连接请求，并根据连接建立的状态判定测试结果。为了能在容器中通过iptables阻止接收对80端口的请求以验证TCP检测失败，下面的配置还在容器上启用了特殊的内核权限NET_ADMIN。 12345678910111213141516171819202122apiVersion: v1kind: Podmetadata: name: liveness-tcpsocket-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 securityContext: capabilities: add: - NET_ADMIN livenessProbe: tcpSocket: port: http periodSeconds: 5 initialDelaySeconds: 20 按照配置，将该清单中的Pod对象创建在集群之上，20秒之后即会进行首次的tcpSocket检测。 12~$ kubectl apply -f liveness-tcpsocket-demo.yaml pod/liveness-tcpsocket-demo created 容器应用demoapp启动后即监听于TCP协议的80端口，tcpSocket检测也就可以成功执行。为了测试效果，可使用下面的命令在Pod的Network名称空间中设置iptables规则以阻止对80端口的请求： 1~$ kubectl exec liveness-tcpsocket-demo -- iptables -A INPUT -p tcp --dport 80 -j REJECT 而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。 123456~ $ kubectl describe pods/liveness-httpget-demo……Events:……Warning Unhealthy 3s (x3 over 23s) kubelet, k8s-node03.ilinux.io Liveness probe failed: dial tcp 10.244.3.19:80: i/o timeout Normal Killing 3s kubelet, k8s-node03.ilinux.io Container demo failed liveness probe, will be restarted 不过，重启容器并不会导致Pod资源的重建操作，网络名称空间的设定附加在pause容器之上，因而添加的iptables规则在应用重启后依然存在，它是一个无法通过重启而解决的问题。若需要手消除该问题，删除添加至Pod中的iptables规则即可。 Pod的重启策略Pod对象的应用容器因程序崩溃、启动状态检测失败、存活状态检测失败或容器申请超出限制的资源等原因都可能导致其被终止，此时是否应该重启则取决于Pod上的restartPolicy（重启策略）字段的定义，该字段支持以下取值。1）Always：无论因何原因、以何种方式终止，kubelet都将重启该Pod，此为默认设定。2）OnFailure：仅在Pod对象以非0方式退出时才将其重启。3）Never：不再重启该Pod。需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一个节点上重新启动Pod对象的相关容器。首次需要重启的容器，其重启操作会立即进行，而再次重启操作将由kubelet延迟一段时间后进行，反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么被终止，直到节点故障、被删除或被驱逐。 容器就绪状态检测Pod对象启动后，应用程序通常需要一段时间完成其初始化过程，例如加载配置或数据、缓存初始化等，甚至有些程序需要运行某类预热过程等，因此通常应该避免在Pod对象启动后立即让其处理客户端请求，而是需要等待容器初始化工作执行完成并转为“就绪”状态，尤其是存在其他提供相同服务的Pod对象的场景更是如此。与存活探针不同的是，就绪状态检测是用来判断容器应用就绪与否的周期性（默认周期为10秒钟）操作，它用于检测容器是否已经初始化完成并可服务客户端请求。与存活探针触发的操作不同，检测失败时，就绪探针不会杀死或重启容器来确保其健康状态，而仅仅是通知其尚未就绪，并触发依赖其就绪状态的其他操作（例如从Service对象中移除此Pod对象），以确保不会有客户端请求接入此Pod对象。就绪探针也支持Exec、HTTP GET和TCP Socket这3种探测方式，且它们各自的定义机制与存活探针相同。因而，将容器定义中的livenessProbe字段名替换为readinessProbe，并略做适应性修改即可定义出就绪性检测的配置来，甚至有些场景中的就绪探针与存活探针的配置可以完全相同。demoapp应用程序通过/readyz暴露了专用于就绪状态检测的接口，它于程序启动约15秒后能够以200状态码响应、以OK为响应结果，也支持用户使用POST请求方法通过readyz参数传递自定义的响应内容，不过所有非OK的响应内容都被响应以5xx的状态码。一个简单的示例如下面的配置清单（readiness-httpget-demo.yaml）所示。 1234567891011121314151617181920apiVersion: v1kind: Podmetadata: name: readiness-httpget-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 80 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 2 periodSeconds: 5 failureThreshold: 3 restartPolicy: Always 下面来测试该Pod就绪探针的作用。按照配置，将Pod对象创建在集群上约15秒后启动首次探测，在该探测结果成功返回之前，Pod将一直处于未就绪状态： 12~ $ kubectl apply -f readiness-httpget-demo.yamlpod/readiness-httpget-demo created 接着运行kubectl get -w命令监视其资源变动信息，由如下命令结果可知，尽管Pod对象处于Running状态，但直到就绪检测命令执行成功后Pod资源才转为“就绪”。 1234~$ kubectl get pods/readiness-httpget-demo -wNAME READY STATUS RESTARTS AGEreadiness-httpget-demo 0/1 Running 0 10sreadiness-httpget-demo 1/1 Running 0 20s Pod运行过程中的某一时刻，无论因何原因导致的就绪状态检测的连续失败都会使得该Pod从就绪状态转变为“未就绪”，并且会从各个通过标签选择器关联至该Pod对象的Service后端端点列表中删除。为了测试就绪状态检测效果，下面修改/readyz响应以非OK内容。 1~$ kubectl exec readiness-httpget-demo -- curl -s -XPOST -d &#x27;readyz=FAIL&#x27; 127.0.0.1/readyz 而后在至少1个检测周期之后，通过该Pod的描述信息可以看到就绪检测失败相关的事件描述，命令及结果如下所示： 123~$ kubectl describe pods/readiness-httpget-demo……Warning Unhealthy 4s (x11 over 54s) kubelet, k8s-node03.ilinux.io Readiness probe failed: HTTP probe failed with statuscode: 521 未定义就绪性检测的Pod对象在进入Running状态后将立即“就绪”，这在容器需要时间进行初始化的场景中可能会导致客户请求失败。因此，生产实践中，必须为关键性Pod资源中的容器定义就绪探针。 容器生命周期容器生命周期接口工作示意图: 容器需要处理来自平台的最重要事件是SIGTERM信号，任何需要“干净”关闭进程的应用程序都需要捕捉该信号进行必要处理，例如释放文件锁、关闭数据库连接和网络连接等，而后尽快终止进程，以避免宽限期过后强制关闭信号SIGKILL的介入。SIGKILL信号是由底层操作系统接收的，而非应用进程，一旦检测到该信号，内核将停止为相应进程提供内核资源，并终止进程正在使用的所有CPU线程，类似于直接切断了进程的电源。但是，容器应用很可能是功能复杂的分布式应用程序的一个组件，仅依赖信号终止进程很可能不足以完成所有的必要操作。因此，容器还需要支持postStart和preStop事件，前者常用于为程序启动前进行预热，后者则一般在“干净”地关闭应用之前释放占用的资源。生命周期钩子函数lifecycle hook是编程语言（例如Angular）中常用的生命周期管理组件，它实现了程序运行周期中的关键时刻的可见性，并赋予用户为此采取某种行动的能力。类似地，容器生命周期钩子使它能够感知自身生命周期管理中的事件，并在相应时刻到来时运行由用户指定的处理程序代码。Kubernetes同样为容器提供了postStart和preStop两种生命周期钩子。 postStart：在容器创建完成后立即运行的钩子句柄（handler），该钩子定义的事件执行完成后容器才能真正完成启动过程，如图4-15中的左图所示；不过Kubernetes无法确保它一定会在容器的主应用程序（由ENTRYPOINT定义）之前运行。 preStop：在容器终止操作执行之前立即运行的钩子句柄，它以同步方式调用，因此在其完成之前会阻塞删除容器的操作；这意味着该钩子定义的事件成功执行并退出，容器终止操作才能真正完成，如图4-15中的右图所示。 钩子句柄的实现方式类似于容器探针句柄的类型，同样有exec、httpGet和tcpSocket这3种，它们各自的配置格式和工作逻辑也完全相同，exec在容器中执行用户定义的一个或多个命令，httpGet在容器中向指定的本地URL发起HTTP连接请求，而tcpSocket则试图与指定的端口建立TCP连接。postStart和preStop句柄定义在容器的lifecycle字段中，其内部一次仅支持嵌套使用一种句柄类型。下面的配置清单（lifecycle-demo.yaml）示例中同时使用了postStart和preStop钩子处理相应的事件。 1234567891011121314151617181920212223242526272829apiVersion: v1kind: Podmetadata: name: lifecycle-demo namespace: defaultspec: containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent securityContext: capabilities: add: - NET_ADMIN livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 80 scheme: HTTP initialDelaySeconds: 5 lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-ports 80&#x27;] preStop: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;while killall python3; do sleep 1; done&#x27;] restartPolicy: Always 示例中的demo容器通过postStart执行iptables命令设置端口重定向规则，将发往该Pod IP的8080端口的所有请求重定向至80端口，从而让容器应用能够同时从8080端口接收请求。demo容器又借助preStop执行killall命令，它假设该命令能够更优雅地终止基于Python3运行的容器应用demoapp。将清单中的Pod对象创建于集群中便可展开后续的测试： 12~$ kubectl apply -f lifecycle-demo.yaml pod/lifecycle-demo created 而后可获取容器内网络名称空间中PREROUTING链上的iptables规则，验证postStart钩子事件的执行结果： 1234~$ kubectl exec lifecycle-demo -- iptables -t nat -nL PREROUTING Chain PREROUTING (policy ACCEPT)target prot opt source destination REDIRECT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8080 redir ports 80 上面的配置清单中有意同时添加了httpGet类型的存活探针，我们可以人为地将探针检测结果置为失败状态，以促使kubelet重启demo容器验证preStop钩子事件的执行。不过，该示例中给出的操作是终止容器应用，那么容器成功重启即验证了相应脚本的运行完成。 多容器Pod容器设计模式中的单节点多容器模型中，初始化容器和Sidecar容器是目前使用较多的模式，尤其是服务网格的发展极大促进了Sidecar容器的应用。 初始化容器初始化代码要首先运行，且只能运行一次，它们常用于验证前提条件、基于默认值或传入的参数初始化对象实例的字段等。Pod中的初始化容器（Init Container）功能与此类似，它们为那些有先决条件的应用容器完成必要的初始设置，例如设置特殊权限、生成必要的iptables规则、设置数据库模式，以及获取最新的必要数据等。有很多场景都需要在应用容器启动之前进行部分初始化操作，如等待其他关联组件服务可用、基于环境变量或配置模板为应用程序生成配置文件、从配置中心获取配置等。初始化容器的典型应用需求有如下几种。 用于运行需要管理权限的工具程序，例如iptables命令等，出于安全等方面的原因，应用容器不适合拥有运行这类程序的权限。 提供主容器镜像中不具备的工具程序或自定义代码。 为容器镜像的构建和部署人员提供了分离、独立工作的途径，部署人员使用专用的初始化容器完成特殊的部署逻辑，从而使得他们不必协同起来制作单个镜像文件。 初始化容器和应用容器处于不同的文件系统视图中，因此可分别安全地使用敏感数据，例如Secrets资源等。 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得以满足。 Pod对象中的所有初始化容器必须按定义的顺序串行运行，直到它们全部成功结束才能启动应用容器，因而初始化容器通常很小，以便它们能够以轻量的方式快速运行。某初始化容器运行失败将会导致整个Pod重新启动（重启策略为Never时例外），初始化容器也必将再次运行，因此需要确保所有初始化容器的操作具有幂等性，以避免无法预知的副作用。Pod资源的spec.initContainers字段以列表形式定义可用的初始化容器，其嵌套可用字段类似于spec.containers。下面的资源清单（init-container-demo.yaml）在Pod对象上定义了一个名为iptables-init的初始化容器示例。 123456789101112131415161718192021222324apiVersion: v1kind: Podmetadata: name: init-container-demo namespace: defaultspec: initContainers: # 定义初始化容器 - name: iptables-init image: ikubernetes/admin-box:latest imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80&#x27;] securityContext: capabilities: add: - NET_ADMIN containers: - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 80 示例中，应用容器demo默认监听TCP协议的80端口，但我们又期望该Pod能够在TCP协议的8080端口通过端口重定向方式为客户端提供服务，因此需要在其网络名称空间中添加一条相应的iptables规则。但是，添加该规则的iptables命令依赖于内核中的网络管理权限，出于安全原因，我们并不期望应用容器拥有该权限，因而使用了拥有网络管理权限的初始化容器来完成此功能。下面先把配置清单中定义的资源创建于集群之上： 12~$ kubectl apply -f init-container-demo.yaml pod/init-container-demo created 随后，在Pod对象init-container-demo的描述信息中的初始化容器信息段可以看到如下内容，它表明初始化容器启动后大约1秒内执行完成返回0状态码并成功退出。 123456789101112Command: /bin/sh -cArgs: iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80State: Terminated Reason: Completed Exit Code: 0 Started: Sun, 30 Aug 2020 11:44:28 +0800 Finished: Sun, 30 Aug 2020 11:44:29 +0800Ready: TrueRestart Count: 0 这表明，向Pod网络名称空间中添加iptables规则的操作已经完成，我们可通过应用容器来请求查看这些规则，但因缺少网络管理权限，该查看请求会被拒绝： 1234~$ kubectl exec init-container-demo -- iptables -t nat -vnLiptables v1.8.3 (legacy): can&#x27;t initialize iptables table `nat&#x27;: Permission denied (you must be root)Perhaps iptables or your kernel needs to be upgraded.command terminated with exit code 3 另一方面，应用容器中的服务却可以正常通过Pod IP的8080端口接收并响应，如下面的命令及执行结果所示： 123~$ podIP=$(kubectl get pods/init-container-demo -o jsonpath=&#123;.status.podIP&#125;)~$ curl http://$&#123;podIP&#125;:8080iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: init-container-demo, … 由此可见，初始化容器及容器的postStop钩子都能完成特定的初始化操作，但postStop必须在应用容器内部完成，它依赖的条件（例如管理权限）也必须为应用容器所有，这无疑会为应用容器引入安全等方面的风险。另外，考虑到应用容器镜像内部未必存在执行初始化操作的命令或程序库，使用初始化容器也就成了不二之选。 Sidecar容器Sidecar容器是Pod中与主容器松散耦合的实用程序容器，遵循容器设计模式，并以单独容器进程运行，负责运行应用的非核心功能，以扩展、增强主容器。Sidecar模式最著名的用例是充当服务网格中的微服务的代理应用（例如Istio中的数据控制平面Envoy），其他典型使用场景包括日志传送器、监视代理和数据加载器等。下面的配置清单（sidecar-container-demo.yaml）中定义了两个容器：一个是运行demoapp的主容器demo，一个运行envoy代理的Sidecar容器proxy。 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: sidecar-container-demo namespace: defaultspec: containers: - name: proxy image: envoyproxy/envoy-alpine:v1.13.1 command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;] lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;wget -O /etc/envoy/envoy.yaml https:// raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/ master/chapter4/envoy.yaml&#x27;] - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: HOST value: &quot;127.0.0.1&quot; - name: PORT value: &quot;8080&quot; Envoy程序是服务网格领域著名的数据平面实现，它在Istio服务网格中以Sidecar的模式同每一个微服务应用程序单独组成一个Pod，负责代理该微服务应用的所有通信事件，并为其提供限流、熔断、超时、重试等多种高级功能。这里我们将demoapp视作一个微服务应用，配置Envoy为其代理并调度入站（Ingress）流量，因而在示例中demo容器基于环境变量被配置为监听127.0.0.1地址上一个特定的8080端口，而proxy容器将监听Pod所有IP地址上的80端口，以接收客户端请求。proxy容器上的postStart事件用于为Envoy代理下载一个适用的配置文件，以便将proxy接收到的所有请求均代理至demo容器。下面说明整个测试过程。先将配置清单中定义的对象创建到集群之上。 12~$ kubectl apply -f sidecar-container-demo.yaml pod/sidecar-container-demo created 随后，等待Pod中的两个容器成功启动且都转为就绪状态，可通过各Pod内端口监听的状态来确认服务已然正常运行。下面命令的结果表示，Envoy已经正常运行并监听了TCP协议的80端口和9901端口（Envoy的内置管理接口）。 123456$ kubectl exec sidecar-container-demo -c proxy -- netstat -tnlp Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:9901 0.0.0.0:* LISTEN 1/envoytcp 0 0 0.0.0.0:80 0.0.0.0:* LISTEN 1/envoytcp 0 0 127.0.0.1:8080 0.0.0.0:* LISTEN - 接下来，我们向Pod的80端口发起HTTP请求，若它能以demoapp的页面响应，则表示代理已然成功运行，甚至可以根据响应头部来判断其是否有代理服务Envoy发来的代理响应，如下面的命令及结果所示。 12345678910~$ podIP=$(kubectl get pods/sidecar-container-demo -o jsonpath=&#123;.status.podIP&#125;)$ curl http://$podIPiKubernetes demoapp v1.0 !! ClientIP: 127.0.0.1, ServerName: sidecar-container-demo, ……~$ curl -I http://$podIPHTTP/1.1 200 OKcontent-type: text/html; charset=utf-8content-length: 108server: envoydate: Sun, 22 May 2020 06:43:04 GMTx-envoy-upstream-service-time: 3 虽然Sidecar容器可以称得上是Pod中的常规容器，但直到v1.18版本，Kubernetes才将其添加作为内置功能。在此之前，Pod中的各应用程序彼此间没有区别，用户无从预测和控制容器的启动及关闭顺序，但多数场景都要求Sidecar容器必须要先于普通应用容器启动以做一些准备工作，例如分发证书、创建存储卷或获取一些数据等，且它们需要晚于其他应用容器终止。Kubernetes从v1.18版本开始支持用户在生命周期字段中将容器标记为Sidecar，这类容器全部转为就绪状态后，普通应用容器方可启动。因而，这个新特性根据生命周期将Pod的容器重新划分成了初始化容器、Sidecar容器和应用容器3类。所有的Sidecar容器都是应用容器，唯一不同之处是，需要手动为Sidecar容器在lifecycle字段中嵌套定义type类型的值为Sidecar。配置格式如下所示： 12345678910spec: containers: - name: proxy image: envoyproxy/envoy-alpine:v1.13.1 lifecycle: type: Sidecar …… - name: demo image: ikubernetes/demoapp:v1.0 …… 另外，可能也有一些场景需要Sidecar容器启动晚于普通应用容器，这种特殊的应用需求，目前可通过OpernKruise项目中的SidecarSet提供的PostSidecar模型来解决。将来，该项目或许支持以DAG的方式来灵活编排容器的启动顺序。 资源需求与资源限制资源需求与限制在Kubernetes上，可由容器或Pod请求与消费的“资源”主要是指CPU和内存（RAM），它可统称为计算资源，另一种资源是事关可用存储卷空间的存储资源。相比较而言，CPU属于可压缩型资源，即资源额度可按需弹性变化，而内存（当前）则是不可压缩型资源，CPU和内存资源的配置主要在Pod对象中的容器上进行，并且每个资源存在如图4-16所示的需求和限制两种类型。 资源需求：定义需要系统预留给该容器使用的资源最小可用值，容器运行时可能用不到这些额度的资源，但用到时必须确保有相应数量的资源可用。 资源限制：定义该容器可以申请使用的资源最大可用值，超出该额度的资源使用请求将被拒绝；显然，该限制需要大于等于requests的值，但系统在某项资源紧张时，会从容器回收超出request值的那部分。 在Kubernetes系统上，1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1 core）相当于1000个微核心（millicores，以下简称为m），因此500m相当于是0.5个核心，即1/2个核心。内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。 容器资源需求下面的配置清单示例（resource-requests-demo.yaml）中的自主式Pod要求为stress容器确保128MiB的内存及1/5个CPU核心（200m）资源可用。Pod运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时stress容器也会尽可能多地占用CPU资源，另外再启动一个专用的CPU压力测试进程（-c 1）。stress-ng是一个多功能系统压力测试具，master/worker模型，master为主进程，负载生成和控制子进程，worker是负责执行各类特定测试的子进程，例如测试CPU的子进程，以及测试RAM的子进程等。 12345678910111213apiVersion: v1kind: Podmetadata: name: stress-podspec: containers: - name: stress image: ikubernetes/stress-ng command: [&quot;/usr/bin/stress-ng&quot;, &quot;-m 1&quot;, &quot;-c 1&quot;, &quot;-metrics-brief&quot;] resources: requests: memory: &quot;128Mi&quot; cpu: &quot;200m&quot; 上面的配置清单中，stress容器请求使用的CPU资源大小为200m，这意味着一个CPU核心足以确保其以期望的最快方式运行。另外，配置清单中期望使用的内存大小为128MiB，不过其运行时未必真的会用到这么多。考虑到内存为非压缩型资源，当超出时存在因OOM被杀死的可能性，于是请求值是其理想中使用的内存空间上限。接下来创建并运行此Pod对象以对其资源限制效果进行检查。因为显示结果涉及资源占用比例等，因此同样的测试配置对不同的系统环境来说，其结果也会有所不同，作者为测试资源需求和资源限制功能而使用的系统环境中，每个节点的可用CPU核心数为8，物理内存空间为16GB。 1~$ kubectl create -f resource-requests-demo.yaml 而后在Pod资源的容器内运行top命令，观察CPU及内存资源占用状态，如下所示。其中{stress-ng-vm}是执行内存压测的子进程，它默认使用256MB的内存空间，{stress-ng-cpu}是执行CPU压测的专用子进程。 123456789~$ kubectl exec stress-pod -- topMem: 2884676K used, 13531796K free, 27700K shrd, 2108K buff, 1701456K cachedCPU: 25% usr 0% sys 0% nic 74% idle 0% io 0% irq 0% sirqLoad average: 0.57 0.60 0.71 3/435 15PID PPID USER STAT VSZ %VSZ CPU %CPU COMMAND9 8 root R 262m 2% 6 13% &#123;stress-ng-vm&#125; /usr/bin/stress-ng7 1 root R 6888 0% 3 13% &#123;stress-ng-cpu&#125; /usr/bin/stress-ng1 0 root S 6244 0% 1 0% /usr/bin/stress-ng -c 1 -m 1 --met…… top命令的输出结果显示，每个测试进程的CPU占用率为13%（实际12.5%），{stress-ng-vm}的内存占用量为262MB（VSZ），此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用范围内尽量多地占用相关的资源。两个测试线程分布于两个CPU核心，以满载的方式运行，系统共有8个核心，因此其使用率为25%（2/8）。另外，节点上的内存资源充裕，所以，尽管容器的内存用量远超128MB，但它依然可以运行。一旦资源紧张时，节点仅保证该容器有1/5个CPU核心（其需求中的定义）可用。在有着8个核心的节点上来说，它的占用率为2.5%，于是每个进程占比为1.25%，多占用的资源会被压缩。内存为非可压缩型资源，该Pod对象在内存资源紧张时可能会因OOM被杀死。对于压缩型的资源CPU来说，若未定义容器的资源请求用量，以确保其最小可用资源量，该Pod占用的CPU资源可能会被其他Pod对象压缩至极低的水平，甚至到该Pod对象无法被调度运行的境地。而对于非压缩型内存资源来说，资源紧缺情形下可能导致相关的容器进程被杀死。因此，在Kubernetes系统上运行关键型业务相关的Pod时，必须要使用requests属性为容器明确定义资源需求。当然，我们也可以为Pod对象定义较高的优先级来改变这种局面。集群中的每个节点都拥有定量的CPU和内存资源，调度器将Pod绑定至节点时，仅计算资源余量可满足该Pod对象需求量的节点才能作为该Pod运行的可用目标节点。也就是说，Kubernetes的调度器会根据容器的requests属性定义的资源需求量来判定哪些节点可接收并运行相关的Pod对象，而对于一个节点的资源来说，每运行一个Pod对象，该Pod对象上所有容器requests属性定义的请求量都要给予预留，直到节点资源被绑定的所有Pod对象瓜分完毕为止。 容器资源限制一旦定义资源限制，分配资源时，可压缩型资源CPU的控制阀可自由调节，容器进程也就无法获得超出其CPU配额的可用值。但是，若进程申请使用超出limits属性定义的内存资源时，该进程将可能被杀死。不过，该进程随后仍可能会被其控制进程重启，例如，当Pod对象的重启策略为Always或OnFailure时，或者容器进程存在有监视和管理功能的父进程等。下面的配置清单文件（resource-limits-demo.yaml）中定义使用simmemleak镜像运行一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而被杀死。 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: memleak-pod labels: app: memleakspec: containers: - name: simmemleak image: ikubernetes/simmemleak imagePullPolicy: IfNotPresent resources: requests: memory: &quot;64Mi&quot; cpu: &quot;1&quot; limits: memory: &quot;64Mi&quot; cpu: &quot;1&quot; 下面将配置清单中定义的Pod对象创建到集群中，测试资源限制的实施效果。 12~$ kubectl apply -f resource-limits-demo.yamlpod/memleak-pod created Pod资源的默认重启策略为Always，于是在simmemleak容器因内存资源达到硬限制而被终止后会立即重启，因此用户很难观察到其因OOM而被杀死的相关信息。不过，多次因内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制（退避算法），即每次重启的时间间隔会不断地拉长，因而用户看到Pod对象的相关状态通常为CrashLoopBackOff。 123~$ kubectl get pods -l app=memleak NAME READY STATUS RESTARTS AGEmemleak-pod 0/1 CrashLoopBackOff 1 24s Pod对象的重启策略在4.5.3节介绍过，这里不再赘述。我们可通过Pod对象的详细描述了解其相关状态，例如下面的命令及部分结果所示。 1234567891011~]$ kubectl describe pods memleak-podName: memleak-pod……Last State: Terminated Reason: OOMKilled Exit Code: 137 Started: Mon, 31 Aug 2020 12:42:50 +0800 Finished: Mon, 31 Aug 2020 12:42:50 +0800 Ready: False Restart Count: 3…… 上面的命令结果中，OOMKilled表示容器因内存耗尽而被终止，因此为limits属性中的memory设置一个合理值至关重要。与资源需求不同的是，资源限制并不影响Pod对象的调度结果，即一个节点上的所有Pod对象的资源限制数量之和可以大于节点拥有的资源量，即支持资源的过载使用（overcommitted）。不过，这么一来，一旦内存资源耗尽，几乎必然地会有容器因OOMKilled而终止。另外需要说明的是，Kubernetes仅会确保Pod对象获得它们请求的CPU时间额度，它们能否取得额外（throttled）的CPU时间，则取决于其他正在运行作业的CPU资源占用情况。例如对于总数为1000m的CPU资源来说，容器A请求使用200m，容器B请求使用500m，在不超出它们各自最大限额的前下，则余下的300m在双方都需要时会以2 : 5（200m : 500m）的方式进行配置。 容器可见资源在容器中运行top等命令观察资源可用量信息时，容器可用资源受限于requests和limits属性中的定义，但容器中可见的资源量依然是节点级别的可用总量。例如，为前面定义的stress-pod添加如下limits属性定义。 123limits: memory: &quot;512Mi&quot; cpu: &quot;400m&quot; 重新创建stress-pod对象，并在其容器内分别列出容器可见的内存和CPU资源总量，命令及结果如下所示。 1234~$ kubectl exec stress-pod -- cat /proc/meminfo | grep ^MemTotalMemTotal: 16416472 kB$ kubectl exec stress-pod -- cat /proc/cpuinfo | grep -c ^processor8 命令结果中显示其可用内存资源总量为16416472 kB（16GB），CPU核心数为8个，这是节点级的资源数量，而非由容器的limits属性所定义的512MiB和400m。较为典型的是在Pod中运行Java应用程序时，若未使用-Xmx选项指定JVM的堆内存可用总量，则会默认设置为主机内存总量的一个空间比例（例如30%），这会导致容器中的应用程序申请内存资源时很快达到上限，而转为OOMKilled状态。另外，即便使用了-Xmx选项设置其堆内存上限，但该设置对非堆内存的可用空间不产生任何限制作用，仍然存在达到容器内存资源上限的可能性。另一个典型代表是在Pod中运行Nginx应用时，其配置参数worker_processes的值设置为auto，则会创建与可见CPU核心数量等同的worker进程数，若容器的CPU可用资源量远小于节点所需资源量时，这种设置在较大的访问负荷下会产生严重的资源竞争，并且会带来更多的内存资源消耗。一种较为妥当的解决方案是使用Downward API将limits定义的资源量暴露给容器，这将在后面的章节中予以介绍。 Pod服务质量类别前面曾提到，Kubernetes允许节点的Pod对象过载使用资源，这意味着节点无法同时满足绑定其上的所有Pod对象以资源满载的方式运行。因而在内存资源紧缺的情况下，应该以何种次序终止哪些Pod对象就变成了问题。事实上，Kubernetes无法自行对此做出决策，它需要借助于Pod对象的服务质量和优先级等完成判定。根据Pod对象的requests和limits属性，Kubernetes把Pod对象归类到BestEffort、Burstable和Guaranteed这3个服务质量类别（Quality of Service，QoS）类别下。 Guaranteed：Pod对象为其每个容器都设置了CPU资源需求和资源限制，且二者具有相同值；同时为每个容器都设置了内存资需求和内存限制，且二者具有相同值。这类Pod对象具有最高级别服务质量。 Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别的设定要求，这类Pod对象具有中等级别服务质量。 BestEffort：不为任何一个容器设置requests或limits属性，这类Pod对象可获得的服务质量为最低级别。一旦内存资源紧缺，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够做到尽可能多地占用资源。若此时系统上已然不存任何BestEffort类别的容器，则接下来将轮到Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod对象存在。 每个运行状态的容器都有其OOM评分，评分越高越优先被杀死。OOM评分主要根据两个维度进行计算：由服务质量类别继承而来的默认分值，以及容器的可用内存资源比例，而同等类别的Pod对象的默认分值相同。下面的代码片段取自pkg/kubelet/qos/policy.go源码文件，它们定义的是各种类别的Pod对象的OOM调节（Adjust）分值，即默认分值。其中，Guaranteed类别Pod资源的Adjust分值为–998，而BestEffort类别的默认分值为1000，Burstable类别的Pod资源的Adjust分值经由相应的算法计算得出。 12345678const ( PodInfraOOMAdj int = -998 KubeletOOMScoreAdj int = -999 DockerOOMScoreAdj int = -999 KubeProxyOOMScoreAdj int = -999 guaranteedOOMScoreAdj int = -998 besteffortOOMScoreAdj int = 1000) 因此，同等级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将先被杀死。例如，图4-17中的同属于Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例为95%，要大于Pod B的80% 需要特别说明的是，OOM是内存耗尽时的处理机制，与可压缩型资源CPU无关，因此CPU资源的需求无法得到保证时，Pod对象仅仅是暂时获取不到相应的资源来运行而已。 综合应用案例下面的配置清单（all-in-one.yaml）中定义的Pod对象all-in-one将前面的用到的大多数配置整合在一起：它有一个初始化容器和两个应用容器，其中sidecar-proxy为Sidecar容器，负责为主容器demo代理服务客户端请求。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465apiVersion: v1kind: Podmetadata: name: all-in-one namespace: defaultspec: initContainers: - name: iptables-init image: ikubernetes/admin-box:latest imagePullPolicy: IfNotPresent command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT --to-port 80&#x27;] securityContext: capabilities: add: - NET_ADMIN containers: - name: sidecar-proxy image: envoyproxy/envoy-alpine:v1.13.1 command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;] args: [&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;] lifecycle: postStart: exec: command: [&#x27;/bin/sh&#x27;,&#x27;-c&#x27;,&#x27;wget -O /etc/envoy/envoy.yaml https:// raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_ Practical_2rd/master/chapter4/envoy.yaml&#x27;] livenessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 readinessProbe: tcpSocket: port: 80 initialDelaySeconds: 5 - name: demo image: ikubernetes/demoapp:v1.0 imagePullPolicy: IfNotPresent env: - name: PORT value: &#x27;8080&#x27; livenessProbe: httpGet: path: &#x27;/livez&#x27; port: 8080 initialDelaySeconds: 5 readinessProbe: httpGet: path: &#x27;/readyz&#x27; port: 8080 initialDelaySeconds: 15 securityContext: runAsUser: 1001 runAsGroup: 1001 resources: requests: cpu: 0.5 memory: &quot;64Mi&quot; limits: cpu: 2 memory: &quot;1024Mi&quot; securityContext: supplementalGroups: [1002, 1003] fsGroup: 2000 配置清单的Pod对象的各容器中，主容器demo在Pod的IP地址上监听TCP协议的8080端口，以接收并响应HTTP请求；Sidecar容器sidecar-proxy监听TCP协议的80端口，接收HTTP请求并将其代理至demo容器的8080端口；初始化容器在Pod的Network名称空间中添加了一条iptables重定向规则，该规则负责把所有发往Pod IP上8080端口的请求重定向至80端口，因而demo容器仅能从127.0.0.1的8080端口接收到请求。读者朋友可将清单中的Pod对象创建到集群上，并逐一测试其各项配置的效果。","categories":[],"tags":[]},{"title":"git及CI/CD","slug":"git使用","date":"2022-02-09T06:41:10.000Z","updated":"2022-02-09T07:04:10.957Z","comments":true,"path":"2022/02/09/git使用/","link":"","permalink":"https://marmotad.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/","excerpt":"","text":"CI/CD的功能CI/CD 是一种通过在应用开发阶段引入自动化来频频繁向客户交付应用的方法。CI/CD 的核心概念是持续集成、持续交付和持续部署。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“集成地狱”）。 具体而言，CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）。这些关联的事务通常被统称为“CI/CD 管道”，由开发和运维团队以敏捷方式协同支持。 CI 是什么？CI 和 CD 有什么区别？CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。 CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。 持续交付通常是指开发人员对应用的更改会自动进行错误测试并上传到存储库（如 GitHub 或容器注册表），然后由运维团队将其部署到实时生产环境中。这旨在解决开发和运维团队之间可见性及沟通较差的问题。因此，持续交付的目的就是确保尽可能减少部署新代码时所需的工作量。 持续部署（另一种“CD”）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用。它主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。 CI/CD 既可能仅指持续集成和持续交付构成的关联环节，也可以指持续集成、持续交付和持续部署这三项构成的关联环节。更为复杂的是，有时“持续交付”也包含了持续部署流程。 归根结底，我们没必要纠结于这些语义，您只需记得 CI/CD 其实就是一个流程（通常形象地表述为管道），用于实现应用开发中的高度持续自动化和持续监控。因案例而异，该术语的具体含义取决于 CI/CD 管道的自动化程度。许多企业最开始先添加 CI，然后逐步实现交付和部署的自动化（例如作为云原生应用的一部分）。 CI 持续集成（Continuous Integration）现代应用开发的目标是让多位开发人员同时处理同一应用的不同功能。但是，如果企业安排在一天内将所有分支源代码合并在一起（称为“合并日”），最终可能造成工作繁琐、耗时，而且需要手动完成。这是因为当一位独立工作的开发人员对应用进行更改时，有可能会与其他开发人员同时进行的更改发生冲突。如果每个开发人员都自定义自己的本地集成开发环境（IDE），而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。 持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。 进一步了解技术细节 CD 持续交付（Continuous Delivery）完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。 在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。 CD 持续部署（Continuous Deployment）对于一个成熟的 CI/CD 管道来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。由于在生产之前的管道阶段没有手动门控，因此持续部署在很大程度上都得依赖精心设计的测试自动化。 实际上，持续部署意味着开发人员对应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这更加便于持续接收和整合用户反馈。总而言之，所有这些 CI/CD 的关联步骤都有助于降低应用的部署风险，因此更便于以小件的方式（而非一次性）发布对应用的更改。不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期投资还是会很大。 版本控制系统什么是版本控制系统版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份以便恢复以前的版本的软件工程技术 版本控制系统解决的问题1、追溯文件历史变更 2、多人团队协同开发 3、代码集中管理 常见的版本控制系统（集中式VS分布式）Subversion集中式版本控制系统 Subversion的特点概括起来主要由以下几条： 每个版本库有唯一的URL（官方地址），每个用户都从这个地址获取代码和数据； 获取代码的更新，也只能连接到这个唯一的版本库，同步以取得最新数据； 提交必须有网络连接（非本地版本库）； 提交需要授权，如果没有写权限，提交会失败； 提交并非每次都能够成功。如果有其他人先于你提交，会提示“改动基于过时的版本，先更新再提交”… 诸如此类； 冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。 好处：每个人都可以一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限。 缺点：中央服务器的单点故障。 若是宕机一小时，那么在这一小时内，谁都无法提交更新、还原、对比等，也就无法协同工作。如果中央服务器的磁盘发生故障，并且没做过备份或者备份得不够及时的话，还会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，被客户端提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人提取出来。 Subversion原理上只关心文件内容的具体差异。每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容。 Git属于分布式的版本控制系统Git记录版本历史只关心文件数据的整体是否发生变化。Git 不保存文件内容前后变化的差异数据。 实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一连接。 在分布式版本控制系统中，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程。 另外，因为Git在本地磁盘上就保存着所有有关当前项目的历史更新，并且Git中的绝大多数操作都只需要访问本地文件和资源，不用连网，所以处理起来速度飞快。用SVN的话，没有网络或者断开VPN你就无法做任何事情。但用Git的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程的镜像仓库。换作其他版本控制系统，这么做几乎不可能，抑或是非常麻烦。 Git具有以下特点： Git中每个克隆(clone)的版本库都是平等的。你可以从任何一个版本库的克隆来创建属于你自己的版本库，同时你的版本库也可以作为源提供给他人，只要你愿意。 Git的每一次提取操作，实际上都是一次对代码仓库的完整备份。 提交完全在本地完成，无须别人给你授权，你的版本库你作主，并且提交总是会成功。 甚至基于旧版本的改动也可以成功提交，提交会基于旧的版本创建一个新的分支。 Git的提交不会被打断，直到你的工作完全满意了，PUSH给他人或者他人PULL你的版本库，合并会发生在PULL和PUSH过程中，不能自动解决的冲突会提示您手工完成。 冲突解决不再像是SVN一样的提交竞赛，而是在需要的时候才进行合并和冲突解决。 Git 也可以模拟集中式的工作模式 Git版本库统一放在服务器中 可以为 Git 版本库进行授权：谁能创建版本库，谁能向版本库PUSH，谁能够读取（克隆）版本库 团队的成员先将服务器的版本库克隆到本地；并经常的从服务器的版本库拉（PULL）最新的更新； 团队的成员将自己的改动推（PUSH）到服务器的版本库中，当其他人和版本库同步（PULL）时，会自动获取改变 Git 的集中式工作模式非常灵活 你完全可以在脱离Git服务器所在网络的情况下，如移动办公／出差时，照常使用代码库 你只需要在能够接入Git服务器所在网络时，PULL和PUSH即可完成和服务器同步以及提交 Git提供 rebase 命令，可以让你的改动看起来是基于最新的代码实现的改动 Git 有更多的工作模式可以选择，远非 Subversion可比 git基本使用配置git 通常只需要配置你是谁，邮箱是什么。就可以知道是谁提交了什么内容 1234[root@localhost ~]# git config --global user.name &quot;fanyang&quot;[root@localhost ~]# git config --global user.email &quot;fanyang@163.com&quot;[root@localhost ~]# git config --global color.ui true[root@localhost ~]# cat .gitconfig git如何提交目录文件到本地仓库1、首先创建git仓库，这个目录里的所有文件都可以被git管理起来，每个文件的修改、删除、GIt都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原” 1234567# 创建git工作目录[root@localhost ~]# mkdir /git[root@localhost ~]# cd /git# 初始化该目录为git仓库[root@localhost git]# git init 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556# 创建新的文件[root@localhost git]# touch file&#123;1..3&#125;[root@localhost git]# lsfile1 file2 file3# 查看git状态[root@localhost git]# git statusOn branch masterNo commits yetUntracked files:# 有三个未提交的文件 (use &quot;git add &lt;file&gt;...&quot; to include in what will be committed) file1 file2 file3nothing added to commit but untracked files present (use &quot;git add&quot; to track)# 添加本地所有文件到本地git缓存[root@localhost git]# git add .# 查看git状态[root@localhost git]# git statusOn branch masterNo commits yetChanges to be committed: (use &quot;git rm --cached &lt;file&gt;...&quot; to unstage)# 有三个新的文件 new file: file1 new file: file2 new file: file3# 添加git描述[root@localhost git]# git commit -m &quot;新增file&#123;1..3&#125;到git仓库&quot;[master (root-commit) 8acf856] 新增file&#123;1..3&#125;到git仓库 3 files changed, 0 insertions(+), 0 deletions(-) create mode 100644 file1 create mode 100644 file2 create mode 100644 file3# 修改file1[root@localhost git]# echo 1 &gt;file1 [root@localhost git]# git statusOn branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: file1no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;修改file1&quot;[master 53 30aef] 修改file1 1 file changed, 1 insertion(+) 文件改名后重新提交到本地git仓库1234567[root@localhost git]# git mv file1 file [root@localhost git]# git status On branch masterChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) renamed: file1 -&gt; file 对比文件差异对比本地文件和暂存区文件的差异12345678910[root@localhost git]# git diff file[root@localhost git]# echo test &gt;&gt; file[root@localhost git]# git diff filediff --git a/file b/fileindex d00491f..c0f2f8d 100644--- a/file+++ b/file@@ -1 +1,2 @@ 1+test 对比暂存区文件和本地git仓库文件差异1234567891011121314151617181920212223[root@localhost git]# git add .[root@localhost git]# git diff file[root@localhost git]# git diff file --cache filefatal: option &#x27;--cache&#x27; must come before non-option arguments[root@localhost git]# git diff file --cached filefatal: option &#x27;--cached&#x27; must come before non-option arguments[root@localhost git]# git diff --cached filediff --git a/file b/filenew file mode 100644index 0000000..c0f2f8d--- /dev/null+++ b/file@@ -0,0 +1,2 @@+1+test[root@localhost git]# [root@localhost git]# git commit -m &quot;修改file文件&quot;[master 8b1ecec] 修改file文件 2 files changed, 2 insertions(+), 1 deletion(-) create mode 100644 file delete mode 100644 file1[root@localhost git]# git diff --cached file 文件回滚操作导致文件被清空（本地目录与暂存区间的撤销）123456789101112131415# 使用以前提交到暂存区的内容覆盖本地目录[root@localhost git]# echo &gt; file[root@localhost git]# git status On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: fileno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git checkout file[root@localhost git]# cat file1test 本地文件误操作提交至暂存区本地仓库覆盖暂存区—–&gt; 暂存区覆盖本地目录 12345678910111213141516171819202122232425262728[root@localhost git]# echo ddd &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# git status On branch masterChanges to be committed: (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage) modified: file[root@localhost git]# git reset HEAD fileUnstaged changes after reset:M file[root@localhost git]# git status On branch masterChanges not staged for commit: (use &quot;git add &lt;file&gt;...&quot; to update what will be committed) (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory) modified: fileno changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)[root@localhost git]# git checkout file[root@localhost git]# git status On branch masternothing to commit, working tree clean[root@localhost git]# cat file1test 多次提交到本地仓库后回滚123456789101112131415161718192021222324252627[root@localhost git]# echo test &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# lsfile file2 file3[root@localhost git]# git commit -m &quot;test&quot;[master c691c60] test 1 file changed, 1 insertion(+)[root@localhost git]# echo test1 &gt;&gt; file[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;test1&quot;[master 3595d2b] test1 1 file changed, 1 insertion(+)[root@localhost git]# git log --oneline 3595d2b (HEAD -&gt; master) test1c691c60 test8b1ecec 修改file文件5330aef 修改file18acf856 新增file&#123;1..3&#125;到git仓库[root@localhost git]# git reset --hard c691c60HEAD is now at c691c60 test[root@localhost git]# git status On branch masternothing to commit, working tree clean[root@localhost git]# cat file1testtest git 回退后，恢复回退前版本123456789101112131415[root@localhost git]# git reflog c691c60 (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to c691c603595d2b HEAD@&#123;1&#125;: commit: test1c691c60 (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: test8b1ecec HEAD@&#123;3&#125;: commit: 修改file文件5330aef HEAD@&#123;4&#125;: commit: 修改file18acf856 HEAD@&#123;5&#125;: commit (initial): 新增file&#123;1..3&#125;到git仓库[root@localhost git]# git rerebase reflog remote repack replace request-pull reset revert [root@localhost git]# git reset --hard c691c60[root@localhost git]# git log --oneline c691c60 (HEAD -&gt; master) test8b1ecec 修改file文件5330aef 修改file18acf856 新增file&#123;1..3&#125;到git仓库 git 分支管理查看、创建、切换分支12345678910111213# 查看分支[root@localhost git]# git branch * master# 创建分支[root@localhost git]# git branch dev[root@localhost git]# git branch dev* master# 切换分支[root@localhost git]# git checkout devSwitched to branch &#x27;dev&#x27;[root@localhost git]# git branch * dev 合并分支master合并到dev—-&gt;测试合并后的dev分支—–&gt;dev分支合并到master 12[root@localhost git]# git merge master[root@localhost git]# git merge dev 删除分支1234[root@localhost git]# git branch dev -dDeleted branch dev (was 06b4943).[root@localhost git]# git branch * masterl,; 标签创建标签123456789# 对当前分支当前的版本打标签[root@localhost git]# git tag -a &quot;v1.0&quot; -m &quot;第一个版本&quot;# 查看当前分支有哪些标签[root@localhost git]# git tag# 查看标签内容[root@localhost git]# git show v1.0 # 对指定的id打标签[root@localhost git]# git tag -a &quot;v1.0&quot; c691c60 -m &quot;未发布的版本&quot;fatal: tag &#x27;v1.0&#x27; already exists 删除标签12[root@localhost git]# git tag -d v0.9 Deleted tag &#x27;v0.9&#x27; (was 8d74209) git 操作远程仓库关联远程仓库1234[root@localhost git]# git remote add origin git@172.18.128.4:root/git-test.git[root@localhost git]# git remote -vorigin git@172.18.128.4:root/git-test.git (fetch)origin git@172.18.128.4:root/git-test.git (push) 将本地仓库内容推送到远程仓库123[root@localhost git]# git add .[root@localhost git]# git commit -m &quot;修改file文件&quot;[root@localhost git]# git push -u origin master 删除远程仓库12[root@localhost git]# git remote remove origin # origin ：用户名称 新用户加入需要做的123456789101112131415161718192021222324252627282930313233343536373839[root@localhost ~]# mkdir /test[root@localhost ~]# cd /test[root@localhost test]# git init Initialized empty Git repository in /test/.git/[root@localhost test]# git status # On branch master## Initial commit#nothing to commit (create/copy files and use &quot;git add&quot; to track)[root@localhost git-test]# git config --global user.name &quot;Your Name&quot;[root@localhost git-test]# git config --global user.email you@example.co[root@localhost test]# git remote add origin git@172.18.128.4:root/git-test.git[root@localhost test]# git remote -vorigin git@172.18.128.4:root/git-test.git (fetch)origin git@172.18.128.4:root/git-test.git (push)[root@localhost test]# git clone git@172.18.128.4:root/git-test.gitCloning into &#x27;git-test&#x27;...[root@localhost test]# lsgit-test[root@localhost test]# cd git-test/[root@localhost git-test]# lsfile file2 file3 file5 file7 file8[root@localhost git-test]# touch file9[root@localhost git-test]# git add .[root@localhost git-test]# git commit -m &quot;new file&quot;# On branch master# Your branch is ahead of &#x27;origin/master&#x27; by 1 commit.# (use &quot;git push&quot; to publish your local commits)#nothing to commit, working directory clean[root@localhost git-test]# git push origin master Counting objects: 3, done.Compressing objects: 100% (2/2), done.Writing objects: 100% (2/2), 233 bytes | 0 bytes/s, done.Total 2 (delta 1), reused 0 (delta 0)To git@172.18.128.4:root/git-test.git 06b4943..b3fdc9f master -&gt; master 同步远程仓库中否代码1[root@localhost git-test]# git pull origin master","categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/categories/CI-CD/"}],"tags":[{"name":"博客           //多个标签可以这样添加","slug":"博客-多个标签可以这样添加","permalink":"https://marmotad.github.io/tags/%E5%8D%9A%E5%AE%A2-%E5%A4%9A%E4%B8%AA%E6%A0%87%E7%AD%BE%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E6%B7%BB%E5%8A%A0/"},{"name":"hexo","slug":"hexo","permalink":"https://marmotad.github.io/tags/hexo/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-02-09T03:10:39.808Z","updated":"2022-02-09T03:08:03.358Z","comments":true,"path":"2022/02/09/hello-world/","link":"","permalink":"https://marmotad.github.io/2022/02/09/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[{"name":"CI/CD","slug":"CI-CD","permalink":"https://marmotad.github.io/categories/CI-CD/"}],"tags":[{"name":"博客           //多个标签可以这样添加","slug":"博客-多个标签可以这样添加","permalink":"https://marmotad.github.io/tags/%E5%8D%9A%E5%AE%A2-%E5%A4%9A%E4%B8%AA%E6%A0%87%E7%AD%BE%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E6%B7%BB%E5%8A%A0/"},{"name":"hexo","slug":"hexo","permalink":"https://marmotad.github.io/tags/hexo/"}]}