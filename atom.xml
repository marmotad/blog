<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>marmotad</title>
  
  <subtitle>blog</subtitle>
  <link href="https://marmotad.github.io/atom.xml" rel="self"/>
  
  <link href="https://marmotad.github.io/"/>
  <updated>2022-02-09T08:07:36.842Z</updated>
  <id>https://marmotad.github.io/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在这里</title>
    <link href="https://marmotad.github.io/2022/02/10/Kubernetes-magedu/"/>
    <id>https://marmotad.github.io/2022/02/10/Kubernetes-magedu/</id>
    <published>2022-02-10T07:41:10.000Z</published>
    <updated>2022-02-09T08:07:36.842Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-Kubernetes集群架构"><a href="#1-Kubernetes集群架构" class="headerlink" title="1 Kubernetes集群架构"></a>1 Kubernetes集群架构</h1><p>Kubernetes属于典型的Server-Client形式的二层架构，在程序级别，Master主要由API Server（kube-apiserver）、Controller-Manager（kube-controller-manager）和Scheduler（kube-scheduler）这3个组件，以及一个用于集群状态存储的etcd存储服务组成，它们构成整个集群的控制平面；而每个Node节点则主要包含kubelet、kube-proxy及容器运行时（Docker是最为常用的实现）3个组件，它们承载运行各类应用容器。</p><h2 id="1-1-Kubernetes系统组件"><a href="#1-1-Kubernetes系统组件" class="headerlink" title="1.1 Kubernetes系统组件"></a>1.1 Kubernetes系统组件</h2><h3 id="1-1-1-Master组件"><a href="#1-1-1-Master组件" class="headerlink" title="1.1.1 Master组件"></a>1.1.1 Master组件</h3><p>Master 它维护有Kubernetes的所有对象记录，负责持续管理对象状态并响应集群中各种资源对象的管理操作，以及确保各资源对象的实际状态与所需状态相匹配。控制平面的各组件支持以单副本形式运行于单一主机，也能够将每个组件以多副本方式同时运行于多个主机上，提高服务可用级别。控制平面各组件及其主要功能如下。</p><h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>API Server是Kubernetes控制平面的前端，支持不同类型应用的生命周期编排，包括部署、缩放和滚动更新等。它还是整个集群的网关接口，由kube-apiserver守护程序运行为服务，通过HTTP/HTTPS协议将RESTful API公开给用户，是发往集群的所有REST操作命令的接入点，用于接收、校验以及响应所有的REST请求，并将结果状态持久存储于集群状态存储系统（etcd）中。</p><h4 id="集群状态存储-ETCD"><a href="#集群状态存储-ETCD" class="headerlink" title="集群状态存储(ETCD)"></a>集群状态存储(ETCD)</h4><p>Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中。</p><h4 id="控制器管理器-kube-controller-manager"><a href="#控制器管理器-kube-controller-manager" class="headerlink" title="控制器管理器(kube-controller-manager)"></a>控制器管理器(kube-controller-manager)</h4><p>控制器负责实现用户通过API Server提交的终态声明，驱动API对象的当前状态逼近或等同于期望状态。Kubernetes提供了驱动Node、Pod、Server、Endpoint、ServiceAccount和Token等API对象的控制器。</p><h4 id="调度器-kube-scheduler"><a href="#调度器-kube-scheduler" class="headerlink" title="调度器(kube-scheduler)"></a>调度器(kube-scheduler)</h4><p>Kubernetes系统上的调度是指为API Server接收到的每一个Pod创建请求，并在集群中为其匹配出一个最佳工作节点。kube-scheduler是默认调度器程序。</p><h3 id="1-1-2-Node组件"><a href="#1-1-2-Node组件" class="headerlink" title="1.1.2 Node组件"></a>1.1.2 Node组件</h3><p>Node组件是集群的“体力”输出者，每个Node会定期向Master报告自身的状态变动，并接受Master的管理。</p><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>kubelet是运行于每个Node之上的“节点代理”服务，负责接收并执行Master发来的指令，以及管理当前Node上Pod对象的容器等任务。<br>kubelet会持续监视当前节点上各Pod的健康状态，包括基于用户自定义的探针进行存活状态探测，并在任何Pod出现问题时将其重建为新实例。它还内置了一个HTTP服务器，监听TCP协议的10248和10250端口：10248端口通过/healthz响应对kubelet程序自身的健康状态进行检测；10250端口用于暴露kubelet API，以验证、接收并响应API Server的通信请求。</p><h4 id="容器运行时环境"><a href="#容器运行时环境" class="headerlink" title="容器运行时环境"></a>容器运行时环境</h4><p>Pod是一组容器组成的集合并包含这些容器的管理机制。kubelet通过CRI（容器运行时接口）可支持多种类型的OCI容器运行时，例如docker、containerd、CRI-O、runC、fraki和Kata Containers等。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>kube-proxy，它把API Server上的Service资源对象转换为当前节点上的iptables或（与）ipvs规则，这些规则能够将那些发往该Service对象ClusterIP的流量分发至它后端的Pod端点之上。kube-proxy是Kubernetes的核心网络组件，它本质上更像是Pod的代理及负载均衡器，负责确保集群中Node、Service和Pod对象之间的有效通信。</p><h3 id="1-1-3-核心附件"><a href="#1-1-3-核心附件" class="headerlink" title="1.1.3 核心附件"></a>1.1.3 核心附件</h3><p>附件（add-ons）用于扩展Kubernetes的基本功能，它们通常运行于Kubernetes集群自身之上，可根据重要程度将其划分为必要和可选两个类别。网络插件是必要附件，管理员需要从众多解决方案中根据需要及项目特性选择，常用的有Flannel、Calico、Canal、Cilium和Weave Net等。KubeDNS通常也是必要附件之一，而Web UI（Dashboard）、容器资源监控系统、集群日志系统和Ingress Controller等是常用附件。</p><h4 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h4><p>Kubernetes使用定制的DNS应用程序实现名称解析和服务发现功能，它自1.11版本起默认使用CoreDNS——一种灵活、可扩展的DNS服务器；之前的版本中用到的是kube-dns项目，SkyDNS则是更早一代的解决方案。</p><h4 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h4><p>基于Web的用户接口，用于可视化Kubernetes集群。</p><h4 id="容器资源监控系统"><a href="#容器资源监控系统" class="headerlink" title="容器资源监控系统"></a>容器资源监控系统</h4><p>Kubernetes常用的指标监控附件有Metrics-Server、kube-state-metrics和Prometheus等。</p><h4 id="集群日志系统"><a href="#集群日志系统" class="headerlink" title="集群日志系统"></a>集群日志系统</h4><p>Kubernetes常用的集中式日志系统是由ElasticSearch、Fluentd和Kibana（称之为EFK）组合提供的整体解决方案。</p><h4 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h4><ul><li>Pod与Service</li></ul><p>Pod本质上是共享Network、IPC和UTS名称空间以及存储资源的容器集合。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220116110055992.png" alt="../img/image-20220116110055992"></p><p>同一Pod内部的容器，它们共享网络协议栈、网络设备、路由、IP地址和端口等网络资源，可以基于本地回环接口lo互相通信。每个Pod上还可附加一组“存储卷”（volume）资源，它们同样可由内部所有容器使用而实现数据共享。持久类型的存储卷还能够确保在容器终止后被重启，甚至容器被删除后数据也不会丢失。<br>同时，这些以Pod形式运行于Kubernetes之上的应用通常以服务类程序居多，其客户端可能来自集群之外，例如现实中的用户，也可能是当前集群中其他Pod中的应用，如图1-10所示。Kubernetes集群的网络模型要求其各Pod对象的IP地址位于同一网络平面内（同一IP网段），各Pod间可使用真实IP地址直接进行通信而无须NAT功能介入，无论它们运行于集群内的哪个工作节点之上，这些Pod对象就像是运行于同一局域网中的多个主机上。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220116110307410.png" alt="../img/image-20220116110307410"></p><p>Service是由基于匹配规则在集群中挑选出的一组Pod对象的集合、访问这组Pod集合的固定IP地址，以及对请求进行调度的方法等功能所构成的一种API资源类型，是Pod资源的代理和负载均衡器。Service匹配Pod对象的规则可用“标签选择器”进行体现，并根据标签来过滤符合条件的资源对象，如图1-11所示。标签是附加在Kubernetes API资源对象之上的具有辨识性的分类标识符，使用键值型数据表达，通常仅对用户具有特定意义。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220116110413899.png" alt="../img/image-20220116110413899"></p><p>每个节点上运行的kube-proxy组件负责管理各Pod与Service之间的网络连接，它并非Kubernetes内置的代理服务器，而是一个基于出站流量的负载均衡器组件。针对每个Service，kube-proxy都会在当前节点上转换并添加相应iptables DNAT规则或ipvs规则，从而将目标地址为某Service对象的ClusterIP的流量调度至该Service根据标签选择器匹配出的Pod对象之上。<br>CoreDNS附件会为集群中的每个Service对象（包括DNS服务自身)生成唯一的DNS名称标识，以及相应的DNS资源记录，服务的DNS名称遵循标准的svc.namespace.svc.cluster-domain格式。例如CoreDNS自身的服务名称为kube-dns.kube-system.svc.cluster.local.，则它的ClusterIP通常是10.96.0.10。<br>除非出于管理目的有意调整，Service资源的名称和ClusterIP在其整个生命周期内都不会发生变动。kubelet会在创建Pod容器时，自动在/etc/resolv.conf文件中配置Pod容器使用集群上CoreDNS服务的ClusterIP作为DNS服务器，因而各Pod可针对任何Service的名称直接请求相应的服务。换句话说，Pod可通过kube-dns.kube-system.svc.cluster.local.来访问集群DNS服务。Ingress资源是Kubernetes将集群外部HTTP/HTTPS流量引入到集群内部专用的资源类型，它仅用于控制流量的规则和配置的集合，其自身并不能进行“流量穿透”，要通过Ingress控制器发挥作用；目前，此类的常用项目有Nginx、Traefik、Envoy、Gloo、kong及HAProxy等。</p><h1 id="第4章-应用部署、运行与管理"><a href="#第4章-应用部署、运行与管理" class="headerlink" title="第4章 应用部署、运行与管理"></a>第4章 应用部署、运行与管理</h1><h2 id="4-1-应用容器与Pod资源"><a href="#4-1-应用容器与Pod资源" class="headerlink" title="4.1　应用容器与Pod资源"></a>4.1　应用容器与Pod资源</h2><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120182840600.png" alt="../img/image-20220120182840600"></p><p>同一Pod中，这些容器共享PID、IPC、Network和UTS名称空间的容器彼此间可通过IPC通信，共享使用主机名和网络接口、IP地址、端口和路由等各种网络资源，因而各容器进程能够通过lo网络接口通信且不能使用相同的网络套接字地址。一个Pod内通常仅应该运行具有强耦合关系的容器，否则除了pause以外，只应该存在单个容器，或者只存在单个主容器和一个以上的辅助类容器（例如服务网格中的Sidecar容器等）。</p><h3 id="4-1-2-容器设计模式"><a href="#4-1-2-容器设计模式" class="headerlink" title="4.1.2　容器设计模式"></a>4.1.2　容器设计模式</h3><h4 id="4-1-1-1-单容器模式"><a href="#4-1-1-1-单容器模式" class="headerlink" title="4.1.1.1 单容器模式"></a>4.1.1.1 单容器模式</h4><p>单容器模式是指将应用程序封装为应用容器运行。该模式需要遵循简单和单一原则，每个容器仅承载一种工作负载。</p><h4 id="4-1-1-2-单节点多容器模式"><a href="#4-1-1-2-单节点多容器模式" class="headerlink" title="4.1.1.2 单节点多容器模式"></a>4.1.1.2 单节点多容器模式</h4><p>单节点多容器模式的常见实现有Sidecar（边车）、适配器（Adapter）、大使（Ambassador）、初始化（Initializer）容器模式等。</p><h5 id="1-Sidecar模式"><a href="#1-Sidecar模式" class="headerlink" title="(1) Sidecar模式"></a>(1) Sidecar模式</h5><p>Sidecar模式是多容器系统设计的最常用模式，它由一个主应用程序（通常是Web应用程序）以及一个辅助容器（Sidecar容器）组成，该辅助容器用于为主容器提供辅助服务以增强主容器的功能，是主应用程序是必不可少的一部分，但却不一定非得存在于应用程序本身内部。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120195851648.png" alt="../img/image-20220120195851648"></p><ul><li><p>sidecar的优势</p><ul><li>辅助应用的运行时环境和编程语言与主应用程序无关，因而无须为每种编程语言分别开发一个辅助工具；</li></ul></li></ul><ul><li><p>二者可基于IPC、lo接口或共享存储进行数据交换，不存在明显的通信延迟；</p></li><li><p>容器镜像是发布的基本单位，将主应用与辅助应用划分为两个容器使得其可由不同团队开发和维护，从而变得方便及高效，单独测试及集成测试也变得可能；</p></li><li><p>容器限制了故障边界，使得系统整体可以优雅降级，例如Sidecar容器异常时，主容器仍可继续提供服务；</p></li><li><p>容器是部署的基本单元，每个功能模块均可独立部署及回滚。<br><font color="red">事实上，这些优势对于其他模型来说同样存在。</font></p></li></ul><h5 id="2-大使模式"><a href="#2-大使模式" class="headerlink" title="(2) 大使模式"></a>(2) 大使模式</h5><p>大使模式本质上是一类代理程序，它代表主容器发送网络请求至外部环境中，因此可以将其视作与客户端（主容器应用）位于同一位置的“外交官”。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120195935929.png" alt="../img/image-20220120195935929"></p><p><font color="red">大使模式的最佳用例之一是提供对数据库的访问。</font>实践中，开发环境、测试环境和生产环境中的主应用程序可能需要分别连接到不同的数据库服务。更好的方案是让应用程序始终通过localhost连接至大使容器，而如何正确连接到目标数据的责任则由大使容器完成。</p><h5 id="3-适配器模式"><a href="#3-适配器模式" class="headerlink" title="(3) 适配器模式"></a>(3) 适配器模式</h5><p>适配器模式（见图4-4）用于为主应用程序提供一致的接口，实现了模块重用，支持标准化和规范化主容器应用程序的输出以便于外部服务进行聚合。大使模式为内部容器提供了简化统一的外部服务视图，适配器模式则刚好反过来，它通过标准化容器的输出和接口，为外界展示了一个简化的应用视图。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120200043248.png" alt="../img/image-20220120200043248"></p><h5 id="4-初始化容器模式"><a href="#4-初始化容器模式" class="headerlink" title="(4) 初始化容器模式"></a>(4) 初始化容器模式</h5><p>初始化容器模式（见图4-5）负责以不同于主容器的生命周期来完成那些必要的初始化任务，</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120200118062.png" alt="../img/image-20220120200118062"></p><p>初始化容器将Pod内部的容器分成了两组：初始化容器和应用程序容器（主容器和Sidecar容器等），初始化容器可以不止一个，但它们需要以特定的顺序串行运行，并需要在启动应用程序容器之前成功终止。不过，多个应用程序容器一般需要并行启动和运行。<br>就Kubernetes来说，除了初始化容器之外，还有一些其他可用的初始化技术，例如admission controllers、admission webhooks和PodPresets等。</p><h4 id="4-1-1-3-多节点模式"><a href="#4-1-1-3-多节点模式" class="headerlink" title="4.1.1.3 多节点模式"></a>4.1.1.3 多节点模式</h4><h5 id="1-领导者选举模式"><a href="#1-领导者选举模式" class="headerlink" title="(1) 领导者选举模式"></a>(1) 领导者选举模式</h5><p>领导者选举模式示意图。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120200251976.png" alt="../img/image-20220120200251976"></p><h5 id="2-工作队列模式"><a href="#2-工作队列模式" class="headerlink" title="(2) 工作队列模式"></a>(2) 工作队列模式</h5><p>分布式应用程序的各组件间存在大量的事件传递需求，当某应用组件需要将信息广播至大量订阅者时，可能需要与多个独立开发的，可能使用了不同平台、编程语言和通信协议的应用程序或服务通信，并且无须订阅者实时响应地通信，它具有解耦子系统、提高伸缩能力和可靠性、支持延迟事件处理、简化异构组件间的集成等优势。图4-7为工作队列模式示意图。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120200324527.png" alt="../img/image-20220120200324527"></p><h5 id="3-分散-聚集"><a href="#3-分散-聚集" class="headerlink" title="(3) 分散/聚集"></a>(3) 分散/聚集</h5><p>分散/聚集模式与工作队列模式非常相似，它同样将大型任务拆分为较小的任务，区别是容器会立即将响应返回给用户，一个很好的例子是MapReduce算法。该模式需要两类组件：一个称为“根”节点或“父”节点的组件，将来自客户端的请求切分成多个小任务并分散到多个节点并行计算；另一类称为“计算”节点或“叶子”节点，每个节点负责运行一部分任务分片并返回结果数据，“根”节点收集这些结果数据并聚合为有意义的数据返回给客户端。开发这类分布式系统需要请求扇出、结果聚合以及与客户端交互等大量的模板代码，但大部分都比较通用。因而要实现该模式，我们只需要分别将两类组件各自构建为容器即可。</p><h3 id="4-1-3-Pod的生命周期"><a href="#4-1-3-Pod的生命周期" class="headerlink" title="4.1.3 Pod的生命周期"></a>4.1.3 Pod的生命周期</h3><p>Kubernetes为Pod资源严格定义了5种相位，并将特定Pod对象的当前相位存储在其内部的子对象PodStatus的phase字段上，因而它总是应该处于其生命进程中以下几个相位之一。</p><ul><li><p>Pending：API Server创建了Pod资源对象并已存入etcd中，但它尚未被调度完成，或仍处于从仓库中下载容器镜像的过程中。</p></li><li><p>Running：Pod已经被调度至某节点，所有容器都已经被kubelet创建完成，且至少有一个容器处于启动、重启或运行过程中。</p></li><li><p>Succeeded：Pod中的所有容器都已经成功终止且不会再重启。</p></li><li><p>Failed：所有容器都已经终止，但至少有一个容器终止失败，即容器以非0状态码退出或已经被系统终止。</p></li><li><p>Unknown：API Server无法正常获取到Pod对象的状态信息，通常是由于其无法与所在工作节点的kubelet通信所致。</p></li></ul><p><font color="red">阶段仅是对Pod对象生命周期运行阶段的概括性描述，而非Pod或内部容器状态的综合汇总，因此Pod对象的status字段中的状态值未必一定是可用的相位，它也有可能是Pod的某个错误状态，例如CrashLoopBackOff或Error等。</font><br>Pod资源的核心职责是运行和维护称为主容器的应用程序容器，在其整个生命周期之中的多种可选行为也是围绕更好地实现该功能而进行，如图4-8所示。其中，初始化容器（init container）是常用的Pod环境初始化方式，健康状态检测（startupProbe、livenessProbe和readinessProbe）为编排工具提供了监测容器运行状态的编程接口，而事件钩子（preStop和postStart）则赋予了应用容器读取来自编排工具上自定义事件的机制。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220120200439355.png" alt="../img/image-20220120200439355"></p><p>若用户给出了上述全部定义，则一个Pod对象生命周期的运行步骤如下。</p><blockquote><p>1）在启动包括初始化容器在内的任何容器之前先创建pause基础容器，它初始化Pod环境并为后续加入的容器提供共享的名称空间。<br>2）按顺序以串行方式运行用户定义的各个初始化容器进行Pod环境初始化；任何一个初始化容器运行失败都将导致Pod创建失败，并按其restartPolicy的策略进行处理，默认为重启。<br>3）待所有初始化容器成功完成后，启动应用程序容器，多容器Pod环境中，此步骤会并行启动所有应用容器，例如主容器和Sidecar容器，它们各自按其定义展开其生命周期；本步骤及后面的几个步骤都将以主容器为例进行说明；容器启动的那一刻会同时运行主容器上定义的PostStart钩子事件，该步骤失败将导致相关容器被重启。<br>4）运行容器启动健康状态监测（startupProbe），判定容器是否启动成功；该步骤失败，同样参照restartPolicy定义的策略进行处理；未定义时，默认状态为Success。<br>5）容器启动成功后，定期进行存活状态监测（liveness）和就绪状态监测（readiness）；存活状态监测失败将导致容器重启，而就绪状态监测失败会使得该容器从其所属的Service对象的可用端点列表中移除。<br>6）终止Pod对象时，会先运行preStop钩子事件，并在宽限期（terminationGrace-PeriodSeconds）结束后终止主容器，宽限期默认为30秒。</p></blockquote><h2 id="4-2-在Pod中运行应用"><a href="#4-2-在Pod中运行应用" class="headerlink" title="4.2 在Pod中运行应用"></a>4.2 在Pod中运行应用</h2><p>Pod资源中可同时存在初始化容器、应用容器和临时容器3种类型的容器，不过创建并运行一个具体的Pod对象时，仅有应用容器是必选项，并且可以仅为其定义单个容器。</p><h2 id="4-2-1-使用单容器Pod资源"><a href="#4-2-1-使用单容器Pod资源" class="headerlink" title="4.2.1 使用单容器Pod资源"></a>4.2.1 使用单容器Pod资源</h2><p>一个Pod对象的核心职责在于以主容器形式运行单个应用，因而定义API资源的关键配置就在于定义该容器，它以对象形式定义在Pod对象的spec.containers字段中，基本格式如下：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">…</span>               <span class="comment"># Pod的标识名，在名称空间中必须唯一</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">…</span>          <span class="comment"># 该Pod所属的名称空间，省略时使用默认名称空间，例如default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span>           <span class="comment"># 定义容器，它是一个列表对象，可包括多个容器的定义，至少得有一个</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span>             <span class="comment"># 容器名称，必选字段，在当前Pod中必须唯一</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">…</span>            <span class="comment"># 创建容器时使用的镜像</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">…</span>  <span class="comment"># 容器镜像下载策略，可选字段</span></span><br></pre></td></tr></table></figure><p>../img/image虽为可选字段，这只是为方便更高级别的管理类资源（例如Deployment等）能覆盖它以实现某种高级管理功能而设置，对于非控制器管理的自主式Pod来说并不能省略该字段。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br></pre></td></tr></table></figure><p>把上面的内容保存于配置文件pod-demo.yaml中，随后即可使用kubectl apply或kubectl create命令进行资源对象创建</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-demo.yaml</span></span><br><span class="line">pod/pod-demo created</span><br></pre></td></tr></table></figure><p>该Pod对象由调度器绑定至特定工作节点后，由相应的kubelet负责创建和维护，实时状态也将同步给API Server并由其存储至etcd中。Pod创建并尝试启动的过程中，可能会经历Pending、ContainerCreating、Running等多种不同的状态，若Pod可正常启动，则kubectl get pods/POD命令输出字段中的状态（STATUS）则显示为Running</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/pod-demo -n default</span></span><br><span class="line">NAME  READY   STATUS  RESTARTS  AGE</span><br><span class="line">pod-demo   1/1     Running   0          5m</span><br></pre></td></tr></table></figure><p>随后即可对Pod中运行着的主容器的服务发起访问请求。镜像demoapp默认运行了一个Web服务程序，该服务监听TCP协议的80端口，镜像可通过“/”、/hostname、/user-agent、/livez、/readyz和/configs等路径服务于客户端的请求。例如，下面的命令先获取到Pod的IP地址，而后对其支持的Web资源路径/和/user-agent分别发出了一个访问请求：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">demoIP=$(kubectl get pods/pod-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line">~ $ curl -s http://$demoIP</span><br><span class="line">iKubernetes demoapp v1.0 ! ClientIP: 10.244.0.0, ServerName: pod-demo, ServerIP: 10.244.2.3!</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s http://<span class="variable">$demoIP</span>/user-agent</span></span><br><span class="line">User-Agent: curl/7.58.0</span><br></pre></td></tr></table></figure><p>容器的../img/imagePullPolicy字段用于为其指定镜像获取策略，可用值包括如下几个。</p><ul><li><p>Always：每次启动Pod时都要从指定的仓库下载镜像。</p></li><li><p>IfNotPresent：仅本地镜像缺失时方才从目标仓库wp下载镜像。</p></li><li><p>Never：禁止从仓库下载镜像，仅使用本地镜像。</p></li></ul><p>对于标签为<font color="red">latest的镜像文件，其默认的镜像获取策略为Always，其他标签的镜像，默认策略则为IfNotPresent。</font>需要注意的是，从私有仓库中下载镜像时通常需要事先到Registry服务器认证后才能进行。认证过程要么需要在相关节点上交互式执行docker login命令，要么将认证信息定义为专有的Secret资源，并配置Pod通过../img/imagePullSecretes字段调用此认证信息完成。<br>删除Pod对象则使用kubectl delete命令。</p><ul><li>命令式命令：kubectl delete pods/NAME。</li><li>命令式对象配置：kubectl delete -f FILENAME。</li></ul><p>若删除后Pod一直处于Terminating状态，则可再一次执行删除命令，并同时使用–force和–grace-period=0选项进行强制删除。</p><h2 id="4-2-2-获取Pod与容器状态详情"><a href="#4-2-2-获取Pod与容器状态详情" class="headerlink" title="4.2.2 获取Pod与容器状态详情"></a>4.2.2 获取Pod与容器状态详情</h2><p>kubectl有多个子命令，用于从不同角度显示对象的状态信息，这些信息有助于用户了解对象的运行状态、属性详情等。</p><ul><li>kubectl describe：显示资源的详情，包括运行状态、事件等信息，但不同的资源类型输出内容不尽相同。</li><li>kubectl logs：查看Pod对象中容器输出到控制台的日志信息；当Pod中运行有多个容器时，需要使用选项-c指定容器名称。</li><li>kubectl exec：在Pod对象某容器内运行指定的程序，其功能类似于docker exec命令，可用于了解容器各方面的相关信息或执行必需的设定操作等，具体功能取决于容器内可用的程序。</li></ul><h4 id="1-打印Pod对象的状态"><a href="#1-打印Pod对象的状态" class="headerlink" title="1 打印Pod对象的状态"></a>1 打印Pod对象的状态</h4><p>kubectl describe pods/NAME -n NAMESPACE命令可打印Pod对象的详细描述信息，包括events和controllers等关系的子对象等，Priority、Status、Containers和Events等字段通常是重点关注的目标字段。<br>另外，也可以通过kubectl get pods/POD -o yaml|json命令的status字段来了解Pod的状态详情，它保存有Pod对象的当前状态。如下命令显示了pod-demo的状态信息，结果输出做了尽可能的省略。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods/pod-demo</span> <span class="string">-o</span> <span class="string">yaml</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">conditions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">lastTransitionTime:</span> <span class="string">&quot;2020-08-16T03:36:48Z&quot;</span></span><br><span class="line">    <span class="attr">message:</span> <span class="string">&#x27;containers with unready status: [demo]&#x27;</span></span><br><span class="line">    <span class="attr">reason:</span> <span class="string">ContainersNotReady</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;False&quot;</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">ContainersReady</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">  <span class="attr">containerStatuses:</span>   <span class="comment"># 容器级别的状态信息</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">containerID:</span> <span class="string">docker://……</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imageID:</span> <span class="string">docker-pullable://ikubernetes/demoapp@sha256:……</span></span><br><span class="line">    <span class="attr">lastState:</span> &#123;&#125;      <span class="comment"># 前一次的状态</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">ready:</span> <span class="literal">true</span>        <span class="comment"># 是否已经就绪</span></span><br><span class="line">    <span class="attr">restartCount:</span> <span class="number">0</span>    <span class="comment"># 重启次数</span></span><br><span class="line">    <span class="attr">started:</span> <span class="literal">true</span>  </span><br><span class="line">    <span class="attr">state:</span>             <span class="comment"># 当前状态</span></span><br><span class="line">      <span class="attr">running:</span></span><br><span class="line">        <span class="attr">startedAt:</span> <span class="string">&quot;2020-08-16T03:36:48Z&quot;</span>   <span class="comment"># 启动时间</span></span><br><span class="line">  <span class="attr">hostIP:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.12</span>  <span class="comment"># 节点IP</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Running</span>       <span class="comment"># Pod当前的相位</span></span><br><span class="line">  <span class="attr">podIP:</span> <span class="number">10.244</span><span class="number">.2</span><span class="number">.3</span>    <span class="comment"># Pod的主IP地址</span></span><br><span class="line">  <span class="attr">podIPs:</span>     <span class="comment"># Pod上的所有IP地址</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">10.244</span><span class="number">.2</span><span class="number">.3</span></span><br><span class="line">  <span class="attr">qosClass:</span> <span class="string">BestEffort</span> <span class="comment"># QoS类别</span></span><br></pre></td></tr></table></figure><p>上面的命令结果中，conditions字段是一个称为PodConditions的数组，它记录了Pod所处的“境况”或者“条件”，其中的每个数组元素都可能由如下6个字段组成。</p><ul><li>lastProbeTime：上次进行Pod探测时的时间戳。</li><li>lastTransitionTime：Pod上次发生状态转换的时间戳。</li><li>message：上次状态转换相关的易读格式信息。</li><li>reason：上次状态转换原因，用驼峰格式的单个单词表示。</li><li>status：是否为状态信息，可取值有True、False和Unknown。</li><li>type：境况的类型或名称，有4个固定值；PodScheduled表示已经与节点绑定；Ready表示已经就绪，可服务客户端请求；Initialized表示所有的初始化容器都已经成功启动；ContainersReady则表示所有容器均已就绪。</li></ul><p>另外，containerStatuses字段描述了Pod中各容器的相关状态信息，包括容器ID、镜像和镜像ID、上一次的状态、名称、启动与否、就绪与否、重启次数和状态等。</p><h4 id="2-查看容器日志"><a href="#2-查看容器日志" class="headerlink" title="2 查看容器日志"></a>2 查看容器日志</h4><p>kubectl logs POD [-c CONTAINER]命令可直接获取并打印控制台日志，不过，若Pod对象中仅运行有一个容器，则可以省略-c选项及容器名称。例如，下面的命令打印了pod-demo中唯一的主容器的控制台日志：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">logs</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="string">*</span> <span class="string">Running</span> <span class="string">on</span> <span class="string">http://0.0.0.0:80/</span> <span class="string">(Press</span> <span class="string">CTRL+C</span> <span class="string">to</span> <span class="string">quit)</span></span><br><span class="line"><span class="number">172.29</span><span class="number">.9</span><span class="number">.1</span> <span class="bullet">-</span> <span class="bullet">-</span> [<span class="number">16</span><span class="string">/Aug/2020</span> <span class="number">03</span><span class="string">:54:42</span>] <span class="string">&quot;GET / HTTP/1.1&quot;</span> <span class="number">200</span> <span class="bullet">-</span></span><br><span class="line"><span class="number">172.29</span><span class="number">.9</span><span class="number">.11</span> <span class="bullet">-</span> <span class="bullet">-</span> [<span class="number">16</span><span class="string">/Aug/2020</span> <span class="number">03</span><span class="string">:54:50</span>] <span class="string">&quot;GET / HTTP/1.1&quot;</span> <span class="number">200</span> <span class="bullet">-</span></span><br></pre></td></tr></table></figure><h5 id="3-在容器中额外运行其他程序"><a href="#3-在容器中额外运行其他程序" class="headerlink" title="3 在容器中额外运行其他程序"></a>3 在容器中额外运行其他程序</h5><p>kubectl exec可以让用户在Pod的某容器中运行用户所需要的任何存在于容器中的程序。在kubectl logs获取的信息不够全面时，此命令可以通过在Pod中运行其他指定的命令（前提是容器中存在此程序）来辅助用户获取更多信息。一个更便捷的使用接口是直接交互式运行容器中的某shell程序。例如，直接查看Pod中的容器运行的进程：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-demo -- ps aux</span></span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:01 python3 /usr/local/bin/demo.py</span><br><span class="line">    8 root      0:00 ps aux</span><br></pre></td></tr></table></figure><p><font color="red">注意:如果Pod中运行多个容器，需要使用-c <container_name>选项指定运行程序的容器名称。</container_name></font></p><p>有时候需要打开容器的交互式shell接口以方便多次执行命令，为kubectl exec命令额外使用-it选项，并指定运行镜像中可用的shell程序就能进入交互式接口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl -it <span class="built_in">exec</span> pod-demo /bin/sh</span></span><br><span class="line">[root@pod-demo /]# hostname</span><br><span class="line">pod-demo</span><br><span class="line">[root@pod-demo /]# netstat -tnl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address     Foreign Address     State   </span><br><span class="line">tcp        0      0 0.0.0.0:80          0.0.0.0:*         LISTEN</span><br></pre></td></tr></table></figure><h3 id="4-2-3-自定义容器应用与参数"><a href="#4-2-3-自定义容器应用与参数" class="headerlink" title="4.2.3　自定义容器应用与参数"></a>4.2.3　自定义容器应用与参数</h3><p>容器镜像启动容器时运行的默认应用程序由其Dockerfile文件中的ENTRYPOINT指令进行定义，传递给程序的参数则通过CMD指令设定，ETRYPOINT指令不存在时，CMD可同时指定程序及其参数。例如，要了解镜像ikubernetes/demoapp:v1.0中定义的ENTRYPOINT和CMD，可以在任何存在此镜像的节点上执行类似如下命令来获取：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Entrypoint&#125;&#125;</span></span><br><span class="line">[/bin/sh -c python3 /usr/local/bin/demo.py]</span><br><span class="line"><span class="meta">~# </span><span class="language-bash">docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Cmd&#125;&#125;</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p>Pod配置中，spec.containers[].command字段可在容器上指定非镜像默认运行的应用程序，且可同时使用spec.containers[].args字段进行参数传递，它们将覆盖镜像中默认定义的参数。若定义了args字段，该字段值将作为参数传递给镜像中默认指定运行的应用程序；而仅定义了command字段时，其值将覆盖镜像中定义的程序及参数。下面的资源配置清单保存在pod-demo-with-cmd-and-args.yaml文件中，它把镜像ikubernetes/demoapp:v1.0的默认应用程序修改为/bin/sh -c，参数定义为python3 /usr/local/bin/demo.py -p 8080，其中的-p选项可修改服务监听的端口为指定的自定义端口</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-demo-with-cmd-and-args</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;python3 /usr/local/bin/demo.py -p 8080&#x27;</span>]</span><br></pre></td></tr></table></figure><p>下面将上述清单中定义的Pod对象创建到集群上，验证其监听的端口是否从默认的80变为了指定的8080：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create -f pod-demo-with-cmd-and-args.yaml</span> </span><br><span class="line">pod/pod-demo-with-cmd-and-args created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-demo-with-cmd-and-args -- netstat -tnl</span>  </span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address    Foreign Address     State </span><br><span class="line">tcp        0      0 0.0.0.0:8080     0.0.0.0:*          LISTEN</span><br></pre></td></tr></table></figure><h3 id="4-2-4-容器环境变量"><a href="#4-2-4-容器环境变量" class="headerlink" title="4.2.4　容器环境变量"></a>4.2.4　容器环境变量</h3><p>容器环境变量需要应用程序支持通过环境变量进行配置，否则用户要在制作Docker镜像时通过entrypoint脚本完成环境变量到程序配置文件的同步。<br>向Pod对象中容器环境变量传递数据的方法有两种：env和envFrom，这里重点介绍第一种方式，第二种方式将在介绍ConfigMap和Secret资源时进行说明。<br>通过环境变量的配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构成的列表。每个环境变量通常由name和value字段构成。</p><ul><li>name <string>：环境变量的名称，必选字段。</string></li><li>value <string>：传递给环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。</string></li></ul><p>示例中使用镜像demoapp中的应用服务器支持通过HOST与PORT环境变量分别获取监听的地址和端口，它们的默认值分别为0.0.0.0和80，下面的配置保存在清单文件pod-using-env.yaml中，它分别为HOST和PORT两个环境变量传递了一个不同的值，以改变容器监听的地址和端口。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-env</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br></pre></td></tr></table></figure><p>下面将清单文件中定义的Pod对象创建至集群中，并查看应用程序监听的地址和端口来验证配置结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-env.yaml</span></span><br><span class="line">pod/pod-using-env created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-using-env -- netstat -tnl</span></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address      Foreign Address    State       </span><br><span class="line">tcp        0      0 127.0.0.1:8080     0.0.0.0:*         LISTEN</span><br></pre></td></tr></table></figure><h3 id="4-2-5-Pod的创建与删除过程"><a href="#4-2-5-Pod的创建与删除过程" class="headerlink" title="4.2.5 Pod的创建与删除过程"></a>4.2.5 Pod的创建与删除过程</h3><h4 id="Pod资源对象的创建过程。"><a href="#Pod资源对象的创建过程。" class="headerlink" title="Pod资源对象的创建过程。"></a>Pod资源对象的创建过程。</h4><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220122090224890.png" alt="../img/image-20220122090224890"></p><blockquote><p>1）用户通过kubectl或其他API客户端提交Pod Spec给API Server。<br>2）API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。<br>3）Scheduler（调度器）通过其watcher监测到API Server创建了新的Pod对象，于是为该Pod对象挑选一个工作节点并将结果信息更新至API Server。<br>4）调度结果信息由API Server更新至etcd存储系统，并同步给Scheduler。<br>5）相应节点的kubelet监测到由调度器绑定于本节点的Pod后会读取其配置信息，并由本地容器运行时创建相应的容器启动Pod对象后将结果回存至API Server。<br>6）API Server将kubelet发来的Pod状态信息存入etcd系统，并将确认信息发送至相应的kubelet。<br>另一方面，Pod可能曾用于处理生产数据或向用户提供服务等，Kubernetes可删除宽限期确保终止操作能够以平滑方式优雅完成，从而用户也可以在正常提交删除操作后获知何时开始终止并最终完成。删除时，用户提交请求后系统即会进行强制删除操作的宽限期倒计时，并将TERM信号发送给Pod对象中每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，Pod对象也随即由API Server删除。如果在等待进程终止的过程中kubelet或容器管理器发生了重启，则终止操作会重新获得一个满额的删除宽限期并重新执行删除操作。</p></blockquote><h4 id="Pod的终止过程"><a href="#Pod的终止过程" class="headerlink" title="Pod的终止过程"></a>Pod的终止过程</h4><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220122090439625.png" alt="../img/image-20220122090439625"></p><p>如图4-10所示，一个典型的Pod对象终止流程如下。</p><blockquote><p>1）用户发送删除Pod对象的命令。<br>2）API服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认为30秒），Pod被视为dead。<br>3）将Pod标记为Terminating状态。<br>4）（与第3步同时运行）kubelet在监控到Pod对象转为Terminating状态的同时启动Pod关闭过程。<br>5）（与第3步同时运行）端点控制器监控到Pod对象的关闭行为时将其从所有匹配到此端点的Service资源的端点列表中移除。<br>6）如果当前Pod对象定义了preStop钩子句柄，在其标记为terminating后即会以同步方式启动执行；如若宽限期结束后，preStop仍未执行完，则重新执行第2步并额外获取一个时长为2秒的小宽限期。<br>7）Pod对象中的容器进程收到TERM信号。<br>8）宽限期结束后，若存在任何一个仍在运行的进程，Pod对象即会收到SIGKILL信号。<br>9）Kubelet请求API Server将此Pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。<br>默认情况下，所有删除操作的宽限期都是30秒，不过kubectl delete命令可以使用–grace-period=<seconds>选项自定义其时长，使用0值则表示直接强制删除指定的资源，不过此时需要同时为命令使用–force选项。</seconds></p></blockquote><h2 id="4-3-暴露容器服务"><a href="#4-3-暴露容器服务" class="headerlink" title="4.3 暴露容器服务"></a>4.3 暴露容器服务</h2><p>不考虑通过Service资源进行服务暴露的情况下，服务于集群外部的客户端的常用方式有两种：一种是在其运行的节点上进行端口映射，由节点IP和选定的协议端口向Pod内的应用容器进行DNAT转发；另一种是让Pod共享其所在的工作节点的网络名称空间，应用进程将直接监听工作节点IP地址和协议端口。</p><h3 id="4-3-1-其他容器端口映射"><a href="#4-3-1-其他容器端口映射" class="headerlink" title="4.3.1 其他容器端口映射"></a>4.3.1 其他容器端口映射</h3><p>其他Kubernetes系统的网络模型中，各Pod的IP地址处于同一网络平面，无论是否为容器指定了要暴露的端口都不会影响集群中其他节点之上的Pod客户端对其进行访问，这意味着，任何在非本地回环接口lo上监听的端口都可直接通过Pod网络被请求。从这个角度来说，容器端口只是信息性数据，它仅为集群用户提供了一个快速了解相关Pod对象的可访问端口的途径，但显式指定容器的服务端口可额外为其赋予一个名称以方便按名称调用。定义容器端口的ports字段的值是一个列表，由一到多个端口对象组成，它的常用嵌套字段有如下几个。</p><ul><li>containerPort <integer>：必选字段，指定在Pod对象的IP地址上暴露的容器端口，有效范围为(0,65536)；使用时，需要指定为容器应用程序需要监听的端口。</integer></li><li>name <string>：当前端口的名称标识，必须符合IANA_SVC_NAME规范且在当前Pod内要具有唯一性；此端口名可被Service资源按名调用。</string></li><li>protocol <string>：端口相关的协议，其值仅支持TCP、SCTP或UDP三者之一，默认为TCP。</string></li></ul><p>需要借助于Pod所在节点将容器服务暴露至集群外部时，还需要使用hostIP与hostPort两个字段来指定占用的工作节点地址和端口。如图4-11所示的Pod A与Pod C可分别通过各自所在节点上指定的hostIP和hostPort服务于客户端请求。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220122092123020.png" alt="../img/image-20220122092123020"></p><ul><li>hostPort <integer>：主机端口，它将接收到的请求通过NAT机制转发至由container-Port字段指定的容器端口。</integer></li><li>hostIP <string>：主机端口要绑定的主机IP，默认为主机之上所有可用的IP地址；该字段通常使用默认值。</string></li></ul><p>下面的资源配置清单示例（pod-using-hostport.yaml）中定义的demo容器指定了要暴露容器上TCP协议的80端口，并将之命名为http，该容器可通过工作节点的10080端口接入集群外部客户端的请求。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-hostport</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">hostPort:</span> <span class="number">10080</span></span><br></pre></td></tr></table></figure><p>在集群中创建配置清单中定义的Pod对象后，需获取其被调度至的目标节点，例如下面第二个命令结果中的k8s-node02.ilinux.io/172.29.9.12，而后从集群外部向该节点的10080端口发起Web请求进行访问测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-hostport.yaml</span> </span><br><span class="line">pod/pod-using-hostport</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/ pod-using-hostport | grep <span class="string">&quot;^Node:&quot;</span></span></span><br><span class="line">Node:         k8s-node02.ilinux.io/172.29.9.12</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl 172.29.9.12:10080</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 172.29.0.1, ServerName: pod-using-hostport, ServerIP: 10.244.2.9!</span><br></pre></td></tr></table></figure><p>注意，hostPort与NodePort类型的Service对象暴露端口的方式不同，NodePort是通过所有节点暴露容器服务，而hostPort则能经由Pod对象所在节点的IP地址进行。</p><h3 id="4-3-2-配置Pod使用节点网络"><a href="#4-3-2-配置Pod使用节点网络" class="headerlink" title="4.3.2 配置Pod使用节点网络"></a>4.3.2 配置Pod使用节点网络</h3><p>同一个Pod对象的各容器运行于一个独立、隔离的Network、UTS和IPC名称空间中，共享同一个网络协议栈及相关的网络设备，但也有些特殊的Pod对象需要运行于所在节点的名称空间中，执行系统级的管理任务（例如查看和操作节点的网络资源甚至是网络设备等），或借助节点网络资源向集群外客户端提供服务等，如图4-12中的右图所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220122092551676.png" alt="../img/image-20220122092551676"></p><p>由kubeadm部署的Kubernetes集群中的kube-apiserver、kube-controller-manager、kube-scheduler，以及kube-proxy和kube-flannel等通常都是第二种类型的Pod对象。网络名称空间是Pod级别的属性，用户配置的Pod对象，仅需要设置其spec.hostNetwork的属性为true即可创建共享节点网络名称空间的Pod对象，如下面保存在pod-using-hostnetwork.yaml文件中的配置清单所示。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-hostnetwork</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>将上面配置清单中定义的pod-using-hostnetwork创建于集群上，并查看主机名称或网络接口的相关属性信息以验证它是否能共享使用工作节点的网络名称空间。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-hostnetwork.yaml</span> </span><br><span class="line">pod/pod-using-hostnetwork created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it pod-using-hostnetwork -- hostname</span></span><br><span class="line">k8s-node01.ilinux.io</span><br></pre></td></tr></table></figure><p>上面第二个命令的结果显示出的主机名称，表示该Pod已然共享了其所在节点的UTS名称空间，以及Network和IPC名称空间。这意味着，Pod对象中运行容器化应用可在其所在的工作节点的IP地址之上监听，这可以通过直接向k8s-node01.ilinux.io节点发起请求来验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl k8s-node01.ilinux.io</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 172.29.9.1, ServerName: k8s-node01.ilinux.io, ServerIP: 172.29.9.11!</span><br></pre></td></tr></table></figure><h2 id="4-4-容器安全上下文"><a href="#4-4-容器安全上下文" class="headerlink" title="4.4 容器安全上下文"></a>4.4 容器安全上下文</h2><p>Kubernetes为安全运行Pod及容器运行设计了安全上下文机制，该机制允许用户和管理员定义Pod或容器的特权与访问控制，以配置容器与主机以及主机之上的其他容器间的隔离级别。安全上下文就是一组用来决定容器是如何创建和运行的约束条件，这些条件代表创建和运行容器时使用的运行时参数。需要提升容器权限时，用户通常只应授予容器执行其工作所需的访问权限，以“最小权限法则”来抑制容器对基础架构及其他容器产生的负面影响。<br>Kubernetes支持用户在Pod及容器级别配置安全上下文，并允许管理员通过Pod安全策略在集群全局级别限制用户在创建和运行Pod时可设定的安全上下文。本节仅描述Pod和容器级别的配置，Pod安全策略的话题将在第9章展开。<br>Pod和容器的安全上下文设置包括以下几个方面。</p><ul><li>自主访问控制（DAC）：传统UNIX的访问控制机制，它允许对象（OS级别，例如文件等）的所有者基于UID和GID设定对象的访问权限。</li><li>Linux功能：Linux为突破系统上传统的两级用户（root和普通用户）授权模型，而将内核管理权限打散成多个不同维度或级别的权限子集，每个子集称为一种“功能”或“能力”，例如CAP_NET_ADMIN、CAP_SYS_TIME、CAP_SYS_PTRACE和CAP_SYS_ADMIN等，从而允许进程仅具有一部分内核管理功能就能完成必要的管理任务。</li><li>seccomp：全称为secure computing mode，是Linux内核的安全模型，用于为默认可发起的任何系统调用进程施加控制机制，人为地禁止它能够发起的系统调用，有效降低了程序被劫持时的危害级别。</li><li>AppArmor：全称为Application Armor，意为“应用盔甲”，是Linux内核的一个安全模块，通过加载到内核的配置文件来定义对程序的约束与控制。</li><li>SELinux：全称为Security-Enhanced Linux，意为安全加强的Linux，是Linux内核的一个安全模块，提供了包括强制访问控制在内的访问控制安全策略机制。</li><li>Privileged模式：即特权模式容器，该模式下容器中的root用户拥有所有的内核功能，即具有真正的管理员权限，它能看到主机上的所有设备，能够挂载文件系统，甚至可以在容器中运行容器；容器默认运行于非特权（unprivileged）模式。</li><li>AllowPrivilegeEscalation：控制是否允许特权升级，即进程是否能够获取比父进程更多的特权；运行于特权模式或具有CAP_SYS_ADMIN能力的容器默认允许特权升级。<br>这些安全上下文相关的特性多数嵌套定义在Pod或容器的securityContext字段中，而且有些特性对应的嵌套字段还不止一个。而seccomp和AppArmor的安全上下文则需要以资源注解的方式进行定义，而且仅能由管理员在集群级别进行Pod安全策略配置。</li></ul><h3 id="4-4-1-配置格式速览"><a href="#4-4-1-配置格式速览" class="headerlink" title="4.4.1 配置格式速览"></a>4.4.1 配置格式速览</h3><p>安全上下文可分别设置Pod级别和容器级别。但有些参数并不适合通用设定，例如特权模式、特权升级、只读根文件系统和内核能力等，它们只可用于容器之上。但也有参数仅可用于Pod级别进行通用设定，例如设置内核参数的sysctl和设置存储卷新件文件默认属组的fsgroup等。下面以Pod资源的配置格式给出了这些配置选项，以便于读者快速预览和了解安全上下文的用法。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span> &#123;<span class="string">…</span>&#125;</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span>         <span class="comment"># Pod级别的安全上下文，对内部所有容器均有效</span></span><br><span class="line">    <span class="string">runAsUser</span> <span class="string">&lt;integer&gt;</span>    <span class="comment"># 以指定的用户身份运行容器进程，默认由镜像中的USER指定</span></span><br><span class="line">    <span class="string">runAsGroup</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 以指定的用户组运行容器进程，默认使用的组随容器运行时设定</span></span><br><span class="line">    <span class="string">supplementalGroups</span>  <span class="string">&lt;[]integer&gt;</span>  <span class="comment"># 为容器中1号进程的用户添加的附加组</span></span><br><span class="line">    <span class="string">fsGroup</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 为容器中的1号进程附加一个专用组，其功能类似于sgid</span></span><br><span class="line">    <span class="string">runAsNonRoot</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否以非root身份运行</span></span><br><span class="line">    <span class="string">seLinuxOptions</span> <span class="string">&lt;Object&gt;</span>  <span class="comment"># SELinux的相关配置</span></span><br><span class="line">    <span class="string">sysctls</span>  <span class="string">&lt;[]Object&gt;</span>         <span class="comment"># 应用到当前Pod名称空间级别的sysctl参数设置列表</span></span><br><span class="line">    <span class="string">windowsOptions</span> <span class="string">&lt;Object&gt;</span>     <span class="comment"># Windows容器专用的设置</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">securityContext:</span>            <span class="comment"># 容器级别的安全上下文，仅在当前容器生效</span></span><br><span class="line">      <span class="string">runAsUser</span> <span class="string">&lt;integer&gt;</span>       <span class="comment"># 以指定的用户身份运行容器进程</span></span><br><span class="line">      <span class="string">runAsGroup</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 以指定的用户组运行容器进程</span></span><br><span class="line">      <span class="string">runAsNonRoot</span> <span class="string">&lt;boolean&gt;</span>    <span class="comment"># 是否以非root身份运行</span></span><br><span class="line">      <span class="string">allowPrivilegeEscalation</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否允许特权升级</span></span><br><span class="line">      <span class="string">capabilities</span> <span class="string">&lt;Object&gt;</span>     <span class="comment"># 为当前容器添加（add）或删除（drop）内核能力</span></span><br><span class="line">        <span class="string">add</span>  <span class="string">&lt;[]string&gt;</span>         <span class="comment"># 添加由列表定义的各内核能力</span></span><br><span class="line">        <span class="string">drop</span>  <span class="string">&lt;[]string&gt;</span>        <span class="comment"># 移除由列表定义的各内核能力</span></span><br><span class="line">      <span class="string">privileged</span> <span class="string">&lt;boolean&gt;</span>      <span class="comment"># 是否运行为特权容器</span></span><br><span class="line">      <span class="string">procMount</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 设置容器的procMount类型，默认为DefaultProcMount；</span></span><br><span class="line">      <span class="string">readOnlyRootFilesystem</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否将根文件系统设置为只读模式</span></span><br><span class="line">      <span class="string">seLinuxOptions</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># SELinux的相关配置</span></span><br><span class="line">      <span class="string">windowsOptions</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># Windows容器专用的设置</span></span><br></pre></td></tr></table></figure><p>Kubernetes默认以非特权模式创建并运行容器，同时禁用了其他与管理功能相关的内核能力，但未额外设定其他上下文参数。</p><h3 id="4-4-2-管理容器进程的运行身份"><a href="#4-4-2-管理容器进程的运行身份" class="headerlink" title="4.4.2 管理容器进程的运行身份"></a>4.4.2 管理容器进程的运行身份</h3><p>制作Docker镜像时，Dockerfile支持以USER指令明确指定运行应用进程时的用户身份。对于未通过USER指令显式定义运行身份的镜像，创建和启动容器时，其进程的默认用户身份为容器中的root用户和root组，该用户有着其他一些附加的系统用户组，例如sys、daemon、wheel和bin等。然而，有些应用程序的进程需要以特定的专用用户身份运行，或者以指定的用户身份运行时才能获得更好的安全特性，这种需求可以在Pod或容器级别的安全上下文中使用runAsUser得以解决，必要时可同时使用runAsGroup设置进程的组身份。<br>下面的资源清单（securitycontext-runasuer-demo.yaml）配置以1001这个UID和GID的身份来运行容器中的demoapp应用，考虑到非特权用户默认无法使用1024以下的端口号，文件中通过环境变量改变了应用监听的端口。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-runasuser-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br></pre></td></tr></table></figure><p>下面的命令先将配置清单中定义的Pod对象securitycontext-runasuser-demo创建到集群上，随后的两条命令验证了容器用户身份确为配置中预设的UID和GID。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f securitycontext-runasuser-demo.yaml</span> </span><br><span class="line">pod/securitycontext-runasuser-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-runasuser-demo -- <span class="built_in">id</span></span> </span><br><span class="line">uid=1001 gid=1001</span><br><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-runasuser-demo -- ps aux</span></span><br><span class="line">PID    USER      TIME  COMMAND</span><br><span class="line">  1    1001      0:00  python3 /usr/local/bin/demo.py</span><br></pre></td></tr></table></figure><p>还可在上面的配置清单中的安全上下文定义中，同时使用supplement-Groups选项定义主进程用户的其他附加用户组，这对于有着复杂权限模型的应用是一个非常有用的选项。<br>另外，若运行容器时使用的镜像文件中已经使用USER指令指定了非root用户的运行身份，我们也可以在安全上下文中使用runAsNonRoot参数定义容器必须使用指定的非root用户身份运行，而无须使用runAsUser参数额外指定用户。</p><h3 id="4-4-3-管理容器的内核功能"><a href="#4-4-3-管理容器的内核功能" class="headerlink" title="4.4.3 管理容器的内核功能"></a>4.4.3 管理容器的内核功能</h3><p>传统UNIX仅实现了特权和非特权两类进程，前者是指以0号UID身份运行的进程，而后者则是从属非0号UID用户的进程。Linux内核从2.2版开始将附加于超级用户的权限分割为多个独立单元，这些单元是线程级别的，它们可配置在每个线程之上，为其赋予特定的管理能力。Linux内核常用的功能包括但不限于如下这些。</p><ul><li>CAP_CHOWN：改变文件的UID和GID。</li><li>CAP_MKNOD：借助系统调用mknod()创建设备文件。</li><li>CAP_NET_ADMIN：网络管理相关的操作，可用于管理网络接口、netfilter上的iptables规则、路由表、透明代理、TOS、清空驱动统计数据、设置混杂模式和启用多播功能等。</li><li>CAP_NET_BIND_SERVICE：绑定小于1024的特权端口，但该功能在重新映射用户后可能会失效。</li><li>AP_NET_RAW：使用RAW或PACKET类型的套接字，并可绑定任何地址进行透明代理。</li><li>CAP_SYS_ADMIN：支持内核上的很大一部分管理功能。</li><li>CAP_SYS_BOOT：重启系统。</li><li>CAP_SYS_CHROOT：使用chroot()进行根文件系统切换，并能够调用setns()修改Mount名称空间。</li><li>CAP_SYS_MODULE：装载内核模块。</li><li>CAP_SYS_TIME：设定系统时钟和硬件时钟。</li><li>CAP_SYSLOG：调用syslog()执行日志相关的特权操作等。</li></ul><p>系统管理员可以通过get命令获取程序文件上的内核功能，并可使用setcap命令为程序文件设定内核功能或取消（-r选项）其已有的内核功能。而为Kubernetes上运行的进程设定内核功能则需要在Pod内容器上的安全上下文中嵌套capabilities字段，添加和移除内核能力还需要分别在下一级嵌套中使用add或drop字段。这两个字段可接受以内核能力名称为列表项，但引用各内核能力名称时需移除CAP_前缀，例如可使用NET_ADMIN和NET_BIND_SERVICE这样的功能名称。<br>下面的配置清单（securitycontext-capabilities-demo.yaml）中定义的Pod对象的demo容器，在安全上下文中启用了内核功能NET_ADMIN，并禁用了CHOWN。demo容器的镜像未定义USER指令，它将默认以root用户的身份运行容器应用。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-capabilities-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>,<span class="string">&quot;-c&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&quot;/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8080 -j </span></span><br><span class="line"><span class="string">    REDIRECT --to-port 80 &amp;&amp; /usr/bin/python3 /usr/local/bin/demo.py&quot;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span> [<span class="string">&#x27;NET_ADMIN&#x27;</span>]</span><br><span class="line">        <span class="attr">drop:</span> [<span class="string">&#x27;CHOWN&#x27;</span>]</span><br></pre></td></tr></table></figure><p>容器中的root用户将默认映射为系统上的普通用户，它实际上并不具有管理网络接口、iptables规则和路由表等相关的权限，但内核功能NET_ADMIN可以为其开放此类权限。但容器中的root用户默认就具有修改容器文件系统上的文件从属关系的能力，而禁用CHOWN功能则关闭了这种操作权限。下面创建该Pod对象并运行在集群上，来验证清单中的配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f securitycontext-capabilities-demo.yaml                               </span><br><span class="line">pod/securitycontext-capabilities-demo created</span><br></pre></td></tr></table></figure><p>而后，检查Pod网络名称空间中netfilter之上的规则，清单中的iptables命令添加的规则位于NAT表的PREROUTING链上。下面的命令结果表示iptables命令已然生成的规则，NET_ADMIN功能启用成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-capabilities-demo -- iptables -t nat -nL PREROUTING</span> </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot    opt    source      destination         </span><br><span class="line">REDIRECT   tcp  --  0.0.0.0/0         0.0.0.0/0       tcp dpt:8080 redir ports 80</span><br></pre></td></tr></table></figure><p>接着，下面用于检查demo容器中的root用户是否能够修改容器文件系统上文件的属主和属组的命令结果表示，其CHOWN功能已然成功关闭。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-capabilities-demo -- <span class="built_in">chown</span> 200.200 /etc/hosts</span></span><br><span class="line">chown: /etc/hosts: Operation not permitted</span><br><span class="line">command terminated with exit code 1</span><br></pre></td></tr></table></figure><p>内核的各项功能均可按其原本的意义在容器的安全上下文中按需打开或关闭，但SYS_ADMIN功能拥有内核中的许多管理权限，实在太过强大，出于安全方面的考虑，用户应该基于最小权限法则组合使用内核功能完成容器运行。</p><h3 id="4-4-4-特权模式容器"><a href="#4-4-4-特权模式容器" class="headerlink" title="4.4.4 特权模式容器"></a>4.4.4 特权模式容器</h3><p>相较于内核功能，SYS_ADMIN赋予了进程很大一部分的系统级管理功能，特权（privileged）容器几乎将宿主机内核的完整权限全部开放给了容器进程，它提供的是远超SYS_ADMIN的授权，包括写操作到/proc和/sys目录以及管理硬件设备等，因而仅应该用到基础架构类的系统级管理容器之上。例如，使用kubeadm部署的集群中，kube-proxy中的容器就运行于特权模式。<br>下面的第一个命令从kube-system名称空间中取出一个kube-proxy相关的Pod对象名称，第二个命令则用于打印该Pod对象的配置清单，限于篇幅，这里仅列出了其中一部分内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">pod-name=$(kubectl get pods -l k8s-app=kube-proxy -n kube-system \</span></span><br><span class="line"><span class="language-bash">         -o jsonpath=&#123;.items[0].metadata.name&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods <span class="variable">$pod</span>-name -n kube-system -o yaml</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">从命令结果中截取的启动容器应用的命令及传递的参数</span></span><br><span class="line">containers:</span><br><span class="line">  - command:</span><br><span class="line">    - /usr/local/bin/kube-proxy</span><br><span class="line">    - --config=/var/lib/kube-proxy/config.conf</span><br><span class="line">    - --hostname-override=$(NODE_NAME)</span><br><span class="line">    ../img/image: ……</span><br><span class="line">    ../img/imagePullPolicy: IfNotPresent</span><br><span class="line">    name: kube-proxy</span><br><span class="line">    resources: &#123;&#125;</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br></pre></td></tr></table></figure><p>上面保留的命令结果的最后两行是定义特权容器的格式，唯一用到的privileged字段只能嵌套在容器的安全上下文中，它使用布尔型值，true表示启用特权容器机制。</p><h3 id="4-4-5-在Pod上使用sysctl"><a href="#4-4-5-在Pod上使用sysctl" class="headerlink" title="4.4.5 在Pod上使用sysctl"></a>4.4.5 在Pod上使用sysctl</h3><p>Linux系统上的sysctl接口允许在运行时修改内核参数，管理员可通过/proc/sys/下的虚拟文件系统接口来修改或查询这些与内核、网络、虚拟内存或设备等各子系统相关的参数。Kubernetes也允许在Pod上独立安全地设置支持名称空间级别的内核参数，它们默认处于启用状态，而节点级别内核参数则被认为是不安全的，它们默认处于禁用状态。<br>截至目前，仅kernel.shm_rmid_forced、net.ipv4.ip_local_port_range和net.ipv4.tcp_syncookies这3个内核参数被Kubernetes视为安全参数，它们可在Pod安全上下文的sysctl参数内嵌套使用，而余下的绝大多数的内核参数都是非安全参数，需要管理员手动在每个节点上通过kubelet选项逐个启用后才能配置到Pod上。例如，在各工作节点上编辑/etc/default/kubelet文件，添加如下内容以允许在Pod上使用指定的两个非安全的内核参数，并重启kubelet服务使之生效。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">KUBELET_EXTRA_ARGS=&#x27;--allowed-unsafe-sysctls=net.core.somaxconn,net.ipv4.ip_unprivileged_port_start&#x27;</span></span><br></pre></td></tr></table></figure><p>net.core.somaxconn参数定义了系统级别入站连接队列最大长度，默认值是128；而net.ipv4.ip_unprivileged_port_start参数定义的是非特权用户可以使用的内核端口起始值，默认为1024，它限制了非特权用户所能够使用的端口范围。<br>下面配置清单（securitycontext-sysctls-demo.yaml）中定义的Pod对象在安全上下文中通过sysctls字段嵌套使用了一个安全的内核参数kernel.shm_rmid_forced，以及一个已经启用的非安全内核参数net.ipv4.ip_unprivileged_port_start，它将该非安全内核参数的值设置为0来允许非特权用户使用11024以内端口的权限。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-sysctls-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">sysctls:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kernel.shm_rmid_forced</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">net.ipv4.ip_unprivileged_port_start</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br></pre></td></tr></table></figure><p>尽管上面配置清单设定了以非特权用户1001的身份运行容器应用，但受上面内核参数的影响，非管理员用户也具有了监听80端口的权限，因而不会遇到无法监听特权端口的情形。下面将配置清单中定义的资源创建在集群之上，来验证设定的结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f securitycontext-sysctls-demo.yaml</span> </span><br><span class="line">pod/securitycontext-sysctls-demo created</span><br></pre></td></tr></table></figure><p>下面的命令结果显示，以普通用户身份运行的demo容器成功监听了TCP协议的80端口。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl exec securitycontext-sysctls-demo -- netstat -tnlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address  Foreign Address    State   PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:80        0.0.0.0:*      LISTEN    1/python3</span><br></pre></td></tr></table></figure><p>在Pod对象之上启用非安全内核参数，其配置结果可能会存在无法预料的结果，在正式使用之前一定要经过充分测试。例如，在某一Pod之上同时配置启用前面示例的两个非安全内核参数可能存在生效结果异常的情况。<br>本节中介绍了设置Pod与容器安全上下文配置方法及几种常用使用方式。从示例中我们可以看出，设置特权容器和添加内核功能等，以及在Pod上共享宿主机的Network和PID名称空间等，对于多项目或多团队共享的Kubernetes集群存在着不小的安全隐患，这就要求管理员应该在集群级别使用Pod安全策略（PodSecurityPolicy），来精心管控这些与安全相关配置的运用能力。</p><h2 id="4-5-容器应用的管理接口"><a href="#4-5-容器应用的管理接口" class="headerlink" title="4.5 容器应用的管理接口"></a>4.5 容器应用的管理接口</h2><h3 id="4-5-1-健康状态监测接口"><a href="#4-5-1-健康状态监测接口" class="headerlink" title="4.5.1　健康状态监测接口"></a>4.5.1　健康状态监测接口</h3><p>监测容器自身运行的API包括分别用于健康状态检测、指标、分布式跟踪和日志等实现类型，如图4-13所示。即便没有完全实现，至少容器化应用也应该提供用于健康状态检测（liveness和readiness）的API，以便编排系统能更准确地判定应用程序的运行状态。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220122102525490.png" alt="../img/image-20220122102525490"></p><p>Kubelet仅能在控制循环中根据容器主进程的运行状态来判断其健康与否，主进程以非0状态码退出代表处于不健康状态，其他均为正常状态。然而，有些异常场景中，仍处于运行状态的进程内部的业务处理机制可能已然处于僵死状态或陷入死循环等，无法正常处理业务请求，对于这种状态的判断便要依赖应用自身专用于健康状态监测的接口。<br>存活状态（liveness）检测用于定期检测容器是否正常运行，就绪状态（readiness）检测用于定期检测容器是否可以接收流量，它们能够通过减少运维问题和提高服务质量来使服务更健壮和更具弹性。Kubernetes在Pod内部的容器资源上提供了livenessProbe和readinessProbe两个字段，分别让用户自定义容器应用的存活状态和就绪状态检测。</p><ul><li>存活状态检测：用于判定容器是否处于“运行”状态；若此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为Success。</li><li>就绪状态检测：用于判断容器是否准备就绪并可对外提供服务；未通过该检测时，端点控制器（例如Service对象）会将其IP地址从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中；未定义就绪状态检测的容器的默认状态为Success。<br>容器探测是Pod对象生命周期中的一项重要日常任务，它由kubelet周期性执行。kubelet可在活动容器上分别执行由用户定义的启动状态检测（startupProbe）、存活状态检测（livenessProbe）和就绪状态检测（readinessProbe），定义在容器上的存活状态和就绪状态操作称为检测探针，它要通过容器的句柄（handler）进行定义。Kubernetes定义了用于容器探测的3种句柄。</li><li>ExecAction：通过在容器中执行一个命令并根据其返回的状态码进行的诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。</li><li>TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常状态，否则为不健康状态。</li><li>HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起http GET请求进行诊断，响应码为2xx或3xx即为成功，否则为失败。<br>上面的每种探测方式都可能存在3种返回结果：Success（成功）、Failure（失败）或Unknown（未知），仅第一种结果表示成功通过检测。<br>另外，Kubernetes自v1.16版本起还支持启动状态（startup）检测。将传统模式开发的大型应用程序迁移至容器编排平台运行时，可能需要相当长的时间进行启动后的初始化，但其初始过程是否正确完成的检测机制和探测参数都可能有别于存活状态检测，例如需要更长的间隔周期和更高的错误阈值等。该类检测的结果处理机制与存活状态检测相同，检测失败时kubelet将杀死容器并根据其restartPolicy决定是否将其重启，而未定义时的默认状态为Success。需要注意的是，一旦定义了启动检测探针，则必须等启动检测成功完成之后，存活探针和就绪探针才可启动。</li></ul><h3 id="4-5-2-容器存活状态检测"><a href="#4-5-2-容器存活状态检测" class="headerlink" title="4.5.2 容器存活状态检测"></a>4.5.2 容器存活状态检测</h3><p>存活性探测是隶属于容器级别的配置，kubelet可基于它判定何时需要重启容器。目前，Kubernetes在容器上支持的存活探针有3种类型：ExecAction、TCPSocketAction和HTTPGetAction。</p><h4 id="1-存活探针配置格式"><a href="#1-存活探针配置格式" class="headerlink" title="1. 存活探针配置格式"></a>1. 存活探针配置格式</h4><p>Pod配置格式中，spec.containers.livenessProbe字段用于定义此类检测，配置格式如下所示。但一个容器之上仅能定义一种类型的探针，即exec、httpGet和tcpSocket三者互斥，它们不可在一个容器同时使用。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="string">exec</span> <span class="string">&lt;Object&gt;</span>                   <span class="comment"># 命令式探针</span></span><br><span class="line">      <span class="string">httpGet</span> <span class="string">&lt;Object&gt;</span>                <span class="comment"># http GET类型的探针</span></span><br><span class="line">      <span class="string">tcpSocket</span> <span class="string">&lt;Object&gt;</span>              <span class="comment"># tcp Socket类型的探针</span></span><br><span class="line">      <span class="string">initialDelaySeconds</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 发起初次探测请求的延后时长</span></span><br><span class="line">      <span class="string">periodSeconds</span> <span class="string">&lt;integer&gt;</span>         <span class="comment"># 请求周期</span></span><br><span class="line">      <span class="string">timeoutSeconds</span> <span class="string">&lt;integer&gt;</span>        <span class="comment"># 超时时长</span></span><br><span class="line">      <span class="string">successThreshold</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 成功阈值</span></span><br><span class="line">      <span class="string">failureThreshold</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 失败阈值</span></span><br></pre></td></tr></table></figure><p>探针之外的其他字段用于定义探测操作的行为方式，用户没有明确定义这些属性字段时，它们会使用各自的默认值:</p><ul><li>initialDelaySeconds <integer>：首次发出存活探测请求的延迟时长，即容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，即容器启动后便立刻进行探测；该参数值应该大于容器的最大初始化时长，以避免程序永远无法启动。</integer></li><li>timeoutSeconds <integer>：存活探测的超时时长，显示为timeout属性，默认为1秒，最小值也为1秒；应该为此参数设置一个合理值，以避免因应用负载较大时的响应延迟导致Pod被重启。</integer></li><li>periodSeconds <integer>：存活探测的频度，显示为period属性，默认为10秒，最小值为1秒；需要注意的是，过高的频率会给Pod对象带来较大的额外开销，而过低的频率又会使得对错误反应不及时。</integer></li><li>successThreshold <integer>：处于失败状态时，探测操作至少连续多少次的成功才被认为通过检测，显示为#success属性，仅可取值为1。</integer></li><li>failureThreshold：处于成功状态时，探测操作至少连续多少次的失败才被视为检测不通过，显示为#failure属性，默认值为3，最小值为1；尽量设置宽容一些的失败计数，能有效避免一些场景中的服务级联失败。<br>使用kubectl describe命令查看配置了存活性探测的Pod对象的详细信息时，其相关容器中会输出类似如下一行内容，它给出了探测方式及其额外的配置属性delay、timeout、period、success和failure及其各自的相关属性值。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Liveness:</span>  <span class="string">……</span> <span class="string">delay=0s</span> <span class="string">timeout=1s</span> <span class="string">period=10s</span> <span class="comment">#success=1 #failure=3</span></span><br></pre></td></tr></table></figure><h4 id="2-exec探针"><a href="#2-exec探针" class="headerlink" title="2 exec探针"></a>2 exec探针</h4><p>exec类型的探针通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，命令状态返回值为0表示“成功”通过检测，其余值均为“失败”状态。spec.containers.livenessProbe.exec字段只有一个可用属性command，用于指定要执行的命令。<br>demoapp应用程序通过/livez输出内置的存活状态检测接口，服务正常时，它以200响应码返回OK，否则为5xx响应码，我们可基于exec探针使用HTTP客户端向该path发起请求，并根据命令的结果状态来判定容器健康与否。系统刚启动时，对该路径的请求将会延迟大约不到5秒的时长，且默认响应值为OK。它还支持由用户按需向该路径发起POST请求，并向参数livez传值来自定义其响应内容。下面是定义在资源清单文件liveness-exec-demo.yaml中的示例。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-exec-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;[ &quot;$(curl -s 127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;</span>]</span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><p>该配置清单中定义的Pod对象为demo容器定义了exec探针，它通过在容器本地执行测试命令来比较curl -s 127.0.0.1/livez的返回值是否为OK以判定容器的存活状态。命令成功执行则表示容器正常运行，否则3次检测失败之后则将其判定为检测失败。首次检测在容器启动5秒之后进行，请求间隔也是5秒。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f liveness-exec-demo.yaml</span> </span><br><span class="line">pod/liveness-exec-demo created</span><br></pre></td></tr></table></figure><p>创建完成后，Pod中的容器demo会正常运行，存活检测探针也不会遇到检测错误而导致容器重启。若要测试存活状态检测的效果，可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> liveness-exec-demo -- curl -s -X POST -d <span class="string">&#x27;livez=FAIL&#x27;</span> 127.0.0.1/livez</span></span><br></pre></td></tr></table></figure><p>而后经过1个检测周期，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令结果中的事件可知，容器因健康状态检测失败而被重启。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/liveness-exec-demo</span></span><br><span class="line">……</span><br><span class="line">Events: </span><br><span class="line">Warning  Unhealthy  17s (x3 over 27s)  kubelet, k8s-node03.ilinux.io  Liveness probe failed:</span><br><span class="line">Normal   Killing     17s             kubelet, k8s-node03.ilinux.io  Container demo failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure><p>另外，下面输出信息中的Containers一段中还清晰显示了容器健康状态检测及状态变化的相关信息：容器当前处于Running状态，但前一次是为Terminated，原因是退出码为137的错误信息，它表示进程是被外部信号所终止。137事实上由两部分数字之和生成：128+signum，其中signum是导致进程终止的信号的数字标识，9表示SIGKILL，这意味着进程是被强行终止的。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Containers:</span></span><br><span class="line">  <span class="attr">demo:</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">    <span class="attr">State:</span>          <span class="string">Running</span></span><br><span class="line">      <span class="attr">Started:</span>      <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:30:02</span> <span class="string">+0800</span></span><br><span class="line">    <span class="attr">Last State:</span>     <span class="string">Terminated</span></span><br><span class="line">      <span class="attr">Reason:</span>       <span class="string">Error</span></span><br><span class="line">      <span class="attr">Exit Code:</span>    <span class="number">137</span></span><br><span class="line">      <span class="attr">Started:</span>      <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:22:20</span> <span class="string">+0800</span></span><br><span class="line">      <span class="attr">Finished:</span>     <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:30:02</span> <span class="string">+0800</span></span><br><span class="line">    <span class="attr">Ready:</span>          <span class="literal">True</span></span><br><span class="line">    <span class="attr">Restart Count:</span>  <span class="number">1</span></span><br><span class="line"><span class="string">……</span></span><br></pre></td></tr></table></figure><p>待容器重启完成后，/livez的响应内容会重置镜像中默认定义的OK，因而其存活状态检测不会再遇到错误，这模拟了一种典型的通过“重启”应用而解决问题的场景。需要特别说明的是，exec指定的命令运行在容器中，会消耗容器的可用计算资源配额，另外考虑到探测操作的效率等因素，探测操作的命令应该尽可能简单和轻量。</p><h4 id="3-HTTP探针"><a href="#3-HTTP探针" class="headerlink" title="3 HTTP探针"></a>3 HTTP探针</h4><p>HTTP探针是基于HTTP协议的探测（HTTPGetAction），通过向目标容器发起一个GET请求，并根据其响应码进行结果判定，2xx或3xx类的响应码表示检测通过。HTTP探针可用配置字段有如下几个。<br>▪host <string>：请求的主机地址，默认为Pod IP；也可以在httpHeaders使用“Host:”来定义。<br>▪port <string>：请求的端口，必选字段。<br>▪httpHeaders &lt;[]Object&gt;：自定义的请求报文头部。<br>▪path <string>：请求的HTTP资源路径，即URL path。<br>▪scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。<br>下面是一个定义在资源清单文件liveness-httpget-demo.yaml中的示例，它使用HTTP探针直接对/livez发起访问请求，并根据其响应码来判定检测结果。</string></string></string></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-httpget-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure><p>上面清单文件中定义的httpGet测试中，请求的资源路径为/livez，地址默认为Pod IP，端口使用了容器中定义的端口名称http，这也是明确为容器指明要暴露的端口的用途之一。下面测试其效果，首先创建此Pod对象：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f liveness-httpget-demo.yaml</span><br><span class="line">pod/liveness-httpget-demo created</span><br></pre></td></tr></table></figure><p>首次检测为延迟5秒，这刚好超过了demoapp的/livez接口默认会延迟响应的时长。镜像中定义的默认响应是以200状态码响应、以OK为响应结果，存活状态检测会成功完成。为了测试存活状态检测的效果，同样可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。<br>~$ kubectl exec liveness-httpget-demo – curl -s -X POST -d ‘livez=FAIL’ 127.0.0.1/livez<br>而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl describe pods/liveness-httpget-demo</span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  7s (x3 over 27s)  kubelet, k8s-node01.ilinux.io  Liveness probe failed: HTTP probe failed with statuscode: 520</span><br><span class="line">  Normal   Killing    7s          kubelet, k8s-node01.ilinux.io  Container demo failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure><p>一般来说，HTTP探针应该针对专用的URL路径进行。<br>这种检测方式仅对分层架构中的当前一层有效，例如，它能检测应用程序工作正常与否的状态，但重启操作却无法解决其后端服务（例如数据库或缓存服务）导致的故障。此时，容器可能会被反复重启，直到后端服务恢复正常。其他两种检测方式也存在类似的问题。</p><h4 id="4-TCP探针"><a href="#4-TCP探针" class="headerlink" title="4 TCP探针"></a>4 TCP探针</h4><p>TCP探针是基于TCP协议进行存活性探测（TCPSocketAction），通过向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。相比较来说，它比基于HTTP协议的探测要更高效、更节约资源，但精准度略低，毕竟连接建立成功未必意味着页面资源可用。<br>spec.containers.livenessProbe.tcpSocket字段用于定义此类检测，它主要有以下两个可用字段：<br>1）host <string>：请求连接的目标IP地址，默认为Pod自身的IP；<br>2）port <string>：请求连接的目标端口，必选字段，可以名称调用容器上显式定义的端口。<br>下面是一个定义在资源清单文件liveness-tcpsocket-demo.yaml中的示例，它向Pod对象的TCP协议的80端口发起连接请求，并根据连接建立的状态判定测试结果。为了能在容器中通过iptables阻止接收对80端口的请求以验证TCP检测失败，下面的配置还在容器上启用了特殊的内核权限NET_ADMIN。</string></string></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-tcpsocket-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">20</span></span><br></pre></td></tr></table></figure><p>按照配置，将该清单中的Pod对象创建在集群之上，20秒之后即会进行首次的tcpSocket检测。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f liveness-tcpsocket-demo.yaml</span>                                      </span><br><span class="line">pod/liveness-tcpsocket-demo created</span><br></pre></td></tr></table></figure><p>容器应用demoapp启动后即监听于TCP协议的80端口，tcpSocket检测也就可以成功执行。为了测试效果，可使用下面的命令在Pod的Network名称空间中设置iptables规则以阻止对80端口的请求：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> liveness-tcpsocket-demo -- iptables -A INPUT -p tcp --dport 80 -j REJECT</span></span><br></pre></td></tr></table></figure><p>而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl describe pods/liveness-httpget-demo</span><br><span class="line">……</span><br><span class="line">Events:</span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  3s (x3 over 23s)  kubelet, k8s-node03.ilinux.io  Liveness probe failed: dial tcp 10.244.3.19:80: i/o timeout</span><br><span class="line">    Normal   Killing    3s      kubelet, k8s-node03.ilinux.io  Container demo     failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure><p>不过，重启容器并不会导致Pod资源的重建操作，网络名称空间的设定附加在pause容器之上，因而添加的iptables规则在应用重启后依然存在，它是一个无法通过重启而解决的问题。若需要手消除该问题，删除添加至Pod中的iptables规则即可。</p><h3 id="4-5-3-Pod的重启策略"><a href="#4-5-3-Pod的重启策略" class="headerlink" title="4.5.3 Pod的重启策略"></a>4.5.3 Pod的重启策略</h3><p>Pod对象的应用容器因程序崩溃、启动状态检测失败、存活状态检测失败或容器申请超出限制的资源等原因都可能导致其被终止，此时是否应该重启则取决于Pod上的restartPolicy（重启策略）字段的定义，该字段支持以下取值。<br>1）Always：无论因何原因、以何种方式终止，kubelet都将重启该Pod，此为默认设定。<br>2）OnFailure：仅在Pod对象以非0方式退出时才将其重启。<br>3）Never：不再重启该Pod。<br>需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一个节点上重新启动Pod对象的相关容器。首次需要重启的容器，其重启操作会立即进行，而再次重启操作将由kubelet延迟一段时间后进行，反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。<br>事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么被终止，直到节点故障、被删除或被驱逐。</p><h4 id="4-5-4-容器就绪状态检测"><a href="#4-5-4-容器就绪状态检测" class="headerlink" title="4.5.4 容器就绪状态检测"></a>4.5.4 容器就绪状态检测</h4><p>Pod对象启动后，应用程序通常需要一段时间完成其初始化过程，例如加载配置或数据、缓存初始化等，甚至有些程序需要运行某类预热过程等，因此通常应该避免在Pod对象启动后立即让其处理客户端请求，而是需要等待容器初始化工作执行完成并转为“就绪”状态，尤其是存在其他提供相同服务的Pod对象的场景更是如此。<br>与存活探针不同的是，就绪状态检测是用来判断容器应用就绪与否的周期性（默认周期为10秒钟）操作，它用于检测容器是否已经初始化完成并可服务客户端请求。与存活探针触发的操作不同，检测失败时，就绪探针不会杀死或重启容器来确保其健康状态，而仅仅是通知其尚未就绪，并触发依赖其就绪状态的其他操作（例如从Service对象中移除此Pod对象），以确保不会有客户端请求接入此Pod对象。<br>就绪探针也支持Exec、HTTP GET和TCP Socket这3种探测方式，且它们各自的定义机制与存活探针相同。因而，将容器定义中的livenessProbe字段名替换为readinessProbe，并略做适应性修改即可定义出就绪性检测的配置来，甚至有些场景中的就绪探针与存活探针的配置可以完全相同。<br>demoapp应用程序通过/readyz暴露了专用于就绪状态检测的接口，它于程序启动约15秒后能够以200状态码响应、以OK为响应结果，也支持用户使用POST请求方法通过readyz参数传递自定义的响应内容，不过所有非OK的响应内容都被响应以5xx的状态码。一个简单的示例如下面的配置清单（readiness-httpget-demo.yaml）所示。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">readiness-httpget-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure><p>下面来测试该Pod就绪探针的作用。按照配置，将Pod对象创建在集群上约15秒后启动首次探测，在该探测结果成功返回之前，Pod将一直处于未就绪状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f readiness-httpget-demo.yaml</span><br><span class="line">pod/readiness-httpget-demo created</span><br></pre></td></tr></table></figure><p>接着运行kubectl get -w命令监视其资源变动信息，由如下命令结果可知，尽管Pod对象处于Running状态，但直到就绪检测命令执行成功后Pod资源才转为“就绪”。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/readiness-httpget-demo -w</span></span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">readiness-httpget-demo   0/1     Running   0          10s</span><br><span class="line">readiness-httpget-demo   1/1     Running   0          20s</span><br></pre></td></tr></table></figure><p>Pod运行过程中的某一时刻，无论因何原因导致的就绪状态检测的连续失败都会使得该Pod从就绪状态转变为“未就绪”，并且会从各个通过标签选择器关联至该Pod对象的Service后端端点列表中删除。为了测试就绪状态检测效果，下面修改/readyz响应以非OK内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> readiness-httpget-demo -- curl -s -XPOST -d <span class="string">&#x27;readyz=FAIL&#x27;</span> 127.0.0.1/readyz</span></span><br></pre></td></tr></table></figure><p>而后在至少1个检测周期之后，通过该Pod的描述信息可以看到就绪检测失败相关的事件描述，命令及结果如下所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/readiness-httpget-demo</span></span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  4s (x11 over 54s)  kubelet, k8s-node03.ilinux.io  Readiness probe failed: HTTP probe failed with statuscode: 521</span><br></pre></td></tr></table></figure><p>未定义就绪性检测的Pod对象在进入Running状态后将立即“就绪”，这在容器需要时间进行初始化的场景中可能会导致客户请求失败。因此，生产实践中，必须为关键性Pod资源中的容器定义就绪探针。</p><h3 id="4-5-5-容器生命周期"><a href="#4-5-5-容器生命周期" class="headerlink" title="4.5.5 容器生命周期"></a>4.5.5 容器生命周期</h3><p>容器生命周期接口工作示意图:</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123090651488.png" alt="../img/image-20220123090651488"></p><p>容器需要处理来自平台的最重要事件是SIGTERM信号，任何需要“干净”关闭进程的应用程序都需要捕捉该信号进行必要处理，例如释放文件锁、关闭数据库连接和网络连接等，而后尽快终止进程，以避免宽限期过后强制关闭信号SIGKILL的介入。SIGKILL信号是由底层操作系统接收的，而非应用进程，一旦检测到该信号，内核将停止为相应进程提供内核资源，并终止进程正在使用的所有CPU线程，类似于直接切断了进程的电源。<br>但是，容器应用很可能是功能复杂的分布式应用程序的一个组件，仅依赖信号终止进程很可能不足以完成所有的必要操作。因此，容器还需要支持postStart和preStop事件，前者常用于为程序启动前进行预热，后者则一般在“干净”地关闭应用之前释放占用的资源。<br>生命周期钩子函数lifecycle hook是编程语言（例如Angular）中常用的生命周期管理组件，它实现了程序运行周期中的关键时刻的可见性，并赋予用户为此采取某种行动的能力。类似地，容器生命周期钩子使它能够感知自身生命周期管理中的事件，并在相应时刻到来时运行由用户指定的处理程序代码。Kubernetes同样为容器提供了postStart和preStop两种生命周期钩子。</p><ul><li>postStart：在容器创建完成后立即运行的钩子句柄（handler），该钩子定义的事件执行完成后容器才能真正完成启动过程，如图4-15中的左图所示；不过Kubernetes无法确保它一定会在容器的主应用程序（由ENTRYPOINT定义）之前运行。</li><li>preStop：在容器终止操作执行之前立即运行的钩子句柄，它以同步方式调用，因此在其完成之前会阻塞删除容器的操作；这意味着该钩子定义的事件成功执行并退出，容器终止操作才能真正完成，如图4-15中的右图所示。</li></ul><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123090725537.png" alt="../img/image-20220123090725537"></p><p>钩子句柄的实现方式类似于容器探针句柄的类型，同样有exec、httpGet和tcpSocket这3种，它们各自的配置格式和工作逻辑也完全相同，exec在容器中执行用户定义的一个或多个命令，httpGet在容器中向指定的本地URL发起HTTP连接请求，而tcpSocket则试图与指定的端口建立TCP连接。<br>postStart和preStop句柄定义在容器的lifecycle字段中，其内部一次仅支持嵌套使用一种句柄类型。下面的配置清单（lifecycle-demo.yaml）示例中同时使用了postStart和preStop钩子处理相应的事件。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lifecycle-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp </span></span><br><span class="line"><span class="string">          --dport 8080 -j REDIRECT --to-ports 80&#x27;</span>]</span><br><span class="line">      <span class="attr">preStop:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;while killall python3; do sleep 1; done&#x27;</span>]</span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure><p>示例中的demo容器通过postStart执行iptables命令设置端口重定向规则，将发往该Pod IP的8080端口的所有请求重定向至80端口，从而让容器应用能够同时从8080端口接收请求。demo容器又借助preStop执行killall命令，它假设该命令能够更优雅地终止基于Python3运行的容器应用demoapp。将清单中的Pod对象创建于集群中便可展开后续的测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f lifecycle-demo.yaml</span> </span><br><span class="line">pod/lifecycle-demo created</span><br></pre></td></tr></table></figure><p>而后可获取容器内网络名称空间中PREROUTING链上的iptables规则，验证postStart钩子事件的执行结果：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> lifecycle-demo -- iptables -t nat -nL PREROUTING</span>  </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt     source    destination         </span><br><span class="line">REDIRECT   tcp  --  0.0.0.0/0    0.0.0.0/0    tcp dpt:8080 redir ports 80</span><br></pre></td></tr></table></figure><p>上面的配置清单中有意同时添加了httpGet类型的存活探针，我们可以人为地将探针检测结果置为失败状态，以促使kubelet重启demo容器验证preStop钩子事件的执行。不过，该示例中给出的操作是终止容器应用，那么容器成功重启即验证了相应脚本的运行完成。</p><h2 id="4-6-多容器Pod"><a href="#4-6-多容器Pod" class="headerlink" title="4.6 多容器Pod"></a>4.6 多容器Pod</h2><p>容器设计模式中的单节点多容器模型中，初始化容器和Sidecar容器是目前使用较多的模式，尤其是服务网格的发展极大促进了Sidecar容器的应用。</p><h3 id="4-6-1-初始化容器"><a href="#4-6-1-初始化容器" class="headerlink" title="4.6.1 初始化容器"></a>4.6.1 初始化容器</h3><p>初始化代码要首先运行，且只能运行一次，它们常用于验证前提条件、基于默认值或传入的参数初始化对象实例的字段等。Pod中的初始化容器（Init Container）功能与此类似，它们为那些有先决条件的应用容器完成必要的初始设置，例如设置特殊权限、生成必要的iptables规则、设置数据库模式，以及获取最新的必要数据等。<br>有很多场景都需要在应用容器启动之前进行部分初始化操作，如等待其他关联组件服务可用、基于环境变量或配置模板为应用程序生成配置文件、从配置中心获取配置等。初始化容器的典型应用需求有如下几种。</p><ul><li>用于运行需要管理权限的工具程序，例如iptables命令等，出于安全等方面的原因，应用容器不适合拥有运行这类程序的权限。</li><li>提供主容器镜像中不具备的工具程序或自定义代码。</li><li>为容器镜像的构建和部署人员提供了分离、独立工作的途径，部署人员使用专用的初始化容器完成特殊的部署逻辑，从而使得他们不必协同起来制作单个镜像文件。</li><li>初始化容器和应用容器处于不同的文件系统视图中，因此可分别安全地使用敏感数据，例如Secrets资源等。</li><li>初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得以满足。</li></ul><p>Pod对象中的所有初始化容器必须按定义的顺序串行运行，直到它们全部成功结束才能启动应用容器，因而初始化容器通常很小，以便它们能够以轻量的方式快速运行。某初始化容器运行失败将会导致整个Pod重新启动（重启策略为Never时例外），初始化容器也必将再次运行，因此需要确保所有初始化容器的操作具有幂等性，以避免无法预知的副作用。<br>Pod资源的spec.initContainers字段以列表形式定义可用的初始化容器，其嵌套可用字段类似于spec.containers。下面的资源清单（init-container-demo.yaml）在Pod对象上定义了一个名为iptables-init的初始化容器示例。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">init-container-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span>   <span class="comment"># 定义初始化容器</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">iptables-init</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/admin-box:latest</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT </span></span><br><span class="line"><span class="string">    --to-port 80&#x27;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>示例中，应用容器demo默认监听TCP协议的80端口，但我们又期望该Pod能够在TCP协议的8080端口通过端口重定向方式为客户端提供服务，因此需要在其网络名称空间中添加一条相应的iptables规则。但是，添加该规则的iptables命令依赖于内核中的网络管理权限，出于安全原因，我们并不期望应用容器拥有该权限，因而使用了拥有网络管理权限的初始化容器来完成此功能。下面先把配置清单中定义的资源创建于集群之上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f init-container-demo.yaml</span> </span><br><span class="line">pod/init-container-demo created</span><br></pre></td></tr></table></figure><p>随后，在Pod对象init-container-demo的描述信息中的初始化容器信息段可以看到如下内容，它表明初始化容器启动后大约1秒内执行完成返回0状态码并成功退出。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Command:</span></span><br><span class="line">  <span class="string">/bin/sh</span></span><br><span class="line">  <span class="string">-c</span></span><br><span class="line"><span class="attr">Args:</span></span><br><span class="line">  <span class="string">iptables</span> <span class="string">-t</span> <span class="string">nat</span> <span class="string">-A</span> <span class="string">PREROUTING</span> <span class="string">-p</span> <span class="string">tcp</span> <span class="string">--dport</span> <span class="number">8080</span> <span class="string">-j</span> <span class="string">REDIRECT</span> <span class="string">--to-port</span> <span class="number">80</span></span><br><span class="line"><span class="attr">State:</span>          <span class="string">Terminated</span></span><br><span class="line">  <span class="attr">Reason:</span>       <span class="string">Completed</span></span><br><span class="line">  <span class="attr">Exit Code:</span>    <span class="number">0</span></span><br><span class="line">  <span class="attr">Started:</span>      <span class="string">Sun,</span> <span class="number">30</span> <span class="string">Aug</span> <span class="number">2020 11:44:28</span> <span class="string">+0800</span></span><br><span class="line">  <span class="attr">Finished:</span>     <span class="string">Sun,</span> <span class="number">30</span> <span class="string">Aug</span> <span class="number">2020 11:44:29</span> <span class="string">+0800</span></span><br><span class="line"><span class="attr">Ready:</span>          <span class="literal">True</span></span><br><span class="line"><span class="attr">Restart Count:</span>  <span class="number">0</span></span><br></pre></td></tr></table></figure><p>这表明，向Pod网络名称空间中添加iptables规则的操作已经完成，我们可通过应用容器来请求查看这些规则，但因缺少网络管理权限，该查看请求会被拒绝：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> init-container-demo -- iptables -t nat -vnL</span></span><br><span class="line">iptables v1.8.3 (legacy): can&#x27;t initialize iptables table `nat&#x27;: Permission denied (you must be root)</span><br><span class="line">Perhaps iptables or your kernel needs to be upgraded.</span><br><span class="line">command terminated with exit code 3</span><br></pre></td></tr></table></figure><p>另一方面，应用容器中的服务却可以正常通过Pod IP的8080端口接收并响应，如下面的命令及执行结果所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/init-container-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl http://<span class="variable">$&#123;podIP&#125;</span>:8080</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: init-container-demo, …</span><br></pre></td></tr></table></figure><p>由此可见，初始化容器及容器的postStop钩子都能完成特定的初始化操作，但postStop必须在应用容器内部完成，它依赖的条件（例如管理权限）也必须为应用容器所有，这无疑会为应用容器引入安全等方面的风险。另外，考虑到应用容器镜像内部未必存在执行初始化操作的命令或程序库，使用初始化容器也就成了不二之选。</p><h3 id="4-6-2-Sidecar容器"><a href="#4-6-2-Sidecar容器" class="headerlink" title="4.6.2 Sidecar容器"></a>4.6.2 Sidecar容器</h3><p>Sidecar容器是Pod中与主容器松散耦合的实用程序容器，遵循容器设计模式，并以单独容器进程运行，负责运行应用的非核心功能，以扩展、增强主容器。Sidecar模式最著名的用例是充当服务网格中的微服务的代理应用（例如Istio中的数据控制平面Envoy），其他典型使用场景包括日志传送器、监视代理和数据加载器等。<br>下面的配置清单（sidecar-container-demo.yaml）中定义了两个容器：一个是运行demoapp的主容器demo，一个运行envoy代理的Sidecar容器proxy。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sidecar-container-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /etc/envoy/envoy.yaml https://</span></span><br><span class="line"><span class="string">          raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/</span></span><br><span class="line"><span class="string">          master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br></pre></td></tr></table></figure><p>Envoy程序是服务网格领域著名的数据平面实现，它在Istio服务网格中以Sidecar的模式同每一个微服务应用程序单独组成一个Pod，负责代理该微服务应用的所有通信事件，并为其提供限流、熔断、超时、重试等多种高级功能。这里我们将demoapp视作一个微服务应用，配置Envoy为其代理并调度入站（Ingress）流量，因而在示例中demo容器基于环境变量被配置为监听127.0.0.1地址上一个特定的8080端口，而proxy容器将监听Pod所有IP地址上的80端口，以接收客户端请求。proxy容器上的postStart事件用于为Envoy代理下载一个适用的配置文件，以便将proxy接收到的所有请求均代理至demo容器。<br>下面说明整个测试过程。先将配置清单中定义的对象创建到集群之上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f sidecar-container-demo.yaml</span> </span><br><span class="line">pod/sidecar-container-demo created</span><br></pre></td></tr></table></figure><p>随后，等待Pod中的两个容器成功启动且都转为就绪状态，可通过各Pod内端口监听的状态来确认服务已然正常运行。下面命令的结果表示，Envoy已经正常运行并监听了TCP协议的80端口和9901端口（Envoy的内置管理接口）。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> sidecar-container-demo -c proxy -- netstat -tnlp</span>    </span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address    Foreign Address   State    PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:9901       0.0.0.0:*       LISTEN      1/envoy</span><br><span class="line">tcp        0      0 0.0.0.0:80         0.0.0.0:*       LISTEN      1/envoy</span><br><span class="line">tcp        0      0 127.0.0.1:8080     0.0.0.0:*       LISTEN      -</span><br></pre></td></tr></table></figure><p>接下来，我们向Pod的80端口发起HTTP请求，若它能以demoapp的页面响应，则表示代理已然成功运行，甚至可以根据响应头部来判断其是否有代理服务Envoy发来的代理响应，如下面的命令及结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/sidecar-container-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">curl http://<span class="variable">$podIP</span></span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 127.0.0.1, ServerName: sidecar-container-demo, ……</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -I http://<span class="variable">$podIP</span></span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">content-type: text/html; charset=utf-8</span><br><span class="line">content-length: 108</span><br><span class="line">server: envoy</span><br><span class="line">date: Sun, 22 May 2020 06:43:04 GMT</span><br><span class="line">x-envoy-upstream-service-time: 3</span><br></pre></td></tr></table></figure><p>虽然Sidecar容器可以称得上是Pod中的常规容器，但直到v1.18版本，Kubernetes才将其添加作为内置功能。在此之前，Pod中的各应用程序彼此间没有区别，用户无从预测和控制容器的启动及关闭顺序，但多数场景都要求Sidecar容器必须要先于普通应用容器启动以做一些准备工作，例如分发证书、创建存储卷或获取一些数据等，且它们需要晚于其他应用容器终止。Kubernetes从v1.18版本开始支持用户在生命周期字段中将容器标记为Sidecar，这类容器全部转为就绪状态后，普通应用容器方可启动。因而，这个新特性根据生命周期将Pod的容器重新划分成了初始化容器、Sidecar容器和应用容器3类。<br>所有的Sidecar容器都是应用容器，唯一不同之处是，需要手动为Sidecar容器在lifecycle字段中嵌套定义type类型的值为Sidecar。配置格式如下所示：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Sidecar</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">……</span></span><br></pre></td></tr></table></figure><p>另外，可能也有一些场景需要Sidecar容器启动晚于普通应用容器，这种特殊的应用需求，目前可通过OpernKruise项目中的SidecarSet提供的PostSidecar模型来解决。将来，该项目或许支持以DAG的方式来灵活编排容器的启动顺序。</p><h2 id="4-7-资源需求与资源限制"><a href="#4-7-资源需求与资源限制" class="headerlink" title="4.7 资源需求与资源限制"></a>4.7 资源需求与资源限制</h2><h3 id="4-7-1-资源需求与限制"><a href="#4-7-1-资源需求与限制" class="headerlink" title="4.7.1 资源需求与限制"></a>4.7.1 资源需求与限制</h3><p>在Kubernetes上，可由容器或Pod请求与消费的“资源”主要是指CPU和内存（RAM），它可统称为计算资源，另一种资源是事关可用存储卷空间的存储资源。<br>相比较而言，CPU属于可压缩型资源，即资源额度可按需弹性变化，而内存（当前）则是不可压缩型资源，CPU和内存资源的配置主要在Pod对象中的容器上进行，并且每个资源存在如图4-16所示的需求和限制两种类型。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123091411880.png" alt="../img/image-20220123091411880"></p><ul><li>资源需求：定义需要系统预留给该容器使用的资源最小可用值，容器运行时可能用不到这些额度的资源，但用到时必须确保有相应数量的资源可用。</li><li>资源限制：定义该容器可以申请使用的资源最大可用值，超出该额度的资源使用请求将被拒绝；显然，该限制需要大于等于requests的值，但系统在某项资源紧张时，会从容器回收超出request值的那部分。</li></ul><p>在Kubernetes系统上，1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1 core）相当于1000个微核心（millicores，以下简称为m），因此500m相当于是0.5个核心，即1/2个核心。内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。</p><h3 id="4-7-2-容器资源需求"><a href="#4-7-2-容器资源需求" class="headerlink" title="4.7.2 容器资源需求"></a>4.7.2 容器资源需求</h3><p>下面的配置清单示例（resource-requests-demo.yaml）中的自主式Pod要求为stress容器确保128MiB的内存及1/5个CPU核心（200m）资源可用。Pod运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时stress容器也会尽可能多地占用CPU资源，另外再启动一个专用的CPU压力测试进程（-c 1）。stress-ng是一个多功能系统压力测试具，master/worker模型，master为主进程，负载生成和控制子进程，worker是负责执行各类特定测试的子进程，例如测试CPU的子进程，以及测试RAM的子进程等。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">stress</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/stress-ng</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/usr/bin/stress-ng&quot;</span>, <span class="string">&quot;-m 1&quot;</span>, <span class="string">&quot;-c 1&quot;</span>, <span class="string">&quot;-metrics-brief&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;200m&quot;</span></span><br></pre></td></tr></table></figure><p>上面的配置清单中，stress容器请求使用的CPU资源大小为200m，这意味着一个CPU核心足以确保其以期望的最快方式运行。另外，配置清单中期望使用的内存大小为128MiB，不过其运行时未必真的会用到这么多。考虑到内存为非压缩型资源，当超出时存在因OOM被杀死的可能性，于是请求值是其理想中使用的内存空间上限。<br>接下来创建并运行此Pod对象以对其资源限制效果进行检查。因为显示结果涉及资源占用比例等，因此同样的测试配置对不同的系统环境来说，其结果也会有所不同，作者为测试资源需求和资源限制功能而使用的系统环境中，每个节点的可用CPU核心数为8，物理内存空间为16GB。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create -f resource-requests-demo.yaml</span></span><br></pre></td></tr></table></figure><p>而后在Pod资源的容器内运行top命令，观察CPU及内存资源占用状态，如下所示。其中{stress-ng-vm}是执行内存压测的子进程，它默认使用256MB的内存空间，{stress-ng-cpu}是执行CPU压测的专用子进程。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- top</span></span><br><span class="line">Mem: 2884676K used, 13531796K free, 27700K shrd, 2108K buff, 1701456K cached</span><br><span class="line">CPU:  25% usr   0% sys   0% nic  74% idle   0% io   0% irq   0% sirq</span><br><span class="line">Load average: 0.57 0.60 0.71 3/435 15</span><br><span class="line">PID  PPID USER  STAT   VSZ %VSZ CPU %CPU COMMAND</span><br><span class="line">9     8  root     R     262m   2%   6  13% &#123;stress-ng-vm&#125; /usr/bin/stress-ng</span><br><span class="line">7     1  root     R     6888   0%   3  13% &#123;stress-ng-cpu&#125; /usr/bin/stress-ng</span><br><span class="line">1     0  root     S     6244   0%   1   0% /usr/bin/stress-ng -c 1 -m 1 --met</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>top命令的输出结果显示，每个测试进程的CPU占用率为13%（实际12.5%），{stress-ng-vm}的内存占用量为262MB（VSZ），此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用范围内尽量多地占用相关的资源。两个测试线程分布于两个CPU核心，以满载的方式运行，系统共有8个核心，因此其使用率为25%（2/8）。另外，节点上的内存资源充裕，所以，尽管容器的内存用量远超128MB，但它依然可以运行。一旦资源紧张时，节点仅保证该容器有1/5个CPU核心（其需求中的定义）可用。在有着8个核心的节点上来说，它的占用率为2.5%，于是每个进程占比为1.25%，多占用的资源会被压缩。内存为非可压缩型资源，该Pod对象在内存资源紧张时可能会因OOM被杀死。<br>对于压缩型的资源CPU来说，若未定义容器的资源请求用量，以确保其最小可用资源量，该Pod占用的CPU资源可能会被其他Pod对象压缩至极低的水平，甚至到该Pod对象无法被调度运行的境地。而对于非压缩型内存资源来说，资源紧缺情形下可能导致相关的容器进程被杀死。因此，在Kubernetes系统上运行关键型业务相关的Pod时，必须要使用requests属性为容器明确定义资源需求。当然，我们也可以为Pod对象定义较高的优先级来改变这种局面。<br>集群中的每个节点都拥有定量的CPU和内存资源，调度器将Pod绑定至节点时，仅计算资源余量可满足该Pod对象需求量的节点才能作为该Pod运行的可用目标节点。也就是说，Kubernetes的调度器会根据容器的requests属性定义的资源需求量来判定哪些节点可接收并运行相关的Pod对象，而对于一个节点的资源来说，每运行一个Pod对象，该Pod对象上所有容器requests属性定义的请求量都要给予预留，直到节点资源被绑定的所有Pod对象瓜分完毕为止。</p><h3 id="4-7-3-容器资源限制"><a href="#4-7-3-容器资源限制" class="headerlink" title="4.7.3 容器资源限制"></a>4.7.3 容器资源限制</h3><p>一旦定义资源限制，分配资源时，可压缩型资源CPU的控制阀可自由调节，容器进程也就无法获得超出其CPU配额的可用值。但是，若进程申请使用超出limits属性定义的内存资源时，该进程将可能被杀死。不过，该进程随后仍可能会被其控制进程重启，例如，当Pod对象的重启策略为Always或OnFailure时，或者容器进程存在有监视和管理功能的父进程等。<br>下面的配置清单文件（resource-limits-demo.yaml）中定义使用simmemleak镜像运行一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而被杀死。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memleak-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: memleak</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: simmemleak</span><br><span class="line">    ../img/image: ikubernetes/simmemleak</span><br><span class="line">    ../img/imagePullPolicy: IfNotPresent</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;1&quot;</span><br></pre></td></tr></table></figure><p>下面将配置清单中定义的Pod对象创建到集群中，测试资源限制的实施效果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f resource-limits-demo.yaml</span></span><br><span class="line">pod/memleak-pod created</span><br></pre></td></tr></table></figure><p>Pod资源的默认重启策略为Always，于是在simmemleak容器因内存资源达到硬限制而被终止后会立即重启，因此用户很难观察到其因OOM而被杀死的相关信息。不过，多次因内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制（退避算法），即每次重启的时间间隔会不断地拉长，因而用户看到Pod对象的相关状态通常为CrashLoopBackOff。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=memleak</span> </span><br><span class="line">NAME      READY     STATUS      RESTARTS   AGE</span><br><span class="line">memleak-pod   0/1       CrashLoopBackOff   1         24s</span><br></pre></td></tr></table></figure><p>Pod对象的重启策略在4.5.3节介绍过，这里不再赘述。我们可通过Pod对象的详细描述了解其相关状态，例如下面的命令及部分结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~]$ </span><span class="language-bash">kubectl describe pods memleak-pod</span></span><br><span class="line">Name:         memleak-pod</span><br><span class="line">……</span><br><span class="line">Last State:     Terminated</span><br><span class="line">      Reason:       OOMKilled</span><br><span class="line">      Exit Code:    137</span><br><span class="line">      Started:      Mon, 31 Aug 2020 12:42:50 +0800</span><br><span class="line">      Finished:     Mon, 31 Aug 2020 12:42:50 +0800</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  3</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>上面的命令结果中，OOMKilled表示容器因内存耗尽而被终止，因此为limits属性中的memory设置一个合理值至关重要。与资源需求不同的是，资源限制并不影响Pod对象的调度结果，即一个节点上的所有Pod对象的资源限制数量之和可以大于节点拥有的资源量，即支持资源的过载使用（overcommitted）。不过，这么一来，一旦内存资源耗尽，几乎必然地会有容器因OOMKilled而终止。<br>另外需要说明的是，Kubernetes仅会确保Pod对象获得它们请求的CPU时间额度，它们能否取得额外（throttled）的CPU时间，则取决于其他正在运行作业的CPU资源占用情况。例如对于总数为1000m的CPU资源来说，容器A请求使用200m，容器B请求使用500m，在不超出它们各自最大限额的前下，则余下的300m在双方都需要时会以2 : 5（200m : 500m）的方式进行配置。</p><h3 id="4-7-4-容器可见资源"><a href="#4-7-4-容器可见资源" class="headerlink" title="4.7.4 容器可见资源"></a>4.7.4 容器可见资源</h3><p>在容器中运行top等命令观察资源可用量信息时，容器可用资源受限于requests和limits属性中的定义，但容器中可见的资源量依然是节点级别的可用总量。例如，为前面定义的stress-pod添加如下limits属性定义。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">limits:</span></span><br><span class="line">   <span class="attr">memory:</span> <span class="string">&quot;512Mi&quot;</span></span><br><span class="line">   <span class="attr">cpu:</span> <span class="string">&quot;400m&quot;</span></span><br></pre></td></tr></table></figure><p>重新创建stress-pod对象，并在其容器内分别列出容器可见的内存和CPU资源总量，命令及结果如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- <span class="built_in">cat</span> /proc/meminfo | grep ^MemTotal</span></span><br><span class="line">MemTotal:       16416472 kB</span><br><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- <span class="built_in">cat</span> /proc/cpuinfo | grep -c ^processor</span></span><br><span class="line">8</span><br></pre></td></tr></table></figure><p>命令结果中显示其可用内存资源总量为16416472 kB（16GB），CPU核心数为8个，这是节点级的资源数量，而非由容器的limits属性所定义的512MiB和400m。<br>较为典型的是在Pod中运行Java应用程序时，若未使用-Xmx选项指定JVM的堆内存可用总量，则会默认设置为主机内存总量的一个空间比例（例如30%），这会导致容器中的应用程序申请内存资源时很快达到上限，而转为OOMKilled状态。另外，即便使用了-Xmx选项设置其堆内存上限，但该设置对非堆内存的可用空间不产生任何限制作用，仍然存在达到容器内存资源上限的可能性。<br>另一个典型代表是在Pod中运行Nginx应用时，其配置参数worker_processes的值设置为auto，则会创建与可见CPU核心数量等同的worker进程数，若容器的CPU可用资源量远小于节点所需资源量时，这种设置在较大的访问负荷下会产生严重的资源竞争，并且会带来更多的内存资源消耗。一种较为妥当的解决方案是使用Downward API将limits定义的资源量暴露给容器，这将在后面的章节中予以介绍。</p><h3 id="4-7-5-Pod服务质量类别"><a href="#4-7-5-Pod服务质量类别" class="headerlink" title="4.7.5 Pod服务质量类别"></a>4.7.5 Pod服务质量类别</h3><p>前面曾提到，Kubernetes允许节点的Pod对象过载使用资源，这意味着节点无法同时满足绑定其上的所有Pod对象以资源满载的方式运行。因而在内存资源紧缺的情况下，应该以何种次序终止哪些Pod对象就变成了问题。事实上，Kubernetes无法自行对此做出决策，它需要借助于Pod对象的服务质量和优先级等完成判定。根据Pod对象的requests和limits属性，Kubernetes把Pod对象归类到BestEffort、Burstable和Guaranteed这3个服务质量类别（Quality of Service，QoS）类别下。</p><ul><li>Guaranteed：Pod对象为其每个容器都设置了CPU资源需求和资源限制，且二者具有相同值；同时为每个容器都设置了内存资需求和内存限制，且二者具有相同值。这类Pod对象具有最高级别服务质量。</li><li>Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别的设定要求，这类Pod对象具有中等级别服务质量。</li><li>BestEffort：不为任何一个容器设置requests或limits属性，这类Pod对象可获得的服务质量为最低级别。<br>一旦内存资源紧缺，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够做到尽可能多地占用资源。若此时系统上已然不存任何BestEffort类别的容器，则接下来将轮到Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod对象存在。</li></ul><p>每个运行状态的容器都有其OOM评分，评分越高越优先被杀死。OOM评分主要根据两个维度进行计算：由服务质量类别继承而来的默认分值，以及容器的可用内存资源比例，而同等类别的Pod对象的默认分值相同。下面的代码片段取自pkg/kubelet/qos/policy.go源码文件，它们定义的是各种类别的Pod对象的OOM调节（Adjust）分值，即默认分值。其中，Guaranteed类别Pod资源的Adjust分值为–998，而BestEffort类别的默认分值为1000，Burstable类别的Pod资源的Adjust分值经由相应的算法计算得出。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const (</span><br><span class="line">    PodInfraOOMAdj        int = -998</span><br><span class="line">    KubeletOOMScoreAdj    int = -999</span><br><span class="line">    DockerOOMScoreAdj     int = -999</span><br><span class="line">    KubeProxyOOMScoreAdj  int = -999</span><br><span class="line">    guaranteedOOMScoreAdj int = -998</span><br><span class="line">    besteffortOOMScoreAdj int = 1000</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>因此，同等级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将先被杀死。例如，图4-17中的同属于Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例为95%，要大于Pod B的80%</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123092311772.png" alt="../img/image-20220123092311772"></p><p>需要特别说明的是，OOM是内存耗尽时的处理机制，与可压缩型资源CPU无关，因此CPU资源的需求无法得到保证时，Pod对象仅仅是暂时获取不到相应的资源来运行而已。</p><h2 id="4-8-综合应用案例"><a href="#4-8-综合应用案例" class="headerlink" title="4.8 综合应用案例"></a>4.8 综合应用案例</h2><p>下面的配置清单（all-in-one.yaml）中定义的Pod对象all-in-one将前面的用到的大多数配置整合在一起：它有一个初始化容器和两个应用容器，其中sidecar-proxy为Sidecar容器，负责为主容器demo代理服务客户端请求。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">all-in-one</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">iptables-init</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/admin-box:latest</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT </span></span><br><span class="line"><span class="string">    --to-port 80&#x27;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sidecar-proxy</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /etc/envoy/envoy.yaml https://</span></span><br><span class="line"><span class="string">          raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_</span></span><br><span class="line"><span class="string">          Practical_2rd/master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&#x27;8080&#x27;</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="number">0.5</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="number">2</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;1024Mi&quot;</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">supplementalGroups:</span> [<span class="number">1002</span>, <span class="number">1003</span>]</span><br><span class="line">    <span class="attr">fsGroup:</span> <span class="number">2000</span></span><br></pre></td></tr></table></figure><p>配置清单的Pod对象的各容器中，主容器demo在Pod的IP地址上监听TCP协议的8080端口，以接收并响应HTTP请求；Sidecar容器sidecar-proxy监听TCP协议的80端口，接收HTTP请求并将其代理至demo容器的8080端口；初始化容器在Pod的Network名称空间中添加了一条iptables重定向规则，该规则负责把所有发往Pod IP上8080端口的请求重定向至80端口，因而demo容器仅能从127.0.0.1的8080端口接收到请求。读者朋友可将清单中的Pod对象创建到集群上，并逐一测试其各项配置的效果。</p><h1 id="4-标签与标签选择器"><a href="#4-标签与标签选择器" class="headerlink" title="4 标签与标签选择器"></a>4 标签与标签选择器</h1><p>它是附加在Kubernetes任何资源对象之上的键值型数据，常用于标签选择器的匹配度检查，从而完成资源筛选。Kubernetes系统的部分基础功能的实现也要依赖标签和标签选择器，例如Service筛选并关联后端Pod对象，由ReplicaSet、StatefulSet和DaemonSet等控制器过滤并关联后端Pod对象等，从而提升用户的资源管理效率。</p><h2 id="4-1-资源标签"><a href="#4-1-资源标签" class="headerlink" title="4.1 资源标签"></a>4.1 资源标签</h2><p>标签可在资源创建时直接指定，也可随时按需添加在活动对象上。一个对象可拥有不止一个标签，而同一个标签也可添加至多个对象之上。下面是较为常用的标签。</p><ul><li>版本标签：”release” : “stable”，”release” : “canary”，”release” : “beta”。</li><li>环境标签：”environment” : “dev”，”environment” : “qa”，”environment” : “prod”。</li><li>应用标签：”app” : “ui”，”app” : “as”，”app” : “pc”，”app” : “sc”。</li><li>架构层级标签：”tier” : “frontend”，”tier” : “backend”，”tier” : “cache”。</li><li>分区标签：”partition” : “customerA”，”partition” : “customerB”。</li><li>品控级别标签：”track” : “daily”，”track” : “weekly”。</li></ul><p>标签中的<font color="red">键名称通常由“键前缀”和“键名”组成，其格式形如KEY_PREFIX/KEY_NAME，键前缀为可选部分。键名至多能使用63个字符，支持字母、数字、连接号（-）、下划线（）、点号（.）等字符，且只能以字母或数字开头。而键前缀必须为DNS子域名格式，且不能超过253个字符。省略键前缀时，键将被视为用户的私有数据。由Kubernetes系统组件或第三方组件自动为用户资源添加的键必须使用键前缀，kubernetes.io/和k8s.io/前缀预留给了Kubernetes的核心组件使用，例如Node对象上常用的kubernetes.io/os、kubernetes.io/arch和kubernetes.io/hostname等。<br><font color="red">标签的键值必须不能多于63个字符，键值要么为空，要么以字母或数字开头及结尾，且中间只能使用字母、数字、连接号（-）、下划线（）或点号（.）等字符。</font></font></p><h3 id="4-1-1-创建资源时定义标签"><a href="#4-1-1-创建资源时定义标签" class="headerlink" title="4.1.1 创建资源时定义标签"></a>4.1.1 创建资源时定义标签</h3><p>创建资源时，可直接在其metadata中嵌套使用labels字段定义要附加的标签项。例如在下面的Namespace资源配置清单文件中，示例ns-with-labels.yaml中使用了两个标签，env=dev和app=eshop。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eshop</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">eshop</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubernetes</span></span><br></pre></td></tr></table></figure><ul><li>可在kubectl get namespaces命令中使用–show-labels选项，以额外显示对象的标签信息。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f ns-with-labels.yaml</span> </span><br><span class="line">namespace/eshop created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop --show-labels</span></span><br><span class="line">NAME  STATUS   AGE   LABELS</span><br><span class="line">demoapp   Active      11s   app=eshop,env=dev</span><br></pre></td></tr></table></figure><ul><li>kubectl get命令上使用-L key1,key2,…选项可指定有特定键的标签信息。例如，仅显示eshop名称空间上的env和app标签：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop -L <span class="built_in">env</span>,app</span></span><br><span class="line">NAME    STATUS   AGE   ENV   APP</span><br><span class="line">eshop       Active     89s    dev   eshop</span><br></pre></td></tr></table></figure><ul><li>kubectl label命令可直接管理活动对象的标签，以按需进行添加或修改等操作。例如为eshop名称空间添加release=beta标签：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop release=beta</span></span><br><span class="line">namespace/eshop labeled</span><br></pre></td></tr></table></figure><ul><li>已经附带了指定键名的标签，使用kubectl label为其设定新的键值时需同时使用–overwrite命令，强制覆盖原有键值。例如，将eshop名称空间的release标签值修改为canary：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop release=canary --overwrite</span></span><br><span class="line">namespace/eshop labeled</span><br></pre></td></tr></table></figure><ul><li>删除活动对象上的标签时同样要使用kubectl label命令，但仅需要指定标签名称并紧跟一个减号“–”，例如，下面的命令首先删除eshop名称空间中的env标签，而后显示其现有的所有标签：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop <span class="built_in">env</span>-</span></span><br><span class="line">namespace/eshop labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop --show-labels</span> </span><br><span class="line">NAME    STATUS   AGE     LABELS</span><br><span class="line">eshop   Active   6m46s   app=eshop,release=beta</span><br></pre></td></tr></table></figure><h2 id="4-2-标签选择器"><a href="#4-2-标签选择器" class="headerlink" title="4.2　标签选择器"></a>4.2　标签选择器</h2><p>标签选择器用于表达标签的查询条件或选择标准，目前Kubernetes API支持两个选择器：基于等值关系（equality-based）的标签选项器与基于集合关系（set-based）的标签选择器。在指定多个选择器时需要以逗号分隔，<font color="red">各选择器之间遵循逻辑“与”，即必须要满足所有条件，而且空值的选择器将不选择任何对象。</font></p><ul><li>基于等值关系的标签选择器</li></ul><p>可用操作符有=、==和!=，其中前两个意义相同，都表示“等值”关系，最后一个表示“不等”。例如env=dev和env!=prod都是基于等值关系的选择器。</p><ul><li>基于集合的标签选择器</li></ul><p>根据标签名的一组值进行筛选，它支持in、notin和exists这3种操作符，例如tier in (frontend,backend)表示所有包含tier标签且其值为frontend或backend的资源对象。</p><ul><li>kubectl get命令的“-l”选项能够指定使用标签选择器筛选目标资源，例如，如下命令显示标签release的值不等于beta，且标签app的值等于eshop的所有名称空间：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces -l <span class="string">&#x27;release!=beta,app=eshop&#x27;</span>  -L app,release</span></span><br><span class="line">NAME      STATUS      AGE   APP     RELEASE</span><br><span class="line">eshop     Active      60m   eshop   canary</span><br></pre></td></tr></table></figure><ul><li>基于集合关系的标签选择器用于基于一组值进行过滤，它支持in、notin和exists 3种操作符，各操作符的使用格式及意义如下。<ul><li>KEY in (VALUE1,VALUE2,…)：指定键名的值存在于给定的列表中即满足条件。</li><li>KEY notin (VALUE1,VALUE2,…)：指定键名的值不存在于给定列表中即满足条件。</li><li>KEY：所有存在此键名标签的资源。</li><li>!KEY：所有不存在此键名标签的资源。</li><li>例如，下面的命令可以过滤出标签键名release的值为beta或canary的所有Namespace对象：</li></ul></li></ul><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl get namespaces -l &#x27;release in (beta,canary)&#x27; -L release</span><br><span class="line">NAME       STATUS     AGE   RELEASE</span><br><span class="line">eshop      Active     63m   canary</span><br></pre></td></tr></table></figure><ul><li>再如，下面的命令可以列出集群中拥有node-role.kubernetes.io标签的各Node对象：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get nodes -l <span class="string">&#x27;node-role.kubernetes.io/master&#x27;</span> -L kubernetes.io/hostname</span></span><br><span class="line">NAME                     STATUS   ROLES    AGE   VERSION   HOSTNAME</span><br><span class="line">k8s-master01.ilinux.io   Ready    master   25d   v1.17.3   k8s-master01.ilinux.io</span><br><span class="line">k8s-master02.ilinux.io   Ready    master   25d   v1.17.3   k8s-master02.ilinux.io</span><br><span class="line">k8s-master03.ilinux.io   Ready    master   25d   v1.17.3   k8s-master03.ilinux.io</span><br></pre></td></tr></table></figure><p>此外，Kubernetes的诸多资源对象必须以标签选择器的方式关联到Pod资源对象，例如Service资源在spec字段中嵌套使用selector字段定义标签选择器，而Deployment与StatefulSet等资源在selector字段中通过matchLabels和matchExpressions构造复杂的标签选择机制。</p><ul><li>matchLabels：直接给定键值对指定标签选择器。</li><li>matchExpressions：基于表达式指定的标签选择器列表，每个选择器形如{key: KEY_NAME, operator: OPERATOR, values: [VALUE1,VALUE2,…]}，选择器列表间为“逻辑与”关系；使用In或NotIn操作符时，其values必须为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空。</li><li>下面的资源清单片段是一个示例，它同时定义了两类标签选择器。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">selector:</span></span><br><span class="line">  <span class="attr">matchLabels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">matchExpressions:</span></span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">tier</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">cache</span>]&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">environment</span>, <span class="attr">operator:</span> <span class="string">Exists</span>, <span class="string">values:</span>&#125;</span><br></pre></td></tr></table></figure><h1 id="5-Service与服务发现"><a href="#5-Service与服务发现" class="headerlink" title="5 Service与服务发现"></a>5 Service与服务发现</h1><p>Service对象的IP地址都仅在Kubernetes集群内可达，它们无法接入集群外部的访问流量。在解决此类问题时，除了可以在单一节点上做端口（hostPort）暴露及让Pod资源共享使用工作节点的网络名称空间（hostNetwork）之外，更推荐用户使用NodePort或LoadBalancer类型的Service资源，或者是有七层负载均衡能力的Ingress资源。</p><h2 id="5-1-Service资源及其实现模型"><a href="#5-1-Service资源及其实现模型" class="headerlink" title="5.1 Service资源及其实现模型"></a>5.1 Service资源及其实现模型</h2><p>Service是Kubernetes的核心资源类型之一。它事实上是一种抽象：通过规则定义出由多个Pod对象组合而成的逻辑集合，以及访问这组Pod的策略。Service关联Pod资源的规则要借助标签选择器完成。</p><h3 id="5-1-1-Service资源概述"><a href="#5-1-1-Service资源概述" class="headerlink" title="5.1.1  Service资源概述"></a>5.1.1  Service资源概述</h3><p>Service资源基于标签选择器把筛选出的一组Pod对象定义成一个逻辑组合，并通过自己的IP地址和端口将请求分发给该组内的Pod对象。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131215422.png" alt="../img/image-20220123131215422"></p><p>Service对象的IP地址（可称为ClusterIP或ServiceIP）是虚拟IP地址，由Kubernetes系统在Service对象创建时在专用网络（Service Network）地址中自动分配或由用户手动指定，并且在Service对象的生命周期中保持不变。Service基于端口过滤到达其IP地址的客户端请求，并根据定义将请求转发至其后端的Pod对象的相应端口之上，因此这种代理机制也称为“端口代理”或四层代理，工作于TCP/IP协议栈的传输层。<br>Service对象会通过API Server持续监视（watch）标签选择器匹配到的后端Pod对象，并实时跟踪这些Pod对象的变动情况，例如IP地址变动以及Pod对象的增加或删除等。不过，Service并不直接连接至Pod对象，它们之间还有一个中间层——Endpoints资源对象，该资源对象是一个由IP地址和端口组成的列表，这些IP地址和端口则来自由Service的标签选择器匹配到的Pod对象。这也是很多场景中会使用“Service的后端端点”这一术语的原因。默认情况下，创建Service资源对象时，其关联的Endpoints对象会被自动创建。</p><h3 id="5-1-2-kube-proxy代理模型"><a href="#5-1-2-kube-proxy代理模型" class="headerlink" title="5.1.2 kube-proxy代理模型"></a>5.1.2 kube-proxy代理模型</h3><p>每个工作节点的kube-proxy组件通过API Server持续监控着各Service及其关联的Pod对象，并将Service对象的创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。客户端、Service及Pod对象的关系如图7-3所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131237759.png" alt="../img/image-20220123131237759"></p><p>Service对象的ClusterIP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现Kubernetes集群网络内部通信，且仅能够以规则中定义的转发服务的请求作为目标地址予以响应，这也是它之所以被称作虚拟IP的原因之一。kube-proxy把请求代理至相应端点的方式有3种：userspace、iptables和ipvs。</p><h4 id="1-userspace代理模型"><a href="#1-userspace代理模型" class="headerlink" title="1 userspace代理模型"></a>1 userspace代理模型</h4><p>此处的userspace是指Linux操作系统的用户空间。在这种模型中，kube-proxy负责跟踪API Server上Service和Endpoints对象的变动（创建或移除），并据此调整Service资源的定义。对于每个Service对象，它会随机打开一个本地端口（运行于用户空间的kube-proxy进程负责监听），任何到达此代理端口的连接请求都将被代理至当前Service资源后端的各Pod对象，至于哪个Pod对象会被选中则取决于当前Service资源的调度方式，默认调度算法是轮询（round-robin）。userspace代理模型工作逻辑如图7-4所示。另外，此类Service对象还会创建iptables规则以捕获任何到达ClusterIP和端口的流量。在Kubernetes 1.1版本之前，userspace是默认的代理模型。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131341607.png" alt="../img/image-20220123131341607"></p><p>在这种代理模型中，请求流量到达内核空间后经由套接字送往用户空间中的kube-proxy进程，而后由该进程送回内核空间，发往调度分配的目标后端Pod对象。因请求报文在内核空间和用户空间来回转发，所以必然导致模型效率不高。</p><h4 id="2-iptables代理模型"><a href="#2-iptables代理模型" class="headerlink" title="2 iptables代理模型"></a>2 iptables代理模型</h4><p>创建Service对象的操作会触发集群中的每个kube-proxy并将其转换为定义在所属节点上的iptables规则，用于转发工作接口接收到的、与此Service资源ClusterIP和端口相关的流量。客户端发来请求将直接由相关的iptables规则进行目标地址转换（DNAT）后根据算法调度并转发至集群内的Pod对象之上，而无须再经由kube-proxy进程进行处理，因而称为iptables代理模型，如图7-5所示。对于每个Endpoints对象，Service资源会为其创建iptables规则并指向其iptables地址和端口，而流量转发到多个Endpoint对象之上的默认调度机制是随机算法。iptables代理模型由Kubernetes v1.1版本引入，并于v1.2版本成为默认的类型。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131409842.png" alt="../img/image-20220123131409842"></p><p>在iptables代理模型中，Service的服务发现和负载均衡功能都使用iptables规则实现，而无须将流量在用户空间和内核空间来回切换，因此更为高效和可靠，但是性能一般，而且受规模影响较大，仅适用于少量Service规模的集群。</p><h4 id="3-ipvs代理模型"><a href="#3-ipvs代理模型" class="headerlink" title="3 ipvs代理模型"></a>3 ipvs代理模型</h4><p>Kubernetes自v1.9版本起引入ipvs代理模型，且自v1.11版本起成为默认设置。在此种模型中，kube-proxy跟踪API Server上Service和Endpoints对象的变动，并据此来调用netlink接口创建或变更ipvs（NAT）规则，如图7-6所示。它与iptables规则的不同之处仅在于客户端请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131440521.png" alt="../img/image-20220123131440521"></p><p>ipvs代理模型中Service的服务发现和负载均衡功能均基于内核中的ipvs规则实现。类似于iptables，ipvs也构建于内核中的netfilter之上，但它使用hash表作为底层数据结构且工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性，适用于存在大量Service资源且对性能要求较高的场景。ipvs代理模型支持rr、lc、dh、sh、sed和nq等多种调度算法。</p><h3 id="5-1-3-Service资源类型"><a href="#5-1-3-Service资源类型" class="headerlink" title="5.1.3　Service资源类型"></a>5.1.3　Service资源类型</h3><p>无论哪一种代理模型，Service资源都可统一根据其工作逻辑分为ClusterIP、NodePort、LoadBalancer和ExternalName这4种类型。</p><h4 id="（1）ClusterIP"><a href="#（1）ClusterIP" class="headerlink" title="（1）ClusterIP"></a>（1）ClusterIP</h4><p>通过集群内部IP地址暴露服务，ClusterIP地址仅在集群内部可达，因而无法被集群外部的客户端访问。此为默认的Service类型。</p><h4 id="（2）NodePort"><a href="#（2）NodePort" class="headerlink" title="（2）NodePort"></a>（2）NodePort</h4><p>NodePort类型是对ClusterIP类型Service资源的扩展，它支持通过特定的节点端口接入集群外部的请求流量，并分发给后端的Server Pod处理和响应。因此，这种类型的Service既可以被集群内部客户端通过ClusterIP直接访问，也可以通过套接字&lt;NodeIP&gt;: &lt;NodePort&gt;与集群外部客户端进行通信，如图7-7所示。显然，若集群外部的请求报文首先到的节点并非Service调度的目标Server Pod所在的节点，<font color="red">该请求必然因需要额外的转发过程（跃点）和更多的处理步骤而产生更多延迟</font></p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131506181.png" alt="../img/image-20220123131506181"></p><h4 id="（3）LoadBalancer"><a href="#（3）LoadBalancer" class="headerlink" title="（3）LoadBalancer"></a>（3）LoadBalancer</h4><p>这种类型的Service依赖于部署在IaaS云计算服务之上并且能够调用其API接口创建软件负载均衡器的Kubernetes集群环境。LoadBalancer Service构建在NodePort类型的基础上，通过云服务商提供的软负载均衡器将服务暴露到集群外部，因此它也会具有NodePort和ClusterIP。简言之，创建LoadBalancer类型的Service对象时会在集群上创建一个NodePort类型的Service，并额外触发Kubernetes调用底层的IaaS服务的API创建一个软件负载均衡器，而集群外部的请求流量会先路由至该负载均衡器，并由该负载均衡器调度至各节点上该Service对象的NodePort，如图7-8所示。该Service类型的优势在于，<font color="red">它能够把来自集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而不是让客户端自行决定连接哪个节点，也避免了因客户端指定的节点故障而导致的服务不可用。</font></p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131528411.png" alt="../img/image-20220123131528411"></p><h4 id="（4）ExternalName"><a href="#（4）ExternalName" class="headerlink" title="（4）ExternalName"></a>（4）ExternalName</h4><p>通过将Service映射至由externalName字段的内容指定的主机名来暴露服务，此主机名需要被DNS服务解析至CNAME类型的记录中。换言之，此种类型不是定义由Kubernetes集群提供的服务，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部服务的一种实现方式，如图7-9所示。因此，这种类型的Service没有ClusterIP和NodePort，没有标签选择器用于选择Pod资源，也不会有Endpoints存在。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123131606248.png" alt="../img/image-20220123131606248"></p><p>总体来说，若需要将Service资源发布至集群外部，应该将其配置为NodePort或Load-Balancer类型，而若要把外部的服务发布于集群内部供Pod对象使用，则需要定义一个ExternalName类型的Service资源，只是这种类型的实现要依赖于v1.7及更高版本的Kubernetes。</p><h2 id="5-2-应用Service资源"><a href="#5-2-应用Service资源" class="headerlink" title="5.2 应用Service资源"></a>5.2 应用Service资源</h2><p>Service是Kubernetes核心API群组（core）中的标准资源类型之一，其管理操作的基本逻辑类似于Namespace和ConfigMap等资源，支持基于命令行和配置清单的管理方式。Service资源配置规范中常用的字段及意义如下所示。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">…</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">type</span> <span class="string">&lt;string&gt;</span>                 <span class="comment"># Service类型，默认为ClusterIP</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;map[string]string&gt;</span>  <span class="comment"># 等值类型的标签选择器，内含“与”逻辑</span></span><br><span class="line">  <span class="string">ports：</span>                       <span class="comment"># Service的端口对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># 端口名称</span></span><br><span class="line">    <span class="string">protocol</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 协议，目前仅支持TCP、UDP和SCTP，默认为TCP</span></span><br><span class="line">    <span class="string">port</span> <span class="string">&lt;integer&gt;</span>              <span class="comment"># Service的端口号，被映射进Pod上的应用程序监听的端口； 而且如果后端Pod有多个端口，并且每个端口都想通过SErvice暴露的话，每个都要单独定义。 最终接收请求的是PodIP和containerPort；</span></span><br><span class="line">    <span class="string">targetPort</span>  <span class="string">&lt;string&gt;</span>        <span class="comment">#  后端目标进程的端口号或名称，名称需由Pod规范定义</span></span><br><span class="line">    <span class="string">nodePort</span> <span class="string">&lt;integer&gt;</span>          <span class="comment"># 节点端口号，仅适用于NodePort和LoadBalancer类型</span></span><br><span class="line">  <span class="string">clusterIP</span>  <span class="string">&lt;string&gt;</span>           <span class="comment"># Service的集群IP，建议由系统自动分配,也支持由用户手动分配</span></span><br><span class="line">  <span class="string">externalTrafficPolicy</span>  <span class="string">&lt;string&gt;</span> <span class="comment"># 外部流量策略处理方式，Local表示由当前节点处理，</span></span><br><span class="line">　　　                            <span class="comment"># Cluster表示向集群范围内调度</span></span><br><span class="line">  <span class="string">loadBalancerIP</span>  <span class="string">&lt;string&gt;</span>        <span class="comment"># 外部负载均衡器使用的IP地址，仅适用于LoadBlancer</span></span><br><span class="line">  <span class="string">externalName</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 外部服务名称，该名称将作为Service的DNS CNAME值</span></span><br></pre></td></tr></table></figure><h3 id="5-2-1-应用ClusterIP-Service资源"><a href="#5-2-1-应用ClusterIP-Service资源" class="headerlink" title="5.2.1　应用ClusterIP Service资源"></a>5.2.1　应用ClusterIP Service资源</h3><p>创建Service对象的常用方法有两种：一是利用此前曾使用过的kubectl create service命令创建，另一个则是利用资源配置清单创建。Service资源对象的期望状态定义在spec字段中，较为常用的内嵌字段为selector和ports，用于定义标签选择器和服务端口。下面的配置清单是定义在services-clusterip-demo.yaml中的一个Service资源示例：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span>       <span class="comment"># 端口名称标识</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span>    <span class="comment"># 协议，支持TCP、UDP和SCTP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span>         <span class="comment"># Service自身的端口号</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span>   <span class="comment"># 目标端口号，即Endpoint上定义的端口号</span></span><br></pre></td></tr></table></figure><p>Service资源的spec.selector仅支持以映射（字典）格式定义的等值类型的标签选择器，例如上面示例中的app: demoapp。定义服务端口的字段spec.ports的值则是一个对象列表，它主要定义Service对象自身的端口与目标后端端口的映射关系。我们可以将示例中的Service对象创建于集群中，通过其详细描述了解其特性，如下面的命令及结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-clusterip-demo.yaml</span> </span><br><span class="line">service/demoapp-svc created</span><br><span class="line">~ $ kubectl describe services/demoapp-svc</span><br><span class="line">Name:              demoapp-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       Selector:  app=demoapp</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.97.72.1</span><br><span class="line">Port:              http  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         &lt;none&gt;</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure><p>上面命令中的结果显示，demoapp-svc默认设定为ClusterIP类型，并得到一个自动分配的IP地址10.97.72.1。创建Service对象的同时会创建一个与之同名且拥有相同标签选择器的Endpoint对象，若该标签选择器无法匹配到任何Pod对象的标签，则Endpoint对象无任何可用端点数据，于是Service对象的Endpoints字段值便成了<none>。<br>我们知道，Service对象自身只是iptables或ipvs规则，它并不能处理客户端的服务请求，而是需要把请求报文通过目标地址转换（DNAT）后转发至后端某个Server Pod，这意味着没有可用的后端端点的Service对象是无法响应客户端任何服务请求的，如下面从集群节点上发起的请求命令结果所示。</none></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~$ curl 10.97.72.1</span><br><span class="line">curl: (7) Failed to connect to 10.97.72.1 port 80: Connection refused</span><br></pre></td></tr></table></figure><p>下面使用命令式命令手动创建一个与该Service对象具有相同标签选择器的Deployment对象demoapp，它默认会自动创建一个拥有标签app: demoapp的Pod对象。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create deploy demoapp --../img/image=ikubernetes/demoapp:v1.0</span></span><br><span class="line">deployment.apps/demoapp created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp</span> </span><br><span class="line">NAME                READY   STATUS   RESTARTS   AGE</span><br><span class="line">demoapp-6c5d545684-g85gl   1/1     Running   0          8s</span><br></pre></td></tr></table></figure><p>Service对象demoapp-svc通过API Server获知这种匹配变动后，会立即创建一个以该Pod对象的IP和端口为列表项的名为demoapp-svc的Endpoints对象，而该Service对象详细描述信息中的Endpoint字段便以此列表项为值，如下面的命令结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME       ENDPOINTS     AGE</span><br><span class="line">demoapp-svc   10.244.2.7:80    42s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.2.7:80</span><br></pre></td></tr></table></figure><p>扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale deployments/demoapp --replicas=3</span></span><br><span class="line">deployment.apps/demoapp scaled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME          ENDPOINTS                       AGE</span><br><span class="line">demoapp-svc   10.244.1.11:80,10.244.2.7:80,10.244.3.9:80   96s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br></pre></td></tr></table></figure><p>扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale deployments/demoapp --replicas=3</span></span><br><span class="line">deployment.apps/demoapp scaled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME          ENDPOINTS                       AGE</span><br><span class="line">demoapp-svc   10.244.1.11:80,10.244.2.7:80,10.244.3.9:80   96s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br></pre></td></tr></table></figure><p>接下来可于集群中的某节点上再次向服务对象demoapp-svc发起访问请求以进行测试，多次的访问请求还可评估负载均衡算法的调度效果，如下面的命令及结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~$ while true; do curl -s 10.97.72.1/hostname; sleep .2; done</span><br><span class="line">ServerName: demoapp-6c5d545684-89w4f</span><br><span class="line">ServerName: demoapp-6c5d545684-zlm2w</span><br><span class="line">ServerName: demoapp-6c5d545684-g85gl</span><br><span class="line">ServerName: demoapp-6c5d545684-g85gl</span><br></pre></td></tr></table></figure><p><font color="red">kubeadm部署的Kubernetes集群的Service代理模型默认为iptables，它使用随机调度算法，因此Service会把客户端请求随机调度至其关联的某个后端Pod对象。请求取样次数越多，其调度效果也越接近算法的目标效果。</font></p><h3 id="7-2-2-应用NodePort-Service资源"><a href="#7-2-2-应用NodePort-Service资源" class="headerlink" title="7.2.2　应用NodePort Service资源"></a>7.2.2　应用NodePort Service资源</h3><p>部署Kubernetes集群系统时会预留一个端口范围，专用于分配给需要用到NodePort的Service对象，该端口范围默认为30000～32767。<font color="red">与Cluster类型的Service资源的一个显著不同之处在于，NodePort类型的Service资源需要显式定义.spec.type字段值为NodePort</font>，必要时还可以手动指定具体的节点端口号。例如下面的配置清单（services-nodeport-demo.yaml）中定义的Service资源对象demoapp-nodeport-svc，它使用了NodePort类型，且人为指定了32223这个节点端口。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-nodeport-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">32223</span></span><br></pre></td></tr></table></figure><p>实践中，并不鼓励用户自定义节点端口，除非能事先确定它不会与某个现存的Service资源产生冲突。无论如何，只要没有特别需要，留给系统自动配置总是较好的选择。将配置清单中定义的Service对象demoapp-nodeport-svc创建于集群之上，以便通过详细描述了解其状态细节。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-nodeport-demo.yaml</span> </span><br><span class="line">service/demoapp-nodeport-svc created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services demoapp-nodeport-svc</span>   </span><br><span class="line">Name:                     demoapp-nodeport-svc</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   &lt;none&gt;</span><br><span class="line">Annotations:              Selector:  app=demoapp</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.97.227.67</span><br><span class="line">Port:                     http  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 http  32223/TCP</span><br><span class="line">Endpoints:                10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure><p>命令结果显示，该Service对象用于调度集群外部流量时使用默认的Cluster策略，该策略优先考虑负载均衡效果，哪怕目标Pod对象位于另外的节点之上而带来额外的网络跃点，因而针对该NodePort的请求将会被分散调度至该Serivce对象关联的所有端点之上。可以在集群外的某节点上对任一工作节点的NodePort端口发起HTTP请求以进行测试。以节点k8s-node03.ilinux.io为例，我们以如下命令向它的IP地址172.29.9.13的32223端口发起多次请求。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> curl -s 172.29.9.13:32223; <span class="built_in">sleep</span> 1; <span class="keyword">done</span></span></span><br><span class="line">…… ClientIP: 10.244.3.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-zlm2w, ServerIP: 10.244.1.11!</span><br><span class="line">…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-g85gl, ServerIP: 10.244.2.7!</span><br></pre></td></tr></table></figure><p>上面命令的结果显示出外部客户端的请求被调度至该Service对象的每一个后端Pod之上，而这些Pod对象可能会分散于集群中的不同节点。命令结果还显示，请求报文的客户端IP地址是最先接收到请求报文的节点上用于集群内部通信的IP地址，而非外部客户端地址，这也能够在Pod对象的应用访问日志中得到进一步验证，如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl logs demoapp-6c5d545684-g85gl | <span class="built_in">tail</span> -n 1</span></span><br><span class="line">10.244.3.0 - - [31/Aug/2020 02:30:00] &quot;GET / HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure><p>NodePort类型的Service对象会对请求报文同时进行源地址转换（SNAT）和目标地址转换（DNAT）操作。<br>另一个外部流量策略Local则仅会将流量调度至请求的目标节点本地运行的Pod对象之上，以减少网络跃点，降低网络延迟，但当请求报文指向的节点本地不存在目标Service相关的Pod对象时将直接丢弃该报文。下面先把demoapp-nodeport-svc的外部流量策略修改为Local，而后再进行访问测试。简单起见，这里使用kubectl patch命令来修改Service对象的流量策略。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch services/demoapp-nodeport-svc -p <span class="string">&#x27;&#123;&quot;spec&quot;: &#123;&quot;externalTrafficPolicy&quot;: &quot;Local&quot;&#125;&#125;&#x27;</span></span>  </span><br><span class="line">service/demoapp-nodeport-svc patched</span><br></pre></td></tr></table></figure><p>-p选项中指定的补丁是一个JSON格式的配置清单片段，它引用了spec.externalTrafficPolicy字段，并为其赋一个新的值。配置完成后，我们再次发起测试请求时会看到，请求都被调度给了目标节点本地运行的Pod对象。另外，Local策略下无须在集群中转发流量至其他节点，也就不用再对请求报文进行源地址转换，Server Pod所看到的客户端IP就是外部客户端的真实地址。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> curl -s 172.29.9.13:32223; <span class="built_in">sleep</span> 1; <span class="keyword">done</span></span>         </span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br></pre></td></tr></table></figure><p>NodePort类型的Service资源同样会被配置ClusterIP，以确保集群内的客户端对该服务的访问请求可在集群范围的通信中完成。</p><h3 id="5-2-3-应用LoadBalancer-Service资源"><a href="#5-2-3-应用LoadBalancer-Service资源" class="headerlink" title="5.2.3 应用LoadBalancer Service资源"></a>5.2.3 应用LoadBalancer Service资源</h3><p>NodePort类型的Service资源虽然能够在集群外部访问，但外部客户端必须事先得知NodePort和集群中至少一个节点IP地址，一旦被选定的节点发生故障，客户端还得自行选择请求访问其他的节点，因而一个有着固定IP地址的固定接入端点将是更好的选择。此外，集群节点很可能是某IaaS云环境中仅具有私有IP地址的虚拟主机，这类地址对互联网客户端不可达，为此类节点接入流量也要依赖于集群外部具有公网IP地址的负载均衡器，由其负责接入并调度外部客户端的服务请求至集群节点相应的NodePort之上。<br>IaaS云计算环境通常提供了LBaaS（Load Balancer as a Service）服务，它允许租户动态地在自己的网络创建一个负载均衡设备。部署在此类环境之上的Kubernetes集群可借助于CCM（Cloud Controller Manager）在创建LoadBalancer类型的Service资源时调用IaaS的相应API，按需创建出一个软件负载均衡器。但CCM不会为那些非LoadBalancer类型的Service对象创建负载均衡器，而且当用户将LoadBalancer类型的Service调整为其他类型时也将删除此前创建的负载均衡器。<font color="red">kubeadm在部署Kubernetes集群时并不会默认部署CCM，有需要的用户需要自行部署。</font><br>对于没有此类API可用的Kubernetes集群，管理员也可以为NodePort类型的Service手动部署一个外部的负载均衡器（推荐使用HA配置模型），并配置将请求流量调度至各节点的NodePort之上，这种方式的缺点是管理员需要手动维护从外部负载均衡器到内部服务的映射关系。<br>从实现方式上来说，LoadBalancer类型的Service就是在NodePort类型的基础上请求外部管理系统的API，并在Kubernetes集群外部额外创建一个负载均衡器，将流量调度至该NodePort Service之上。Kubernetes以异步方式请求创建负载均衡器，并将有关配置保存在Service对象的.status.loadBalancer字段中。下面是定义在services-loadbalancer-demo.yam配置清单中的LoadBalancer类型Service资源，在最简单的配置模型中，用户仅需要修改NodePort Service服务定义中type字段的值为LoadBalancer即可。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-loadbalancer-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure><p>Service对象的loadBalancerIP负责承接外部发来的流量，该IP地址通常由云服务商系统动态配置，或者借助.spec.loadBalancerIP字段显式指定，但有些云服务商不支持用户设定该IP地址，这种情况下，即便提供了也会被忽略。外部负载均衡器的流量会直接调度至Service后端的Pod对象之上，而如何调度流量则取决于云服务商，有些环境可能还需要为Service资源的配置定义添加注解，必要时请自行参考云服务商文档说明。另外，LoadBalancer Service还支持使用.spec. loadBalancerSourceRanges字段指定负载均衡器允许的客户端来源的地址范围。</p><h3 id="5-2-4-外部IP"><a href="#5-2-4-外部IP" class="headerlink" title="5.2.4 外部IP"></a>5.2.4 外部IP</h3><p>若集群中部分或全部节点除了有用于集群通信的节点IP地址之外，还有可用于外部通信的IP地址，如图7-10中的EIP-1和EIP-2，那么我们还可以在Service资源上启用spec.externalIPs字段来基于这些外部IP地址向外发布服务。所有路由到指定的外部IP（externalIP）地址某端口的请求流量都可由该Service代理到后端Pod对象之上，如图7-10所示。从这个角度来说，请求流量到达外部IP与节点IP并没有本质区别，但外部IP却可能仅存在于一部分的集群节点之上，而且它不受Kubernetes集群管理，需要管理员手动介入其配置和回收等操作任务中。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123132640583.png" alt="../img/image-20220123132640583"></p><p>外部IP地址可结合ClusterIP、NodePort或LoadBalancer任一类型的Service资源使用，而到达外部IP的请求流量会直接由相关联的Service调度转发至相应的后端Pod对象进行处理。假设示例Kubernetes集群中的k8s-node01节点上拥有一个可被路由到的IP地址172.29.9.26，我们期望能够将demoapp的服务通过该外部IP地址发布到集群外部，则可以使用下列配置清单（services-externalip-demo.yaml）中的Service资源实现。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-externalip-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">externalIPs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.26</span></span><br></pre></td></tr></table></figure><p>节点k8s-node01故障也必然导致该外部IP上公开的服务不再可达，除非该IP地址可以浮动到其他节点上。如今，大多数云服务商都支持浮动IP的功能，该IP地址可绑定在某个主机，并在其故障时通过某种触发机制自动迁移至其他主机。在不具有浮动IP功能的环境中进行测试之前，需要先在k8s-node01上（或根据规划的其他的节点上）手动配置172.29.9.26这个外部IP地址。而且，在模拟节点故障并手动将外部IP地址配置在其他节点进行浮动IP测试时，还需要清理之前的ARP地址缓存。</p><h2 id="5-3-Service与Endpoint资源"><a href="#5-3-Service与Endpoint资源" class="headerlink" title="5.3 Service与Endpoint资源"></a>5.3 Service与Endpoint资源</h2><p>端点是指通过LAN或WAN连接的能够用于网络通信的硬件设备，它在广义上可以指代任何与网络连接的设备。在Kubernetes语境中，端点通常代表Pod或节点上能够建立网络通信的套接字，并由专用的资源类型Endpoint进行定义和跟踪。</p><h3 id="5-3-1-Endpoint与容器探针"><a href="#5-3-1-Endpoint与容器探针" class="headerlink" title="5.3.1 Endpoint与容器探针"></a>5.3.1 Endpoint与容器探针</h3><p>Service对象借助于Endpoint资源来跟踪其关联的后端端点，但Endpoint是“二等公民”，<font color="red">Service对象可根据标签选择器直接创建同名的Endpoint对象</font>，不过用户几乎很少有直接使用该类型资源的需求。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">services-readiness-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span>      <span class="comment"># 定义Deployment对象，它使用Pod模板创建Pod对象</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span>         <span class="comment"># 该Deployment对象要求满足的Pod对象数量</span></span><br><span class="line">  <span class="attr">selector:</span>           <span class="comment"># Deployment对象的标签选择器，用于筛选Pod对象并完成计数</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">  <span class="attr">template:</span>           <span class="comment"># 由Deployment对象使用的Pod模板，用于创建足额的Pod对象</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span>    <span class="comment"># 定义探针类型和探测方式</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">15</span>   <span class="comment"># 初次检测延迟时长</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span>         <span class="comment"># 检测周期</span></span><br></pre></td></tr></table></figure><p>Endpoint对象会根据就绪状态把同名Service对象标签选择器筛选出的后端端点的IP地址分别保存在subsets.addresses字段和subsets.notReadyAddresses字段中，通过API Server持续、动态跟踪每个端点的状态变动，并即时反映到端点IP所属的字段。仅那些位于subsets.addresses字段的端点地址可由相关的Service用作后端端点。此外，相关Service对象标签选择器筛选出的Pod对象数量的变动也将会导致Endpoint对象上的端点数量变动。<br>上面配置清单中定义Endpoint对象services-readiness-demo会筛选出Deployment对象demoapp2创建的两个Pod对象，将它们的IP地址和服务端口创建为端点对象。但延迟15秒启动的容器探针会导致这两个Pod对象至少要在15秒以后才能转为“就绪”状态，这意味着在上面配置清单中的Service资源创建后至少15秒之内无可用后端端点，例如下面的资源创建和Endpoint资源监视命令结果中，在20秒之后，Endpoint资源services-readiness-demo才得到第一个可用的后端端点IP。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-readiness-demo.yaml</span> </span><br><span class="line">service/services-readiness-demo created</span><br><span class="line">deployment.apps/demoapp2 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/services-readiness-demo -w</span> </span><br><span class="line">NAME                      ENDPOINTS                      AGE</span><br><span class="line">services-readiness-demo                                  6s</span><br><span class="line">services-readiness-demo   10.244.1.15:80                 20s</span><br><span class="line">services-readiness-demo   10.244.1.15:80,10.244.2.9:80   31s</span><br></pre></td></tr></table></figure><p>因任何原因导致的后端端点就绪状态检测失败，都会触发Endpoint对象将该端点的IP地址从subsets.addresses字段移至subsets.notReadyAddresses字段。例如，我们使用如下命令人为地将地址10.244.2.9的Pod对象中的容器就绪状态检测设置为失败，以进行验证。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -X POST -d <span class="string">&#x27;readyz=FAIL&#x27;</span> 10.244.2.9/readyz</span></span><br></pre></td></tr></table></figure><p>等待至少3个检测周期共30秒之后，获取Endpoint对象services-readiness-demo的资源清单的命令将返回类似如下信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/services-readiness-demo -o yaml</span></span><br><span class="line">……</span><br><span class="line">subsets:</span><br><span class="line">- addresses:</span><br><span class="line">  - ip: 10.244.1.15</span><br><span class="line">    nodeName: k8s-node01.ilinux.io</span><br><span class="line">    targetRef:</span><br><span class="line">      kind: Pod</span><br><span class="line">      name: demoapp2-85595465d-dhbzs</span><br><span class="line">      namespace: default</span><br><span class="line">      resourceVersion: &quot;321388&quot;</span><br><span class="line">      uid: 8d2a3bb6-c628-4558-917a-f8f6df9b8573</span><br><span class="line">  notReadyAddresses:</span><br><span class="line">  - ip: 10.244.2.9</span><br><span class="line">    nodeName: k8s-node02.ilinux.io</span><br><span class="line">    targetRef:</span><br><span class="line">      kind: Pod</span><br><span class="line">      name: demoapp2-85595465d-z7w5h</span><br><span class="line">      namespace: default</span><br><span class="line">      resourceVersion: &quot;323328&quot;</span><br><span class="line">      uid: 380050ae-4e32-4724-af22-e079ab2ec02e</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br></pre></td></tr></table></figure><p>该故障端点重新转回就绪状态后，Endpoints对象会将其移回subsets.addresses字段中。这种处理机制确保了Service对象不会将客户端请求流量调度给那些处于运行状态但服务未就绪（notReady）的端点。</p><h3 id="5-3-2-自定义Endpoint资源"><a href="#5-3-2-自定义Endpoint资源" class="headerlink" title="5.3.2 自定义Endpoint资源"></a>5.3.2 自定义Endpoint资源</h3><p>除了借助Service对象的标签选择器自动关联后端端点外，Kubernetes也支持自定义Endpoint对象，用户可通过配置清单创建具有固定数量端点的Endpoint对象，而调用这类Endpoint对象的同名Service对象无须再使用标签选择器。Endpoint资源的API规范如下。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoint</span></span><br><span class="line"><span class="attr">metadata:</span>                  <span class="comment"># 对象元数据</span></span><br><span class="line">  <span class="attr">name:</span></span><br><span class="line">  <span class="attr">namespace:</span></span><br><span class="line"><span class="attr">subsets:</span>                   <span class="comment"># 端点对象的列表</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">addresses:</span>               <span class="comment"># 处于“就绪”状态的端点地址对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">hostname</span>  <span class="string">&lt;string&gt;</span>     <span class="comment"># 端点主机名</span></span><br><span class="line">    <span class="string">ip</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 端点的IP地址，必选字段</span></span><br><span class="line">    <span class="string">nodeName</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 节点主机名</span></span><br><span class="line">    <span class="string">targetRef：</span>            <span class="comment"># 提供了该端点的对象引用</span></span><br><span class="line">      <span class="string">apiVersion</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 被引用对象所属的API群组及版本</span></span><br><span class="line">      <span class="string">kind</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 被引用对象的资源类型，多为Pod</span></span><br><span class="line">      <span class="string">name</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 对象名称</span></span><br><span class="line">      <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 对象所属的名称空间</span></span><br><span class="line">      <span class="string">fieldPath</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 被引用的对象的字段，在未引用整个对象时使用，通常仅引用</span></span><br><span class="line">                           <span class="comment"># 指定Pod对象中的单容器，例如spec.containers[1]</span></span><br><span class="line">      <span class="string">uid</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 对象的标识符</span></span><br><span class="line">  <span class="attr">notReadyAddresses:</span>       <span class="comment"># 处于“未就绪”状态的端点地址对象列表，格式与address相同</span></span><br><span class="line">  <span class="attr">ports:</span>                   <span class="comment"># 端口对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 端口名称</span></span><br><span class="line">    <span class="string">port</span> <span class="string">&lt;integer&gt;</span>         <span class="comment"># 端口号，必选字段</span></span><br><span class="line">    <span class="string">protocol</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 协议类型，仅支持UDP、TCP和SCTP，默认为TCP</span></span><br><span class="line">    <span class="string">appProtocol</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 应用层协议</span></span><br></pre></td></tr></table></figure><p>自定义Endpoint常将那些不是由编排程序编排的应用定义为Kubernetes系统的Service对象，从而让客户端像访问集群上的Pod应用一样请求外部服务。例如，假设要把Kubernetes集群外部一个可经由172.29.9.51:3306或172.29.9.52:3306任一端点访问的MySQL数据库服务引入集群中，便可使用如下清单中的配置完成。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-external</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.51</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.52</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-external</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure><p>显然，非经Kubernetes管理的端点，其就绪状态难以由Endpoint通过注册监视特定的API资源对象进行跟踪，因而用户需要手动维护这种调用关系的正确性。<br>Endpoint资源提供了在Kubernetes集群上跟踪端点的简单途径，但对于有着大量端点的Service来说，将所有的网络端点信息都存储在单个Endpoint资源中，会对Kubernetes控制平面组件产生较大的负面影响，且每次端点资源变动也会导致大量的网络流量。EndpointSlice（端点切片）通过将一个服务相关的所有端点按固定大小（默认为100个）切割为多个分片，提供了一种更具伸缩性和可扩展性的端点替代方案。<br>EndpointSlice由引用的端点资源组成，类似于Endpoint，它可由用户手动创建，也可由EndpointSlice控制器根据用户在创建Service资源时指定的标签选择器筛选集群上的Pod对象自动创建。单个EndpointSlice资源默认不能超过100个端点，小于该数量时，EndpointSlice与Endpoint存在1:1的映射关系且性能相同。EndpointSlice控制器会尽可能地填满每一个EndpointSlice资源，但不会主动进行重新平衡，新增的端点会尝试添加到现有的EndpointSlice资源上，若超出现有任何EndpointSlice对象的可用的空余空间，则将创建新的EndpointSlice，而非分散填充。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpointslice -n kube-system</span></span><br><span class="line">NAME           ADDRESSTYPE        PORTS            ENDPOINTS        AGE</span><br><span class="line">kube-dns-mbdj5   IPv4          53,9153,53   10.244.0.6,10.244.0.7   13d</span><br></pre></td></tr></table></figure><p>EndpointSlice资源根据其关联的Service与端口划分成组，每个组隶属于同一个Service。更具体的使用方式请参考Kubernetes的相关文档。</p><h2 id="5-4-深入理解Service资源"><a href="#5-4-深入理解Service资源" class="headerlink" title="5.4 深入理解Service资源"></a>5.4 深入理解Service资源</h2><p>本质上，Service对象代表着由kube-proxy借助于自身的程序逻辑（userspace）、iptables或ipvs，甚至是某种形式的组合所构建出的流量代理和调度转发机制，每个Service对象的创建、更新与删除都会经由kube-proxy反映为程序配置、iptables规则或ipvs规则的相应操作。</p><h3 id="5-4-1-iptables代理模型"><a href="#5-4-1-iptables代理模型" class="headerlink" title="5.4.1 iptables代理模型"></a>5.4.1 iptables代理模型</h3><p>由集群中每个节点上的kube-proxy进程将Service定义、转换且配置于节点内核上的iptables规则。每个Service的定义主要由Service流量匹配规则、流量调度规则和以每个后端Endpoint为单位的DNAT规则组成，这些规则负责完成Service资源的核心功能。此外，iptables代理模型还会额外在filter表和mangle表上使用一些辅助类的规则。</p><h4 id="1-ClusterIP-Service"><a href="#1-ClusterIP-Service" class="headerlink" title="1 ClusterIP Service"></a>1 ClusterIP Service</h4><p>ClusterIP类型Service资源的请求流量是指以某个特定Service对象的ClusterIP（或称为Service_IP）为目标地址，同时以Service_Port为目标端口的报文，它们可能源自Kubernetes集群中某个特定节点上的Pod、独立容器（非托管至Kubernetes集群）或进程，也可能源自节点之外。通常，源自独立容器或节点外部的请求报文的源IP地址为Pod网络（例如Flannel默认的10.244.0.0/16）之外的IP地址。<br>Cluster类型Service对象的相关规则主要位于KUBE-SERVICES、KUBE-MARQ-MASK和KUBE-POSTROUTING这3个自定义链，以及那些以KUBE-SVC或KUBE-SEP为前缀的各个自定义链上，用于实现Service流量筛选、分发和目标地址转换（端点地址），以及为非源自Pod网络的请求报文进行源地址转换。各相关的规则链及调用关系如图7-11所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123133725398.png" alt="../img/image-20220123133725398"></p><p>▪KUBE-SERVICES：包含所有ClusterIP类型Service的流量匹配规则，由PREROUTING和OUTPUT两个内置链直接调用。每个Service对象包含两条规则定义，对于所有发往该Service（目标IP为Service_IP且目标端口为Service_Port）的请求报文：前一条规则用于为非源自Pod网络（! -s 10.244.0.0/16）中的请求报文打上特有的防火墙标记，而打标签的操作则要借助KUBE-MARQ-MASK自定义链中的规则，后一条规则负责将所有报文转至专用的以KUBE-SVC为名称前缀的自定义链，后缀是Service信息的HASH值。<br>▪KUBE-MARQ-MASK：专用目的自定义链，所有转至该自定义链的报文都将被打上特有的防火墙标记（0x4000），以便于将特定类型的报文定义为单独的分类，进而在将该类报文转发到目标端点之前由POSTROUTING规则链进行源地址转换。<br>▪KUBE-SVC-<HASH>：定义一个服务的流量调度规则，它通过随机调度算法将请求分发给该Service的所有后端端点，每个后端端点定义在以KUBE-SEP为前缀名称的自定义链上，后缀是端点信息的hash值。<br>▪KUBE-SEP-<HASH>：定义一个端点相关的流量处理规则。它通常包含两条规则：前一条用于为那些源自该端点自身（-s ep_ip）的流量请求调用自定义链KUBE-MARQ-MASK，打上特有的防火墙标记；后一条负责对发往该端点的所有流量进行目标IP地址和端口转换，新目标为该端点的IP和端口（-j DNAT –to-destination ep_ip:ep_port）。<br>▪KUBE-POSTROUTING：专用的自定义链，由内置链POSTROUTING无条件调用，负责对带特有防火墙标记0x4000的请求报文进行源地址转换或地址伪装（MASQUERADE），新的源地址为报文离开协议栈时流经接口的主IP地址。<br>我们可通过实际存在的Service对象来验证这些设定，以7.2.1节创建的demoapp-svc为例，在集群中的任何一个工作节点上使用iptables -t nat -vnL或iptables -t nat -S命令打印与它相关的iptables规则。下面的命令打印了该Service对象用于流量匹配的相关规则，它定义在KUBE-SERVICES自定义链上。</HASH></HASH></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;default/demoapp-svc&quot;</span><br><span class="line">-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-ZAGXFVDPX7HH4UMW</span><br></pre></td></tr></table></figure><p>第一条规则用于将那些发往demoapp-svc的、来自10.244.0.0/16网络之外的请求报文交由自定义链KUBE-MARK-MASQ上的规则添加专用标记0x4000，该条规则如下所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-MARK-MASQ | grep &quot;^-A&quot;</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure><p>添加标记的处理并不会短路iptables规则链KUBE-SERVICES对流量的处理，因此所有发往demoapp-svc的流量还会继续由后一条规则指向的、以KUBE-SVC为名称前缀的自定义链KUBE-SVC-ZAGXFVDPX7HH4UMW中的规则处理。该自定义链专用于为demoapp-svc中的所有可用端点定义流量调度规则，它包含如下3条规则：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SVC-ZAGXFVDPX7HH4UMW | grep &quot;^-A&quot; </span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-HDIVJIPCJU2JBJVX</span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ZAFCYSF77K72PY72</span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-SEP-FUO5ALUGHUE426HZ</span><br></pre></td></tr></table></figure><p>注意<br>所有以KUBE-SEP和KUBE-SVC为前缀的自定义链的名称在重新创建Service或重启Kubernetes集群后都有可能发生改变，但它们的引用关系不变。</p><p>这3条规则的处理目标分别为3个以KUBE-SEP为名称前缀的自定义链，每个链上定义了一个端点的流量处理规则，因而意味着该Service对象共有3个Endpoint对象，所有流量将在这3个Endpoint之间随机（–mode random）分配。到达KUBE-SVC-ZAGXFVDPX7HH4UMW的流量将由这3条规则以“短路”方式进行匹配检查和处理，任何一条规则处理后都不会再匹配后续的其他规则。第一条规则将处理大约1/3（–probability 0.33333333349）的流量，余下的所有流量（即由第一条规则处理后余下的2/3）将由第二条规则处理一半（–probability 0.50000000000），再余下的所有流量都将由第三条规则处理，因此3个Endpoint将各自得到大约1/3的流量。<br>每个Endpoint专用的自定义链以KUBE-SEP为名称前缀，它包含某单点端点相关的流量处理规则。以专用IP地址为10.244.1.11的Endpoint对象为例，它对应于自定义链KUBE-SEP-HZPGLN57HG6GZW4O，该链下包含两个iptables规则，如下面的命令结果所示：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~#  iptables -t nat -S KUBE-SEP-HDIVJIPCJU2JBJVX | grep &quot;^-A&quot;                         </span><br><span class="line">-A KUBE-SEP-HDIVJIPCJU2JBJVX -s 10.244.1.11/32 -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-HDIVJIPCJU2JBJVX -p tcp -m comment --comment &quot;default/demoapp-svc:http&quot; -m tcp -j DNAT --to-destination 10.244.1.11:80</span><br></pre></td></tr></table></figure><p>Pod对象也可能会向自己所属的Service对象发起访问请求，而且该请求经由OUTPUT链到达KUBE-SERVICES链后存在被调度回当前Pod对象的可能性。第一条规则就是为该类报文添加专有的流量标记。第二条规则将接收到的所有流量进行目标地址转换（DNAT），新的目标为10.244.1.11:80，它对应Kubernetes集群上由Service对象demoapp-svc匹配到的一个特定Pod对象。<br>不难猜测，特定节点（例如前面示例中的k8s-node01）接收到的请求报文的源地址为Pod网络中的IP地地址的，必然源自该节点或节点上的Pod对象。它们的IP地址位于该节点的PodCIDR之中，这些流量离开节点之前无须进行源地址转换，因而目标端点直接响应给客户端IP就能够正确到达请求方。而请求报文的源地址并非为Pod网络中的IP地址的，例如请求方为该节点上的某独立容器，则Service必须在其离开本节点之前，将请求报文的源地址转换为该节点上报文离开时要经由接口的IP地址（例如cni0上的10.244.1.0），以确保响应报文可正确回送至该节点，并由该节点响应给相应的客户端，由内置链POSTROUTING所调用的自定义链KUBE-POSTROUTING上的规则便用于实现此类功能。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-POSTROUTING | grep &quot;^-A&quot;</span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>由此可见，对于集群内部的后端端点来说，它们收到的请求报文的源地址，要么是Pod的IP地址，要么是节点IP地址，因而直接发送响应报文给请求方即可。但那些本身并非源自Pod或节点的请求的响应报文，还需要由节点自动执行一次目标地址转换，以便把报文送达真正的请求方。注意<br>kube-proxy也支持在iptables代理模型上使用masquerade all，从而对通过ClusterIP地址访问的所有请求进行源地址转换，但在大多数场景中，这都不是必要的选择。2. NodePort Service<br>相较于ClusterIP类型来说，所有发往NodePort类型的Service对象的请求流量的目标IP和端口分别是节点IP和NodePort，这类报文无法由KUBE-SERVICES自定义链上那些基于Service IP和Service Port定义的流量匹配规则所匹配，但会由该自定义链上的最后一条规则转给KUBE-NODEPORTS自定义链。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | tail -n 1</span><br><span class="line">-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br></pre></td></tr></table></figure><p>KUBE-NODEPORTS链以类似ClusterIP Service拦截规则的方式定义了NodePort Service对象的拦截规则，其中每个Service对象包含两条规则定义。对于所有发往该Service（目标IP为该NodeIP，目标端口为NodePort）的请求报文：前一条规则为发往该Service对象的所有请求报文，基于KUBE-MARQ-MASK自定义链中的规则打上特有的防火墙标记；后一条规则负责将这些报文转至专用的、以KUBE-SVC为前缀的自定义链。以前面创建的demoapp-nodeport-svc为例，它拥有以下两条iptables规则。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~#  iptables -t nat -S KUBE-NODEPORTS | grep &quot;default/demoapp-nodeport-svc&quot;</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-SVC-HCTPASJ7WVWOBYLM</span><br></pre></td></tr></table></figure><p>我们已经知道，Service对象的专用自定义链定义了一组调度规则，以调度发往该Service对象匹配的所有后端端点的相关流量，而其中的每一个后端端点又有自己专用的自定义链，用于对请求报文进行目标地址转换。另外，NodePort类型的Service为所有从NodePort进入的请求报文都打了特有防火墙标记，因此这些请求报文会按照POSTROUTING和KUBE-POSTROUTING链上的规则将源地址转换为该报文离开节点时所经由的接口的IP地址。这些处理步骤与ClusterIP类型的Service对象几乎完全相同。完整的处理流程如图7-12所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123134247463.png" alt="../img/image-20220123134247463"></p><p>对于集群内部的后端端点来说，它们收到的请求报文的源地址都是节点IP地址。以Flannel插件环境中10.244.1.0/24这个Pod CIDR为例，该IP地址可能是flannel.1接口上的10.244.1.0/24，也可能是cni0上的10.244.1.1/24。于是，后端端点会把报文响应给请求报文进入时的节点，再由该节点将目标地址转换为客户端IP后发送。<br>但是，对于将外部流量策略定义为Local的NodePort Service对象来说，由于流量报文不会在集群内跨节点转发，也就没有必要对请求报文进行SNAT操作，所以后端端点可以看到真实的客户端IP。它的具体处理流程如图7-13所示。<br><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123134329215.png" alt="../img/image-20220123134329215"></p><p>1）KUBE-SERVICES链把目标地址指向当前节点的报文，并转给KUBE-NODEPORTS处理。<br>2）对于一个Local策略的NodePort Service来说，KUBE-NODEPORTS会定义两条规则：前一条负则将源地址位于127.0.0.0/8网络的请求报文借助KUBE-MARK-MASQ打上0x4000防火墙标记；后一条则将报文转给该Service专用的KUBE-XLB-<HASH>自定义链。<br>3）KUBE-XLB-<HASH>自定义链将源自Pod网络（10.244.0.0/16）的请求报文以类似ClusterIP Service使用的方式进行处理，只转换请求报文目标地址；将源自当前节点所处的本地网络中的请求报文，按照常规的NodePort Service使用的方式进行处理，并同时转换源地址和目标地址；而将其他类型的请求报文直接转交给指定的本地后端端点处理，这也体现了本地流量策略的真正意义。<br>显然，若某节点自身未运行NodePort Service后端Pod，则本地策略类型的请求将得到失败的响应结果。提示<br>未配置外部IP地址的LoadBalancer类型的Service对象的工作方式与NodePort类型几乎完全相同，这里不再专门描述。3. External IP<br>在iptables中，外部IP表现为一种专有的Service访问入口。在KUBE-SERVICES自定义链上，每个外部IP都有3条相关的iptables规则：第1条用于为发往该外部IP的服务端口的请求流量，借助KUBE-MARK-MASQ自定义链打上特有的防火墙标记0x4000；第2条将这些请求流量中从非物理接口进入且源地址类型不是本地地址的流量，交由相应Service的专用自定义链进行流量分发；第3条用于将这些流量中目标地址类型是本地地址的请求报文，也交由相应Service的专用自定义链进行流量分发。具体的处理过程如图7-14所示。</HASH></HASH></p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123134355773.png" alt="../img/image-20220123134355773"></p><p>以前面定义的default/demoapp-externalip-svc中使用的外部IP 172.29.9.26为例，下面的命令可以在KUBE-SERVICES获取到相应的专用规则。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;172.29.9.26&quot;           </span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56</span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m addrtype --dst-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56</span><br></pre></td></tr></table></figure><p>由此可见，尽管外部IP需要结合ClusterIP、NodePort或LoadBalancer中任一类型的Service对象使用，但到达外部IP的服务请求流量却有着专用的拦截规则，请求报文也是交由相应Service的专用自定义链直接进行向后分发。</p><h3 id="7-4-2-ipvs代理模型"><a href="#7-4-2-ipvs代理模型" class="headerlink" title="7.4.2 ipvs代理模型"></a>7.4.2 ipvs代理模型</h3><p>由前一节的介绍可知，单个Service对象的iptables数量与后端端点的数量正相关，对于拥有较多Service对象和大规模Pod对象的Kubernetes集群，每个节点的内核上将充斥着大量的iptables规则。Service对象的变动会导致所有节点刷新netfilter上的iptables规则，而且每次的Service请求也都将经历多次的规则匹配检测和处理过程，这会占用节点上相当比例的系统资源。因此，iptables代理模型不适用于Service和Pod数量较多的集群。ipvs代理模型通过将流量匹配和分发功能配置为少量ipvs规则，有效降低了对系统资源的占用，从而能够承载更大规模的Kubernetes集群。</p><ol><li>调整kube-proxy代理模型<br>kube-proxy使用的代理模型定义在配置文件中，kubeadm部署的Kubernetes集群以DaemonSet控制器编排kube-proxy在每个节点上运行一个实例，配置文件则以kube-system名称空间中名为kube-proxy的ConfigMap对象的形式提供，默认使用iptables代理模型。在测试集群环境中，可直接使用kubectl edit configmaps/kube-proxy -n kube-system命令编辑该ConfigMap对象，将代理模型修改为ipvs，配置要点如下所示。</li></ol><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    apiVersion: kubeproxy.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="string">    bindAddress: 0.0.0.0</span></span><br><span class="line"><span class="string">    ……</span></span><br><span class="line"><span class="string">    iptables:                 # iptables配置细节</span></span><br><span class="line"><span class="string">      masqueradeAll: false    # 是否将通过ClusterIP访问的流量全部进行SNAT</span></span><br><span class="line"><span class="string">      masqueradeBit: null</span></span><br><span class="line"><span class="string">      minSyncPeriod: 0s</span></span><br><span class="line"><span class="string">      syncPeriod: 0s</span></span><br><span class="line"><span class="string">    ipvs:                     # ipvs配置细节</span></span><br><span class="line"><span class="string">      excludeCIDRs: null</span></span><br><span class="line"><span class="string">      minSyncPeriod: 0s</span></span><br><span class="line"><span class="string">      scheduler: &quot;&quot;           # 调度算法，默认为rr</span></span><br><span class="line"><span class="string">      strictARP: false</span></span><br><span class="line"><span class="string">      syncPeriod: 0s</span></span><br><span class="line"><span class="string">      tcpFinTimeout: 0s</span></span><br><span class="line"><span class="string">      tcpTimeout: 0s</span></span><br><span class="line"><span class="string">      udpTimeout: 0s</span></span><br><span class="line"><span class="string">    kind: KubeProxyConfiguration</span></span><br><span class="line"><span class="string">    metricsBindAddress: &quot;&quot;</span></span><br><span class="line"><span class="string">    mode: &quot;ipvs&quot;              # 代理模型，空值代表是iptables</span></span><br><span class="line"><span class="string">    nodePortAddresses: null</span></span><br><span class="line"><span class="string">    ……</span></span><br></pre></td></tr></table></figure><p>配置完成后，以灰度模式手动逐个或分批次删除kube-system名称空间中kube-proxy旧版本的Pod实例，全部更新完成后便切换到了ipvs代理模型。或者，在测试环境中，可以直接使用如下命令一次性完成所有实例的强制更新。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods -l k8s-app=kube-proxy -n kube-system</span></span><br></pre></td></tr></table></figure><p>提示<br>用于生产环境时，建议在部署Kubernetes集群时直接选定要使用的代理模型，或在集群部署完成后立即调整代理模型，而后再部署其他应用。</p><ol start="2"><li>ipvs代理模型下的Service资源<br>相较于iptables代理模型的复杂表示逻辑，ipvs的代理逻辑也较为简单，它仅有两个关键配置要素。首先，kube-proxy会在每个节点上创建一个名为kube-ipvs0的虚拟网络接口，并将集群上所有Service对象的ClusterIP和ExternalIP配置到该接口，使相应IP地址的流量都可被当前节点捕获。其次，kube-proxy会为每个Service生成相关的ipvs虚拟服务器（Virtual Server）定义，该虚拟服务器的真实服务器（Real Server）是由相应Service对象的后端端点组成，到达虚拟服务器VIP（虚拟IP地址）上的服务端口的请求流量由默认或指定的调度算法分发至相关的各真实服务器。<br>但kube-proxy对ClusterIP和NodePort类型Service对象的虚拟服务定义方式略有不同。对于每个ClusterIP类型的Service，kube-proxy仅针对Service_IP生成单个虚拟服务，协议和端口遵循Service的定义。以前面创建的demoapp-svc为例，它的ClusterIP是10.97.72.1，它的虚拟服务定义如下，这些可以通过ipvsadm -Ln命令在集群中的任意一个节点上获取。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep -A 3 &quot;10.97.72.1&quot;</span><br><span class="line">TCP  10.97.72.1:80 rr</span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.1.11:80              Masq    1      0          0</span>         </span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.2.7:80               Masq    1      0          0</span>         </span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.3.9:80               Masq    1      0          0</span></span><br></pre></td></tr></table></figure><p>而对于NodePort类型Service，kube-proxy会针对kube-ipvs0上的Service_IP:Service_Port，以及当前节点上的所有活动接口的主IP地址的NodePort各定义一个虚拟服务，下面的命令用于获取前面创建的NodePort类型Service对象的demoapp-nodeport-svc的相关虚拟服务的定义。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep -E &quot;31398|10.97.56.1&quot;</span><br><span class="line">TCP  172.29.9.11:31398 rr     # 节点IP</span><br><span class="line">TCP  10.97.56.1:80 rr         # ClusterIP</span><br><span class="line">TCP  10.244.1.0:31398 rr      # flannel.1接口IP</span><br><span class="line">TCP  10.244.1.1:31398 rr      # cni0接口IP</span><br><span class="line">TCP  127.0.0.1:31398 rr       # lo接口IP</span><br><span class="line">TCP  172.17.0.1:31398 rr      # docker0接口IP</span><br></pre></td></tr></table></figure><p>LoadBalancer类型Service的配置方式与NodePort类型相似，这里不再单独说明。另外，对于每个ExternalI，kube-proxy也会根据每个ExternalIP:Service_Port的组合生成一个虚拟服务，下面的命令及结果显示出前面创建的外部IP地址172.29.9.26相关的虚拟服务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep  &quot;172.29.9.26&quot;</span><br><span class="line">TCP  172.29.9.26:80 rr</span><br></pre></td></tr></table></figure><p>上述每种Service类型对应的所有虚拟服务内部同样都使用NAT模式进行请求代理，除了更加多样的调度算法选择外，它的转发性能并没有显著提升，不过因为避免了使用大量的iptables规则，所以系统资源开销显著降低。ipvs仅实现了代理和调度机制，Service资源中的报文过滤和源地址转换等功能，依旧要由iptables完成，但相应的规则数量较少且较为固定。</p><h2 id="7-5-Kubernetes服务发现"><a href="#7-5-Kubernetes服务发现" class="headerlink" title="7.5 Kubernetes服务发现"></a>7.5 Kubernetes服务发现</h2><p>Kubernetes系统上的Service为Pod中的服务类应用提供了一个固定的访问入口，但Pod客户端中的应用还需要借助服务发现机制获取特定服务的IP和端口。</p><h3 id="7-5-1-服务发现概述"><a href="#7-5-1-服务发现概述" class="headerlink" title="7.5.1  服务发现概述"></a>7.5.1  服务发现概述</h3><p>服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），由服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费方周期性地从注册中心获取服务提供者的最新位置信息，从而“发现”要访问的目标服务资源。复杂的服务发现机制还会让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。<br>根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。</p><ul><li>客户端发现：由客户端到服务注册中心发现其依赖的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。</li><li>服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。<br>服务注册中心是服务发现得以落地的核心组件。</li></ul><p>在传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统，它们可提供基本的数据存储功能，但距离实现完整的服务发现机制还有大量的二次开发任务需要完成。而且，它们更注重数据一致性而不得不弱化可用性（分布式系统的CAP理论），这背离了微服务发现场景中更注重服务可用性的需求。<br>Netflix的Eureka是专用于服务发现的分布式系统，遵从“存在少量的错误数据，总比完全不可用要好”的设计原则，服务发现和可用性是其核心目标，能够在多种故障期间保持服务发现和服务注册的功能。另一个同级别的实现是Consul，它于服务发现的基础功能之外还提供了多数据中心的部署等一众出色的特性。<br>尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目结合古老的DNS技术和时髦的Go语言、Raft算法，并构建于etcd存储系统之上，为Kubernetes系统实现了一种独特且实用的服务发现机制。Kubernetes在v1.3版本引入的KubeDNS由kubedns、dnsmasq和sidecar这3个部分组合而成。第一个部分包含kubedns和skydns两个组件，前者负责将Service和Endpoint转换为SkyDNS可以理解的格式；第二部分用于增强解析功能；第三部分为前两者添加健康状态检查机制，因而我们可以把KubeDNS视为SkyDNS的增强版。<br>而另一个基于DNS较新的服务发现项目是由CNCF孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链实现所有功能，也允许用户自行开发和添加必要的插件，但所有功能运行在单个容器之中。另外，CoreDNS使用Caddy作为底层的Web Server，可以支持以UDP、TLS、gRPC和HTTPS等方式对外提供DNS服务。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。</p><h3 id="7-5-2-基于环境变量的服务发现"><a href="#7-5-2-基于环境变量的服务发现" class="headerlink" title="7.5.2 基于环境变量的服务发现"></a>7.5.2 基于环境变量的服务发现</h3><p>创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的Link兼容的变量。</p><h4 id="（1）Kubernetes-Service环境变量"><a href="#（1）Kubernetes-Service环境变量" class="headerlink" title="（1）Kubernetes Service环境变量"></a>（1）Kubernetes Service环境变量</h4><p>Kubernetes为每个Service资源生成包括以下形式的环境变量在内的一系列环境变量，在同一名称空间中创建的Pod对象都会自动拥有这些变量：</p><ul><li>{SVCNAME}SERVICE_HOST</li><li>{SVCNAME}_SERVICE_PORT注意<br>如果SVCNAME中使用了连接线，Kubernetes会在定义环境变量时将其转换为下划线。</li></ul><h4 id="（2）Docker-Link形式的环境变量"><a href="#（2）Docker-Link形式的环境变量" class="headerlink" title="（2）Docker Link形式的环境变量"></a>（2）Docker Link形式的环境变量</h4><p>Docker使用–link选项实现容器连接时所设置的环境变量形式，具体使用方式请参考Docker的相关文档。在创建Pod对象时，Kubernetes也会把与此形式兼容的一系列环境变量注入Pod对象中。<br>例如，在Service资源demoapp-svc创建后创建的Pod对象中查看可用的环境变量，其中以DEMOAPP_SVC_SERVICE开头的为Kubernetes Service环境变量，名称中不包含SERVICE字符串的环境变量为Docker Link形式的环境变量。下面的命令创建了一个临时Pod对象，并在其命令行列出与demoapp-svc的相关环境变量。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-pod --../img/image=ikubernetes/admin-toolbox:v1.0 -it --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@client-pod /]# printenv | grep DEMOAPP_SVC</span><br><span class="line">DEMOAPP_SVC_SERVICE_PORT_HTTP=80</span><br><span class="line">DEMOAPP_SVC_SERVICE_HOST=10.97.72.1</span><br><span class="line">DEMOAPP_SVC_SERVICE_PORT=80</span><br><span class="line">DEMOAPP_SVC_PORT=tcp://10.97.72.1:80</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_ADDR=10.97.72.1</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_PORT=80</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_PROTO=tcp</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP=tcp://10.97.72.1:80</span><br></pre></td></tr></table></figure><p>基于环境变量的服务发现功能简单、易用，但存在一定局限，例如只有那些与新建Pod对象在同一名称空间中且事先存在的Service对象的信息才会以环境变量形式注入，而那些不在同一名称空间，或者在Pod资源创建之后才创建的Service对象的相关环境变量则不会被添加。</p><h3 id="7-5-3-基于DNS的服务发现"><a href="#7-5-3-基于DNS的服务发现" class="headerlink" title="7.5.3 基于DNS的服务发现"></a>7.5.3 基于DNS的服务发现</h3><p>名称解析和服务发现是Kubernetes系统许多功能得以实现的基础服务，ClusterDNS通常是集群安装完成后应该立即部署的附加组件。Kubernetes集群上的每个Service资源对象在创建时都会被自动指派一个遵循&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的名称，并由ClusterDNS为该名称自动生成资源记录，service、ns和zone分别代表服务的名称、名称空间的名称和集群的域名。例如demoapp-svc的DNS名称为demoapp-svc.default.svc.cluster.local.，其中cluster.local.是未明确指定域名后缀的集群默认使用的域名。<br>无论使用kubeDNS还是CoreDNS，它们提供的基于DNS的服务发现解决方案都会负责为该DNS名称解析相应的资源记录类型以实现服务发现。以拥有ClusterIP的多种Service资源类型（ClusterIP、NodePort和LoadBalancer）为例，每个Service对象都会具有以下3个类型的DNS资源记录。</p><ul><li>1）根据ClusterIP的地址类型，为IPv4生成A记录，为IPv6生成AAAA记录。<ul><li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;cluster-ip&gt;</li><li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;cluster-ip&gt;</li></ul></li><li>2）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。<ul><li>_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.&lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li></ul></li><li>3）对于每个给定的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4）都要生成PTR记录，它们各自的格式如下所示：<ul><li>&lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li><li>h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.<br>例如，</li></ul></li></ul><p>前面在default名称空间中创建的Service对象demoapp-svc的地址为10.97.72.1，且为TCP协议的80端口取名http，对于默认的cluster.local域名来说，它会拥有如下3个DNS资源记录。</p><ul><li>A记录：demoapp-svc.default.svc.cluster.local. 30 IN A 10.97.72.1</li><li>SRV记录：_http._tcp.demoapp-svc.default.svc.cluster.local. 30 IN SRV 0 100 80 demoapp- svc.default.svc.cluster.local.</li><li>PTR记录：1.72.97.10.in-addr.arpa. 30 IN PTR demoapp-svc.default.svc.cluster.local。</li></ul><p>kubelet会为创建的每一个容器在/etc/resolv.conf配置文件中生成DNS查询客户端依赖的必要配置，相关的配置信息源自kubelet的配置参数。各容器的DNS服务器由clusterDNS参数的值设定，它的取值为kube-system名称空间中的Service对象kube-dns的ClusterIP，默认为10.96.0.10，而DNS搜索域的值由clusterDomain参数的值设定，若部署Kubernetes集群时未特别指定，其值将为cluster.local、svc.cluster.local和NAMESPACENAME.svc.cluster.local。下面的示例取自集群上一个随机选择的Pod中的容器。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p>上述search参数中指定的DNS各搜索域，是以次序指定的几个域名后缀，它们各自的域名如下所示。</p><ul><li>&lt;ns&gt;.svc.&lt;zone&gt;：附带有特定名称空间的域名，例如default.svc.cluster.local。</li><li>svc. &lt;zone&gt;：附带了Kubernetes标识Service专用子域svc的域名，例如svc.cluster.local。</li><li>&lt;zone&gt;：集群本地域名，例如cluster.local。</li></ul><p>各容器能够直接向集群上的ClusterDNS发起服务名称和端口名称解析请求完成服务发现，各名称也支持短格式，由搜索域自动补全相关的后缀。我们可以在Kubernetes集群上通过任意一个有nslookup等DNS测试工具的容器进行测试。下面基于此前创建专用于测试的客户端Pod对象client-pod的交互式接口完成后续测试操作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client-pod -- /bin/sh</span></span><br><span class="line">[root@client-pod /]#</span><br></pre></td></tr></table></figure><p>接下来便可以进行名称解析测试。例如，下面的命令用于请求同一名称空间（default）中的服务名称demoapp-svc的解析结果，并获得了正确的返回值。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=A demoapp-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   demoapp-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.97.72.1</span><br></pre></td></tr></table></figure><p>ClusterDNS解析demoapp-svc服务名称的搜索次序依次是default.svc.cluster.local、svc.cluster.local和cluster.local，因此基于DNS的服务发现不受Service资源所在名称空间和创建时间的限制。上面的解析结果也正是默认的default名称空间中创建的demoapp-svc服务的IP地址。<br>SRV记录中的端口名称的格式_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，同样可使用短格式名称。下面的命令用于请求解析demoapp-svc上的http端口，它返回的结果为80。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod ~]# nslookup -query=SRV _http._tcp.demoapp-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">_http._tcp.demoapp-svc.default.svc.cluster.local        service = 0 100 80 demoapp-svc.default.svc.cluster.local.</span><br></pre></td></tr></table></figure><p>请求解析其他名称空间中的Service对象名称时需要明确指定服务名称和名称空间，下面以kube-dns.kube-system为例进行解析请求。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=A kube-dns.kube-system</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   kube-dns.kube-system.svc.cluster.local</span><br><span class="line">Address: 10.96.0.10</span><br></pre></td></tr></table></figure><p>端口名称解析时同样需要指定Service名称及其所在的名称空间，下面的命令用于请求解析kube-dns.kube-system上的metrics端口，它返回了9153的端口号。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=SRV _metrics._tcp.kube-dns.kube-system</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">_metrics._tcp.kube-dns.kube-system.svc.cluster.local    service = 0 100 9153 kube-dns.kube-system.svc.cluster.local.</span><br></pre></td></tr></table></figure><p>为了减少搜索次数，无论是否处于同一名称空间，客户端都可以直接使用FQDN格式的名称解析Service名称和端口名称，这也是在某应用的配置文件中引用其他服务时建议遵循的方式。</p><h3 id="7-5-4-Pod的DNS解析策略与配置"><a href="#7-5-4-Pod的DNS解析策略与配置" class="headerlink" title="7.5.4 Pod的DNS解析策略与配置"></a>7.5.4 Pod的DNS解析策略与配置</h3><p>Kubernetes还支持在单个Pod资源规范上自定义DNS解析策略和配置，它们分别使用spec.dnsPolicy和spec.dnsConfig进行定义，并组合生效。目前，Kubernetes支持如下DNS解析策略，它们定义在spec.dnsPolicy字段上。</p><ul><li>Default：从运行所在的节点继承DNS名称解析相关的配置。</li><li>ClusterFirst：在集群DNS服务器上解析集群域内的名称，其他域名的解析则交由从节点继承而来的上游名称服务器。</li><li>ClusterFirstWithHostNet：专用于在设置了hostNetwork的Pod对象上使用的ClusterFirst策略，任何配置了hostNetwork的Pod对象都应该显式使用该策略。</li><li>None：用于忽略Kubernetes集群的默认设定，而仅使用由dnsConfig自定义的配置。<br>Pod资源的自定义DNS配置要通过嵌套在spec.dnsConfig字段中的如下几个字段进行，它们的最终生效结果要结合dnsPolicy的定义生成。</li><li>nameservers &lt;[]string&gt;：DNS名称服务器列表，它附加在由dnsPolicy生成的DNS名称服务器之后。</li><li>searches &lt;[]string&gt;：DNS名称解析时的搜索域，它附加在dnsPolicy生成的搜索域之后。</li><li>options &lt;[]Object&gt;：DNS解析选项列表，它将会同dnsPolicy生成的解析选项合并成最终生效的定义。<br>下面配置清单示例（pod-with-dnspolicy.yaml）中定义的Pod资源完全使用自定义的配置，它通过将dnsPolicy设置为None而拒绝从节点继承DNS配置信息，并在dnsConfig中自定义了要使用的DNS服务、搜索域和DNS选项。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-with-dnspolicy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">dnsConfig:</span></span><br><span class="line">    <span class="attr">nameservers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">223.6</span><span class="number">.6</span><span class="number">.6</span></span><br><span class="line">    <span class="attr">searches:</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="string">svc.cluster.local</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cluster.local</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ilinux.io</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;5&quot;</span></span><br></pre></td></tr></table></figure><p>将上述配置清单中定义的Pod资源创建到集群之上，它最终会生成类似如下内容的/etc/resolv.conf配置文件。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">nameserver 223.5.5.5</span><br><span class="line">nameserver 223.6.6.6</span><br><span class="line">search svc.cluster.local cluster.local ilinux.io</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure><p>上面配置中的搜索域要求，即便是客户端与目标服务位于同一名称空间，也要求在短格式的服务名称上显式指定其所处的名称空间。感兴趣的读者可自行测试其效果。</p><h3 id="7-5-5-配置CoreDNS"><a href="#7-5-5-配置CoreDNS" class="headerlink" title="7.5.5 配置CoreDNS"></a>7.5.5 配置CoreDNS</h3><p>CoreDNS是高度模块化的DNS服务器，几乎全部功能均由可插拔的插件实现。CoreDNS调用的插件及相关的配置定义在称为Corefile的配置文件中。CoreDNS主要用于定义各服务器监听地址和端口、授权解析的区域以及加载的插件等，配置格式如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZONE:[PORT] &#123;</span><br><span class="line">    [PLUGIN]...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>参数说明如下。</p><ul><li>ZONE：定义该服务器授权解析的区域，它监听由PORT指定的端口。</li><li>PLUGIN：定义要加载的插件，每个插件可能存在一系列属性，而每个属性还可能存在可配置的参数。<br>由kubeadm在部署Kubernetes集群时自动部署的CoreDNS的Corefile存储为kube-system名称空间中名为coredns的ConfigMap对象，定义了一个监听53号端口授权解析根区域的服务器，详细的配置信息及各插件的简单说明如下所示。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    .:53 &#123;</span></span><br><span class="line"><span class="string">        errors  # 将错误日志发往标准输出stdout</span></span><br><span class="line"><span class="string">        health &#123;  </span></span><br><span class="line"><span class="string">           lameduck 5s</span></span><br><span class="line"><span class="string">        &#125;       # 通过http://localhost:8080/health报告健康状态</span></span><br><span class="line"><span class="string">        ready   # 待所有插件就绪后通过8181端口响应“200 OK”以报告就绪状态</span></span><br><span class="line"><span class="string">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span></span><br><span class="line"><span class="string">           pods insecure</span></span><br><span class="line"><span class="string">           fallthrough in-addr.arpa ip6.arpa</span></span><br><span class="line"><span class="string">           ttl 30</span></span><br><span class="line"><span class="string">        &#125;       # Kubernetes系统的本地区域及专用的名称解析配置</span></span><br><span class="line"><span class="string">        prometheus :9153  # 通过http://localhost:9153/metrics输出指标数据</span></span><br><span class="line"><span class="string">        forward . /etc/resolv.conf  # 非Kubernetes本地域名的解析转发逻辑</span></span><br><span class="line"><span class="string">        cache 30     # 缓存时长</span></span><br><span class="line"><span class="string">        loop         # 探测转发循环并终止其过程</span></span><br><span class="line"><span class="string">        reload       # Corefile内容改变时自动重载配置信息</span></span><br><span class="line"><span class="string">        loadbalance  # A、AAAA或MX记录的负载均衡器，使用round-robin算法</span></span><br><span class="line"><span class="string">    &#125;</span></span><br></pre></td></tr></table></figure><p>在该配置文件中，专用于Kubernetes系统上的名称解析服务由名为kubernetes的插件进行定义，该插件负责处理指定的权威区域中的所有查询，例如上面示例中的正向解析区域cluster.local，以及反向解析区域in-addr.arpa和ip6.arpa。该插件支持多个配置参数，例如endpoint、tls、kubeconfig、namespaces、labels、pods、ttl和fallthrough等，上面示例中用到的3个参数的功能如下。</p><ul><li>1）pods POD-MODE：设置用于处理基于Pod IP地址的A记录的工作模式，以便在直接同Pod建立SSL通信时验证证书信息；默认值为disabled，表示不处理Pod请求，总是响应NXDOMAIN；在其他可用值中，insecure表示直接响应A记录而无须向Kubernetes进行校验，目标在于兼容kube-dns；而verified表示仅在指定的名称空间中存在一个与A记录中的IP地址相匹配的Pod对象时才会将结果响应给客户端。</li><li>2）fallthrough [ZONES…]：常规情况下，该插件的权威区域解析结果为NXDOMAIN时即为最终结果，而该参数允许将该响应的请求继续转给后续的其他插件处理；省略指定目标区域时表示生效于所有区域，否则，将仅生效于指定的区域。</li><li>3）ttl：自定义响应结果的可缓存时长，默认为5秒，可用值范围为[0,3600]。<br>那些非由kubernetes插件所负责解析的本地匹配的名称，将由forward插件定义的方式转发给其他DNS服务器进行解析，示例中的配置表示将根区域的解析请求转发给主机配置文件/etc/resolv.conf中指定的DNS服务器进行。若要将请求直接转发给指定的DNS服务器，则将该文件路径替换为目标DNS服务器的IP地址即可，多个IP地址之间以空白字符分隔。例如，下面的配置示例表示将除了ilinux.io区域之外的其他请求转给223.5.5.5或223.6.6.6进行解析。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">. &#123;</span><br><span class="line">  forward . 223.5.5.5 223.6.6.6 &#123;</span><br><span class="line">    except ilinux.io</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>CoreDNS的各插件与相关的配置属性、参数及详细使用方式请参考官方文档中的介绍：<a href="https://coredns.io/plugins/%E3%80%82">https://coredns.io/plugins/。</a></p><h2 id="7-6-Headless-Service资源解析"><a href="#7-6-Headless-Service资源解析" class="headerlink" title="7.6 Headless Service资源解析"></a>7.6 Headless Service资源解析</h2><p>常规的ClusterIP、NodePort和LoadBalancer类型的Service对象可通过不同的入口来接收和分发客户端请求，且它们都拥有集群IP地址（ClusterIP）。然而，个别场景也可能不必或无须使用Service对象的负载均衡功能以及集群IP地址，而是借助ClusterDNS服务来代替实现这部分功能。Kubernetes把这类不具有ClusterIP的Service资源形象地称为Headless Service，该Service的请求流量无须kube-proxy处理，也不会有负载均衡和路由相关的iptables或ipvs规则。至于ClusterDNS如何自动配置Headless Service，则取决于Service标签选择器的定义。</p><ul><li>有标签选择器：由端点控制器自动创建与Service同名的Endpoint资源，而ClusterDNS则将Service名称的A记录直接解析为后端各端点的IP而非ClusterIP。</li><li>无标签选择器：ClusterDNS的配置分为两种情形，为ExternalName类型的服务（配置了spec.externalName字段）创建CNAME记录，而为与该Service同名的Endpoint对象上的每个端点创建一个A记录。<br>显然，ClusterDNS对待无标签选择器的第二种情形的Headless Service与对待有标签选择器的Headless </li></ul><p>Service的方式相同，区别仅在于相应的Endpoint资源是否由端点控制器基于标签选择器自动创建。通常，我们把无标签选择器的第一种情形（使用CNAME记录）的Headless Service当作一种独立的Service类型使用，即ExternalName Service，而将那些把Service名称使用A记录解析为端点IP地址的类型统一称为Headless Service。</p><h3 id="7-6-1-ExternalName-Service"><a href="#7-6-1-ExternalName-Service" class="headerlink" title="7.6.1 ExternalName Service"></a>7.6.1 ExternalName Service</h3><p>ExternalName Service是一种特殊类型的Service资源，它不需要使用标签选择器关联任何Pod对象，也无须定义任何端口或Endpoints，但必须要使用spec.externalName属性定义一个CNAME记录，用于返回真正提供服务的服务名称的别名。ClusterDNS会为这种类型的Service资源自动生成&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN CNAME &lt;extname&gt;.格式的DNS资源记录。<br>下面配置清单示例（externalname-redis-svc.yaml）中定义了一个名为externalname-redis-svc的Service资源，它使用DNS CNAME记录指向集群外部的redis.ik8s.io这一FQDN。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">externalname-redis-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ExternalName</span></span><br><span class="line">  <span class="attr">externalName:</span> <span class="string">redis.ik8s.io</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">6379</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">selector:</span> &#123;&#125;</span><br></pre></td></tr></table></figure><p>待Service资源externalname-redis-svc创建完成后，各Pod对象即可通过短格式或FQDN格式的Service名称访问相应的服务。ClusterDNS会把该名称以CNAME格式解析为.spec.externalName字段中的名称，而后通过DNS服务将其解析为相应主机的IP地址。我们可通过此前Pod对象client-pod对该名称进行解析测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client -- /bin/sh</span>                                            </span><br><span class="line">[root@client-pod /]#</span><br></pre></td></tr></table></figure><p>未指定解析类型的，nslookup命令会对解析得到的CNAME结果自动进行更进一步的解析。例如下面命令中，请求解析externalname-redis-svc.default.svc.cluster.local名称得到CNAME格式的结果redis.ik8s.io将被进一步解析为A记录格式的结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup externalname-redis-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">externalname-redis-svc.default.svc.cluster.local    canonical name = redis.ik8s.io.</span><br><span class="line">Name:   redis.ik8s.io</span><br><span class="line">Address: 1.2.3.4</span><br></pre></td></tr></table></figure><p>ExternalName用于通过DNS别名将外部服务发布到Kubernetes集群上，这类的DNS别名同本地服务的DNS名称具有相同的形式。因而Pod对象可像发现和访问集群内部服务一样来访问这些发布到集群之上的外部服务，这样隐藏了服务的位置信息，使得各工作负载能够以相同的方式调用本地和外部服务。等到了能够或者需要把该外部服务引入到Kubernetes集群上之时，管理员只需要修改相应ExternalName Service对象的类型为集群本地服务即可。</p><h3 id="7-6-2-Headless-Service"><a href="#7-6-2-Headless-Service" class="headerlink" title="7.6.2 Headless Service"></a>7.6.2 Headless Service</h3><p>除了为每个Service资源对象在创建时自动指派一个遵循<service>.<ns>.svc.<zone>格式的DNS名称，ClusterDNS还会为Headless Service中的每个端点指派一个遵循<hostname>. <service>.<ns>.svc.<zone>格式的DNS名称，因此，每个Headless Service资源对象的名称都会由ClusterDNS自动生成以下几种类型的资源记录。</zone></ns></service></hostname></zone></ns></service></p><ul><li>1）根据端点IP地址的类型，在Service名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。<ul><li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt;</li><li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt;</li></ul></li><li>2）根据端点IP地址的类型，在端点自身的hostname名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。<ul><li>&lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt;</li><li>&lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt;</li></ul></li><li>3）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。</li><li>_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li><li>4）对于每个给定的每个端点的主机名称的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4），都要生成PTR记录，它们各自的格式如下所示。<ul><li>&lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li><li>h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.<br>定义Service资源时，只需要将其ClusterIP字段的值显式设置为None即可将其定义为Headless类型。下面是一个Headless Service资源配置示例，它拥有标签选择器，因而能够自动创建同名的Endpoint资源。</li></ul></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-headless-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br></pre></td></tr></table></figure><p>将上面定义的Headless Service资源创建到集群上，我们从其资源详细描述中可以看出，demoapp-headless-svc没有ClusterIP，但因标签选择器能够匹配到Pod资源，因此它拥有端点记录。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f demoapp-headless-svc.yaml</span> </span><br><span class="line">service/demoapp-headless-svc created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe svc demoapp-headless-svc</span></span><br><span class="line">Name:              demoapp-headless-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       Selector:  app=demoapp</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              http  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.244.1.16:80,10.244.2.10:80,10.244.3.11:80</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>根据Headless Service的工作特性可知，它记录在ClusterDNS的A记录的相关解析结果是后端端点的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源。下面依然通过Pod对象client-pod的交互式接口进行测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client-pod -- /bin/sh</span></span><br><span class="line">[root@client-pod /]# nslookup -query=A demoapp-headless-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.3.11</span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.1.16</span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.2.10</span><br></pre></td></tr></table></figure><p>其解析结果正是Headless Service通过标签选择器关联到的所有Pod资源的IP地址。于是，客户端向此Service对象发起的请求将直接接入Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源是由DNS服务器接收到查询请求时以轮询方式返回的IP地址。<br>另一方面，每个IP地址的反向解析记录（PTR）对应的FQDN名称是相应端点所在主机的主机名称。对于Kubernetes上的容器来说，其所在主机的主机名是指Pod对象上的主机名称，它由Pod资源的spec.hostname字段和spec.subdomain组合定义，格式为&lt;hostname&gt;.subdomain&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，其中的&lt;subdomain&gt;可省略。若此两者都未定义，则&lt;hostname&gt;值取自IP地址，IP地址a.b.c.d对应的主机名为a-b-c-d，如下面命令的解析结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=PTR 10.244.3.11</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line">11.3.244.10.in-addr.arpa        name = 10-244-3-11.demoapp-headless-svc.default.svc.cluster.local.</span><br></pre></td></tr></table></figure><p>StatefulSet控制器对象是Headless Service资源的一个典型应用场景，相关话题将会在第8章中详细描</p><h2 id="7-7-本章小结"><a href="#7-7-本章小结" class="headerlink" title="7.7 本章小结"></a>7.7 本章小结</h2><p>本章重点讲解了Kubernetes的Service资源基础概念、类型、实现机制及其发布方式等话题，并介绍了服务发现及Headless Service。<br>▪Service资源通过标签选择器为一组任务负载创建一个统一的访问入口，它把客户端请求代理调度至后端各端点。<br>▪Service支持userspace、iptables和ipvs代理模型，iptables模式更为成熟稳定，而ipvs则在有大规模Service的场景中有着更好的性能表现。<br>▪ClusterIP是最基础的Service类型，它仅适用于集群内通信，NodePort和LoadBalancer能够将服务发布到集群外部；外部IP能够与这3种类型的Service组合使用，从而开放特定的IP接入外部流量。<br>▪Endpoint和EndpointSlice用于跟踪端点资源，并将端点信息提供给Service等。<br>▪Headless Service是没有ClusterIP的Service资源类型，它要么结合externalName以CNAME资源记录的形式映射至其他服务，要么以A记录或AAAA记录的形式解析至端点IP地址。</p><h1 id="第5章-存储卷与数据持久化"><a href="#第5章-存储卷与数据持久化" class="headerlink" title="第5章 存储卷与数据持久化"></a>第5章 存储卷与数据持久化</h1><h2 id="5-1-存储卷基础"><a href="#5-1-存储卷基础" class="headerlink" title="5.1 存储卷基础"></a>5.1 存储卷基础</h2><p>Pod本身有生命周期，其应用容器及生成的数据自身均无法独立于该生命周期之外持久存在，并且同一Pod中的容器可共享PID、Network、IPC和UTS名称空间，但Mount和USER名称空间却各自独立，因而跨容器的进程彼此间默认无法基于共享的存储空间交换文件或数据。因而，借助特定的存储机制甚至是独立于Pod生命周期的存储设备完成数据持久化也是必然之需。</p><h3 id="5-1-1-存储卷概述"><a href="#5-1-1-存储卷概述" class="headerlink" title="5.1.1 存储卷概述"></a>5.1.1 存储卷概述</h3><p>存储卷是定义在Pod资源之上可被其内部的所有容器挂载的共享目录，该目录关联至宿主机或某外部的存储设备之上的存储空间，可由Pod内的多个容器同时挂载使用。Pod存储卷独立于容器自身的文件系统，因而也独立于容器的生命周期，它存储的数据可于容器重启或重建后继续使用。图5-1展示了Pod容器与存储卷之间的关系。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123092524003.png" alt="../img/image-20220123092524003"></p><p>每个工作节点基于本地内存或目录向Pod提供存储空间，也能够使用借助驱动程序挂载的网络文件系统或附加的块设备，例如使用挂载至本地某路径上的NFS文件系统等。Kubernetes系统具体支持的存储卷类型要取决于存储卷插件的内置定义，如图5-2所示，不过Kubernetes也支持管理员基于扩展接口配置使用第三方存储。另外，Kubernetes甚至还支持一些有着特殊功用的存储卷，例如将外部信息投射至Pod之中的ConfigMap、Secret和Downward API等。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123092541384.png" alt="../img/image-20220123092541384"></p><p>存储卷并非Kubernetes上一种独立的API资源类型，它隶属于Pod资源，且与所属的特定Pod对象有着相同的生命周期，因而通过API Server管理声明了存储卷资源的Pod对象时也会相应触发存储卷的管理操作。在具体的执行过程中，首选由调度器将该Pod对象绑到一个工作节点之上，若该Pod定义存储卷尚未被挂载，Controller Manager中的AD控制器（Attach/Detach Controller）会先借助相应的存储卷插件把远程的存储设备附加到该目标节点，而由内置在kubelet中的Pod管理器（Pod Manager）触发本地的存储卷操作实现，它借助存储卷管理器（Volume Manager）调用存储卷插件进行关联并驱动相应存储服务，并完成设备的挂载、格式化和卸载等操作。存储卷独立于Pod对象中容器的生命周期，从而使得容器重启或更新之后数据依然可用，但删除Pod对象时也必将删除其存储卷。<br>Kubernetes系统内置了多种类型的存储卷插件，因而能够直接支持多种类型存储系统（即存储服务方），例如CephFS、NFS、RBD、iscsi和vSphereVolume等。定义Pod资源时，用户可在其spec.volumes字段中嵌套配置选定的存储卷插件，并结合相应的存储服务来使用特定类型的存储卷，甚至使用CS或flexVolume存储卷插件来扩展支持更多的存储服务系统。<br>对Pod对象来说，卷类型主要是为关联适配的存储系统时提供相关的配置参数。例如，关联节点本地的存储目录与关联GlusterFS存储系统所需要的配置参数差异巨大，因此指定了存储卷类型也就限定了其关联到的后端存储设备。目前，Kubernetes支持的存储卷可简单归为以下类别，它们也各自有着不少的实现插件。</p><ul><li>1）临时存储卷：emptyDir。</li><li>2）本地存储卷：hostPath和local。</li><li>3）网络存储卷：<ul><li>云存储——awsElasticBlockStore、gcePersistentDisk、azureDisk和azureFile。</li><li>网络文件系统——NFS、GlusterFS、CephFS和Cinder。</li><li>网络块设备——iscsi、FC、RBD和vSphereVolume。</li><li>网络存储平台——Quobyte、PortworxVolume、StorageOS和ScaleIO。</li></ul></li><li>4）特殊存储卷：Secret、ConfigMap、DownwardAPI和Projected。</li><li>5）扩展支持第三方存储的存储接口（Out-of-Tree卷插件）：CSI和FlexVolume。</li></ul><p><font color="red">Kubernetes内置提供的存储卷插件可归类为In-Tree类型，它们同Kubernetes源代码一同发布和迭代，而由存储服务商借助于CSI或FlexVolume接口扩展的独立于Kubernetes代码的存储卷插件则统称为Out-Of-Tree类型</font>，集群管理员也可根据需要创建自定义的扩展插件，目前CSI是较为推荐的扩展接口，如图5-3所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123092624623.png" alt="../img/image-20220123092624623"></p><p>尽管网络存储基本都具有持久存储能力，但它们都要求Pod资源清单的编写人员了解可用的真实网络存储的基础结构，并且能够准确配置用到的每一种存储服务。例如，要创建基于Ceph RBD的存储卷，用户必须要了解Ceph集群服务器（尤其是Monitor服务器）的地址，并且能够理解接入Ceph集群的必要配置及其意义。</p><h3 id="5-1-2-配置Pod存储卷"><a href="#5-1-2-配置Pod存储卷" class="headerlink" title="5.1.2 配置Pod存储卷"></a>5.1.2 配置Pod存储卷</h3><p>在Pod中定义使用存储卷的配置由两部分组成：一部分通过.spec.volumes字段定义在Pod之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储系统的访问接口进行定义；另一部分是嵌套定义在容器的volumeMounts字段上的存储卷挂载列表，它只能挂载当前Pod对象中定义的存储卷。不过，定义了存储卷的Pod内的容器也可以选择不挂载任何存储卷。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 存储卷名称标识，仅可使用DNS标签格式的字符，在当前Pod中必须唯一</span></span><br><span class="line">     <span class="string">VOL_TYPE</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 存储卷插件及具体的目标存储系统的相关配置</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># 要挂载的存储卷的名称，必须匹配存储卷列表中某项的定义</span></span><br><span class="line">      <span class="string">mountPath</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 容器文件系统上的挂载点路径</span></span><br><span class="line">      <span class="string">readOnly</span> <span class="string">&lt;boolean&gt;</span>        <span class="comment"># 是否挂载为只读模式，默认为“否”</span></span><br><span class="line">      <span class="string">subPath</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 挂载存储卷上的一个子目录至指定的挂载点</span></span><br><span class="line">      <span class="string">subPathExpr</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 挂载由指定模式匹配到的存储卷的文件或目录至挂载点</span></span><br><span class="line">      <span class="string">mountPropagation</span> <span class="string">&lt;string&gt;</span> <span class="comment"># 挂载卷的传播模式</span></span><br></pre></td></tr></table></figure><p>Pod配置清单中的.spec.volumes字段的值是一个对象列表，每个列表项定义一个存储卷，它由存储卷名称（.spec.volumes.name &lt;String&gt;）和存储卷对象（.spec.volumes.VOL_TYPE &lt;Object&gt;）组成，其中VOL_TYPE是使用的存储卷类型名称，它的内嵌字段随类型的不同而不同，具体参数需要参阅Pod上各存储卷插件的相关文档说明。<br>定义好的存储卷可由当前Pod资源内的各容器进行挂载。Pod中仅有一个容器时，使用存储卷的目的通常在于数据持久化，以免重启时导致数据丢失，而只有多个容器挂载同一个存储卷时，“共享”才有了具体的意义。挂载卷的传播模式（mountPropagation）就是用于配置容器将其挂载卷上的数据变动传播给同一Pod中的其他容器，甚至是传播给同一个节点上的其他Pod的一个特性，该字段的可用值包括如下几项。</p><ul><li>None：该挂载卷不支持传播机制，当前容器不向其他容器或Pod传播自己的挂载操作，也不会感知主机后续在该挂载卷或其任何子目录上执行的挂载变动；此为默认值。</li><li>HostToContainer：主机向容器的单向传播，即当前容器能感知主机后续对该挂载卷或其任何子目录上执行的挂载变动。</li><li>Bidirectional：主机和容器间的双向传播，当前容器创建的存储卷挂载操作会传播给主机及使用了同一存储卷的所有Pod的所有容器，也能感知主机上后续对该挂载卷或其任何子目录上执行的挂载变动；该行为存在破坏主机操作系统的危险，因而仅可用于特权模式下的容器中。</li></ul><h2 id="5-2-临时存储卷"><a href="#5-2-临时存储卷" class="headerlink" title="5.2 临时存储卷"></a>5.2 临时存储卷</h2><p>Kubernetes支持的存储卷类型中，emptyDir存储卷的生命周期与其所属的Pod对象相同，它无法脱离Pod对象的生命周期提供数据存储功能，因此通常仅用于数据缓存或临时存储。不过，基于emptyDir构建的gitRepo存储卷可以在Pod对象的生命周期起始时，从相应的Git仓库中克隆相应的数据文件到底层的emptyDir中，也就使得它具有了一定意义上的持久性。</p><h3 id="5-2-1-emptyDir存储卷"><a href="#5-2-1-emptyDir存储卷" class="headerlink" title="5.2.1 emptyDir存储卷"></a>5.2.1 emptyDir存储卷</h3><p>emptyDir存储卷可以理解为Pod对象上的一个临时目录，类似于Docker上的“Docker挂载卷”，在Pod启动时被创建，而在Pod对象被移除时一并被删除。因此，emptyDir存储卷只能用于某些特殊场景中，例如同一Pod内的多个容器间的文件共享，或作为容器数据的临时存储目录用于数据缓存系统等。<br>emptyDir存储卷嵌套定义在.spec.volumes.emptyDir字段中，可用字段主要有两个。</p><ul><li>medium：此目录所在的存储介质的类型，可用值为default或Memory，默认为default，表示使用节点的默认存储介质；Memory表示使用基于RAM的临时文件系统tmpfs，总体可用空间受限于内存，但性能非常好，通常用于为容器中的应用提供缓存存储。</li><li>sizeLimit：当前存储卷的空间限额，默认值为nil，表示不限制；不过，在medium字段值为Memory时，建议务必定义此限额。<br>下面是一个使用了emptyDir存储卷的简单示例，它保存在volumes-emptydir-demo.yaml配置文件中。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-emptydir-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-downloader</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/admin-box</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /data/envoy.yaml https://raw.</span></span><br><span class="line"><span class="string">    githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/</span></span><br><span class="line"><span class="string">    master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">envoy</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/envoy</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">    <span class="attr">emptyDir:</span></span><br><span class="line">      <span class="attr">medium:</span> <span class="string">Memory</span></span><br><span class="line">      <span class="attr">sizeLimit:</span> <span class="string">16Mi</span></span><br></pre></td></tr></table></figure><p>在该示例清单中，为Pod对象定义了一个名为config-file-store的、基于emptyDir存储插件的存储卷。初始化容器将该存储卷挂载至/data目录后，下载envoy.yaml配置文件并保存于该挂载点目录下。主容器将该存储卷挂载至/etc/envoy目录，再通过自定义命令让容器应用在启动时加载的配置文件/etc/envoy/envoy.yaml上，如图5-4所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123092733861.png" alt="../img/image-20220123092733861"></p><p>Pod资源的详细信息中会显示存储卷的相关状态，包括其是否创建成功（Events字段中输出）、相关的类型及参数（Volumes字段中输出），以及容器中的挂载状态等信息（Containers字段中输出）。如下面的命令结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods volumes-emptydir-demo</span></span><br><span class="line">……</span><br><span class="line">Init Containers:</span><br><span class="line">  config-file-downloader:</span><br><span class="line">  ……</span><br><span class="line">    Mounts:</span><br><span class="line">      /data from config-file-store (rw)</span><br><span class="line">  ……</span><br><span class="line">Containers:</span><br><span class="line">  envoy:</span><br><span class="line">    Mounts:</span><br><span class="line">      /etc/envoy from config-file-store (ro)</span><br><span class="line">  ……</span><br><span class="line">Volumes:</span><br><span class="line">  config-file-store:</span><br><span class="line">    Type:       EmptyDir (a temporary directory that shares a pod&#x27;s lifetime)</span><br><span class="line">    Medium:     Memory</span><br><span class="line">    SizeLimit:  16Mi</span><br><span class="line">……</span><br></pre></td></tr></table></figure><p>为Envoy下载的配置文件中定义了一个监听所有可用IP地上TCP 80端口的Ingress侦听器，以及一个监听所有可用IP地址上TCP的9901端口的Admin接口，这与Envoy镜像中默认配置文件中的定义均有不同。下面命令的结果显示它吻合自定义配置文件的内容。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> volumes-emptydir-demo -- netstat -tnl</span></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       </span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      </span><br><span class="line">tcp        0      0 0.0.0.0:9901            0.0.0.0:*               LISTEN  </span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/volumes-emptydir-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl <span class="variable">$podIP</span>:9901/listeners</span></span><br><span class="line">listener_0::0.0.0.0:80</span><br></pre></td></tr></table></figure><p>emptyDir卷简单易用，但仅能用于临时存储。另外存在一些类型的存储卷构建在emptyDir之上，并额外提供了它所没有功能，例如将于下一节介绍的gitRepo存储卷。</p><h3 id="5-2-2-gitRepo存储卷"><a href="#5-2-2-gitRepo存储卷" class="headerlink" title="5.2.2 gitRepo存储卷"></a>5.2.2 gitRepo存储卷</h3><p>gitRepo存储卷可以看作是emptyDir存储卷的一种实际应用，使用该存储卷的Pod资源可以通过挂载目录访问指定的代码仓库中的数据。使用gitRepo存储卷的Pod资源在创建时，会首先创建一个空目录（emptyDir）并克隆（clone）一份指定的Git仓库中的数据至该目录，而后再创建容器并挂载该存储卷。<br>定义gitRepo类型的存储卷时，其可嵌套使用字段有如下3个。</p><ul><li>repository &lt;string&gt;：Git仓库的URL，必选字段。</li><li>directory &lt;string&gt;：目标目录名称，但名称中不能包含“..”字符；“.”表示将仓库中的数据直接克隆至存储卷映射的目录中，其他字符则表示将数据克隆至存储卷上以用户指定的字符串为名称的子目录中。</li><li>revision &lt;string&gt;：特定revision的提交哈希码。</li></ul><p><font color="red">注意:使用gitRepo存储卷的Pod资源运行的工作节点上必须安装有Git程序，否则克隆仓库的操作将无法完成。</font></p><p>下面的配置清单示例（volumes-gitrepo-demo.yaml）中的Pod资源在创建时，会先创建一个空目录，将指定的Git仓库<a href="https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%8B%E9%9A%86%E4%B8%80%E4%BB%BD%E7%9B%B4%E6%8E%A5%E4%BF%9D%E5%AD%98%E5%9C%A8%E6%AD%A4%E7%9B%AE%E5%BD%95%E4%B8%AD%EF%BC%8C%E8%80%8C%E5%90%8E%E5%B0%86%E6%AD%A4%E7%9B%AE%E5%BD%95%E5%88%9B%E5%BB%BA%E4%B8%BA%E5%AD%98%E5%82%A8%E5%8D%B7html%EF%BC%8C%E5%86%8D%E7%94%B1%E5%AE%B9%E5%99%A8nginx%E5%B0%86%E6%AD%A4%E5%AD%98%E5%82%A8%E5%8D%B7%E6%8C%82%E8%BD%BD%E5%88%B0/usr/share/nginx/html%E7%9B%AE%E5%BD%95%E4%B8%8A%E3%80%82">https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git中的数据克隆一份直接保存在此目录中，而后将此目录创建为存储卷html，再由容器nginx将此存储卷挂载到/usr/share/nginx/html目录上。</a></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-gitrepo-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">    <span class="attr">gitRepo:</span></span><br><span class="line">      <span class="attr">repository:</span> </span><br><span class="line">      <span class="string">https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git</span></span><br><span class="line">      <span class="attr">directory:</span> <span class="string">.</span></span><br><span class="line">      <span class="attr">revision:</span> <span class="string">&quot;master&quot;</span></span><br></pre></td></tr></table></figure><p>访问此Pod资源中的nginx服务，即可看到它来自Git仓库中的页面资源。不过，gitRepo存储卷在其创建完成后不会再与指定的仓库执行同步操作，这意味着在Pod资源运行期间，如果仓库中的数据发生了变化，gitRepo存储卷不会同步到这些内容。当然，此时可以为Pod资源创建一个Sidecar容器来执行此类的同步操作，尤其是数据来源于私有仓库时，通过Sidecar容器完成认证等必要步骤后再进行克隆操作就更为必要。<br>gitrRepo存储卷构建于emptyDir之上，其生命周期与Pod资源一样，故使用中不应在此类存储卷中保存由容器生成的重要数据。另外，gitRepo存储插件即将废弃，建议在初始化容器或Sidecar容器中运行git命令来完成相应的功能。</p><h2 id="5-3-hostPath存储卷"><a href="#5-3-hostPath存储卷" class="headerlink" title="5.3 hostPath存储卷"></a>5.3 hostPath存储卷</h2><p>hostPath存储卷插件是将工作节点上某文件系统的目录或文件关联到Pod上的一种存储卷类型，其数据具有同工作节点生命周期一样的持久性。hostPath存储卷使用的是工作节点本地的存储空间，所以仅适用于特定情况下的存储卷使用需求，例如将工作节点上的文件系统关联为Pod的存储卷，从而让容器访问节点文件系统上的数据，或者排布分布式存储系统的存储设备等。hostPath存储卷在运行有管理任务的系统级Pod资源，以及Pod资源需要访问节点上的文件时尤为有用。<br>配置hostPath存储卷的嵌套字段有两个：一个用于指定工作节点上的目录路径的必选字段path；另一个用于指定节点之上存储类型的type。hostPath支持使用的节点存储类型有如下几种。</p><ul><li>DirectoryOrCreate：指定的路径不存在时，自动将其创建为0755权限的空目录，属主和属组均为kubelet。</li><li>Directory：事先必须存在的目录路径。</li><li>FileOrCreate：指定的路径不存在时，自动将其创建为0644权限的空文件，属主和属组均为kubelet。</li><li>File：事先必须存在的文件路径。</li><li>Socket：事先必须存在的Socket文件路径。</li><li>CharDevice：事先必须存在的字符设备文件路径。</li><li>BlockDevice：事先必须存在的块设备文件路径。</li><li>“”：空字符串，默认配置，在关联hostPath存储卷之前不进行任何检查。</li></ul><p>这类Pod对象通常受控于DaemonSet类型的Pod控制器，它运行在集群中的每个工作节点上，负责收集工作节点上系统级的相关数据，因此使用hostPath存储卷也理所应当。然而，基于同一个模板创建Pod对象仍可能会因节点上文件的不同而存在着不同的行为，而且在节点上创建的文件或目录默认仅root用户可写，若期望容器内的进程拥有写权限，则需要将该容器运行于特权模式，不过这存在潜在的安全风险。<br>下面是定义在配置清单volumes-hostpath-demo.yaml中的Pod对象，容器中的filebeat进程负责收集工作节点及容器相关的日志信息并发往Redis服务器，它使用了3个hostPath类型的存储卷，第一个指向了宿主机的日志文件目录/var/logs，后面两个则与宿主机上的Docker运行时环境有关。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">vol-hostpath-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">filebeat</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">ikubernetes/filebeat:5.6.7-alpine</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REDIS_HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">redis.ilinux.io:6379</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">LOG_LEVEL</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">info</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/log</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/log</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/run/docker.sock</span></span><br></pre></td></tr></table></figure><p>上面配置清单中Pod对象的正确运行要依赖于REDIS_HOST和LOG_LEVEL环境变量，它们分别用于定义日志缓冲队列服务和日志级别。如果有可用的Redis服务器，我们就可通过环境变量REDIS_HOST将其对应的主机名或IP地址传递给Pod对象，待Pod对象准备好之后即可通过Redis服务器查看到由该Pod发送的日志信息。测试时，我们仅需要给REDIS_HOST环境变量传递一个任意值（例如清单中的redis.ilinux.io）便可直接创建Pod对象，只不过该Pod中容器的日志会报出无法解析指定主机名的错误，但这并不影响存储卷的配置和使用。<br>对于由Deployment或StatefulSet等一类控制器管控的、使用了hostPath存储卷的Pod对象来说，需要注意在基于资源可用状态的调度器调度Pod对象时，并不支持参考目标节点之上hostPath类型的存储卷，在Pod对象被重新调度至其他节点时，容器进程此前创建的文件或目录则大多不会存在。一个常用的解决办法是通过在Pod对象上使用nodeSelector或者nodeAffinity赋予该Pod对象指定要绑定到的具体节点来影响调度器的决策，但即便如此，管理员仍然不得不手动管理涉及的多个节点之上的目录，低效且易错。因此，hostPath存储卷虽然能持久保存数据，但对于由调度器按需调度的应用来说并不适用。</p><h2 id="5-4-网络存储卷"><a href="#5-4-网络存储卷" class="headerlink" title="5.4 网络存储卷"></a>5.4 网络存储卷</h2><p>5.4.1 NFS存储卷<br>Kubernetes的NFS存储卷用于关联某事先存在的NFS服务器上导出的存储空间到Pod对象中以供容器使用，该类型的存储卷在Pod对象终止后仅是被卸载而非被删除。而且，NFS是文件系统级共享服务，它支持同时存在的多路挂载请求，可由多个Pod对象同时关联使用。定义NFS存储卷时支持嵌套使用以下几个字段。</p><ul><li>server &lt;string&gt;：NFS服务器的IP地址或主机名，必选字段。</li><li>path &lt;string&gt;：NFS服务器导出（共享）的文件系统路径，必选字段。</li><li>readOnly &lt;boolean&gt;：是否以只读方式挂载，默认为false。</li></ul><p>Redis基于内存存储运行，数据持久化存储的需求通过周期性地将数据同步到主机磁盘之上完成，因此将Redis抽象为Pod对象部署运行于Kubernetes系统之上时，需要考虑节点级或网络级的持久化存储卷的支持，本示例就是以NFS存储卷为例，为Redis进程提供跨Pod对象生命周期的数据持久化功能。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-nfs-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">999</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">      <span class="attr">nfs:</span>   <span class="comment"># NFS存储卷插件</span></span><br><span class="line">        <span class="attr">server:</span> <span class="string">nfs.ilinux.io</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/data/redis</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>上面的示例定义在名为volumes-nfs-demo.yaml资源清单文件中，容器镜像文件redis:alpine默认会以redis用户（UID是999）运行redis-server进程，并将数据持久保存在容器文件系统上的/data目录中，因而需要确保UID为999的用户有权限读写该目录。与此对应，NFS服务器上用于该Pod对象的存储卷的导出目录（本示例中为/data/redis目录）也需要确保让UID为999的用户拥有读写权限，因而需要在nfs.ilinux.io服务器上创建该用户，将该用户设置为/data/redis目录的属主，或通过facl设置该用户拥有读写权限。<br>以Ubuntu Server18.04为例，在一个专用的主机（nfs.ilinux.io）上以root用户设定所需的NFS服务器的步骤如下。</p><ul><li>1）安装NFS Server程序包，Ubuntu 18.04上的程序包名为nfs-kernel-server。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">apt -y install nfs-kernel-server</span></span><br></pre></td></tr></table></figure><ul><li>2）设定基础环境，包括用户、数据目录及相应授权。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">mkdir</span> /data/redis</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash">useradd -u 999 redis</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">chown</span> redis /data/redis</span></span><br></pre></td></tr></table></figure><ul><li>3）编辑/etc/exports配置文件，填入类似如下内容：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/redis     172.29.0.0/16(rw,no_root_squash) 10.244.0.0/16(rw,no_root_squash)</span><br></pre></td></tr></table></figure><ul><li>4）启动NFS服务器：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">systemctl start nfs-server</span></span><br></pre></td></tr></table></figure><ul><li>5）在各工作节点安装NFS服务客户端程序包，Ubuntu 18.04上的程序包名为nfs-common。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">apt install -y nfs-common</span></span><br></pre></td></tr></table></figure><p>待上述步骤执行完成后，切换回Kubernetes集群可运行kubectl命令的主机之上，运行命令创建配置清单中的Pod对象：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-nfs-demo.yaml</span> </span><br><span class="line">pod/volumes-nfs-demo created</span><br></pre></td></tr></table></figure><p>资源创建完成后，可通过其命令客户端redis-cli创建测试数据，并手动触发其与存储系统同步，下面加粗部分的字体为要执行的Redis命令。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it volumes-nfs-demo -- redis-cli</span></span><br><span class="line">127.0.0.1:6379&gt; set mykey &quot;hello ilinux.io&quot;</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get mykey</span><br><span class="line">&quot;hello ilinux.io&quot;</span><br><span class="line">127.0.0.1:6379&gt; BGSAVE</span><br><span class="line">Background saving started</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br></pre></td></tr></table></figure><p>为了测试其数据持久化效果，下面先删除此前创建的Pod对象vol-nfs-pod，而后待重建该Pod对象后检测数据是否依然能够访问。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods/volumes-nfs-demo</span></span><br><span class="line">pod &quot;volumes-nfs-demo&quot; deleted</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-nfs-demo.yaml</span> </span><br><span class="line">pod/volumes-nfs-demo created</span><br></pre></td></tr></table></figure><p>待其重建完成后，通过再次创建的Pod资源的详细描述信息可以观察到它挂载使用NFS存储卷的相关状态，也可通过下面的命令来检查redis-server中是否还保存有此前存储的数据。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it volumes-nfs-demo -- redis-cli</span></span><br><span class="line">127.0.0.1:6379&gt; get mykey</span><br><span class="line">&quot;hello ilinux.io&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure><p>上面的命令结果显示出此前创建的键mykey及其数据在Pod对象删除并重建后依然存在，这表明删除Pod对象后，其关联的外部存储设备及数据并不会被一同删除，因而才具有了跨Pod生命周期的数据持久性。若需要在删除Pod后清除具有持久存储功能的存储设备上的数据，则需要用户或管理员通过存储系统的管理接口手动进行。</p><h3 id="5-4-2-RBD存储卷"><a href="#5-4-2-RBD存储卷" class="headerlink" title="5.4.2 RBD存储卷"></a>5.4.2 RBD存储卷</h3><p>Ceph是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和对象存储3种存储接口。它是个高度可配置的系统，并提供了一个命令行界面用于监视和控制其存储集群。Kubernetes支持通过RBD卷插件和CephFS卷插件，基于Ceph存储系统为Pod提供存储卷。要配置Pod对象使用RBD存储卷，需要事先满足以下前提条件。<br>▪存在某可用的Ceph RBD存储集群，否则需要创建一个。<br>▪在Ceph RBD集群中创建一个能满足Pod资源数据存储需要的存储映像。<br>▪在Kubernetes集群内的各节点上安装Ceph客户端程序包（ceph-common）。<br>定义RBD类型的存储卷时需要指定要连接的目标服务器和认证信息等配置，它们依赖如下几个可用的嵌套字段。<br>▪monitors &lt;[]string&gt;：Ceph存储监视器，逗号分隔的字符串列表；必选字段。<br>▪../img/image <string>：rados ../img/image（映像）的名称，必选字段。<br>▪pool <string>：Ceph存储池名称，默认为rbd。<br>▪user <string>：Ceph用户名，默认为admin。<br>▪keyring <string>：用户认证到Ceph集群时使用的keyring文件路径，默认为/etc/ceph/keyring。<br>▪secretRef <Object>：用户认证到Ceph集群时使用的保存有相应认证信息的Secret资源对象，该字段会覆盖由keyring字段提供的密钥信息。<br>▪readOnly <boolean>：是否以只读方式访问。<br>▪fsType：要挂载的存储卷的文件系统类型，至少应该是节点操作系统支持的文件系统，例如Ext4、xfs、NTFS等，默认为Ext4。<br>下面提供的RBD存储卷插件使用示例定义在volumes-rbd-demo.yaml配置清单文件中，它使用kube用户认证到Ceph集群中，并关联RDB存储池kube中的存储映像redis-img1为Pod对象volumes-rbd-demo的存储卷，由容器进程挂载至/data目录进行数据存取。</boolean></Object></string></string></string></string></p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-rbd-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">      <span class="attr">rbd:</span></span><br><span class="line">        <span class="attr">monitors:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.1:6789&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.2:6789&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.3:6789&#x27;</span></span><br><span class="line">        <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">        <span class="string">../img/image:</span> <span class="string">redis-img1</span></span><br><span class="line">        <span class="attr">fsType:</span> <span class="string">xfs</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">user:</span> <span class="string">kube</span></span><br><span class="line">        <span class="attr">keyring:</span> <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br></pre></td></tr></table></figure><p>RBD存储卷插件依赖Ceph存储集群作为存储系统，这里假设其监视器（MON）的地址为172.29.200.1、172.29.200.2和172.29.200.3，集群上的存储池kube中需要有事先创建好的存储映像redis-img1。客户端访问集群时要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了用户的keyring文件。该示例实现的逻辑架构如图5-5所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123095502153.png" alt="../img/image-20220123095502153"></p><p>为了完成示例中定义的资源的测试，需要事先完成如下几个步骤。<br>1）在Ceph集群上的kube存储池中创建用作Pod存储卷的RBD映像文件，并设置映像特性。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">rbd create --pool kube --size 1G redis-img1</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash">rbd feature <span class="built_in">disable</span> -p kube redis-img1 object-map fast-diff deep-flatten</span></span><br></pre></td></tr></table></figure><p>2）在Ceph集群上创建存储卷客户端账号并进行合理授权。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">ceph auth get-or-create client.kube mon <span class="string">&#x27;allow r&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=kube&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    -o /etc/ceph/ceph.client.kube.keyring</span></span><br></pre></td></tr></table></figure><p>3）在Kubernetes集群的各工作节点上执行如下命令安装Ceph客户端库。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">echo</span> deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \</span></span><br><span class="line"><span class="language-bash">    | <span class="built_in">tee</span> /etc/apt/sources.list.d/ceph.list</span> </span><br><span class="line"><span class="meta">~# </span><span class="language-bash">apt update &amp;&amp; apt install ceph-common</span></span><br></pre></td></tr></table></figure><p>4）在Ceph集群某节点上执行如下命令，以复制Ceph集群的配置文件及客户端认证使用的keyring文件到Kubernetes集群的各工作节点之上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="keyword">for</span> kubehost <span class="keyword">in</span> k8s-node01 k8s-node02 k8s-node03; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="language-bash">    scp -p /etc/ceph/&#123;ceph.conf,ceph.client.kube.keyring&#125; <span class="variable">$&#123;kubehost&#125;</span>:/etc/ceph/; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><p>待完成如上必要的准备步骤后，便可执行如下命令将前面定义在volumes-rbd-demo.yaml中的Pod资源创建在Kubernetes集群上进行测试。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-rbd-demo.yaml</span> </span><br><span class="line">pod/volumes-rbd-demo created</span><br></pre></td></tr></table></figure><p>随后从集群上的Pod对象volumes-rbd-demo的详细描述中获取存储的相关状态信息，确保其创建操作得以成功执行。下面是相关的存储卷信息示例。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Volumes:</span></span><br><span class="line">  <span class="attr">redis-rbd-vol:</span></span><br><span class="line">    <span class="attr">Type:</span>  <span class="string">RBD</span> <span class="string">(a</span> <span class="string">Rados</span> <span class="string">Block</span> <span class="string">Device</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">CephMonitors:</span>  [<span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span>]</span><br><span class="line">    <span class="attr">RBD../img/image:</span>      <span class="string">redis-img1</span></span><br><span class="line">    <span class="attr">FSType:</span>        <span class="string">xfs</span></span><br><span class="line">    <span class="attr">RBDPool:</span>       <span class="string">kube</span></span><br><span class="line">    <span class="attr">RadosUser:</span>     <span class="string">kube</span></span><br><span class="line">    <span class="attr">Keyring:</span>       <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br><span class="line">    <span class="attr">SecretRef:</span>     <span class="string">nil</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>      <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>删除Pod对象仅会解除它对RBD映像的引用而非级联删除它，因而RBD映像及数据将依然存在，除非管理员手动进行删除。我们可使用类似前一节测试Redis数据持久性的方式来测试本示例中的容器数据的持久能力，这里不再给出具体步骤。另外，实践中，应该把认证到Ceph集群上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用keyring字段引用相应用户的keyring文件。</p><h3 id="5-4-3-CephFS存储卷"><a href="#5-4-3-CephFS存储卷" class="headerlink" title="5.4.3 CephFS存储卷"></a>5.4.3 CephFS存储卷</h3><p>CephFS（Ceph文件系统）是在分布式对象存储RADOS之上构建的POSIX兼容的文件系统，它致力于为各种应用程序提供多用途、高可用和高性能的文件存储。CephFS将文件元数据和文件数据分别存储在各自专用的RADOS存储池中，其中MDS通过元数据子树分区等支持高吞吐量的工作负载，而数据则由客户端直接相关的存储池直接进行读写操作，其扩展能跟随底层RADOS存储的大小进行线性扩展。Kubernetes的CephFS存储卷插件以CephFS为存储方案为Pod提供存储卷，因而可受益于CephFS的存储扩展和性能优势。<br>CephFS存储卷插件嵌套定义于Pod资源的spec.volumes.cephfs字段中，它支持通过如下字段的定义接入到存储预配服务中。</p><ul><li>monitors &lt;[]string&gt;：Ceph存储监视器，为逗号分隔的字符串列表；必选字段。</li><li>user &lt;string&gt;：Ceph集群用户名，默认为admin。</li><li>secretFile &lt;string&gt;：用户认证到Ceph集群时使用的Base64格式的密钥文件（非keyring文件），默认为/etc/ceph/user.secret。</li><li>secretRef &lt;Object&gt;：用户认证到Ceph集群过程中加载其密钥时使用的Kubernetes Secret资源对象。</li><li>path &lt;string&gt;：挂载的文件系统路径，默认为CephFS文件系统的根（/），可以使用CephFS文件系统上的子路径，例如/kube/namespaces/default/redis1等。</li><li>readOnly &lt;boolean&gt;：是否挂载为只读模式，默认为false。</li></ul><p>下面提供的CephFS存储卷插件使用示例定义在volumes-cephfs-demo.yaml配置清单文件中，它使用fsclient用户认证到Ceph集群中，并关联CephFS上的子路径/kube/namespaces/default/redis1，作为Pod对象volumes-cephfs-demo的存储卷，并由容器进程挂载至/data目录进行数据存取。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-cephfs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span> </span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/data&quot;</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-cephfs-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-cephfs-vol</span></span><br><span class="line">    <span class="attr">cephfs:</span></span><br><span class="line">      <span class="attr">monitors:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/kube/namespaces/default/redis1</span></span><br><span class="line">      <span class="attr">user:</span> <span class="string">fsclient</span></span><br><span class="line">      <span class="attr">secretFile:</span> <span class="string">&quot;/etc/ceph/fsclient.key&quot;</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>Kubernetes集群上需要启用了CephFS，并提供了满足条件的用户账号及授权才能使用CephFS存储卷插件。客户端访问集群时需要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了保存在/etc/ceph/fsclient.key文件中的CephFS专用用户认证信息。要完成示例清单中定义的资源的测试，需要事先完成如下几个步骤。</p><ul><li>1）将授权访问CephFS的用户fsclient的Secret文件fsclient.key复制到Kubernetes集群的各工作节点，以便kubelet可加载并使用它。在生成fsclient.key的Ceph节点上执行如下命令以复制必要的文件。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="keyword">for</span> kubehost <span class="keyword">in</span> k8s-node01 k8s-node02 k8s-node03; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="language-bash">    scp -p /etc/ceph/fsclient.key /etc/ceph/ceph.conf <span class="variable">$&#123;kubehost&#125;</span>:/etc/ceph/; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure><ul><li>2）在Kubernetes集群的各工作节点上执行如下命令，以安装Ceph客户端库。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">echo</span> deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \</span></span><br><span class="line"><span class="language-bash">    | <span class="built_in">tee</span> /etc/apt/sources.list.d/ceph.list</span> </span><br><span class="line"><span class="meta">~# </span><span class="language-bash">apt update &amp;&amp; apt install ceph-common</span></span><br></pre></td></tr></table></figure><ul><li>3）在Kubernetes的某工作节点上手动挂载CephFS，以创建由Pod对象使用的数据目录。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">mount -t ceph ceph01:6789:/ /mnt -o name=fsclient,secretfile=/etc/ceph/fsclient.key</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">mkdir</span> -p /mnt/kube/namespaces/default/redis1</span></span><br></pre></td></tr></table></figure><p>上述准备步骤执行完成后即可运行如下命令创建清单volumes-cephfs-demo.yaml中定义的Pod资源，并进行测试：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-cephfs-demo.yaml</span>        </span><br><span class="line">pod/volumes-cephfs-demo created</span><br></pre></td></tr></table></figure><p>随后通过Pod对象volumes-cephfs-demo的详细描述了解其创建及运行状态，若一切无误，则相应的存储卷会显示出类似如下的描述信息：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Volumes:</span></span><br><span class="line">  <span class="attr">redis-cephfs-vol:</span></span><br><span class="line">    <span class="attr">Type:</span>        <span class="string">CephFS</span> <span class="string">(a</span> <span class="string">CephFS</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">Monitors:</span>    [<span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span>]</span><br><span class="line">    <span class="attr">Path:</span>        <span class="string">/kube/namespaces/default/redis1</span></span><br><span class="line">    <span class="attr">User:</span>        <span class="string">fsclient</span></span><br><span class="line">    <span class="attr">SecretFile:</span>  <span class="string">/etc/ceph/fsclient.key</span></span><br><span class="line">    <span class="attr">SecretRef:</span>   <span class="string">nil</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>    <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>删除Pod对象仅会卸载其挂载的CephFS文件系统（或子目录），因而文件系统（或目录）及相关数据将依然存在，除非管理员手动进行删除。我们可使用类似5.4.1节中测试Redis数据持久性的方式来测试本示例中的容器数据的持久性，这里不再给出具体步骤。另外在实践中，应该把认证到CephFS文件系统上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用secretFile字段引用相应用户密钥信息文件。</p><h2 id="5-4-4-GlusterFS存储卷"><a href="#5-4-4-GlusterFS存储卷" class="headerlink" title="5.4.4 GlusterFS存储卷"></a>5.4.4 GlusterFS存储卷</h2><p>GlusterFS（Gluster File System）是一个开源的分布式文件系统，是水平扩展存储解决方案Gluster的核心，它具有强大的横向扩展能力，通过扩展能够支持PB级的存储容量和数千个客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据，它基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能，是另一种流行的分布式存储解决方案。Kubernetes的GlusterFS存储卷插件依赖于GlusterFS存储集群作为存储方案。要配置Pod资源使用GlusterFS存储卷，需要事先满足以下前提条件：</p><blockquote><p>1）存在某可用的GlusterFS存储集群，否则要创建一个。<br>2）在GlusterFS集群中创建一个能满足Pod资源数据存储需要的卷。<br>3）在Kubernetes集群内的各节点上安装GlusterFS客户端程序包（glusterfs和glusterfs-fuse）。<br>GlusterFS存储卷嵌套定义在Pod资源的spec.volumes.glusterfs字段中，它常用的配置字段有如下几个。</p><ul><li><p>endpoints &lt;string&gt;：Endpoints资源的名称，此资源需要事先存在，用于提供Gluster集群的部分节点信息作为其访问入口；必选字段。</p></li><li><p>path &lt;string&gt;：用到的GlusterFS集群的卷路径，例如kube-redis；必选字段。</p></li><li><p>readOnly &lt;boolean&gt;：是否为只读卷。</p></li></ul></blockquote><p>下面提供的GlusterFS存储卷插件使用示例定义在volumes-glusterfs-demo.yaml配置清单文件中，它通过glusterfs-endpoints资源中定义的GlusterFS集群节点信息接入集群，并以kube-redis卷作为Pod资源的存储卷。glusterfs-endpoints资源需要在Kubernetes集群中事先创建，而kube-redis则需要先于Gluster集群创建。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-glusterfs-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">      <span class="attr">glusterfs:</span></span><br><span class="line">        <span class="attr">endpoints:</span> <span class="string">glusterfs-endpoints</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">kube-redis</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure><p>用于访问Gluster集群的相关节点信息要事先保存在某特定的Endpoint资源中，例如上面示例中调用的glusterfs-endpoints。此类的Endpoint资源依赖用户根据实际需求手动创建，例如，下面保存在glusterfs-endpoints.yaml文件中的资源示例定义了3个接入相关的Gluster存储集群的节点：gfs01.ilinux.io、gfs02.ilinux.io和gfs03.ilinux.io，其中的端口信息仅为满足Endpoint资源的必选字段要求，因此其值可以随意填写。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">glusterfs-endpoints</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs01.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs02.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs03.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br></pre></td></tr></table></figure><p>准备好必要的存储供给条件后，先创建Endpoint资源glusterfs-endpoints，之后创建Pod资源vol-glusterfs-pod，即可测试其数据持久存储的效果</p><h2 id="5-5-持久存储卷"><a href="#5-5-持久存储卷" class="headerlink" title="5.5 持久存储卷"></a>5.5 持久存储卷</h2><p>通过5.4节网络存储卷及使用示例可知，用户必须要清晰了解用到的网络存储系统的访问细节才能完成存储卷相关的配置任务，例如RBD存储卷插件配置中的监视器（monitor）、存储池（pool）、存储映像（../img/image）和密钥环（keyring）等来自于Ceph存储系统中的概念，这就要求用户对该类存储系统有着一定的了解才能够顺利使用。这与Kubernetes向用户和开发隐藏底层架构的目标有所背离，最好对存储资源的使用也能像计算资源一样，用户和开发人员既无须了解Pod资源究竟运行在哪个节点，也不用了解存储系统是什么设备、位于何处以及如何访问。<br>PV（PersistentVolume）与PVC（PersistentVolumeClaim）就是在用户与存储服务之间添加的一个中间层，管理员事先根据PV支持的存储卷插件及适配的存储方案（目标存储系统）细节定义好可以支撑存储卷的底层存储空间，而后由用户通过PVC声明要使用的存储特性来绑定符合条件的最佳PV定义存储卷，从而实现存储系统的使用与管理职能的解耦，大大简化了用户使用存储的方式。<br>PV和PVC的生命周期由Controller Manager中专用的PV控制器（PV Controller）独立管理，这种机制的存储卷不再依附并受限于Pod对象的生命周期，从而实现了用户和集群管理员的职责相分离，也充分体现出Kubernetes把简单留给用户，把复杂留给自己的管理理念。</p><h3 id="5-5-1-PV与PVC基础"><a href="#5-5-1-PV与PVC基础" class="headerlink" title="5.5.1 PV与PVC基础"></a>5.5.1 PV与PVC基础</h3><p>PV是由集群管理员于全局级别配置的预挂载存储空间，它通过支持的存储卷插件及给定的配置参数关联至某个存储系统上可用数据存储的一段空间，这段存储空间可能是Ceph存储系统上的一个存储映像、一个文件系统（CephFS）或其子目录，也可能是NFS存储系统上的一个导出目录等。PV将存储系统之上的存储空间抽象为Kubernetes系统全局级别的API资源，由集群管理员负责管理和维护。<br>将PV提供的存储空间用于Pod对象的存储卷时，用户需要事先使用PVC在名称空间级别声明所需要的存储空间大小及访问模式并提交给Kubernetes API Server，接下来由PV控制器负责查找与之匹配的PV资源并完成绑定。随后，用户在Pod资源中使用persistentVolumeClaim类型的存储卷插件指明要使用的PVC对象的名称即可使用其绑定到的PV所指向的存储空间，如图5-6所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123100113403.png" alt="../img/image-20220123100113403"></p><p>由此可见，尽管PVC及PV将存储资源管理与使用的职责分离至用户和集群管理员两类不同的人群之上，简化了用户对存储资源的使用机制，但也对二者之间的协同能力提出了要求。管理员需要精心预测和规划集群用户的存储使用需求，提前创建出多种规格的PV，以便于在用户声明PVC后能够由PV控制器在集群中找寻到合适的甚至是最佳匹配的PV进行绑定。<br>不难揣测，这种通过管理员手动创建PV来满足PVC需求的静态预配（static provisioning）存在着不少的问题。<br>第一，集群管理员难以预测出用户的真实需求，很容易导致某些类型的PVC无法匹配到PV而被挂起，直到管理员参与到问题的解决过程中。<br>第二，那些能够匹配到PV的PVC也很有可能存在资源利用率不佳的状况，例如一个声明使用5G存储空间的PVC绑定到一个20GB的PV之上。<br>更好的解决方案是一种称为动态预配、按需创建PV的机制。集群管理员要做的仅是事先借助存储类（StorageClass）的API资源创建出一到多个“PV模板”，并在模板中定义好基于某个存储系统创建PV所依赖的存储组件（例如Ceph RBD存储映像或CephfFS文件系统等）时需要用到的配置参数。创建PVC时，用户需要为其指定要使用PV模板（StorageClass资源），而后PV控制器会自动连接相应存储类上定义的目标存储系统的管理接口，请求创建匹配该PVC需求的存储组件，并将该存储组件创建为Kubernetes集群上可由该PVC绑定的PV资源。<br>需要说明的是，静态预配的PV可能属于某存储类，也可能没有存储类，这取决于管理员的设定。但动态PV预配依赖存储类的辅助，PVC必须向一个事先存在的存储类发起动态分配PV的请求，没有指定存储类的PVC不支持使用动态预配PV的方式。<br><font color="red">PV和PVC是一对一的关系：一个PVC仅能绑定一个PV，而一个PV在某一时刻也仅可被一个PVC所绑定。</font>为了能够让用户更精细地表达存储需求，PV资源对象的定义支持存储容量、存储类、卷模型和访问模式等属性维度的约束。相应地，PVC资源能够从访问模式、数据源、存储资源容量需求和限制、标签选择器、存储类名称、卷模型和卷名称等多个不同的维度向PV资源发起匹配请求并完成筛选。</p><h3 id="5-5-2-PV的生命周期"><a href="#5-5-2-PV的生命周期" class="headerlink" title="5.5.2 PV的生命周期"></a>5.5.2 PV的生命周期</h3><p>从较为高级的实现上来讲，Kubernetes系统与存储相关的组件主要有存储卷插件、存储卷管理器、PV/PVC控制器和AD控制器（Attach/Detach Controller）这4种，如图5-7所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123100142256.png" alt="../img/image-20220123100142256"></p><p>存储卷插件：Kubernetes存储卷功能的基础设施，是存储任务相关操作的执行方；它是存储相关的扩展接口，用于对接各类存储设备。</p><ul><li><p>存储卷管理器：kubelet内置管理器组件之一，用于在当前节点上执行存储设备的挂载（mount）、卸载（unmount）和格式化（format）等操作；另外，存储卷管理器也可执行节点级别设备的附加（attach）及拆除（detach）操作。</p></li><li><p>PV控制器：负责PV及PVC的绑定和生命周期管理，并根据需求进行存储卷的预配和删除操作；</p></li><li><p>AD控制器：专用于存储设备的附加和拆除操作的组件，能够将存储设备关联（attach）至目标节点或从目标节点之上剥离（detach）。<br>这4个组件中，存储卷插件是其他3个组件的基础库，换句话说，PV控制器、AD控制器和存储卷管理器均构建于存储卷插件之上，以提供不同维度管理功能的接口，具体的实现逻辑均由存储卷插件完成。<br>除了创建、删除PV对象，以及完成PV和PVC的状态迁移等生命周期管理之外，PV控制器还要负责绑定PVC与PV对象，而且PVC只能在绑定到PV之后方可由Pod作为存储卷使用。创建后未能正确关联到存储设备的PV将处于Pending状态，直到成功关联后转为Available状态。而后一旦该PV被某个PVC请求并成功绑定，其状态也就顺应转为Bound，直到相应的PVC删除后而自动解除绑定，PV才会再次发生状态转换，此时的状态为（Released），随后PV的去向将由其“回收策略”（reclaim policy）所决定，具体如下。</p></li></ul><blockquote><p>1）Retain（保留）：删除PVC后将保留其绑定的PV及存储的数据，但会把该PV置为Released状态，它不可再被其他PVC所绑定，且需要由管理员手动进行后续的回收操作：首先删除PV，接着手动清理其关联的外部存储组件上的数据，最后手动删除该存储组件或者基于该组件重新创建PV。<br>2）Delete（删除）：对于支持该回收策略的卷插件，删除一个PVC将同时删除其绑定的PV资源以及该PV关联的外部存储组件；动态的PV回收策略继承自StorageClass资源，默认为Delete。多数情况下，管理员都需要根据用户的期望修改此默认策略，以免导致数据非计划内的删除。<br>3）Recycle（回收）：对于支持该回收策略的卷插件，删除PVC时，其绑定的PV所关联的外部存储组件上的数据会被清空，随后，该PV将转为Available状态，可再次接受其他PVC的绑定请求。不过，该策略已被废弃。</p></blockquote><p>相应地，创建后的PVC也将处于Pending状态，仅在遇到条件匹配、状态为Available的PV，且PVC请求绑定成功才会转为Bound状态。PV和PVC的状态迁移如图5-8所示。总结起来，PV和PVC的生命周期存在以几个关键阶段。</p><blockquote><p>1）存储预配（provision）：存储预配是指为PVC准备PV的途径，Kubernetes支持静态和动态两种PV预配方式，前者是指由管理员以手动方式创建PV的操作，而后者则是由PVC基于StorageClass定义的模板，按需请求创建PV的机制。<br>2）存储绑定：用户基于一系列存储需求和访问模式定义好PVC后，PV控制器即会为其查找匹配的PV，完成关联后它们二者同时转为已绑定状态，而且动态预配的PV与PVC之间存在强关联关系。无法找到可满足条件的PV的PVC将一直处于Pending状态，直到有符合条件的PV出现并完成绑定为止。<br>3）存储使用：Pod资源基于persistenVolumeClaim存储卷插件的定义，可将选定的PVC关联为存储卷并用于内部容器应用的数据存取。<br>4）存储回收：存储卷的使用目标完成之后，删除PVC对象可使得此前绑定的PV资源进入Released状态，并由PV控制器根据PV回收策略对PV作出相应的处置。目前，可用的回收策略有Retaine、Delete和Recycle这3种。<br>如前所述，处于绑定状态的PVC删除后，相应的PV将转为Released状态，之后的处理机制依赖于其回收策略。而处于绑定状态的PV将会导致相应的PVC转为Lost状态，而无法再由Pod正常使用，除非PVC再绑定至其他Available状态的PV之上，但应用是否能正常运行，则取决于对此前数据的依赖度。另一方面，为了避免使用中的存储卷被移除而导致数据丢失，Kubernetes自1.9版引入了“PVC保护机制”，其目的在于，用户删除了仍被某Pod对象使用中的PVC时，Kubernetes不会立即移除该PVC，而是会推迟到它不再被任何Pod对象使用后方才真正执行删除操作。处于保护阶段的PVC资源的status字段值为Termination，并且其Finalizers字段值中包含有kubernetes.io/pvc-protection。</p></blockquote><h3 id="5-5-3-静态PV资源"><a href="#5-5-3-静态PV资源" class="headerlink" title="5.5.3 静态PV资源"></a>5.5.3 静态PV资源</h3><p>PersistentVolume是隶属于Kubernetes核心API群组中的标准资源类型，它的目标在于通过存储卷插件机制，将支持的外部存储系统上的存储组件定义为可被PVC声明所绑定的资源对象。但PV资源隶属于Kubernetes集群级别，因而它只能由集群管理员进行创建。这种由管理员手动定义和创建的PV被人们习惯地称为静态PV资源。<br>PV支持的存储卷插件类型是Pod对象支持的存储卷插件类型的一个子集，它仅涵盖Pod支持的网络存储卷类别中的所有存储插件以及local卷插件。除了存储卷插件之外，PersistentVolume资源规范Spec字段主要支持嵌套以下几个通用字段，它们用于定义PV的容量、访问模式和回收策略等属性。</p><ul><li>capacity &lt;map[string]string&gt;：指定PV的容量；目前，Capacity仅支持存储容量设定，将来应该还可以指定IOPS和吞吐量（throughput）。</li><li>accessModes &lt;[]string&gt;：指定当前PV支持的访问模式；存储系统支持的存取能力大体可分为ReadWriteOnce（单路读写）、ReadOnlyMany（多路只读）和ReadWrite-Many（多路读写）3种类型，某个特定的存储系统可能会支持其中的部分或全部的能力。</li><li>persistentVolumeReclaimPolicy &lt;string&gt;：PV空间被释放时的处理机制；可用类型仅为Retain（默认）、Recycle或Delete。目前，仅NFS和hostPath支持Recycle策略，也仅有部分存储系统支持Delete策略。</li><li>volumeMode &lt;string&gt;：该PV的卷模型，用于指定此存储卷被格式化为文件系统使用还是直接使用裸格式的块设备；默认值为Filesystem，仅块设备接口的存储系统支持该功能。</li><li>storageClassName &lt;string&gt;：当前PV所属的StorageClass资源的名称，指定的存储类需要事先存在；默认为空值，即不属于任何存储类。</li><li>mountOptions &lt;string&gt;：挂载选项组成的列表，例如ro、soft和hard等。</li><li>nodeAffinity &lt;Object&gt;：节点亲和性，用于限制能够访问该PV的节点，进而会影响与该PV关联的PVC的Pod的调度结果。</li></ul><p>PV的访问模式用于反映它关联的存储系统所支持的某个或全部存取能力，例如NFS存储系统支持以上3种存取能力，于是NFS PV可以仅支持ReadWriteOnce访问模式。需要注意的是，PV在某个特定时刻仅可基于一种模式进行存取，哪怕它同时支持多种模式。</p><h4 id="1-NFS-PV示例"><a href="#1-NFS-PV示例" class="headerlink" title="1 NFS PV示例"></a>1 NFS PV示例</h4><p>下面的配置示例来自于pv-nfs-demo.yaml资源清单，它定义了一个使用NFS存储系统的PV资源，它将空间大小限制为5GB，并支持多路的读写操作。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-nfs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">mountOptions:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hard</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">nfsvers=4.1</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span>  <span class="string">&quot;/data/redis002&quot;</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">nfs.ilinux.io</span></span><br></pre></td></tr></table></figure><p>在NFS服务器nfs.ilinux.io上导出/data/redis002目录后，便可使用如下命令创建该PV资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pv-nfs-demo.yaml</span></span><br><span class="line">persistentvolume/pv-nfs-demo created</span><br></pre></td></tr></table></figure><p>若能够正确关联到指定的后端存储，该PV对象的状态将显示为Available，否则其状态为Pending，直至能够正确完成存储资源关联或者被删除。我们同样可使用describe命令来获取PV资源的详细描述信息。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pv/pv-nfs-demo</span></span><br><span class="line">Name:            pv-nfs-demo</span><br><span class="line">……  </span><br><span class="line">Status:          Available</span><br><span class="line">……        </span><br><span class="line">Source:</span><br><span class="line">    Type:      NFS (an NFS mount that lasts the lifetime of a pod)</span><br><span class="line">    Server:    nfs.ilinux.io</span><br><span class="line">    Path:      /data/redis002</span><br><span class="line">    ReadOnly:  false</span><br><span class="line">Events:        &lt;none&gt;</span><br></pre></td></tr></table></figure><p>描述信息中的Available表明该PV已经可以接受PVC的绑定请求，并在绑定完成后转变其状态至Bound。2. RBD PV示例<br>下面是另一个PV资源的配置清单（pv-rbd-demo.yaml），它使用了RBD存储后端，空间大小等同于指定的RBD存储映像的大小（这里为2GB），并限定支持的访问模式为RWO，回收策略为Retain。除此之外，该PV资源还拥有一个名为usedof的资源标签，该标签可被PVC的标签选择器作为筛选PV资源的标准之一。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-rbd-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">usedof:</span> <span class="string">redisdata</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">rbd:</span></span><br><span class="line">    <span class="attr">monitors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph01.ilinux.io</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph02.ilinux.io</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph03.ilinux.io</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">pv-test</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">kube</span></span><br><span class="line">    <span class="attr">keyring:</span> <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br><span class="line">    <span class="attr">fsType:</span> <span class="string">xfs</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br></pre></td></tr></table></figure><p>将RBD卷插件内嵌字段相关属性值设定为Ceph存储系统的实际的环境，包括监视器地址、存储池、存储映像、用户名和认证信息（keyring或secretRef）等。测试时，请事先部署好Ceph集群，参考5.4.2节中设定专用用户账号和Kubernetes集群工作节点的方式，准备好基础环境，并在Ceph集群的管理节点运行如下命令创建用到的存储映像：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd create pv-test --size 2G --pool kube</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd feature <span class="built_in">disable</span> -p kube pv-test object-map fast-diff deep-flatten</span></span><br></pre></td></tr></table></figure><p>待所有准备工作就绪后，即可运行如下命令创建示例清单中定义的PV资源pv-rbd-demo：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl apply -f pv-rbd-demo.yaml</span> </span><br><span class="line">persistentvolume/pv-rbd-demo created</span><br></pre></td></tr></table></figure><p>我们同样可以使用describe命令了解pv-rbd-demo的详细描述，若处于Pending状态则需要详细检查存储卷插件的定义是否能吻合存储系统的真实环境。</p><h3 id="5-5-4-PVC资源"><a href="#5-5-4-PVC资源" class="headerlink" title="5.5.4 PVC资源"></a>5.5.4 PVC资源</h3><p>PersistentVolumeClaim也是Kubernetes系统上标准的API资源类型之一，它位于核心API群组，属于名称空间级别。用户提交新建的PVC资源最初处于Pending状态，由PV控制器找寻最佳匹配的PV并完成二者绑定后，两者都将转入Bound状态，随后Pod对象便可基于persistentVolumeClaim存储卷插件配置使用该PVC对应的持久存储卷。<br>定义PVC时，用户可通过访问模式（accessModes）、数据源（dataSource）、存储资源空间需求和限制（resources）、存储类、标签选择器、卷模型和卷名称等匹配标准来筛选集群上的PV资源，其中，resources和accessModes是最重要的筛选标准。PVC的Spec字段的可嵌套字段有如下几个。</p><ul><li>accessModes &lt;[]string&gt;：PVC的访问模式；它同样支持RWO、RWX和ROX这3种模式。</li><li>dataSrouces &lt;Object&gt;：用于从指定的数据源恢复该PVC卷，它目前支持的数据源包括一个现存的卷快照对象（snapshot.storage.k8s.io/VolumeSnapshot）、一个既有的PVC对象（PersistentVolumeClaim）或一个既有的用于数据转存的自定义资源对象（resource/object）。</li><li>resources &lt;Object&gt;：声明使用的存储空间的最小值和最大值；目前，PVC的资源限定仅支持空间大小一个维度。</li><li>selector &lt;Object&gt;：筛选PV时额外使用的标签选择器（matchLabels）或匹配条件表达式（matchExpressions）。</li><li>storageClassName &lt;string&gt;：该PVC资源隶属的存储类资源名称；指定了存储类资源的PVC仅能在同一个存储类下筛选PV资源，否则就只能从所有不具有存储类的PV中进行筛选。</li><li>volumeMode &lt;string&gt;：卷模型，用于指定此卷可被用作文件系统还是裸格式的块设备；默认值为Filesystem。</li><li>volumeName &lt;string&gt;：直接指定要绑定的PV资源的名称。<br>下面通过匹配前一节中创建的PV资源的两个具体示例来说明PVC资源的配置方法，两个PV资源目前的状态如下所示，它仅截取了命令结果中的一部分。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$</span><span class="language-bash">kubectl get pv</span></span><br><span class="line">NAME        CAPACITY  ACCESS MODES  RECLAIM POLICY       STATUS        CLAIM   </span><br><span class="line">pv-nfs-demo   5Gi        RWX            Retain           Available                                  </span><br><span class="line">pv-rbd-demo   2Gi        RWO            Retain           Available</span><br></pre></td></tr></table></figure><h4 id="1-NFS-PVC示例"><a href="#1-NFS-PVC示例" class="headerlink" title="1 NFS PVC示例"></a>1 NFS PVC示例</h4><p>下面的配置清单（pvc-demo-0001.yaml）定义了一个名为pvc-nfs-demo的PVC资源示例，它仅定义了期望的存储空间范围、访问模式和卷模式以筛选集群上的PV资源。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-demo-0001</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteMany&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br></pre></td></tr></table></figure><p>显然，此前创建的两个PV资源中，pv-nfs-demo能够完全满足该PVC的筛选条件，因而创建示例清单中的资源后，它能够迅速绑定至PV之上，如下面的创建和资源查看命令结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-demo-0001.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-demo-0001 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc pvc-nfs-0001</span></span><br><span class="line">NAME   STATUS  VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">pvc-demo-0001  Bound  pv-nfs-demo  5Gi      RWX                        3s</span><br></pre></td></tr></table></figure><p>被PVC资源pvc-demo-0001绑定的PV资源pv-nfs-demo的状态也将从Available转为Bound，如下面的命令结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pv/pv-nfs-demo -o jsonpath=&#123;.status.phase&#125;</span></span><br><span class="line">Bound</span><br></pre></td></tr></table></figure><p>集群上的PV资源数量很多时，用户可通过指定多维度的过滤条件来缩小PV资源的筛选范围，以获取到最佳匹配。2. RBD PVC示例<br>下面这个定义在pvc-demo-0002.yaml中的配置清单定义了一个PVC资源，除了期望的访问模式、卷模型和存储空间容量边界之外，它使用了标签选择器来匹配PV资源的标签。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-demo-0002</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">usedof:</span> <span class="string">&quot;redisdata&quot;</span></span><br></pre></td></tr></table></figure><p>配置清单中的资源PVC/pvc-demo-0002特地为绑定此前创建的资源PV/pv-rbd-demo而创建，其筛选条件可由该PV完全满足，因而创建配置清单中的PVC/pvc-demo-0002资源后会即刻绑定于PV/pv-rbd-demo之上，如下面命令的结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-demo-0002.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-demo-0002 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-demo-0002</span></span><br><span class="line">NAME   STATUS  VOLUME  CAPACITY   ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">pvc-demo-0002   Bound   pv-rbd-demo  2Gi     RWO                       10s</span><br></pre></td></tr></table></figure><p>删除一个PVC将导致其绑定的PV资源转入Released状态，并由相应的回收策略完成资源回收。反过来，直接删除一个仍由某PVC绑定的PV资源，会由PVC保护机制延迟该删除操作至相关的PVC资源被删除。</p><h3 id="5-5-5-在Pod中使用PVC"><a href="#5-5-5-在Pod中使用PVC" class="headerlink" title="5.5.5 在Pod中使用PVC"></a>5.5.5 在Pod中使用PVC</h3><p>需要特别说明的是，PVC资源隶属名称空间级别，它仅可被同一名称空间中的Pod对象通过persistentVolumeClaim插件所引用并作为存储卷使用，该存储卷插件可嵌套使用如下两个字段。</p><ul><li>claimName：要调用的PVC存储卷的名称，PVC卷要与Pod在同一名称空间中。</li><li>readOnly：是否强制将存储卷挂载为只读模式，默认为false。<br>下面的配置清单（volumes-pvc-demo.yaml）定义了一个Pod资源，该配置清单将5.5.2节中直接使用RBD存储的Pod资源改为了调用指定的PVC存储卷。</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-pvc-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="string">../img/image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="string">../img/imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">pvc-demo-0002</span></span><br></pre></td></tr></table></figure><h3 id="5-5-6-存储类"><a href="#5-5-6-存储类" class="headerlink" title="5.5.6 存储类"></a>5.5.6 存储类</h3><p>存储类也是Kubernetes系统上的API资源类型之一，它位于storage.k8s.io群组中。存储类通常由集群管理员为管理PV资源之便而按需创建的存储资源类别（逻辑组），例如可将存储系统按照其性能高低或者综合服务质量级别分类（见图5-9）、依照备份策略分类，甚至直接按管理员自定义的标准分类等。存储类也是PVC筛选PV时的过滤条件之一，这意味着PVC仅能在其隶属的存储类之下找寻匹配的PV资源。不过，Kubernetes系统自身无法理解“类别”到底意味着什么，它仅仅把存储类中的信息当作PV资源的特性描述使用。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123100927132.png" alt="../img/image-20220123100927132"></p><p>存储类的最重要功能之一便是对PV资源动态预配机制的支持，它可被视作动态PV资源的创建模板，能够让集群管理员从维护PVC和PV资源之间的耦合关系的束缚中解脱出来。需要用到具有持久化功能的存储卷资源时，用户只需要向满足其存储特性要求的存储类声明一个PVC资源，存储类将会根据该声明创建恰好匹配其需求的PV对象。</p><h4 id="1-StorageClass资源"><a href="#1-StorageClass资源" class="headerlink" title="1 StorageClass资源"></a>1 StorageClass资源</h4><p>StorageClass资源的期望状态直接与apiVersion、kind和metadata定义在同一级别而无须嵌套在spec字段中，它支持使用的字段包括如下几个。</p><ul><li>allowVolumeExpansion &lt;boolean&gt;：是否支持存储卷空间扩展功能。</li><li>allowedTopologies &lt;[]Object&gt;：定义可以动态配置存储卷的节点拓扑，仅启用了卷调度功能的服务器才会用到该字段；每个卷插件都有自己支持的拓扑规范，空的拓扑选择器表示无拓扑限制。</li><li>provisioner &lt;string&gt;：必选字段，用于指定存储服务方（provisioner，或称为预配器），存储类要基于该字段值来判定要使用的存储插件，以便适配到目标存储系统；Kubernetes内置支持许多的provisioner，它们的名字都以kubernetes.io/为前缀，例如kubernetes.io/glusterfs等。</li><li>parameters &lt;map[string]string&gt;：定义连接至指定的provisioner类别下的某特定存储时需要使用的各相关参数；不同provisioner的可用参数各不相同。</li><li>reclaimPolicy &lt;string&gt;：由当前存储类动态创建的PV资源的默认回收策略，可用值为Delete（默认）和Retain两个；但那些静态PV的回收策略则取决于它们自身的定义。</li><li>volumeBindingMode &lt;string&gt;：定义如何为PVC完成预配和绑定，默认值为Volume-BindingImmediate；该字段仅在启用了存储卷调度功能时才能生效。</li><li>mountOptions &lt;[]string&gt;：由当前类动态创建的PV资源的默认挂载选项列表。</li></ul><p>下面是一个定义在storageclass-rbd-demo.yaml配置文件中的StorageClass资源清单，它定义了一个以Ceph存储系统的RBD接口为后端存储的StorageClass资源fast-rbd，因此，其存储预配标识为kubernetes.io/rbd。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fast-rbd</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">monitors:</span> <span class="string">ceph01.ilinux.io:6789,ceph02.ilinux.io:6789</span></span><br><span class="line">  <span class="attr">adminId:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">adminSecretName:</span> <span class="string">ceph-admin-secret</span></span><br><span class="line">  <span class="attr">adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userId:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userSecretName:</span> <span class="string">ceph-kube-secret</span></span><br><span class="line">  <span class="attr">userSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">  <span class="string">../img/imageFormat:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">  <span class="string">../img/imageFeatures:</span> <span class="string">&quot;layering&quot;</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Retain</span></span><br></pre></td></tr></table></figure><p>不同的provisioner的parameters字段中可嵌套使用的字段各有不同，上面示例中Ceph RBD存储服务可使用的各字段及意义如下。</p><ul><li>monitors &lt;string&gt;：Ceph存储系统的监视器访问接口，多个套接字间以逗号分隔。</li><li>adminId &lt;string&gt;：有权限在指定的存储池中创建../img/image的管理员用户名，默认为admin。</li><li>adminSecretName &lt;string&gt;：存储有管理员账号认证密钥的Secret资源名称。</li><li>adminSecretNamespace &lt;string&gt;：管理员账号相关的Secret资源所在的名称空间。</li><li>pool &lt;string&gt;：Ceph存储系统的RBD存储池名称，默认为rbd。</li><li>userId &lt;string&gt;：用于映射RBD镜像的Ceph用户账号，默认同adminId字段。</li><li>userSecretName &lt;string&gt;：存储有用户账号认证密钥的Secret资源名称。</li><li>userSecretNamespace &lt;string&gt;：用户账号相关的Secret资源所在的名称空间。</li><li>fsType &lt;string&gt;：存储映像格式化的文件系统类型，默认为ext4。</li><li>../img/imageFormat &lt;string&gt;：存储映像的格式，其可用值仅有“1”和“2”，默认值为“2”。</li><li>../img/imageFeatures &lt;string&gt;：“2”格式的存储映像支持的特性，目前仅支持layering，默认为空值，并且不支持任何功能。</li></ul><p>存储类接入其他存储系统时使用的参数请参考<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/%E3%80%82">https://kubernetes.io/docs/concepts/storage/storage-classes/。</a><br><font color="red">与Pod或PV资源上的RBD卷插件配置格式不同的是，StorageClass上的RBD供给者参数不支持使用keyring直接认证到Ceph，它仅能引用Secret资源中存储的认证密钥完成认证操作。</font>因而，我们需要先将Ceph用户admin和kube的认证密钥分别创建为Secret资源对象。</p><ul><li>1）在Ceph管理节点上分别获取admin和kube的认证密钥，不同Ceph集群上的输出结果应该有所不同：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">ceph auth get-key client.admin</span> </span><br><span class="line">AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">ceph auth get-key client.kube</span></span><br><span class="line">AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA=</span><br></pre></td></tr></table></figure><ul><li>2）在Kubernetes集群管理客户端上使用kubectl命令分别将二者创建为Secret资源，在具体测试操作中，需要将其中的密钥分别替换为前一步中的命令输出结果：</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic ceph-admin-secret --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/rbd&quot;</span> \</span></span><br><span class="line"><span class="language-bash">  --from-literal=key=<span class="string">&#x27;AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">  --namespace=kube-system</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic ceph-kube-secret --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/rbd&quot;</span> \</span></span><br><span class="line"><span class="language-bash">  --from-literal=key=<span class="string">&#x27;AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA==&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">  --namespace=kube-system</span></span><br></pre></td></tr></table></figure><p>示例中使用的账号及存储池的管理方式请参考5.4节和5.5节给出的步骤。待相关Secret资源准备完成后，将示例清单中的StorageClass资源创建在集群上，即可由PVC或PV资源将其作为存储类</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f storageclass-rbd-demo.yaml</span> </span><br><span class="line">storageclass.storage.k8s.io/fast-rbd created</span><br></pre></td></tr></table></figure><p>我们还可以使用kubectl get sc/NAME命令打印存储类的相关信息，或者使用kubectl describe sc NAME命令获取详细描述来进一步了解其状态。2. PV动态预配<br>动态PV预配功能的使用有两个前提条件：支持动态PV创建功能的卷插件，以及一个使用了对应于该存储卷插件的后端存储系统的StorageClass资源。不过，Kubernetes并非内置支持所有的存储卷插件的PV动态预配功能，具体信息如图5-10所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123101116752.png" alt="../img/image-20220123101116752"></p><p>RBD存储卷插件，结合5.5.4节中定义关联至Ceph RBD存储系统接口的存储类资源fast-rbd就能实现PV的动态预配功能，用户于该存储类中创建PVC资源后，运行于kube-controller-manager守护进程中的PV控制器会根据fast-rbd存储类的定义接入Ceph存储系统创建出相应的存储映像，并在自动创建一个关联至该存储映像的PV资源后，将其绑定至PVC资源。<br>动态PV预配的过程中，PVC控制器会调用相关存储系统的管理接口API或专用的客户端工具来完成后端存储系统上的存储组件管理。以Ceph RBD为例，PV控制器会以存储类参数adminId中指定的用户身份调用rbd命令创建存储映像。然而，以kubeadm部署且运行为静态Pod资源的kube-controller-manager容器并未自行附带此类工具，如ceph-common程序包。常见的解决方案有3种：在Kubernetes系统上部署kubernetes-incubator/external-storage中的rbd-provisioner，从而以外置的方式提供相关工具程序，或基于CSI卷插件使用ceph-csi项目来支持更加丰富的卷功能，或定制kube-controller-manager的容器镜像，为其安装ceph-common程序包。本节将给出第三种方式的实现过程。提示<br>若以二进制程序包部署Kubernetes集群，则直接在Master节点安装ceph-common就能解决问题。<br>首先，我们使用如下的Dockerfile文件，并基于现有kube-controller-manager镜像文件为其额外安装ceph-common程序包，随后重新打包为容器镜像。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ARG KUBE_VERSION=&quot;v1.19.0&quot;</span><br><span class="line"></span><br><span class="line">FROM registry.aliyuncs.com/google_containers/kube-controller-manager:$&#123;KUBE_VERSION&#125;</span><br><span class="line"></span><br><span class="line">RUN apt update &amp;&amp; apt install -y wget  gnupg lsb-release</span><br><span class="line"></span><br><span class="line">ARG CEPH_VERSION=&quot;octopus&quot;</span><br><span class="line">RUN wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key </span><br><span class="line">add - &amp;&amp; \</span><br><span class="line">     echo deb https://mirrors.aliyun.com/ceph/debian-$&#123;CEPH_VERSION&#125;/ $(lsb_</span><br><span class="line">     release -sc) main &gt; /etc/apt/sources.list.d/ceph.list &amp;&amp; \</span><br><span class="line">     apt update &amp;&amp; \</span><br><span class="line">     apt install -y ceph-common ceph-fuse</span><br><span class="line"></span><br><span class="line">RUN rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/*</span><br></pre></td></tr></table></figure><p>将上面的内容保存于某专用目录下（例如kube-controller-manager）的名为Dockerfile的文件中，而后使用如下命令将其打包为镜像即可。其中，构建时参数KUBE_VERSION和CEPH_VERSION可分别修改为适用的版本。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">cd</span> kube-controller-manager</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">docker ../img/image build . --build-args KUBE_VERSION= <span class="string">&quot;v1.19.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    --build-args CEPH_VERSION=“octopus”\</span></span><br><span class="line"><span class="language-bash">    -t ikubernetes/kube-controller-manager:v1.19.0</span></span><br></pre></td></tr></table></figure><p>而后，将该镜像分发至各Master节点，并分别修改它们的/etc/kubernetes/manifests/kube-controller-manager.yaml配置清单中的容器镜像为定制的镜像ikubernetes/kube-controller-manager:v1.19.0，待Controller Manager相关的Pod自动重启后即可进行动态PV的创建测试。下面是定义于pvc-dyn-rbd-demo.yaml配置清单中的PVC资源，它向存储类fast-rbd声明了需要的存储空间及访问模式。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-dyn-rbd-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">fast-rbd</span></span><br></pre></td></tr></table></figure><p>将示例清单中的PVC资源创建至Kubernetes集群之上，便会触发PV控制器在指定的存储类中自动创建匹配的PV资源。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-dyn-rbd-demo.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-dyn-rbd-demo created</span><br></pre></td></tr></table></figure><p>下面的命令显示出该PVC资源已经绑定到了一个名为pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce的PV之上。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-dyn-rbd-demo -o jsonpath=&#123;.spec.volumeName&#125;</span></span><br><span class="line">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span><br></pre></td></tr></table></figure><p>如下命令输出的该PV的详细描述之中，Annotations中的kubernetes.io/createdby: rbd-dynamic-provisioner表示它是由rbd-dynamic-provisioner动态创建，而Source段中的信息更能印证这种结论。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">describe</span> <span class="string">pv</span> <span class="string">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span></span><br><span class="line"><span class="attr">Name:</span>            <span class="string">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span></span><br><span class="line"><span class="attr">Labels:</span>          <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Annotations:     kubernetes.io/createdby:</span> <span class="string">rbd-dynamic-provisioner</span></span><br><span class="line">                 <span class="attr">pv.kubernetes.io/bound-by-controller:</span> <span class="literal">yes</span></span><br><span class="line">                 <span class="attr">pv.kubernetes.io/provisioned-by:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">Finalizers:</span>      [<span class="string">kubernetes.io/pv-protection</span>]</span><br><span class="line"><span class="attr">StorageClass:</span>    <span class="string">fast-rbd</span></span><br><span class="line"><span class="attr">Status:</span>          <span class="string">Bound</span></span><br><span class="line"><span class="attr">Claim:</span>           <span class="string">default/pvc-sc-rbd-demo</span></span><br><span class="line"><span class="attr">Reclaim Policy:</span>  <span class="string">Delete</span>      <span class="comment"># 回收策略</span></span><br><span class="line"><span class="attr">Access Modes:</span>    <span class="string">RWO</span>         <span class="comment"># 访问模式</span></span><br><span class="line"><span class="attr">VolumeMode:</span>      <span class="string">Filesystem</span>  <span class="comment"># 卷模式</span></span><br><span class="line"><span class="attr">Capacity:</span>        <span class="string">3Gi</span>         <span class="comment"># 卷空间容量</span></span><br><span class="line"><span class="attr">Node Affinity:</span>   <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Message:</span>         </span><br><span class="line"><span class="attr">Source:</span>  <span class="comment"># 数据源标识</span></span><br><span class="line">    <span class="attr">Type:</span>          <span class="string">RBD</span> <span class="string">(a</span> <span class="string">Rados</span> <span class="string">Block</span> <span class="string">Device</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> </span><br><span class="line">    <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">CephMonitors:</span>  [<span class="string">ceph01.ilinux.io:6789</span> <span class="string">ceph02.ilinux.io:6789</span> <span class="string">ceph03.ilinux.</span></span><br><span class="line">    <span class="string">io:6789</span>]</span><br><span class="line">    <span class="attr">RBD../img/image:</span>      <span class="string">kubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e</span></span><br><span class="line">    <span class="attr">FSType:</span>        <span class="string">ext4</span></span><br><span class="line">    <span class="attr">RBDPool:</span>       <span class="string">kube</span></span><br><span class="line">    <span class="attr">RadosUser:</span>     <span class="string">kube</span></span><br><span class="line">    <span class="attr">Keyring:</span>       <span class="string">/etc/ceph/keyring</span></span><br><span class="line">    <span class="attr">SecretRef:</span>     <span class="string">&amp;SecretReference&#123;Name:ceph-kube-secret,Namespace:kube-system,&#125;</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>      <span class="literal">false</span></span><br><span class="line"><span class="attr">Events:</span>            <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure><p>上面命令结果中显示出，该PV的容量、访问模式和卷模式均符合PVC所声明的要求，并且能够通过下面的命令验证相关的存储映像已经存在于Ceph存储集群之上：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd <span class="built_in">ls</span> -p kube</span></span><br><span class="line">kubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e</span><br></pre></td></tr></table></figure><p>另外，该PV继承自存储类fast-rbd中的回收策略为Delete，这也是存储类默认使用的回收策略，因此，删除其绑定的PVC对象也将删除该PV对象。对于多数持久存储场景而言，这可能是存在着一定风险的策略，建议定义存储类时手动修改该策略。感兴趣的读者可自行测试这种级联删除的效果。</p><h2 id="5-6-容器存储接口CSI"><a href="#5-6-容器存储接口CSI" class="headerlink" title="5.6 容器存储接口CSI"></a>5.6 容器存储接口CSI</h2><p>存储卷管理器通过调用存储卷插件实现当前节点上存储卷相关的附加、分离、挂载/卸载等操作，对于未被Kubernetes内置（In-Tree）的卷插件所支持的存储系统或服务来说，扩展定义新的卷插件是解决问题的唯一途径。但将存储供应商提供的第三方存储代码打包到Kubernetes的核心代码可能会导致可靠性及安全性方面的问题，因而这就需要一种简单、便捷的、外置于Kubernetes代码树（Out-Of-Tree）的扩展方式，FlexVolume和CSI（容器存储接口）就是这样的存储卷插件，换句话说，它们自身是内置的存储卷插件，但实现的却是第三方存储卷的扩展接口。</p><h3 id="5-6-1-CSI基础"><a href="#5-6-1-CSI基础" class="headerlink" title="5.6.1 CSI基础"></a>5.6.1 CSI基础</h3><p>FlexVolume是Kubernetes自v1.8版本进入GA（高可用）阶段的一种存储插件扩展方式，它要求将外部插件的二进制文件部署在预先配置的路径中（例如/usr/libexec/kubernetes/kubelet-plugins/volume/exec/），并设定系统环境满足其正常运行所需要的全部依赖关系。事实上，一个FlexVolume类型的插件就是一款可被kubelet驱动的可执行文件，它实现了特定存储的挂载、卸载等存储插件接口，而对该类插件的调用相当于请求运行该程序文件，并要求返回JSON格式的响应内容。<br>第三方需要提供的CSI组件主要是两个CSI存储卷驱动程序，一个是节点插件（Identity+Node），用于同kubelet交互实现存储卷的挂载和卸载等功能，另一个是自定义控制器（Identity+Controller），负责处理来自API Server的存储卷管理请求，例如创建和删除等，它的功能类似于控制器管理器中的PV控制器，如图5-11中实线的圆角方框所示。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220207105317056.png" alt="../img/image-20220207105317056"></p><p>kubelet对存储卷的挂载和卸载操作将通过UNIX Socket调用在同一主机上运行的外部CSI卷驱动程序完成。初始化外部CSI卷驱动程序时，kubelet必须调用CSI方法NodeGetInfo才能将Kubernetes的节点名称映射为CSI的节点标识（NodeID）。于是，为了降低部署外部容器化的CSI卷驱动程序时的复杂度，Kubernetes团队提供了一个以Sidecar容器运行的应用——Kubernetes CSI Helper，以辅助自动完成UNIX Sock套接字注册及NodeID的初始化，如图5-11中的node-driver-registrar容器所示。<br>不受Kubernetes信任的第三方卷驱动程序运行为独立的容器，它无法直接同控制器管理器通信，而是要借助于Kubernetes API Server进行；换句话说，CSI存储卷驱动需要注册监视（watch）API Server上的特定资源并针对存储卷管理器面向其存储卷的请求执行预配、删除、附加和分离等操作。同样为了降低外部容器化CSI卷驱动及控制器程序部署的复杂度，Kubernetes团队提供了一到多个以Sidecar容器运行的代理应用Kubernetes to CSI来负责监视Kubernetes API，并触发针对CSI卷驱动程序容器的相应操作，如图5-11中的external-attacher和external-privisioner等，它们各自的简要功能如下所示。</p><ul><li>external-privisioner：CSI存储卷的创建和删除。</li><li>external-attacher：CSI存储卷的附加和分离。</li><li>external-resizer：CSI存储卷的容量调整（扩缩容）。</li><li>external-snapshotter：CSI存储卷的快照管理（创建和删除等）。</li></ul><p>尽管Kubernetes并未指定CSI卷驱动程序的打包标准，但它提供了以下建议，以简化容器化CSI卷驱动程序的部署。</p><blockquote><p>1）创建一个独立CSI卷驱动容器镜像，由其实现存储卷插件的标准行为，并在运行时通过UNIX Socket公开其API。<br>2）将控制器级别的各辅助容器（external-privisioner和external-attacher等）以Sidecar的形式同带有自定义控制器功能的CSI卷驱动程序容器运行在同一个Pod中，而后借助StatefulSet或Deployment控制器资源确保各辅助容器可正常运行相应数目的实例副本，将负责各容器间通信的UNIX Socket存储到共享的emptyDir存储卷上。<br>3）将节点上需要的辅助容器node-driver-registrar以Sidecar的形式与运行CSI卷驱动程序的容器运行在同一Pod中，而后借助DaemonSet控制器资源确保辅助容器可在每个节点上运行一个实例。<br>下一节将以Longhorn存储系统为例简单说明CSI卷插件解决方案的部署及简单使用方式。</p></blockquote><h3 id="5-6-2-Longhorn存储系统"><a href="#5-6-2-Longhorn存储系统" class="headerlink" title="5.6.2　Longhorn存储系统"></a>5.6.2　Longhorn存储系统</h3><p> Longhorn是由Rancher实验室创建的一款云原生的、轻量级、可靠且易用的开源分布式块存储系统，后来由CNCF孵化。它借助CSI存储卷插件以外置的存储解决方案形式运行。Longhorn遵循微服务的原则，利用容器将小型独立组件构建为分布式块存储，并使用编排工具来协调这些组件，从而形成弹性分布式系统。部署到Kubernetes集群上之后，Longhorn会自动将集群中所有节点上可用的本地存储（默认为/var/lib/longhorn/目录所在的设备）聚集为存储集群，而后利用这些存储管理分布式、带有复制功能的块存储，且支持快照及数据备份操作。<br> 面向现代云环境设计的存储系统的控制器随着待编排存储卷数量的急速增加也变得高度复杂。为了摆脱这种困境，Longhorn充分利用了近年来关于如何编排大量容器的关键技术，采用微服务的设计模式，将大型复杂的存储控制器切分为每个存储卷一个专用的、小型存储控制器，而后借助现代编排工具来管理这些控制器，从而将每个CSI卷构建为一个独立的微服务。如图5-12所示的存储架构中，3个Pod分别使用了一个Longhorn存储卷，每个卷有一个专用的控制器（Engine）资源和两个副本（Replica）资源，它们都是为了便于描述其应用而由Longhorn引入的自定义资源类型。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123101611997.png" alt="../img/image-20220123101611997"></p><p>Engine容器仅负责单个存储卷的管理，其生命周期与存储卷相同，因而它并非真正的CSI插件级别的卷控制器或节点插件。Longhorn上负责处理来自Kubernetes CSI卷插件的API调用，以及完成存储卷管理的组件是Longhorn Manager（node-driver-registrar），它是一个容器化应用且受DaemonSet控制器资源编排，在Kubernetes集群的每个节点上运行一个副本。Longhorn Manager持续监视Kubernetes API上与Longhorn存储卷相关的资源变动，一旦发现新的资源创建，它负责在该卷附加的节点（即Pod被Kubernetes调度器绑定的目标节点）上创建一个Engine资源对象，并在副本相关的每个目标节点上相应创建一个Replica资源对象。<br>Kubernetes集群内部通过CSI插件接口调用Longhorn插件以管理相关类型的存储卷，而Longhorn存储插件则基于Longhorn API与Longhorn Manager进行通信，卷管理之外的其他功能则要依赖Longhorn UI完成，例如快照、备份、节点和磁盘的管理等。另外，Longhorn的块设备存储卷的实现建立在iSCSI协议之上，因而需要调用Longhorn存储卷的Pod所在节点必须部署了相关的程序包，例如open-iscsi或iscsiadm等。<br>目前版本（v1.0.1）的Longhorn要求运行于v.1.13或更高版本的Docker环境下，以及v.1.4或更高版本的Kubernetes之上，并且要求各节点部署了open-iscsi、curl、findmnt、grep、awk、blkid和lsblk等程序包。基础环境准备完成后，我们使用类似如下的命令即能完成Longhorn应用的部署。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f \</span></span><br><span class="line"><span class="language-bash">    https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml</span></span><br></pre></td></tr></table></figure><p>该部署清单会在默认的longhorn-system名称空间下部署csi-attacher、csi-provisioner、csi-resizer、engine-../img/image-ei、longhorn-csi-plugin和longhorn-manager等应用相关的Pod对象，待这些Pod对象成功转为Running状态之后即可测试使用Longhorn CSI插件。<br>该部署清单还会默认创建如下面资源清单中定义的名为longhorn的StorageClass资源，它以部署好的Longhorn为后端存储系统，支持存储卷动态预配机制。我们也能够以类似的方式定义基于该存储系统的、使用了不同配置的其他StorageClass资源，例如仅有一个副本以用于测试场景或对数据可靠性要求并非特别高的应用等。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span>               <span class="comment"># 资源类型</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span>    <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">longhorn</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">driver.longhorn.io</span>  <span class="comment"># 存储供给驱动</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span>       <span class="comment"># 是否支持存储卷弹性扩缩容</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">numberOfReplicas:</span> <span class="string">&quot;3&quot;</span>          <span class="comment"># 副本数量</span></span><br><span class="line">  <span class="attr">staleReplicaTimeout:</span> <span class="string">&quot;2880&quot;</span>    <span class="comment"># 过期副本超时时长</span></span><br><span class="line">  <span class="attr">fromBackup:</span> <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>随后，我们随时可以按需创建基于该存储类的PVC资源来使用Longhorn存储系统上的持久存储卷提供的存储空间。下面的示例资源清单（pvc-dyn-longhorn-demo.yaml）便定义了一个基于Longhorn存储类的PVC，它请求使用2GB的空间。</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-dyn-longhorn-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br></pre></td></tr></table></figure><p>如前所述，Longhorn存储设备支持动态预配，于是以默认创建的存储类Longhorn为模板的PVC在无满足其请求条件的PV时，可由控制器自动创建出适配的PV卷来。下面两条命令及结果也反映了这种预配机制。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-dyn-longhorn-demo.yaml</span> </span><br><span class="line">persistentvolumeclaim/pvc-dyn-longhorn-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-dyn-longhorn-demo</span></span><br><span class="line">NAME                    STATUS  VOLUME                                 CAPACITY…</span><br><span class="line">pvc-dyn-longhorn-demo   Bound   pvc-c67415ae-560b-49c7-8515-3467f4160794   2Gi…</span><br></pre></td></tr></table></figure><p>对于每个存储卷，Longhorn存储系统都会使用自定义的Volumes类型资源对象维持及跟踪其运行状态，每个Volumes资源都会有一个Engines资源对象作为其存储控制器，如下面的两个命令及结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get volumes -n longhorn-system</span></span><br><span class="line">NAME                                  AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794   90s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get engines -n longhorn-system</span></span><br><span class="line">NAME                                            AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204   2m10s</span><br></pre></td></tr></table></figure><p>Engines资源对象的详细描述或资源规范中的spec和status字段记录有当前资源的详细信息，包括关联的副本、purge状态、恢复状态和快照信息等，为了节约篇幅，下面的命令仅给出了部分运行结果。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe engines pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \</span></span><br><span class="line"><span class="language-bash">          -n longhorn-system</span></span><br><span class="line">……</span><br><span class="line">Spec:</span><br><span class="line">  Backup Volume:     </span><br><span class="line">  Desire State:      stopped</span><br><span class="line">  Disable Frontend:  false</span><br><span class="line">  Engine ../img/image:      longhornio/longhorn-engine:v1.0.1</span><br><span class="line">  Frontend:          blockdev</span><br><span class="line">  Log Requested:     false</span><br><span class="line">  Node ID:               # 绑定的节点，它必须与调用了该存储卷的Pod运行于同一节点</span><br><span class="line">  Replica Address Map:   # 关联的存储卷副本</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3:  10.244.3.58:10000</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050:  10.244.2.53:10000</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db:  10.244.1.61:10000</span><br><span class="line">  Volume Name:  pvc-c67415ae-560b-49c7-8515-3467f4160794</span><br><span class="line">  Volume Size:  2147483648</span><br></pre></td></tr></table></figure><p>Replicas也是Longhorn提供的一个独立资源类型，每个资源对象对应着一个存储卷副本，如下面的命令结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicas -n longhorn-system</span></span><br><span class="line">NAME                                         AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3   2m36s</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050   2m36s</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db   2m36s</span><br></pre></td></tr></table></figure><p>基于Longhorn存储卷的PVC被Pod引用后，Pod所在的节点便是该存储卷Engine对象运行所在的节点，Engine的状态也才会由Stopped转为Running。示例清单volumes-pvc-longhorn-demo.yaml定义了一个调用pvc/pvc-dyn-longhorn-demo资源的Pod资源，因而该Pod所在的节点便是该PVC后端PV相关的Engine绑定的节点，如下面3个命令及其结果所示。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-pvc-longhorn-demo.yaml</span> </span><br><span class="line">pod/volumes-pvc-longhorn-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/volumes-pvc-longhorn-demo -o jsonpath=<span class="string">&#x27;&#123;.spec.nodeName&#125;&#x27;</span></span></span><br><span class="line">k8s-node03.ilinux.io</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get engines/pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \</span></span><br><span class="line"><span class="language-bash">    -n longhorn-system -o jsonpath=<span class="string">&#x27;&#123;.spec.nodeID&#125;&#x27;</span></span></span><br><span class="line">k8s-node03.ilinux.io</span><br></pre></td></tr></table></figure><p>由以上Longhorn存储系统的部署及测试结果可知，该存储系统不依赖于任何外部存储设备，仅基于Kubernetes集群工作节点本地的存储即能正常提供存储卷服务，且支持动态预配等功能。但应用于生产环境时，还是有许多步骤需要优化，例如将数据存储与操作系统等分离到不同的磁盘设备，是否可以考虑关闭底层的RAID设备等，具体请参考Longhorn文档中的最佳实践。<br>为了便于通过Kubernetes集群外部的浏览器访问该用户接口，我们需要把相关的Service对象的类型修改为NodePort。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch svc/longhorn-frontend -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&#x27;</span> -n longhorn-system</span></span><br><span class="line">service/longhorn-frontend patched</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get svc/longhorn-frontend -n longhorn-system -o jsonpath=<span class="string">&#x27;&#123;.spec.ports[0].nodePort&#125;&#x27;</span></span></span><br><span class="line">30180</span><br></pre></td></tr></table></figure><p>随后，我们经由任意一个节点的IP地址节点端口（例如上面命令中自动分配而来的30180）即可访问该UI，如图5-13所示。节点、存储卷、备份和系统设置导航标签各自给出了相关功能的配置入口，感兴趣的读者可自行探索其使用细节。</p><p><img src="/blog.github.io/2022/02/10/Kubernetes-magedu/../img/image-20220123102448325.png" alt="../img/image-20220123102448325"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;1-Kubernetes集群架构&quot;&gt;&lt;a href=&quot;#1-Kubernetes集群架构&quot; class=&quot;headerlink&quot; title=&quot;1 Kubernetes集群架构&quot;&gt;&lt;/a&gt;1 Kubernetes集群架构&lt;/h1&gt;&lt;p&gt;Kubernetes属于典型</summary>
      
    
    
    
    <category term="Kubernetes" scheme="https://marmotad.github.io/categories/Kubernetes/"/>
    
    
    <category term="博客           //多个标签可以这样添加" scheme="https://marmotad.github.io/tags/%E5%8D%9A%E5%AE%A2-%E5%A4%9A%E4%B8%AA%E6%A0%87%E7%AD%BE%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E6%B7%BB%E5%8A%A0/"/>
    
    <category term="hexo" scheme="https://marmotad.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>git及CI/CD</title>
    <link href="https://marmotad.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/"/>
    <id>https://marmotad.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/</id>
    <published>2022-02-09T06:41:10.000Z</published>
    <updated>2022-02-09T07:04:10.957Z</updated>
    
    <content type="html"><![CDATA[<h2 id="CI-CD的功能"><a href="#CI-CD的功能" class="headerlink" title="CI/CD的功能"></a>CI/CD的功能</h2><p>CI/CD 是一种通过在应用开发阶段引入<font color="red">自动化</font>来频<font color="red">频繁</font>向客户交付应用的方法。CI/CD 的核心概念是<font color="red">持续集成、持续交付和持续部署</font>。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“<a href="https://www.solutionsiq.com/agile-glossary/integration-hell/">集成地狱</a>”）。</p><p>具体而言，<font color="red">CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）</font>。这些关联的事务通常被统称为“CI/CD 管道”，由<a href="https://www.redhat.com/zh/topics/devops">开发和运维团队</a>以敏捷方式协同支持。</p><h2 id="CI-是什么？CI-和-CD-有什么区别？"><a href="#CI-是什么？CI-和-CD-有什么区别？" class="headerlink" title="CI 是什么？CI 和 CD 有什么区别？"></a>CI 是什么？CI 和 CD 有什么区别？</h2><p>CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。</p><p>CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。</p><p>持续<em>交付</em>通常是指开发人员对应用的更改会自动进行错误测试并上传到存储库（如 <a href="https://redhatofficial.github.io/#!/main">GitHub</a> 或容器注册表），然后由运维团队将其部署到实时生产环境中。这旨在解决开发和运维团队之间可见性及沟通较差的问题。因此，持续交付的目的就是确保尽可能减少部署新代码时所需的工作量。</p><p>持续<em>部署</em>（另一种“CD”）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用。它主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。</p><p><img src="/blog.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/ci-cd-flow-desktop_1-16443901866861.png" alt="CI/CD 流程"></p><p>CI/CD 既可能仅指持续集成和持续交付构成的关联环节，也可以指持续集成、持续交付和持续部署这三项构成的关联环节。更为复杂的是，有时“持续交付”也包含了持续部署流程。</p><p>归根结底，我们没必要纠结于这些语义，您只需记得 CI/CD 其实就是一个流程（通常形象地表述为管道），用于实现应用开发中的高度持续自动化和持续监控。因案例而异，该术语的具体含义取决于 CI/CD 管道的自动化程度。许多企业最开始先添加 CI，然后逐步实现交付和部署的自动化（例如作为<a href="https://www.redhat.com/zh/topics/cloud-native-apps">云原生应用</a>的一部分）。</p><hr><h2 id="CI-持续集成（Continuous-Integration）"><a href="#CI-持续集成（Continuous-Integration）" class="headerlink" title="CI 持续集成（Continuous Integration）"></a>CI 持续集成（Continuous Integration）</h2><p><a href="https://www.redhat.com/zh/solutions/cloud-native-development">现代应用开发</a>的目标是让多位开发人员同时处理同一应用的不同功能。但是，如果企业安排在一天内将所有分支源代码合并在一起（称为“<a href="https://thedailywtf.com/articles/Happy_Merge_Day!">合并日</a>”），最终可能造成工作繁琐、耗时，而且需要手动完成。这是因为当一位独立工作的开发人员对应用进行更改时，有可能会与其他开发人员同时进行的更改发生冲突。如果每个开发人员都自定义自己的本地<a href="https://www.redhat.com/zh/topics/middleware/what-is-ide">集成开发环境（IDE）</a>，而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。</p><p>持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。</p><p><a href="https://developers.redhat.com/blog/2017/09/06/continuous-integration-a-typical-process/">进一步了解技术细节</a></p><hr><h2 id="CD-持续交付（Continuous-Delivery）"><a href="#CD-持续交付（Continuous-Delivery）" class="headerlink" title="CD 持续交付（Continuous Delivery）"></a>CD 持续交付（Continuous Delivery）</h2><p>完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。</p><p>在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。</p><hr><h2 id="CD-持续部署（Continuous-Deployment）"><a href="#CD-持续部署（Continuous-Deployment）" class="headerlink" title="CD 持续部署（Continuous Deployment）"></a>CD 持续部署（Continuous Deployment）</h2><p>对于一个成熟的 CI/CD 管道来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。由于在生产之前的管道阶段没有手动门控，因此持续部署在很大程度上都得依赖精心设计的测试自动化。</p><p>实际上，持续部署意味着开发人员对应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这更加便于持续接收和整合用户反馈。总而言之，所有这些 CI/CD 的关联步骤都有助于降低应用的部署风险，因此更便于以小件的方式（而非一次性）发布对应用的更改。不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期投资还是会很大。</p><h2 id="版本控制系统"><a href="#版本控制系统" class="headerlink" title="版本控制系统"></a>版本控制系统</h2><h3 id="什么是版本控制系统"><a href="#什么是版本控制系统" class="headerlink" title="什么是版本控制系统"></a>什么是版本控制系统</h3><p>版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份<font color="red">以便恢复以前的版本</font>的软件工程技术</p><h3 id="版本控制系统解决的问题"><a href="#版本控制系统解决的问题" class="headerlink" title="版本控制系统解决的问题"></a>版本控制系统解决的问题</h3><p>1、追溯文件历史变更</p><p>2、多人团队协同开发</p><p>3、代码集中管理</p><h3 id="常见的版本控制系统（集中式VS分布式）"><a href="#常见的版本控制系统（集中式VS分布式）" class="headerlink" title="常见的版本控制系统（集中式VS分布式）"></a>常见的版本控制系统（集中式VS分布式）</h3><h4 id="Subversion集中式版本控制系统"><a href="#Subversion集中式版本控制系统" class="headerlink" title="Subversion集中式版本控制系统"></a>Subversion集中式版本控制系统</h4><ul><li><p>Subversion的特点概括起来主要由以下几条：</p><blockquote><ul><li>每个版本库有唯一的URL（官方地址），每个用户都从这个地址获取代码和数据；</li><li>获取代码的更新，也只能连接到这个唯一的版本库，同步以取得最新数据；</li><li>提交必须有网络连接（非本地版本库）；</li><li>提交需要授权，如果没有写权限，提交会失败；</li><li>提交并非每次都能够成功。如果有其他人先于你提交，会提示“改动基于过时的版本，先更新再提交”… 诸如此类；</li><li>冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。</li></ul></blockquote></li></ul><p><img src="/blog.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/20190113235821878.png" alt="在这里插入图片描述"></p><p><font color="red">好处：</font>每个人都可以一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限。</p><p><font color="blue">缺点：</font>中央服务器的单点故障。</p><p>若是宕机一小时，那么在这一小时内，谁都无法提交更新、还原、对比等，也就无法协同工作。如果中央服务器的磁盘发生故障，并且没做过备份或者备份得不够及时的话，还会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，被客户端提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人提取出来。</p><p>Subversion<font color="red">原理上只关心文件内容的具体差异</font>。每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容。</p><h4 id="Git属于分布式的版本控制系统"><a href="#Git属于分布式的版本控制系统" class="headerlink" title="Git属于分布式的版本控制系统"></a>Git属于分布式的版本控制系统</h4><p>Git记录版本历史<font color="red">只关心文件数据的整体是否发生变化</font>。Git <font color="red">不保存文件内容前后变化的差异数据</font>。</p><p>实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一连接。</p><p>在分布式版本控制系统中，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程。</p><p>另外，因为Git在本地磁盘上就保存着所有有关当前项目的历史更新，并且Git中的绝大多数操作都只需要访问本地文件和资源，不用连网，所以处理起来速度飞快。用SVN的话，没有网络或者断开VPN你就无法做任何事情。但用Git的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程的镜像仓库。换作其他版本控制系统，这么做几乎不可能，抑或是非常麻烦。</p><ul><li>Git具有以下特点：</li></ul><blockquote><ul><li>Git中每个克隆(clone)的版本库都是平等的。你可以从任何一个版本库的克隆来创建属于你自己的版本库，同时你的版本库也可以作为源提供给他人，只要你愿意。</li><li>Git的每一次提取操作，实际上都是一次对代码仓库的完整备份。</li><li>提交完全在本地完成，无须别人给你授权，你的版本库你作主，并且提交总是会成功。</li><li>甚至基于旧版本的改动也可以成功提交，提交会基于旧的版本创建一个新的分支。</li><li>Git的提交不会被打断，直到你的工作完全满意了，PUSH给他人或者他人PULL你的版本库，合并会发生在PULL和PUSH过程中，不能自动解决的冲突会提示您手工完成。</li><li>冲突解决不再像是SVN一样的提交竞赛，而是在需要的时候才进行合并和冲突解决。</li><li>Git 也可以模拟集中式的工作模式</li><li>Git版本库统一放在服务器中</li><li>可以为 Git 版本库进行授权：谁能创建版本库，谁能向版本库PUSH，谁能够读取（克隆）版本库</li><li>团队的成员先将服务器的版本库克隆到本地；并经常的从服务器的版本库拉（PULL）最新的更新；</li><li>团队的成员将自己的改动推（PUSH）到服务器的版本库中，当其他人和版本库同步（PULL）时，会自动获取改变</li><li>Git 的集中式工作模式非常灵活</li><li>你完全可以在脱离Git服务器所在网络的情况下，如移动办公／出差时，照常使用代码库</li><li>你只需要在能够接入Git服务器所在网络时，PULL和PUSH即可完成和服务器同步以及提交</li><li>Git提供 rebase 命令，可以让你的改动看起来是基于最新的代码实现的改动</li><li>Git 有更多的工作模式可以选择，远非 Subversion可比</li></ul></blockquote><h2 id="git基本使用"><a href="#git基本使用" class="headerlink" title="git基本使用"></a>git基本使用</h2><h3 id="配置git"><a href="#配置git" class="headerlink" title="配置git"></a>配置git</h3><ul><li>通常只需要配置你是谁，邮箱是什么。就可以知道是谁提交了什么内容</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# git config --global user.name &quot;fanyang&quot;</span><br><span class="line">[root@localhost ~]# git config --global user.email &quot;fanyang@163.com&quot;</span><br><span class="line">[root@localhost ~]# git config --global color.ui true</span><br><span class="line">[root@localhost ~]# cat .gitconfig</span><br></pre></td></tr></table></figure><p><img src="/blog.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/image-20201119202333081.png" alt="image-20201119202333081"></p><h3 id="git如何提交目录文件到本地仓库"><a href="#git如何提交目录文件到本地仓库" class="headerlink" title="git如何提交目录文件到本地仓库"></a>git如何提交目录文件到本地仓库</h3><p>1、首先创建git仓库，这个目录里的所有文件都可以被git管理起来，每个文件的修改、删除、GIt都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建git工作目录</span></span><br><span class="line">[root@localhost ~]# mkdir /git</span><br><span class="line">[root@localhost ~]# cd /git</span><br><span class="line"><span class="meta"># </span><span class="language-bash">初始化该目录为git仓库</span></span><br><span class="line">[root@localhost git]# git  init</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建新的文件</span></span><br><span class="line">[root@localhost git]<span class="comment"># touch file&#123;1..3&#125;</span></span><br><span class="line">[root@localhost git]<span class="comment"># ls</span></span><br><span class="line">file1  file2  file3</span><br><span class="line"><span class="comment"># 查看git状态</span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line"><span class="comment"># 有三个未提交的文件</span></span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to include <span class="keyword">in</span> what will be committed)</span><br><span class="line"></span><br><span class="line">file1</span><br><span class="line">file2</span><br><span class="line">file3</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use <span class="string">&quot;git add&quot;</span> to track)</span><br><span class="line"><span class="comment"># 添加本地所有文件到本地git缓存</span></span><br><span class="line">[root@localhost git]<span class="comment"># git add .</span></span><br><span class="line"><span class="comment"># 查看git状态</span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use <span class="string">&quot;git rm --cached &lt;file&gt;...&quot;</span> to unstage)</span><br><span class="line"><span class="comment"># 有三个新的文件</span></span><br><span class="line">new file:   file1</span><br><span class="line">new file:   file2</span><br><span class="line">new file:   file3</span><br><span class="line"><span class="comment"># 添加git描述</span></span><br><span class="line">[root@localhost git]<span class="comment"># git commit -m &quot;新增file&#123;1..3&#125;到git仓库&quot;</span></span><br><span class="line">[master (root-commit) 8acf856] 新增file&#123;1..3&#125;到git仓库</span><br><span class="line"> 3 files changed, 0 insertions(+), 0 deletions(-)</span><br><span class="line"> create mode 100644 file1</span><br><span class="line"> create mode 100644 file2</span><br><span class="line"> create mode 100644 file3</span><br><span class="line"><span class="comment"># 修改file1</span></span><br><span class="line">[root@localhost git]<span class="comment"># echo 1 &gt;file1 </span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line">Changes not staged <span class="keyword">for</span> commit:</span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to update what will be committed)</span><br><span class="line">  (use <span class="string">&quot;git checkout -- &lt;file&gt;...&quot;</span> to discard changes <span class="keyword">in</span> working directory)</span><br><span class="line"></span><br><span class="line">modified:   file1</span><br><span class="line"></span><br><span class="line">no changes added to commit (use <span class="string">&quot;git add&quot;</span> and/or <span class="string">&quot;git commit -a&quot;</span>)</span><br><span class="line">[root@localhost git]<span class="comment"># git add .</span></span><br><span class="line">[root@localhost git]<span class="comment"># git commit -m &quot;修改file1&quot;</span></span><br><span class="line">[master 53 30aef] 修改file1</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="文件改名后重新提交到本地git仓库"><a href="#文件改名后重新提交到本地git仓库" class="headerlink" title="文件改名后重新提交到本地git仓库"></a>文件改名后重新提交到本地git仓库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git mv file1 file </span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">renamed:    file1 -&gt; file</span><br></pre></td></tr></table></figure><h3 id="对比文件差异"><a href="#对比文件差异" class="headerlink" title="对比文件差异"></a>对比文件差异</h3><h4 id="对比本地文件和暂存区文件的差异"><a href="#对比本地文件和暂存区文件的差异" class="headerlink" title="对比本地文件和暂存区文件的差异"></a>对比本地文件和暂存区文件的差异</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git diff file</span><br><span class="line">[root@localhost git]# echo test &gt;&gt; file</span><br><span class="line">[root@localhost git]# git diff file</span><br><span class="line">diff --git a/file b/file</span><br><span class="line">index d00491f..c0f2f8d 100644</span><br><span class="line">--- a/file</span><br><span class="line">+++ b/file</span><br><span class="line">@@ -1 +1,2 @@</span><br><span class="line"> 1</span><br><span class="line">+test</span><br></pre></td></tr></table></figure><h4 id="对比暂存区文件和本地git仓库文件差异"><a href="#对比暂存区文件和本地git仓库文件差异" class="headerlink" title="对比暂存区文件和本地git仓库文件差异"></a>对比暂存区文件和本地git仓库文件差异</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git diff file</span><br><span class="line">[root@localhost git]# git diff file --cache file</span><br><span class="line">fatal: option &#x27;--cache&#x27; must come before non-option arguments</span><br><span class="line">[root@localhost git]# git diff file --cached file</span><br><span class="line">fatal: option &#x27;--cached&#x27; must come before non-option arguments</span><br><span class="line">[root@localhost git]# git diff --cached file</span><br><span class="line">diff --git a/file b/file</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000..c0f2f8d</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/file</span><br><span class="line">@@ -0,0 +1,2 @@</span><br><span class="line">+1</span><br><span class="line">+test</span><br><span class="line">[root@localhost git]# </span><br><span class="line">[root@localhost git]# git commit -m &quot;修改file文件&quot;</span><br><span class="line">[master 8b1ecec] 修改file文件</span><br><span class="line"> 2 files changed, 2 insertions(+), 1 deletion(-)</span><br><span class="line"> create mode 100644 file</span><br><span class="line"> delete mode 100644 file1</span><br><span class="line">[root@localhost git]# git diff --cached file</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="文件回滚"><a href="#文件回滚" class="headerlink" title="文件回滚"></a>文件回滚</h3><h4 id="操作导致文件被清空（本地目录与暂存区间的撤销）"><a href="#操作导致文件被清空（本地目录与暂存区间的撤销）" class="headerlink" title="操作导致文件被清空（本地目录与暂存区间的撤销）"></a>操作导致文件被清空（本地目录与暂存区间的撤销）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用以前提交到暂存区的内容覆盖本地目录</span></span><br><span class="line">[root@localhost git]# echo &gt; file</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">modified:   file</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br><span class="line">[root@localhost git]# git checkout file</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br></pre></td></tr></table></figure><h4 id="本地文件误操作提交至暂存区"><a href="#本地文件误操作提交至暂存区" class="headerlink" title="本地文件误操作提交至暂存区"></a>本地文件误操作提交至暂存区</h4><p>本地<font color="red">仓库</font>覆盖暂存区—–&gt; 暂存区覆盖本地<font color="red">目录</font></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# echo ddd &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">modified:   file</span><br><span class="line"></span><br><span class="line">[root@localhost git]# git reset HEAD file</span><br><span class="line">Unstaged changes after reset:</span><br><span class="line">Mfile</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">modified:   file</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br><span class="line">[root@localhost git]# git checkout file</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br></pre></td></tr></table></figure><h4 id="多次提交到本地仓库后回滚"><a href="#多次提交到本地仓库后回滚" class="headerlink" title="多次提交到本地仓库后回滚"></a>多次提交到本地仓库后回滚</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# echo test &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add  .</span><br><span class="line">[root@localhost git]# ls</span><br><span class="line">file  file2  file3</span><br><span class="line">[root@localhost git]# git commit -m &quot;test&quot;</span><br><span class="line">[master c691c60] test</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line">[root@localhost git]# echo test1 &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git commit -m &quot;test1&quot;</span><br><span class="line">[master 3595d2b] test1</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line">[root@localhost git]# git log --oneline </span><br><span class="line">3595d2b (HEAD -&gt; master) test1</span><br><span class="line">c691c60 test</span><br><span class="line">8b1ecec 修改file文件</span><br><span class="line">5330aef 修改file1</span><br><span class="line">8acf856 新增file&#123;1..3&#125;到git仓库</span><br><span class="line">[root@localhost git]# git reset --hard c691c60</span><br><span class="line">HEAD is now at c691c60 test</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br><span class="line">test</span><br></pre></td></tr></table></figure><h4 id="git-回退后，恢复回退前版本"><a href="#git-回退后，恢复回退前版本" class="headerlink" title="git 回退后，恢复回退前版本"></a>git 回退后，恢复回退前版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git reflog </span><br><span class="line">c691c60 (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to c691c60</span><br><span class="line">3595d2b HEAD@&#123;1&#125;: commit: test1</span><br><span class="line">c691c60 (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: test</span><br><span class="line">8b1ecec HEAD@&#123;3&#125;: commit: 修改file文件</span><br><span class="line">5330aef HEAD@&#123;4&#125;: commit: 修改file1</span><br><span class="line">8acf856 HEAD@&#123;5&#125;: commit (initial): 新增file&#123;1..3&#125;到git仓库</span><br><span class="line">[root@localhost git]# git re</span><br><span class="line">rebase         reflog         remote         repack         replace        request-pull   reset          revert </span><br><span class="line">[root@localhost git]# git reset --hard c691c60</span><br><span class="line">[root@localhost git]# git log --oneline </span><br><span class="line">c691c60 (HEAD -&gt; master) test</span><br><span class="line">8b1ecec 修改file文件</span><br><span class="line">5330aef 修改file1</span><br><span class="line">8acf856 新增file&#123;1..3&#125;到git仓库</span><br></pre></td></tr></table></figure><h3 id="git-分支管理"><a href="#git-分支管理" class="headerlink" title="git 分支管理"></a>git 分支管理</h3><h4 id="查看、创建、切换分支"><a href="#查看、创建、切换分支" class="headerlink" title="查看、创建、切换分支"></a>查看、创建、切换分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看分支</span></span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* master</span><br><span class="line"><span class="meta"># </span><span class="language-bash">创建分支</span></span><br><span class="line">[root@localhost git]# git branch dev</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">  dev</span><br><span class="line">* master</span><br><span class="line"><span class="meta"># </span><span class="language-bash">切换分支</span></span><br><span class="line">[root@localhost git]# git checkout dev</span><br><span class="line">Switched to branch &#x27;dev&#x27;</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* dev</span><br></pre></td></tr></table></figure><h4 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h4><p>master合并到dev—-&gt;测试合并后的dev分支—–&gt;dev分支合并到master</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git merge master</span><br><span class="line">[root@localhost git]# git merge dev</span><br></pre></td></tr></table></figure><h4 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git branch dev -d</span><br><span class="line">Deleted branch dev (was 06b4943).</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* masterl,;</span><br></pre></td></tr></table></figure><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><h4 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">对当前分支当前的版本打标签</span></span><br><span class="line">[root@localhost git]# git tag -a &quot;v1.0&quot; -m &quot;第一个版本&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看当前分支有哪些标签</span></span><br><span class="line">[root@localhost git]# git tag</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看标签内容</span></span><br><span class="line">[root@localhost git]# git show v1.0 </span><br><span class="line"><span class="meta"># </span><span class="language-bash">对指定的<span class="built_in">id</span>打标签</span></span><br><span class="line">[root@localhost git]# git tag -a &quot;v1.0&quot; c691c60 -m &quot;未发布的版本&quot;</span><br><span class="line">fatal: tag &#x27;v1.0&#x27; already exists</span><br></pre></td></tr></table></figure><h4 id="删除标签"><a href="#删除标签" class="headerlink" title="删除标签"></a>删除标签</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git tag -d v0.9 </span><br><span class="line">Deleted tag &#x27;v0.9&#x27; (was 8d74209)</span><br></pre></td></tr></table></figure><h3 id="git-操作远程仓库"><a href="#git-操作远程仓库" class="headerlink" title="git 操作远程仓库"></a>git 操作远程仓库</h3><h4 id="关联远程仓库"><a href="#关联远程仓库" class="headerlink" title="关联远程仓库"></a>关联远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git remote add origin git@172.18.128.4:root/git-test.git</span><br><span class="line">[root@localhost git]# git remote -v</span><br><span class="line">origingit@172.18.128.4:root/git-test.git (fetch)</span><br><span class="line">origingit@172.18.128.4:root/git-test.git (push)</span><br></pre></td></tr></table></figure><h4 id="将本地仓库内容推送到远程仓库"><a href="#将本地仓库内容推送到远程仓库" class="headerlink" title="将本地仓库内容推送到远程仓库"></a>将本地仓库内容推送到远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git commit -m &quot;修改file文件&quot;</span><br><span class="line">[root@localhost git]# git push -u origin master </span><br></pre></td></tr></table></figure><h4 id="删除远程仓库"><a href="#删除远程仓库" class="headerlink" title="删除远程仓库"></a>删除远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git remote remove origin </span><br><span class="line"><span class="meta"># </span><span class="language-bash">origin ：用户名称</span></span><br></pre></td></tr></table></figure><h3 id="新用户加入需要做的"><a href="#新用户加入需要做的" class="headerlink" title="新用户加入需要做的"></a>新用户加入需要做的</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# mkdir /test</span><br><span class="line">[root@localhost ~]# cd /test</span><br><span class="line">[root@localhost test]# git init </span><br><span class="line">Initialized empty Git repository in /test/.git/</span><br><span class="line">[root@localhost test]# git status </span><br><span class="line"><span class="meta"># </span><span class="language-bash">On branch master</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># Initial commit</span></span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">nothing to commit (create/copy files and use <span class="string">&quot;git add&quot;</span> to track)</span></span><br><span class="line">[root@localhost git-test]#     git config --global user.name &quot;Your Name&quot;</span><br><span class="line">[root@localhost git-test]#     git config --global user.email you@example.co</span><br><span class="line">[root@localhost test]# git remote add origin git@172.18.128.4:root/git-test.git</span><br><span class="line">[root@localhost test]# git remote -v</span><br><span class="line">origingit@172.18.128.4:root/git-test.git (fetch)</span><br><span class="line">origingit@172.18.128.4:root/git-test.git (push)</span><br><span class="line">[root@localhost test]# git clone git@172.18.128.4:root/git-test.git</span><br><span class="line">Cloning into &#x27;git-test&#x27;...</span><br><span class="line"></span><br><span class="line">[root@localhost test]# ls</span><br><span class="line">git-test</span><br><span class="line">[root@localhost test]# cd git-test/</span><br><span class="line">[root@localhost git-test]# ls</span><br><span class="line">file  file2  file3  file5  file7  file8</span><br><span class="line">[root@localhost git-test]# touch file9</span><br><span class="line">[root@localhost git-test]# git add .</span><br><span class="line">[root@localhost git-test]# git commit -m &quot;new file&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">On branch master</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Your branch is ahead of <span class="string">&#x27;origin/master&#x27;</span> by 1 commit.</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">  (use <span class="string">&quot;git push&quot;</span> to publish your <span class="built_in">local</span> commits)</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">nothing to commit, working directory clean</span></span><br><span class="line">[root@localhost git-test]# git push origin master </span><br><span class="line">Counting objects: 3, done.</span><br><span class="line">Compressing objects: 100% (2/2), done.</span><br><span class="line">Writing objects: 100% (2/2), 233 bytes | 0 bytes/s, done.</span><br><span class="line">Total 2 (delta 1), reused 0 (delta 0)</span><br><span class="line">To git@172.18.128.4:root/git-test.git</span><br><span class="line">   06b4943..b3fdc9f  master -&gt; master</span><br></pre></td></tr></table></figure><h3 id="同步远程仓库中否代码"><a href="#同步远程仓库中否代码" class="headerlink" title="同步远程仓库中否代码"></a>同步远程仓库中否代码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git-test]# git pull origin master </span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;CI-CD的功能&quot;&gt;&lt;a href=&quot;#CI-CD的功能&quot; class=&quot;headerlink&quot; title=&quot;CI/CD的功能&quot;&gt;&lt;/a&gt;CI/CD的功能&lt;/h2&gt;&lt;p&gt;CI/CD 是一种通过在应用开发阶段引入&lt;font color=&quot;red&quot;&gt;自动化&lt;/fon</summary>
      
    
    
    
    <category term="CI/CD" scheme="https://marmotad.github.io/categories/CI-CD/"/>
    
    
    <category term="博客           //多个标签可以这样添加" scheme="https://marmotad.github.io/tags/%E5%8D%9A%E5%AE%A2-%E5%A4%9A%E4%B8%AA%E6%A0%87%E7%AD%BE%E5%8F%AF%E4%BB%A5%E8%BF%99%E6%A0%B7%E6%B7%BB%E5%8A%A0/"/>
    
    <category term="hexo" scheme="https://marmotad.github.io/tags/hexo/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://marmotad.github.io/2022/02/09/hello-world/"/>
    <id>https://marmotad.github.io/2022/02/09/hello-world/</id>
    <published>2022-02-09T03:10:39.808Z</published>
    <updated>2022-02-09T03:08:03.358Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
