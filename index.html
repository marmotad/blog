<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marmotad.github.io","root":"/blog/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="myBlog">
<meta property="og:type" content="website">
<meta property="og:title" content="marmotad">
<meta property="og:url" content="https://marmotad.github.io/index.html">
<meta property="og:site_name" content="marmotad">
<meta property="og:description" content="myBlog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://marmotad.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>marmotad</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="marmotad" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">marmotad</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/" class="post-title-link" itemprop="url">Pod资源调度器</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-02-24 16:13:36 / 修改时间：16:42:16" itemprop="dateCreated datePublished" datetime="2022-02-24T16:13:36+08:00">2022-02-24</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h1><p>本章讲解了Kubernetes Scheduler的基本工作逻辑，并详细讲解了几种高级调度功能的使用方法。</p>
<ul>
<li>经典调度策略：经过预选、优选、选定和绑定等步骤完成Pod调度，仅支持Predicate、Priority和Bind扩展点，且必须先由default-scheduler调度后才能生效。</li>
<li>调度框架：支持QueueSort、PreFilter、Filter、PostFilter、PreScore、Score、Reserve、PreBind、Bind和Unreserve等扩展点，将内置的预选函数和优选函数全部实现为调度插件，并支持用户自定义插件。</li>
<li>NodeAffinity可用于让Pod选择期望运行的节点或节点类型。</li>
<li>PodAffinity与PodAntiAffinity可用于在指定的拓扑中以特定的要求放置Pod。</li>
<li>PodSpreadContraints允许在指定的拓扑间均匀地分布Pod，是更具一般性的分布形式，支持多重约束，并能结合PodAffinity实现更灵活的分布机制。</li>
<li>污点给节点提供了主动排斥Pod对象的方式，仅那些可以容忍节点污点的Pod可运行在相关节点之上。</li>
<li>优选级和抢占功能为优化集群资源利用率、确保更重要工作负载的运行提供了可行性。</li>
</ul>
<h1 id="Pod资源调度器"><a href="#Pod资源调度器" class="headerlink" title="Pod资源调度器"></a>Pod资源调度器</h1><p>控制平面组件Kubernetes Scheduler就是为工作负载挑选最佳运行节点的调度器。</p>
<h2 id="Kubernetes调度器"><a href="#Kubernetes调度器" class="headerlink" title="Kubernetes调度器"></a>Kubernetes调度器</h2><p>对于每个未绑定至任何工作节点的Pod对象，无论是新创建、被节点驱逐或节点故障等，Kubernetes Scheduler都要使用调度算法从集群中挑选一个最佳目标节点来运行它，如图11-1所示。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223143559546.png" alt="image-20220223143559546">Kubernetes在v1.15版本之后引入的调度框架重构了此前使用的经典调度器架构，它以插件化的方式在多个扩展点实现了调度器的绝大多数功能，替代了经典调度器中以预选（predicate）函数和优选（priority）函数为核心的调度载体，并支持通过Scheduler Extender进行Webhook式扩展的架构，为用户扩展使用自定义调度插件提供了便捷的接口。</p>
<h3 id="调度器基础"><a href="#调度器基础" class="headerlink" title="调度器基础"></a>调度器基础</h3><p>Kubernetes内置了适合绝大多数场景中需求的默认调度器，它支持同时使用算法基于原生及可定制的工具来选出集群中最适合运行当前Pod资源的一个节点。Scheduler是API Server的客户端，它注册并监听所有Pod资源规范中spec.nodeName字段的状态变动信息，并对该字段值为“空”的每个Pod对象启动调度机制。<br>调度器需要根据特定的调度要求对现有节点进行预选，以过滤掉那些无法满足Pod运行条件的节点，而后对满足过滤条件的各个节点进行打分，并按综合得分进行排序，最后从优先级排序结果中挑选出得分最高节点作为适合目标Pod的最佳节点。如果中间任何一个步骤返回了错误信息，调度器就会中止调度过程。调度流程的最后，调度程序在binding（绑定）的过程中将调度决策通知给API Server，如图11-2所示，而后由相应节点的代理程序kubelet启动Pod的创建和启动等过程。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223143631380.png" alt="image-20220223143631380"></p>
<blockquote>
<p>1）节点预选：基于一系列预选规则（例如NodeAffinity和VolumeBinding等）对每个节点进行检查，将那些不符合筛选条件的节点过滤掉；没有节点满足目标Pod的资源需求时，该Pod将被置于Pending状态，直到出现至少一个能够满足条件的节点为止。<br>2）节点优选：根据优先算法（例如ImageLocality和PodTopologySpread等）对预选出的节点打分，并根据最终得分进行优先级排序。<br>3）从优先级排序结果中挑出优先级最高的节点运行Pod对象，最高优先级节点数量多于一个时，则从中随机选择一个作为Pod可绑定的目标Node对象。<br>带有“通用”性质的调度器能在大多数Pod调度场景中工作得很好，但也必定存在无法满足的需求场景，例如根据GPU资源用量调度深度学习类的应用Pod的场景等，扩展新的调度方式成为这类场景中必然要解决的问题。Kubernetes Scheduler支持源代码二次开发、多调度器、Scheduler Extender和Scheduler Framework等几种扩展方式。</p>
</blockquote>
<p>Kubernetes同时提供多个调度器程序的扩展方式剥离了与原调度器程序的耦合关系，这种方式仅要求在那些需要使用自定义调度机制的Pod资源上通过pod.spec.scheduler字段来指定使用的调度器名称即可，如图11-3所示。显然，多个独立的调度器彼此间无协作在集群全局紧密地协作。</p>
<p><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223143700032.png" alt="image-20220223143700032"></p>
<p>另一种扩展方式是基于Scheduler Extender（Webhook）在指定的扩展点对kube-scheduler进行功能扩展，如图11-4所示。但kube-scheduler这种扩展方式也存在不少的问题，例如它仅支持predicate、priority和bind这3个有限的扩展点，通过Webhook进行扩展有一定程度上的性能开销、很难中止调度过程，也无法使用调度器默认的缓存功能等。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223143752224.png" alt="image-20220223143752224">Kubernetes自v1.15版本引入调度框架（Scheduling Framework）为现有的调度程序添加了一组新的“插件”API，从而调度器支持以插件形式对kube-scheduler进行功能扩展，如图11-5所示。与Scheduler Extender不同的是，调度框架支持多插件并存机制，这些插件根据其功能可以在调度的一个或多个扩展点对原有的调度器进行扩展。调度器插件可根据自身的功能注册到一个或多个扩展点并由调度器进行调用，它们或许能够影响调度决策，也可能仅提供有助于调度决策的信息。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223143819333.png" alt="image-20220223143819333"><br>调度框架将每次调度一个Pod的整个过程进一步细分为“调度周期”和“绑定周期”两个阶段，前者负责为Pod选择一个最佳调度节点，后者为完成Pod到节点的绑定执行必要的检测或初始化操作等，它们联合起来称为“调度上下文”。在图11-5中可以看出，调度框架提供了多个扩展点，事实上，其中的Filter相当于传统调度器上的Predicate，Score则是Priority，Bind则保持了原有的名称。</p>
<ul>
<li>QueueSort：注册到该扩展点的插件负责对调度队列中的Pod资源进行排序，但一次仅支持启用单个插件；Pod排序队列的存在使得优选级调度及优选级抢占成为可能。</li>
<li>PreFilter：PreFilter类的插件用于预处理Pod相关信息，或者检查Pod和集群必须满足的条件，任何错误都将会导致调度过程中止而返回。</li>
<li>Filter：该类型的插件负责过滤无法满足Pod资源运行条件的节点，对于每一个节点，调度程序都会按顺序调用每个插件对其进行逐一评估，任何插件拒斥该节点都会直接导致该插件被排除，且不再由后续的插件进行检查。节点过滤能够以并行方式运行，并且在一个调度周期内可以多次调用该扩展点上的插件。</li>
<li>PostFilter：该类插件对成功通过过滤插件检查的节点执行过滤后操作，较早版本的调度框架不支持PreScore，该扩展点后来被重命名为PreScore，而Kubernetes v1.19版本又重新添加了该扩展点。</li>
<li>PreScore：该类插件对成功通过过滤插件检查的节点进行预评分，并生成可由各Score插件共享的状态结果，任何错误都将导致调度过程中止而返回。</li>
<li>Score和NormalizeScore：Score类插件负责对成功通过过滤的节点进行评分和排序，对于每个节点，调度程序会调用每个插件为其打分；NormalizeScore扩展点中注册的插件可为Score扩展点中的同名插件提供节点得分修正逻辑，以使得其满足特定的规范（节点得分满足[MinNodeScore，MaxNodeScore]的范围要求），不提供NormalizeScore插件的话，Score插件自身必须确保得分满足该规范，否则调度周期将被中止。</li>
<li>Reserve：信息类扩展点，一般用于为给定的Pod保留目标节点上的特定资源时提供状态信息，以避免将Pod绑定到目标节点的过程中发生资源争用。</li>
<li>Permit：该类型插件用于准许（approve）、阻止（deny）或延迟（wait)Pod资源的绑定，所有插件都返回approve才意味着该Pod可进入绑定周期，任意一个插件返回deny都会导致Pod重新返回调度队列，并触发Unreserve类型的插件，而返回wait则意味着Pod将保持在该阶段，直接批准而返回approve或超时而返回deny。</li>
<li>PreBind：负责执行绑定Pod之前所需要的所有任务，例如设置存储卷等；任何插件返回错误都会导致该Pod被打回调度队列。</li>
<li>Bind：所有的PreBind类插件完成之后才能运行该类插件，以将Pod绑定至目标节点上，各插件依照其配置的顺序进行调用，或者由某个特定的插件全权“处理”该Pod，从而跳过后续的其他插件。</li>
<li>PostBind：信息类扩展点，相关插件在Pod成功绑定之后被调用，通常用于设置清单关联的资源。</li>
<li>Unreserve：信息类扩展点，对于在Reserve扩展点预留了资源的Pod对象，因被其他扩展点插件所拒绝时，可由该扩展点通知取消为其预留的资源；一般来说，注册到该扩展点的插件也必将注册到Reserve扩展点之上。</li>
</ul>
<p>调度器框架允许单个调度器插件实现多个扩展点，这意味着，我们可以按需在一个插件中只实现一个扩展点，也可以同时实现多个扩展点，例如，内置的插件InterPodAffinity同时实现了PreFilter、Filter、PreScore和Score扩展点。不过，除非特别有必要，否则应该尽力避免将同一个功能需求在不同的插件中重复实现。<br>Kubernetes自v1.18版本起将Scheduling Framework作为调度器的默认实现，此前基于Predicate、Priority和Bind的调度与扩展逻辑正式进入废弃阶段。为了便于描述，我们把Kubernetes v1.17及之前版本中使用的调度器模型称为经典调度器，将其使用的调度模型及配置称为经典调度策略。</p>
<h3 id="经典调度策略"><a href="#经典调度策略" class="headerlink" title="经典调度策略"></a>经典调度策略</h3><p>Kubernetes通用调度程序提供的经典调度策略使用Predicate和Priority函数实现核心调度功能，并支持多调度器和Extender的扩展方式。预选函数是节点过滤器，负责根据待调度Pod的计算资源和存储资源需求，以及节点亲和关系及反亲和关系规范等来过滤节点。优选函数是节点优选级排序工具，负责基于各节点上当前的资源水位、Pod与Node的亲和或反亲和关系、Pod之间的亲和或反亲和关系，以及尽可能将同一组Pod资源合理分散到不同节点上的方式对过滤后的节点进行优选级排序，最高优选级的节点即为待调度Pod资源的最佳运行节点。</p>
<h4 id="节点预选"><a href="#节点预选" class="headerlink" title="节点预选"></a>节点预选</h4><p>预选操作会针对所有或特定样本数量的节点进行，对于每一个节点，调度器将使用配置的预选函数以特定次序进行逐一筛查，其中任何一个预选函数的否决都将导致该节点被过滤掉。若不存在任何一个满足条件的节点，则Pod将被置于Pending状态，直到至少有一个节点用。Kubernetes的每个版本支持的预选函数都可能会发生变动，图11-6给出了Kubernetes v1.17支持的各预选函数及它们的应用次序，实线边框标识的为kube-scheduler程序默认启用的函数。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223173452196.png" alt="image-20220223173452196">这些预选函数根据指定判定标准及各Node对象和当前Pod对象能否适配，按照实现的主要目标大体可分为如下几类。</p>
<ul>
<li><p>节点存储卷数量限制检测：MaxEBSVolumeCount、MaxGCEPDVolumeCount、MaxCSIVolumeCount、MaxAzureDiskVolumeCount和MaxCinderVolumeCount。</p>
</li>
<li><p>检测节点状态是否适合运行Pod：CheckNodeUnschedulable和CheckNodeLabel-Presence。</p>
</li>
<li><p>Pod与节点的匹配度检测：Hostname、PodFitsHostPorts、MatchNodeSelector、NoDisk-Conflict、PodFitsResources、PodToleratesNodeTaints、PodToleratesNodeNoExecuteTaints、CheckVolumeBinding和NoVolumeZoneConflict。</p>
</li>
<li><p>Pod间的亲和关系判定：MatchInterPodAffinity。</p>
</li>
<li><p>将一组Pod打散至集群或特定的拓扑结构中：CheckServiceAffinity和EvenPods-Spread。</p>
</li>
</ul>
<p>在Kubernetes Scheduler上启用相应的预选函数才能实现相关调度机制的节点过滤需求，下面给出了这些于Kubernetes v1.17版本中支持的各预选函数的简要功能，其中仅ServiceAffinity和CheckNodeLabelPresence支持自定义配置，余下的均为静态函数。</p>
<blockquote>
<p>1）CheckNodeUnschedulable：检查节点是否被标识为Unschedulable，以及是否将Pod调度到该类节点之上。<br>2）HostName：若Pod资源通过spec. nodeName明确指定了要绑定的目标节点，则节点名称与与该字段值相同的节点才会被保留。<br>3）PodFitsHostPorts：若Pod容器定义了ports.hostPort属性，该预选函数负责检查其值指定的端口是否已被节点上的其他容器或服务所占用，该端口已被占用的节点将被过滤掉。<br>4）MatchNodeSelector：若Pod资源规范上定义了spec.nodeSelector字段，则仅拥有匹配该标签选择器的标签节点才会被保留。<br>5）NoDiskConflict：检查Pod对象请求的存储卷在此节点是否可用，不存在冲突则通过检查。<br>6）PodFitsResources：检查节点是否有足够资源（例如CPU、内存和GPU等）满足Pod的运行需求。节点声明其资源可用容量，而Pod定义其资源需求，于是调度器会判断节点是否有足够的可用资源运行Pod对象，无法满足则返回失败原因（例如，CPU或内存资源不足等）。调度器评判资源消耗的标准是节点已分配资源量（各容器的requests值之和），而非节点上各Pod已用资源量，但那些在注解中标记为关键性的Pod资源则不受该预选函数控制。<br>7）PodToleratesNodeTaints：检查Pod的容忍度（spec.tolerations字段）是否能够容忍该节点上的污点，不过它仅关注带有NoSchedule和NoExecute两个效用标识的污点。<br>8）PodToleratesNodeNoExecuteTaints：检查Pod的容忍度是否能接纳节点上定义的NoExecute类型的污点。<br>9）CheckNodeLabelPresence：检查节点上某些标签的存在性，要检查的标签以及其可否存在取决于用户的定义。在集群中的部署节点以regions/zones/racks类标签的拓扑方式编制，且基于该类标签对相应节点进行了位置标识时，预选函数可以根据位置标识将Pod调度至此类节点之上。<br>10）CheckServiceAffinity：根据调度的目标Pod对象所属的Service资源已关联的其他Pod对象的位置（所运行节点）来判断当前Pod可运行的目标节点，目的在于将同一Service对象的Pod放置在同一拓扑内（如同一个rack或zone）的节点上以提高效率。<br>11）MaxEBSVolumeCount：检查节点上已挂载的EBS存储卷数量是否超过了设置的最大值。<br>12）MaxGCEPDVolumeCount：检查节点上已挂载的GCE PD存储卷数量是否超过了设置的最大值，默认值为16。<br>13）MaxCSIVolumeCount：检查节点上已挂载的CSI存储卷数量是否超过了设置的最大值。<br>14）MaxAzureDiskVolumeCount：检查节点上已挂载的Azure Disk存储卷数量是否超过了设置的最大值，默认值为16。<br>15）MaxCinderVolumeCount：检查节点上已挂载的Cinder存储卷数量是否超过了设置的最大值。<br>16）CheckVolumeBinding：检查节点上已绑定和未绑定的PVC是否能满足Pod的存储卷需求，对于已绑定的PVC，此预选函数检查给定节点是否能兼容相应PV，而对于未绑定的PVC，预选函数搜索那些可满足PVC申请的可用PV，并确保它可与给定的节点兼容。<br>17）NoVolumeZoneConflict：在给定了存储故障域的前提下，检测节点上的存储卷是否可满足Pod定义的需求。<br>18）EvenPodsSpread：检查节点是否能满足Pod规范中topologySpreadConstraints字段定义的约束，以支持Pod的拓扑感知调度。<br>19）MatchInterPodAffinity：检查给定节点是否能满足Pod对象的亲和性或反亲和性条件，用于实现Pod亲和性调度或反亲和性调度。</p>
</blockquote>
<h4 id="节点优选"><a href="#节点优选" class="headerlink" title="节点优选"></a>节点优选</h4><p>成功通过预选函数过滤的节点将生成一个列表，调度流程随后进入优先级排序阶段。各优选函数主要评定成功通过过滤检查的节点对运行该Pod资源的适配程度。对于每个节点，调度器会使用各个拥有权重值的优选函数分别为其打分（0～10之间的分数），优选函数得出的初始分值乘以其权重为该函数最终分值，而各个优选函数的最终分值之和则是该节点的最终得分，如下面的公式所示。因此，通用调度器为优选函数提供的权重属性赋予了管理员定义优先函数倾向性的能力。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">finalScoreNode = (weight1 * priorityFunc1) + (weight2 * priorityFunc2) + …</span><br></pre></td></tr></table></figure>

<p>下面仍然以Kubernetes v1.17版本为例来说明其支持的各优选函数及其功能，图11-7给出了该版本支持的各优选函数，实线边框标识的为kube-scheduler程序默认启用的优选函数。这些优选函数依然可大体分为节点资源对Pod的适配、节点对Pod的亲和性或反亲和性、Pod间的亲和性或反亲和性，以及将Pod打散为几个评估目标。</p>
<p><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223173521002.png" alt="image-20220223173521002"></p>
<p>下面对这些经典优选函数进行介绍。</p>
<ul>
<li>LeastRequestedPriority：优先将Pod打散至集群中的各节点之上，以让各节点有近似的计算资源消耗比例，适用于集群规模变动较少的场景；其分值由节点空闲资源与节点总容量的比值计算而来，即由CPU或内存资源的总容量减去节点上已有Pod对象需求的容量总和，再减去当前要创建的Pod对象的需求容量得到的结果除以总容量。CPU和内存具有相同权重，资源空闲比例越高的节点得分也就越高，其计算公式为：(cpu((capacity – sum(requested)) * 10 / capacity) + memory((capacity – sum(requested)) * 10 / capacity))/ 2。</li>
<li>MostRequestedPriority：与优选函数LeastRequestedPriority评估节点得分的方法相似，但二者不同的是，当前函数将给予计算资源占用比例更大的节点以更高的得分，计算公式为：(cpu((sum(requested)) * 10 / capacity) + memory((sum(requested)) * 10 / capacity))/ 2。该函数的目标在于优先让节点以满载的方式承载Pod资源，从而能够使用更少的节点数，因而较适用于节点规模可弹性伸缩的集群中，以最大化地节约节点数量。</li>
<li>BalancedResourceAllocation：以CPU和内存资源占用率的相近程度作为评估标准，二者越接近的节点权重越高。该优选函数不能单独使用，它需要和Least-RequestedPriority组合使用来平衡优化节点资源的使用状态，选择在部署当前Pod资源后系统资源更为均衡的节点。</li>
<li>ResourceLimitsPriority：以是否能够满足Pod资源限制为评估标准，能够满足Pod对CPU或（和）内存资源限制的节点将计1分，节点未声明可分配资源或Pod未定义资源限制时不影响节点计分。</li>
<li>RequestedToCapacityRatio：该函数允许用户自定义节点各类资源（例如CPU和内存等）的权重，以便提高大型集群中稀缺资源的利用率；该函数的行为可以通过名为requestedToCapacityRatioArguments的配置选项进行控制，它由shape和resources两个参数组成。</li>
<li>NodeAffinityPriority：节点亲和调度机制，它根据Pod资源规范中的spec.node-Selector来对给定节点进行匹配度检查，成功匹配到的条目越多则节点得分越高。不过，其评估过程使用表示首选亲和的标签选择器PreferredDuringSchedulingIgnoredDuringExecution。</li>
<li>ImageLocalityPriority：镜像亲和调度机制，它根据给定节点上是否拥有运行当前Pod对象的容器所依赖的镜像文件来计算节点得分值，没有Pod对象所依赖的任何镜像文件的节点得分为0，而存在相关镜像文件的各节点中，被Pod依赖到的镜像文件的体积之和越大的节点得分越高。</li>
<li>TaintTolerationPriority：基于Pod资源对节点的污点容忍调度偏好进行优先级评估，它将Pod对象的tolerations列表与节点的污点进行匹配度检查，成功匹配的条目越多，则节点得分越低。</li>
<li>SelectorSpreadPriority：尽可能分散Pod至不同节点上的调度机制，它首先查找标签选择器能匹配当前Pod标签的ReplicationController、ReplicaSet和StatefulSet等控制器对象，而后查找可由这类对象的标签选择器匹配的现存各Pod对象及其所在的节点，而运行此类Pod对象越少的节点得分越高。简单来说，如其名称所示，此优选函数尽量把同一标签选择器匹配到的Pod资源打散到不同的节点上运行。</li>
<li>ServiceSpreadingPriority：类似于SelectorSpreadPriority，它首先查找标签选择器能匹配当前Pod标签的Service对象，而后查找可由这类Service对象的标签选择器匹配的现存各Pod对象及其所在的节点，而运行此类Pod对象越少的节点得分越高。</li>
<li>EvenPodsSpreadPriority：用于将一组特定的Pod对象在指定的拓扑结构上进行均衡打散，打散条件定义在Pod对象的spec.topologySpreadConstraints字段上，它内嵌labelSelector字段指定标签选择器以匹配符合条件的Pod对象，使用topologyKey字段指定目标拓扑结构，使用maxSkew描述最大允许的不均衡数量，而无法满足指定调度条件时的评估策略则由whenUnsatisfiable字段定义，它有两个可用取值，默认值DoNotSchedule表示不予调度，而ScheduleAnyway则表示以满足最小不均衡值的标准进行调度。</li>
<li>EqualPriority：设定所有节点具有相同的权重1。</li>
<li>InterPodAffinityPriority：遍历Pod对象的亲和性条目，并将那些能够匹配到给定节点的条目的权重相加，结果值越大的节点得分越高。</li>
<li>NodePreferAvoidPodsPriority：此优选级函数权限默认为10000，它根据节点是否设置了注解信息scheduler.alpha.kubernetes.io/preferAvoidPods来计算其优选级。计算方式是，给定的节点无此注解信息时，其得分为10乘以权重10000，存在此注解信息时，由ReplicationController或ReplicaSet控制器管控的Pod对象的得分为0，其他Pod对象会被忽略（得最高分）。<br>这些优选函数中，LeastRequestedPriority和BalancedResourceAllocationPriority的目标是根据节点的可分配资源状态优先打散Pod并均衡分配至集群节点，而MostRequestedPriority的目标刚好相反，它是优先将Pod“堆满”一个节点后再启用下一个，因而它们彼此间互斥。<br>另外，除了程序中的默认配置，kube-scheduler启用的预选函数和优选函数也能够通过称为调度策略的配置文件进行自定义。自定义的调度配置文件遵循JSON格式且必须命名为policy.cfg，启用后它将完全覆盖默认的调度策略，因此需要用到的任何预选函数或优选函数必须要在该文件中显式声明。</li>
</ul>
<h3 id="调度器插件"><a href="#调度器插件" class="headerlink" title="调度器插件"></a>调度器插件</h3><p>Kubernetes 1.19版提供了20多个调度器插件，调度周期中与过滤和打分相关的插件同样大体用于检查节点与Pod的匹配度、节点自身的调度限制、Pod与节点的亲和性或反亲和性、Pod间的亲和性与反亲和性，以及将Pod打散到集群或指定拓扑结构中的节点上等不同的目标。</p>
<ul>
<li>PrioritySort：用于为调度队列提供基于Pod优先级的排序方式，仅实现了QueueSort扩展点。</li>
<li>DefaultPreemption：用于为调度流程提供默认的抢占逻辑，仅实现了PostFilter扩展点。</li>
<li>ImageLocality：功能类似于同名的优选函数，仅负责实现Score扩展点。</li>
<li>TaintToleration：用于实现基于Pod容忍度和Node污点的调度机制，它实现了Filter、PreScore和Score这3个扩展点。</li>
<li>NodeName：纯过滤器，仅实现了Filter扩展点，负责检查节点名称与Pod资源规范中的spec.nodeName值是否一致。</li>
<li>NodePorts：检查Pod请求使用的节点端口在该节点上是否可用，实现了PreFilter和Filter扩展点。</li>
<li>NodePreferAvoidPods：根据节点上的注解scheduler.alpha.kubernetes.io/preferAvoidPods对节点进行打分，默认权重较高（10000），它仅实现了Score扩展点。</li>
<li>NodeAffinity：负责实现基于Pod规范的nodeSelector或节点亲和（nodeAffinity）的调度机制，它支持Filter和Score扩展点。</li>
<li>NodeUnschedulable：纯过滤器，仅实现了Filter扩展点，负责将那些.spec.unschedulable字段值为true的节点过滤掉。</li>
<li>NodeLabel：根据节点上配置的标签进行节点过滤和打分，实现了Filter和Score两个扩展点。</li>
<li>VolumeBinding：检查Pod请求的存储卷在节点上是否可用，实现了Filter、Reserve、Unreserve和PreBind扩展点。</li>
<li>VolumeRestrictions：检查节点上挂载的某种特定类型的存储卷是否能满足限制，仅实现了Filter扩展点。</li>
<li>VolumeZone：检查节点上的存储卷是否能满足zone限制，仅实现了Filter扩展点。</li>
<li>InterPodAffinity：用于实现Pod间的亲和或反亲和调度，实现了PreFilter、Filter、PreScore和Score扩展点。</li>
<li>DefaultTopologySpread：倾向于将Service、ReplicaSets或StatefulSets的Pod对象打散并分布到集群不中同的节点之上；该插件实现的扩展点有PreScore和Score两个。</li>
<li>PodTopologySpread：用于控制Pod在集群region/zone/rack/node故障域或者用户自定义的拓扑域中的分布，是支撑Pod topologySpreadConstraints特性的基础组件；该插件实现的扩展点有PreFilter、Filter、PreScore和Score。</li>
<li>ServiceAffinity：同一Service下的Pod对象的反亲和调度机制，倾向于将该Pod资源同其自身所属的Service对象的其他Pod分散运行于不同的节点，实现的扩展点有PreFilter、Filter和Score。</li>
</ul>
<p>经典调度器中的预选函数PodFitsResources，以及优选函数LeastRequested、MostRequested、BalancedResourceAllocation和RequestedToCapacityRatio的功能被整合在名为noderesources的插件目录下，但它们仍以独立的插件名称工作，因而仍可被视作独立的调度器插件，下面是这几个插件的功能说明。</p>
<ul>
<li>NodeResourcesFit：在功能上对应于预选函数PodFitsResources，仅用于实现Filter扩展点。</li>
<li>NodeResourcesLeastAllocated：功能上对应于优选函数LeastRequestedPriority，仅用于实现Score扩展点。</li>
<li>NodeResourcesBalancedAllocation：功能上对应于优选函数BalancedResource-Allocation，仅用于实现Score扩展点。</li>
<li>NodeResourcesMostAllocated：功能上对应于优选函数MostRequestedPriority，且功能与NodeResourcesLeastAllocated互斥，二者通常不应该同时使用，仅用于实现Score扩展点。</li>
<li>RequestedToCapacityRatio：功能上对应于优选函数RequestedToCapacityRatio，通常仅用于实现Score扩展点。</li>
</ul>
<p>经典调度器中使用的检测节点上特定类型存储卷数量限制的预选函数（例如MaxEBSVolumeCount和MaxCSIVolumeCount等）也被整合进同一个插件目录（nodevolumelimits）中，但它们各自仍然作为独立的插件使用，且仅能用于实现Filter扩展点。下面几个是这类插件的功能说明。</p>
<ul>
<li>NodeVolumeLimits：功能同预选函数MaxCSIVolumeCount，检测节点是否满足指定的CSI存储插件类型上的存储卷数量限制。</li>
<li>EBSLimits：功能同预选函数MaxEBSVolumeCount，检测节点是否满足EBS存储卷数量限制，默认为16个。</li>
<li>AzureDiskLimits：功能同预选函数MaxAzureDiskVolumeLimit，检测节点是否满足AzureDisk存储卷数量限制。</li>
<li>CinderLimits：功能同预选函数MaxCinderVolumeCount，检测节点是否可满足Cinder存储卷数量限制。</li>
<li>GCEPDLimits：功能同预选函数MaxGCEPDVolumeCount，检测节点是否满足GCE PD存储卷数量限制。<br>目前，可用于绑定周期的内置调度器插件仅DefaultBinder一个，它为调度流程提供默认的Bind机制，而且仅实现了Bind扩展点。</li>
</ul>
<p>与经典调度器使用调度策略进行配置有所不同的是，调度框架使用调度配置来为调度器提供自定义的配置信息。启用了调度框架的kube-scheduler会自动创建一个名为default-scheduler的Profile文件，它默认启用除NodeResourcesMostAllocated、RequestedToCapacityRatio、CinderLimits、NodeLabel和ServiceAffinity之外的其他调度插件。</p>
<h3 id="配置调度器"><a href="#配置调度器" class="headerlink" title="配置调度器"></a>配置调度器</h3><p>调度器程序kube-scheduler使用KubeSchedulerConfiguration格式的配置文件，且支持通过–config选项加载用户自定义的遵循该格式的配置文件。该类配置文件遵循YAML或JSON数据规范，隶属于kubescheduler.config.k8s.io这一API群组。截至本章编写时，该API群组存在v1alpha2和v1beta1两个主要版本，Kubernetes v1.18及之后的版本才能支持v1alpha2，且自Kubernetes v1.20版本晋升为v1beta1。下面列出了v1alpha2版本的简要配置格式。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1alpha2</span> <span class="comment"># v1alpha2版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"><span class="attr">AlgorithmSource:</span>  <span class="comment"># 指定调度算法配置源，从v1alpha2版本起该配置进入废弃阶段</span></span><br><span class="line">  <span class="string">Policy：</span>                        <span class="comment"># 基于调度策略的调度算法配置源</span></span><br><span class="line">    <span class="attr">File:</span>                         <span class="comment"># 文件格式的调度策略</span></span><br><span class="line">      <span class="string">Path</span> <span class="string">&lt;string&gt;:</span>              <span class="comment"># 调度策略文件policy.cfg的位置</span></span><br><span class="line">    <span class="attr">ConfigMap:</span>                    <span class="comment"># ConfigMap格式的调度策略</span></span><br><span class="line">      <span class="string">Namespace</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 调度策略ConfigMap资源隶属的名称空间</span></span><br><span class="line">      <span class="string">Name</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># ConfigMap资源的名称</span></span><br><span class="line">  <span class="string">Provider</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># 配置使用的调度算法的名称，例如DefaultProvider</span></span><br><span class="line"><span class="attr">LeaderElection:</span> &#123;&#125;                <span class="comment"># 多kube-scheduler实例并存时使用的领导选举算法</span></span><br><span class="line"><span class="attr">ClientConnection:</span> &#123;&#125;              <span class="comment"># 与API Server通信时提供给代理服务器的配置信息</span></span><br><span class="line"><span class="string">HealthzBindAddress</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 响应健康状态检测的服务器监听的地址和端口</span></span><br><span class="line"><span class="string">MetricsBindAddress</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 响应指标抓取请求的服务器监听的地址和端口</span></span><br><span class="line"><span class="string">DisablePreemption</span> <span class="string">&lt;bool&gt;</span>          <span class="comment"># 是否禁用抢占模式，false表示不禁用</span></span><br><span class="line"><span class="string">PercentageOfNodesToScore</span> <span class="string">&lt;int32&gt;</span>  <span class="comment"># 需要过滤出的可用节点百分比</span></span><br><span class="line"><span class="string">BindTimeoutSeconds</span>  <span class="string">&lt;int64&gt;</span>       <span class="comment"># 绑定操作的超时时长，必须使用非负数</span></span><br><span class="line"><span class="string">PodInitialBackoffSeconds</span>  <span class="string">&lt;int64&gt;</span> <span class="comment"># 不可调度Pod的初始补偿时长，默认值为1</span></span><br><span class="line"><span class="string">PodMaxBackoffSeconds</span> <span class="string">&lt;int64&gt;</span>      <span class="comment"># 不可调度Pod的最大补偿时长，默认为10</span></span><br><span class="line"><span class="string">Profiles</span> <span class="string">&lt;[]string&gt;</span> <span class="comment"># 加载的KubeSchedulerProfile配置列表，v1alpha2支持加载多个配置列表</span></span><br><span class="line"><span class="string">Extenders</span> <span class="string">&lt;[]Extender&gt;</span>            <span class="comment"># 加载的Extender列表</span></span><br></pre></td></tr></table></figure>

<p>由上面的KubeSchedulerConfiguration配置格式可知，目前Kubernetes Scheduler支持调度策略和调度配置两种调度器配置机制，前者遵循传统调度器的预选、优选和选择等工作逻辑，而后者则仅能够由新式调度框架通过扩展点来支持。kube-scheduler默认会生成KubeSchedulerConfiguration格式的配置，我们可通过其命令行选项–write-config-to将其输出到指定的文件中。</p>
<h4 id="调度策略"><a href="#调度策略" class="headerlink" title="调度策略"></a>调度策略</h4><p>调度策略通过指定预选策略和优选函数分别实现节点过滤与计分功能，相关的策略可保存在配置文件或ConfigMap资源中，而后在KubeSchedulerConfiguration配置中由AlgorithmSource字段引用，或者直接在kube-scheduler程序上使用选项进行指定。传统的调度策略简要配置格式如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Policy</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="string">Predicates</span> <span class="string">&lt;[]object&gt;</span>                 <span class="comment"># Predicate对象列表</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Name</span> <span class="string">&lt;string&gt;</span>                       <span class="comment"># Predicate名称</span></span><br><span class="line">  <span class="string">Argument</span> <span class="string">&lt;Object&gt;</span>                   <span class="comment"># 可选字段，仅允许自定义配置的Predicate支持</span></span><br><span class="line"><span class="string">Priorities</span> <span class="string">&lt;[]object&gt;</span>                 <span class="comment"># Priority对象列表</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Name</span> <span class="string">&lt;string&gt;</span>                       <span class="comment"># Priority名称</span></span><br><span class="line">  <span class="string">Weight</span> <span class="string">&lt;int&gt;</span>                        <span class="comment"># 权重</span></span><br><span class="line">  <span class="string">Argument</span> <span class="string">&lt;Object&gt;</span>                   <span class="comment"># 可选字段，仅允许自定义配置的Priority支持</span></span><br><span class="line"><span class="string">Extenders</span> <span class="string">&lt;[]object&gt;</span>                  <span class="comment"># 加载的Extender列表</span></span><br><span class="line"><span class="string">HardPodAffinitySymmetricWeight</span> <span class="string">&lt;int&gt;</span>  <span class="comment"># Pod强制亲和调度关联的隐式首选亲和规则权重</span></span><br><span class="line"><span class="string">AlwaysCheckAllPredicates</span> <span class="string">&lt;bool&gt;</span>       <span class="comment"># 是否禁用Predicate进行节点过滤时的短路模式</span></span><br></pre></td></tr></table></figure>

<p>下面示例（policy.cfg）定义了一个不同于程序默认配置的调度策略，它启用了Even-PodsSpreadPriority策略支持的Pod规范中由topologySpreadConstraints定义的约束规则。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Policy</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">predicates:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">GeneralPredicates</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MaxCSIVolumeCountPred</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CheckVolumeBinding</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">EvenPodsSpread</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MatchInterPodAffinity</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CheckNodeUnschedulable</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NoDiskConflict</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NoVolumeZoneConflict</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MatchNodeSelector</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PodToleratesNodeTaints</span></span><br><span class="line"><span class="attr">priorities:</span></span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">LeastRequestedPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">BalancedResourceAllocation</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">ServiceSpreadingPriority</span>, <span class="attr">weight:</span> <span class="number">2</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">EvenPodsSpreadPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">TaintTolerationPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">ImageLocalityPriority</span>, <span class="attr">weight:</span> <span class="number">2</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">SelectorSpreadPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">InterPodAffinityPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br><span class="line"><span class="bullet">-</span> &#123;<span class="attr">name:</span> <span class="string">EqualPriority</span>, <span class="attr">weight:</span> <span class="number">1</span>&#125;</span><br></pre></td></tr></table></figure>

<p>我们随后为kube-scheduler提供一个自定义的KubeSchedulerConfiguration配置文件，让它通过文件路径来引用自定义的调度策略。下面的示例（kubeschedconf-v1alpha1-demo.yaml）指定了基于v1alpha1的API版本从指定的文件处加载调度策略配置文件policy.cfg。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"><span class="attr">bindTimeoutSeconds:</span> <span class="number">600</span></span><br><span class="line"><span class="attr">algorithmSource:</span></span><br><span class="line">  <span class="attr">policy:</span></span><br><span class="line">    <span class="attr">file:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/kubernetes/scheduler/policy.cfg</span></span><br><span class="line">  <span class="attr">provider:</span> <span class="string">DefaultProvider</span></span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line">  <span class="attr">kubeconfig:</span> <span class="string">&quot;/etc/kubernetes/scheduler.conf&quot;</span></span><br><span class="line"><span class="attr">disablePreemption:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>将这两个文件放在控制平面节点k8s-master01主机上的某个目录下（例如/etc/kubernetes/ scheduler），然后编辑/etc/kubernetes/manifests/kube-scheduler.yaml文件，为kube-scheduler添加存储卷以及–config选项来引用它，其中关键的配置部分如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">kube-scheduler</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--config=/etc/kubernetes/scheduler/kubeschedconf-v1alpha1-demo.yaml</span></span><br><span class="line">   <span class="string">……</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/kubernetes/scheduler</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/kubernetes/scheduler</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Directory</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">scheduler-config</span></span><br></pre></td></tr></table></figure>

<p>待Kubernetes Scheduler的Pod重新加载配置并启动完成后，我们可以在日志中看到加载指定Predicate和Priority函数的信息。接下来即可通过创建Pod对象来测试自定义调度策略的生效效果。另外，通过KubeSchedulerConfiguration引用指定的Extender，我们还能够使用调度器的经典扩展方式来添加外挂扩展。</p>
<h4 id="调度配置"><a href="#调度配置" class="headerlink" title="调度配置"></a>调度配置</h4><p>调度配置支持管理员为调度框架的各扩展点指定要调用的插件，相关的配置定义在KubeSchedulerConfiguration配置文件的profile字段中。自API群组kubescheduler.config.k8s.io的v1alpha2版本起始，kube-scheduler支持同时使用多个Profile，每个Profile拥有唯一的名称标识，并可由Pod资源在spec.schedulerName显式调用。调度框架默认会创建一个名为default-scheduler的配置文件，它启用了大部分的调度插件，而且是Pod资源默认使用的调度器。Profile的配置格式如下所示，配置默认的调度器或添加新的调度器时，需要在程序默认启用的调度插件的基础上“启用”或“禁用”指定的插件。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">SchedulerName</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 当前Profile的名称</span></span><br><span class="line"><span class="string">Plugins</span> <span class="string">&lt;Object&gt;</span>            <span class="comment"># 插件配置对象</span></span><br><span class="line">  <span class="string">&lt;ExtendPoint&gt;</span> <span class="string">&lt;Object&gt;</span>    <span class="comment"># 配置指定的扩展点，例如QueueSort，每个扩展点按名称指定</span></span><br><span class="line">    <span class="string">Enabled</span> <span class="string">&lt;[]Plugin&gt;</span>      <span class="comment"># 启用的插件列表</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Name</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 插件名称</span></span><br><span class="line">      <span class="string">Weight</span> <span class="string">&lt;int32&gt;</span>        <span class="comment"># 插件权重，仅Score扩展点支持</span></span><br><span class="line">    <span class="string">Disabled</span> <span class="string">&lt;[]Plugin&gt;</span>     <span class="comment"># 禁用的插件列表</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">Name</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 插件名称</span></span><br><span class="line">      <span class="string">Weight</span> <span class="string">&lt;int32&gt;</span>        <span class="comment"># 插件权重</span></span><br><span class="line"><span class="string">PluginConfig</span> <span class="string">&lt;[]Object&gt;</span>     <span class="comment"># 插件特有的配置</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">Name</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># 插件名称</span></span><br><span class="line"><span class="string">Args</span> <span class="string">&lt;Object&gt;</span>               <span class="comment"># 配置信息</span></span><br></pre></td></tr></table></figure>

<p>下面的KubeSchedulerConfiguration配置示例（kubeschedconf-v1alpha2-demo.yaml）中，在profile字段中默认的default-scheduler之外添加了一个名为demo-scheduler的自定义调度器，它采用了优先在节点上堆叠Pod的调度方式，适合集群可弹性伸缩的环境中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubescheduler.config.k8s.io/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeSchedulerConfiguration</span></span><br><span class="line"><span class="attr">clientConnection:</span></span><br><span class="line">  <span class="attr">kubeconfig:</span> <span class="string">&quot;/etc/kubernetes/scheduler.conf&quot;</span></span><br><span class="line"><span class="attr">disablePreemption:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">profiles:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">schedulerName:</span> <span class="string">default-scheduler</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">schedulerName:</span> <span class="string">demo-scheduler</span></span><br><span class="line">  <span class="attr">plugins:</span></span><br><span class="line">    <span class="attr">score:</span></span><br><span class="line">      <span class="attr">disabled:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NodeResourcesBalancedAllocation</span></span><br><span class="line">        <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NodeResourcesLeastAllocated</span></span><br><span class="line">        <span class="attr">weight:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">enabled:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">NodeResourcesMostAllocated</span></span><br><span class="line">        <span class="attr">weight:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>

<p>采用与前一节类似的配置方式，让kube-scheduler重新加载自定义的KubeScheduler-Configuration配置文件后，即可借助Deployment控制器创建多个Pod副本进行测试，唯一的特殊要求是要在Pod模板上使用spec.schedulerName指定调度器为demo-scheduler。按照定义，调度器将所有副本堆满一个节点后，才会启用另一个节点。本章提供的用于测试的配置清单示例scheduler-test.yaml中定义了一个Deployment对象，Pod模板定义了使用demo-scheduler调度器，且请求使用1000MB的CPU资源和512MiB的内存资源，它会在第一个节点无法容纳某个Pod对象的CPU或内存资源需求时转而使用第二个节点，感兴趣的读者可自行测试。<br>对于Kubernetes v1.20及上的版本，我们也提供了一个用于测试的示例配置文件kubeschedconf-v1beta1-demo.yaml，感兴趣的读者可自行测试其使用机制。</p>
<h2 id="节点亲和调度"><a href="#节点亲和调度" class="headerlink" title="节点亲和调度"></a>节点亲和调度</h2><p>节点亲和是调度程序用来确定Pod对象调度位置（哪个或哪类节点）的调度法则，这些规则基于节点上的自定义标签和Pod对象上指定的标签选择器进行定义，而支持这种调度机制的有NodeName和NodeAffinity调度插件。简单来说，节点亲和调度机制支持Pod资源定义自身对期望运行的某类节点的倾向性，倾向于运行指定类型的节点即为“亲和”关系，否则即为“反亲和”关系。<br>在Pod上定义节点亲和规则时有两种类型的节点亲和关系：强制（required）亲和和首选（preferred）亲和，或分别称为硬亲和与软亲和，本书会不加区别地使用这两种称呼。强制亲和限定了调度Pod资源时必须要满足的规则，无可用节点时Pod对象会被置为Pending状态，直到满足规则的节点出现。相比较来说，首选亲和规则实现的是一种柔性调度限制，它同样倾向于将Pod运行在某类特定的节点之上，但无法满足调度需求时，调度器将选择一个无法匹配规则的节点，而非将Pod置于Pending状态。<br>在Pod规范上定义节点亲和规则的关键点有两个：一是给节点规划并配置合乎期望的标签；二是为Pod对象定义合理的标签选择器。正如preferredDuringSchedulingIgnoredDuringExecution和requiredDuringSchedulingIgnoredDuringExecution字段名字中的后半段符串IgnoredDuringExecution隐含的意义所指，在Pod资源基于节点亲和规则调度至某节点之后，因节点标签发生了改变而变得不再符合Pod定义的亲和规则时，调度器也不会将Pod从此节点上移出，因而亲和调度仅在调度执行的过程中进行一次即时的判断，而非持续地监视亲和规则是否能够得以满足。图11-8简单给出这种亲和关系作用机制的示意图，简便起见，图中将requiredDuringSchedulingIgnoredDuringExecution字段缩写为required一词。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220224150309500.png" alt="image-20220224150309500"></p>
<h3 id="Pod节点选择器"><a href="#Pod节点选择器" class="headerlink" title="Pod节点选择器"></a>Pod节点选择器</h3><p>Pod资源可以使用.spec.nodeName直接指定要运行的目标节点，也可以基于.spec.nodeSelector指定的标签选择器过滤符合条件的节点作为可用目标节点，最终选择则基于打分机制完成。因此，后者也称为节点选择器。用户事先为特定部分的Node资源对象设定好标签，而后即可配置Pod通过节点选择器实现类似于节点的强制亲和调度。<br>由kubeadm部署的Kubernetes集群默认会为每个节点附加kubernetes.io/arch、kubernetes.io/hostname和kubernetes.io/os等标签，而且主节点还会有一个node-role.kubernetes.io/master标签，其中kubernetes.io/hostname适合NodeName类型的调度。无法满足nodeSelector的调度需求时，我们还可以使用kubectl label nodes/NODE命令为其附加自定义标签。例如，下面为k8s-node01.ilinux.io和k8s-node03.ilinux.io节点设置无值的gpu标签以标识其拥有GPU设备，并在标签设置完成后验证设置结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes/k8s-node01.ilinux.io gpu=</span></span><br><span class="line">node/k8s-node01.ilinux.io labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes/k8s-node03.ilinux.io gpu=</span></span><br><span class="line">node/k8s-node03.ilinux.io labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get nodes -l <span class="string">&#x27;gpu&#x27;</span> -o custom-columns=NAME:.metadata.name</span></span><br><span class="line">NAME</span><br><span class="line">k8s-node01.ilinux.io</span><br><span class="line">k8s-node03.ilinux.io</span><br></pre></td></tr></table></figure>

<p>下面配置清单示例（pod-with-nodeselector.yaml）中定义的Pod资源使用节点选择器定义了节点亲和机制，它倾向于运行在拥有GPU设备的节点上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-with-nodeselector</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">gpu:</span> <span class="string">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<p>按照规划，pod/pod-withnodeselector资源仅可能会运行在节点k8s-node01或k8s-node03之上，将如上资源清单中定义的Pod资源创建到集群中，通过查看其运行的节点即可判定调度效果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-with-nodeselector.yaml</span></span><br><span class="line">pod/pod-with-nodeselector created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pod/pod-with-nodeselector -o jsonpath=&#123;.spec.nodeName&#125;</span></span><br><span class="line">k8s-node03.ilinux.io</span><br></pre></td></tr></table></figure>

<p>事实上，多数情况下用户都无须关心Pod对象的具体运行位置，除非Pod依赖的特殊条件仅能由部分节点满足时，例如GPU和SSD等。即便如此，也应该尽量避免使用.spec.nodeName静态指定Pod对象的运行位置，而是应该让调度器基于标签和标签选择器为Pod挑选匹配的工作节点。另外，Pod规范中的.spec.nodeSelector仅支持简单等值关系的节点选择器，而.spec.affinity.nodeAffinity支持更灵活的节点选择器表达式，而且可以实现硬亲和与软亲和逻辑。</p>
<h3 id="强制节点亲和"><a href="#强制节点亲和" class="headerlink" title="强制节点亲和"></a>强制节点亲和</h3><p>Pod规范中的.spec.affinity.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution字段用于定义节点的强制亲和关系，它的值是一个对象列表，可由一到多个nodeSelectorTerms对象组成，彼此间为“逻辑或”关系。nodeSelectorTerms用于定义节点选择器，其值为对象列表，它支持matchExpressions和matchFields两种复杂的表达机制。</p>
<ul>
<li>matchExpressions：标签选择器表达式，基于节点标签进行过滤；可重复使用以表达不同的匹配条件，各条件间为“或”关系。</li>
<li>matchFields：以字段选择器表达的节点选择器；可重复使用以表达不同的匹配条件，各条件间为“或”关系。<br>每个匹配条件可由一到多个匹配规则组成，例如某个matchExpressions条件下可同时存在两个表达式规则，</li>
</ul>
<p>如下面的示例所示，同一条件下的各条规则彼此间为“逻辑与”关系。这意味着某节点满足nodeSelectorTerms中的任意一个条件即可，但满足某个条件指的是可完全匹配该条件下定义的所有规则。<br>下面配置清单示例（node-affinity-required-demo.yaml）中，Pod模板使用了强制节点亲和约束，它要求Pod只能运行在那些拥有gpu标签且不具有node-role.kubernetes.io/master标签的节点之上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-affinity-required</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">node-affinity-required</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">node-affinity-required</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">            <span class="attr">nodeSelectorTerms:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">gpu</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">DoesNotExist</span></span><br></pre></td></tr></table></figure>

<p>2.1节中，我们为k8s-node01和k8s-node03设定了gpu标签，而k8s-master01拥有主节点标识的标签。因此按照期望，5个Pod副本仅会运行在k8s-node01和k8s-node02之上，下面的命令结果也完全证实了我们的设定。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f node-affinity-required-demo.yaml</span></span><br><span class="line">deployment.apps/node-affinity-required created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l ctlr=node-affinity-required \</span></span><br><span class="line"><span class="language-bash">     -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</span></span><br><span class="line">NAME                                     NODE</span><br><span class="line">node-affinity-required-5c469987c-2fdql   k8s-node03.ilinux.io</span><br><span class="line">node-affinity-required-5c469987c-hfcvn   k8s-node01.ilinux.io</span><br><span class="line">node-affinity-required-5c469987c-mt4gg   k8s-node03.ilinux.io</span><br><span class="line">node-affinity-required-5c469987c-pm56j   k8s-node01.ilinux.io</span><br><span class="line">node-affinity-required-5c469987c-vwbtt   k8s-node03.ilinux.io</span><br></pre></td></tr></table></figure>

<p>另外，调度器调度Pod时，支撑节点亲和机制的NodeName和NodeAffinity等插件仅是其中组调度条件，Kubernetes配置的其他插件依然会参与到Pod的调度过程。例如可以为Pod模板中的demoapp容器添加如下资源请求后重新进行测试来验证调度器的工作模型，为了便于测试，我们将修改后的配置保存到单独的配置清单node-affinity-and-resourcefits.yaml中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">cpu:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">memory:</span> <span class="string">2Gi</span></span><br></pre></td></tr></table></figure>

<p>注册到Filter扩展点的调度插件NodeResourcesFit负责检查节点的可分配资源是否能容纳Pod的资源请求，计算方式是节点上的资源总量（CPU和内存分别计算）减去已运行在该节点上的所有Pod对象的requests资源量之和。考虑本书试验环境中使用的4个节点（其中一个为master）配置相同，均为4核心CPU和8GB内存，每个节点至少都运行着kube-proxy和calico-node一类节点代理等系统应用。因而，每个节点仅能满足配置示例中的单个Pod副本的运行需求。于是，配置中定义的5个Pod实例中将有3个Pod的调度结果处于Pending状态，如下命令的结果也证实了我们的猜想与设定。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f node-affinity-and-resourcefits.yaml</span></span><br><span class="line">deployment.apps/node-affinity-and-resourcefits created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l ctlr=node-affinity-and-resourcefits -o custom-columns=NAME:.metadata.name,STATUS:.status.phase</span></span><br><span class="line">NAME                                               STATUS</span><br><span class="line">node-affinity-and-resourcefits-5c98b765b5-5kxqb    Pending</span><br><span class="line">node-affinity-and-resourcefits-5c98b765b5-f9qwb    Pending</span><br><span class="line">node-affinity-and-resourcefits-5c98b765b5-g6k25    Running</span><br><span class="line">node-affinity-and-resourcefits-5c98b765b5-kl4db     Running</span><br><span class="line">node-affinity-and-resourcefits-5c98b765b5-lrtfm     Pending</span><br></pre></td></tr></table></figure>

<p>节点硬亲和机制实现的功能与节点选择器（nodeSelector）相似，但亲和性支持使用标签匹配表达式或字段选择器来挑选节点，提供了灵活且强大的选择机制，因此可被理解为新一代的节点选择器。</p>
<h3 id="首选节点亲和"><a href="#首选节点亲和" class="headerlink" title="首选节点亲和"></a>首选节点亲和</h3><p>节点首选亲和机制为节点选择机制提供了一种柔性控制逻辑，被调度的Pod对象不再是“必须”，而是“应该”放置到某些特定节点之上，但条件不满足时，该Pod也能够接受被编排到其他不符合条件的节点之上。另外，多个软亲和条件并存时，它还支持为每个条件定义weight属性以区别它们优先级，取值范围是1～100，数字越大优先级越高。下面配置清单示例（node-affinity-preferred-demo.yaml）中，Pod模板定义了两个节点软亲和约束条件，它们有着不同的权重。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">node-affinity-preferred</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">node-affinity-preferred</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">node-affinity-preferred</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">1500m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">nodeAffinity:</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">60</span></span><br><span class="line">            <span class="attr">preference:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">gpu</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">Exists</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">30</span></span><br><span class="line">            <span class="attr">preference:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">zone</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span> [<span class="string">&quot;foo&quot;</span>,<span class="string">&quot;bar&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>示例中，Pod资源模板定义了节点软亲和，以选择尽量运行在指定范围内拥有gpu标签或者zone标签的节点之上，其中gpu标签是更为重要的倾向性规则，它的权重为60，相比较来说zone标签的重要性低了一级，因为它的权重为30。这么一来，如果集群中拥有足够多的节点，它将被此规则分为4类：在指定范围内拥有gpu标签和zone标签、仅满足gpu一个标签条件、仅满足zone一个标签条件，以及不满足任何标签筛选条件的节点，如图11-9所示。</p>
<p><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223145001045.png" alt="image-20220223145001045"></p>
<p>本书所用的测试环境共有3个节点（图11-9虚线内的节点），各自有4颗CPU和8GiB的内存资源，以配置清单示例中定义的节点亲和规则来说，它们的倾向性权重分别如图11-9中标识的信息所示。在创建所需的5个Pod对象副本时，各Pod会由调度器根据节点软亲和及资源匹配度进行调度。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes k8s-node01.ilinux.io zone=foo</span></span><br><span class="line">node/k8s-node01.ilinux.io labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes k8s-node02.ilinux.io zone=bar</span></span><br><span class="line">node/k8s-node02.ilinux.io labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l ctlr=node-affinity-preferred \</span></span><br><span class="line"><span class="language-bash">      -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</span></span><br><span class="line">NAME                                         NODE</span><br><span class="line">node-affinity-preferred-6f975c57b5-6dknl     k8s-node01.ilinux.io</span><br><span class="line">node-affinity-preferred-6f975c57b5-fjfbf     k8s-node01.ilinux.io</span><br><span class="line">node-affinity-preferred-6f975c57b5-jsgcr     k8s-node03.ilinux.io</span><br><span class="line">node-affinity-preferred-6f975c57b5-ksmqh     k8s-node03.ilinux.io</span><br><span class="line">node-affinity-preferred-6f975c57b5-vh9jg     k8s-node02.ilinux.io</span><br></pre></td></tr></table></figure>

<p>我们有意为容器添加了资源需求以影响调度器的工作方式，因此即便k8s-node01节点的倾向程度更大，但无法满足Pod副本的资源需求时，它将转而使用k8s-node03，直到该节点资源也分配完毕才使用更低倾向性的k8s-node02节点。</p>
<h2 id="Pod亲和调度"><a href="#Pod亲和调度" class="headerlink" title="Pod亲和调度"></a>Pod亲和调度</h2><p>出于高效通信等需求，偶尔需要把一些Pod对象组织在相近的位置（同一节点、机架、区域或地区等），例如应用程序的Pod及其后端提供数据服务的Pod等，我们可以认为这是一类具有亲和关系的Pod对象。偶尔，出于安全或分布式容灾等原因，也会需要把一些Pod对象与其所运行的位置隔离开来，例如在IDC中的区域运行某应用的单个代理Pod对象等，我们可把这类Pod对象间的关系称为反亲和。<br>当然，我们也能够通过Pod与节点的亲和关系来变相完成Pod对象间的亲和或反亲和特性，但这要求我们必须明确指定Pod可运行的节点标签，显然这并非较优的选择。理想的实现方式是允许调度器把第一个Pod放置在任何位置，而后与其有着亲和或反亲和关系的其他Pod据此动态完成位置编排，这就是Pod亲和调度与反亲和调度的功用。Pod间的亲和关系也存在强制亲和及首选亲和的区别，它们表示的约束意义同节点亲和相似。<br>Pod间的亲和及反亲和关系主要由调度插件InterPodAffinity来支撑，它既要负责节点过滤，也要完成节点的优先级排序。而经典调度策略则使用内置的MatchInterPodAffinity预选策略和terPodAffinityPriority优选函数进行各节点的优选级评估。</p>
<h3 id="位置拓扑"><a href="#位置拓扑" class="headerlink" title="位置拓扑"></a>位置拓扑</h3><p>Pod亲和调度的目标在于确保相关Pod对象运行在“同一位置”，而反亲和调度要求它们不能运行在“同一位置”。而节点位置的定义取决于节点拓扑结构的定义，若拓扑方式不同，则对如图11-10中所示的Pod-A和Pod-B是否在同一位置的判定结果也可能有所不同。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223145059121.png" alt="image-20220223145059121">如果基于各节点的kubernetes.io/hostname标签作为评判标准，显然“同一位置”意味着同一个节点，而不同节点有不同位置，如图11-11所示。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223145118480.png" alt="image-20220223145118480">而如果根据图11-12所划分的故障转移域来评判，k8s-node01和k8s-node02属于同一位置，而k8s-node03和k8s-node04属于另一个意义上的同一位置。</p>
<p><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223145135847.png" alt="image-20220223145135847"></p>
<p>因而，在定义Pod对象的亲和与反亲和关系时，首先需要借助标签选择器来选择同一类Pod对象，而后根据筛选出的同类现有Pod对象所在节点的标签来判定“同一位置”的具体所指，而后针对亲和关系将该Pod放置在同一位置中优先级最高的节点之上，或者针对反亲和关系将该Pod编排至不同拓扑优先级最高的节点上。<br>Pod间的亲和关系定义在spec.affinity.podAffinity字段中，而反亲和关系定义在spec.affinity.podAntiAffinity字段中，它们各自的约束特性也存在强制与首选两种，它们都支持使用如下关键字段。</p>
<ul>
<li>topologyKey &lt;string&gt;：拓扑键，用来划分拓扑结构的节点标签，在指定的键上具有相同值的节点归属为同一拓扑；必选字段。</li>
<li>labelSelector &lt;Object&gt;：Pod标签选择器，用于指定该Pod将针对哪类现有Pod的位置来确定可放置的位置。</li>
<li>namespaces &lt;[]string&gt;：用于指示labelSelector字段的生效目标名称空间，默认为当前Pod所属的同一名称空间。</li>
</ul>
<h3 id="Pod间的强制亲和"><a href="#Pod间的强制亲和" class="headerlink" title="Pod间的强制亲和"></a>Pod间的强制亲和</h3><p>Pod间强制约束的亲和调度也定义在requiredDuringSchedulingIgnoredDuringExecution字段中。下面的示例中先定义了一个假设为被依赖的存储应用Redis，而后定义了一个依赖该存储的demoapp应用，后者使用Pod间强制亲和约束，期望与Redis运行在同一位置。为了尽量接近真实环境来模拟这种约束机制，这里模拟k8s-node01和k8s-node02位于同一个机架rack001上，而k8s-node03位于另一个机架rack002上，并以节点标签rack作为topologyKey，即确定节点位置拓扑的键。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span>  <span class="comment"># Pod的标签，将被demoapp Pod选择作为参照系</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">redis:6.0-alpine</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-affinity-required</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">pod-affinity-required</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">pod-affinity-required</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAffinity:</span>            <span class="comment"># Pod亲和调度</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 强制亲和定义</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span>      <span class="comment"># Pod对象标签选择器，用于确定放置当前Pod的参照系</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">app</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis&quot;</span>]&#125;</span><br><span class="line">              <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">ctlr</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis&quot;</span>]&#125;</span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">zone</span>   <span class="comment"># 拓扑键，用于确定节点位置拓扑的节点标签，必选</span></span><br></pre></td></tr></table></figure>

<p>我们可以推断出，若Redis的单个Pod副本被调度至rack001标识的位置上，则demoapp的所有Pod副本都会运行在该位置的k8s-node01或k8s-node02节点上；否则，它们都会运行在rack002标识的位置上。为了测试效果，我们先为节点打上相应的标签。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes k8s-node01.ilinux.io rack=rack001</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes k8s-node02.ilinux.io rack=rack001</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label nodes k8s-node03.ilinux.io rack=rack002</span></span><br></pre></td></tr></table></figure>

<p>接着，我们将配置清单示例中的Redis和demoapp应用部署在集群上，以测试demoapp Pod是否与Redis Pod存在强制亲和关系。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-affinity-required-demo.yaml</span> </span><br><span class="line">deployment.apps/redis created</span><br><span class="line">deployment.apps/pod-affinity-required created</span><br></pre></td></tr></table></figure>

<p>随后，可通过资源的详细描述中的Events信息或者资源规范查看Redis Pod副本的运行位置。下面的内容取自Redis Pod的详细描述中的事件，它显示Redis Pod对象default/redis-844696cc84-fp4xz被default-scheduler调度至k8s-node02节点之上，该节点位于rack001之上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Events:</span></span><br><span class="line">  <span class="string">……</span></span><br><span class="line"> <span class="string">Normal</span>  <span class="string">Scheduled</span>  <span class="string">&lt;unknown&gt;</span>  <span class="string">default-scheduler</span>              <span class="string">Successfully</span> </span><br><span class="line"> <span class="string">assigned</span> <span class="string">default/redis-844696cc84-fp4xz</span> <span class="string">to</span> <span class="string">k8s-node02.ilinux.io</span></span><br></pre></td></tr></table></figure>

<p>因而，deployment/demoapp的所有Pod对象也必将运行在rack001机架的k8s-node01或k8s-node02节点之上，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,ctlr=pod-affinity-required \</span></span><br><span class="line"><span class="language-bash">-o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</span></span><br><span class="line">NAME                                      NODE</span><br><span class="line">pod-affinity-required-778d4ff894-cblpr    k8s-node01.ilinux.io</span><br><span class="line">pod-affinity-required-778d4ff894-h4lkk    k8s-node02.ilinux.io</span><br><span class="line">pod-affinity-required-778d4ff894-jkh4p    k8s-node01.ilinux.io</span><br><span class="line">pod-affinity-required-778d4ff894-lt8p9    k8s-node02.ilinux.io</span><br><span class="line">pod-affinity-required-778d4ff894-zddz9    k8s-node01.ilinux.io</span><br></pre></td></tr></table></figure>

<p>Pod间的亲和调度能够将有密切关系或密集通信的应用约束在同一位置，通过降低通信延迟来降低性能损耗。需要注意的是，若节点上的标签在运行时发生更改导致不能再满足Pod上的亲和关系定义时，该Pod将继续在该节点上运行而不会被重新调度。另外，labelSelector属性仅匹配与被调度的Pod在同一名称空间中的Pod资源，不过也可以通过为其添加namespace字段以指定其他名称空间。</p>
<h3 id="Pod间的首选亲和"><a href="#Pod间的首选亲和" class="headerlink" title="Pod间的首选亲和"></a>Pod间的首选亲和</h3><p>因满足位置关系的节点上的可分配计算资源、存储卷和节点端口等原因导致Pod间的强制亲和关系在无法得到满足时，调度器会将Pod对象置于Pending状态，但首选亲和约束则只是尽力满足这种亲和约束，当无法保证这种亲和关系时，调度器则会将Pod对象调度至集群中其他位置的节点之上。而对位置关系要求不甚严格的应用之间的部署需求，首选亲和倒也不失为一种折中的选择。<br>Pod间的柔性亲和约束也使用preferredDuringSchedulingIgnoredDuringExecution字段进行定义，它同样允许用户定义具有不同权重的多重亲和条件，以定义出多个不同适配级别位置。下面资源示意示例（pod-affinity-preferred-demo.yaml）中先是定义了一个Redis应用，它由调度器自行选定目标节点，但demoapp应用Pod将以柔性亲和的方式期望与Redis Pod运行在同一节点（带有kubernetes.io/hostname标签），当条件无法满足时，则期望运行在同一区域（zone标签），否则也能接受运行在集群中的其他任何节点之上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">redis-preferred</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">redis-preferred</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span>  <span class="comment"># Redis Pod的标签，它也将是demoapp Pod亲和关系依赖的关键要素</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">redis-preferred</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">redis:6.0-alpine</span></span><br><span class="line">        <span class="attr">resources:</span>  <span class="comment"># 资源请求，用于影响节点的可承载Pod数量</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">500m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">512Mi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-affinity-preferred</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">pod-affinity-preferred</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">pod-affinity-preferred</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">        <span class="attr">resources:</span></span><br><span class="line">          <span class="attr">requests:</span></span><br><span class="line">            <span class="attr">cpu:</span> <span class="string">1500m</span></span><br><span class="line">            <span class="attr">memory:</span> <span class="string">1Gi</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAffinity:</span>     <span class="comment"># Pod亲和关系定义</span></span><br><span class="line">          <span class="attr">preferredDuringSchedulingIgnoredDuringExecution:</span>  <span class="comment"># 柔性亲和</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">100</span>  <span class="comment"># 最大权重的亲和条件</span></span><br><span class="line">            <span class="attr">podAffinityTerm:</span></span><br><span class="line">              <span class="attr">labelSelector:</span></span><br><span class="line">                <span class="attr">matchExpressions:</span></span><br><span class="line">                <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">app</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis&quot;</span>]&#125;</span><br><span class="line">                <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">ctlr</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis-prefered&quot;</span>]&#125;</span><br><span class="line">              <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span>  <span class="comment"># 确定节点位置拓扑的标签</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">weight:</span> <span class="number">50</span>   <span class="comment"># 第二权重的亲和条件</span></span><br><span class="line">            <span class="attr">podAffinityTerm:</span></span><br><span class="line">              <span class="attr">labelSelector:</span></span><br><span class="line">                <span class="attr">matchExpressions:</span></span><br><span class="line">                <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">app</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis&quot;</span>]&#125;</span><br><span class="line">                <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">ctlr</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;redis-prefered&quot;</span>]&#125;</span><br><span class="line">              <span class="attr">topologyKey:</span> <span class="string">rack</span>  <span class="comment"># 确定节点位置拓扑的第二标签，扩大了前一条件位置范围</span></span><br></pre></td></tr></table></figure>

<p>我们沿用3.2节的集群环境，假设Redis Pod被调度至k8s-node01节点之上，则demoapp Pod同样更倾向运行在该节点，当条件无法满足时，调度器将以该节点所在的zone为标准来挑选同一个zone中的另一节点k8s-node02，最后是k8s-node03。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l <span class="string">&quot;ctlr in (redis-preferred,pod-affinity-preferred)&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName</span></span><br><span class="line">NAME                                        NODE</span><br><span class="line">pod-affinity-preferred-66495459b4-2kgn4     k8s-node01.ilinux.io</span><br><span class="line">pod-affinity-preferred-66495459b4-q64tf     k8s-node02.ilinux.io</span><br><span class="line">pod-affinity-preferred-66495459b4-snhdr     k8s-node03.ilinux.io</span><br><span class="line">pod-affinity-preferred-66495459b4-xp9rl     k8s-node02.ilinux.io</span><br><span class="line">redis-preferred-78bd44b79d-ztdbv            k8s-node01.ilinux.io</span><br></pre></td></tr></table></figure>

<p>假设Redis被调度到了k8s-node03之上，则该节点无更多资源容纳demoapp Pod时，且同一zone内再无其他节点，则相关的Pod只能被无差别地调度至k8s-node01和k8s-node02之上。Pod间的柔性亲和关系尽力保证有紧密关系的Pod运行在一起的同时，避免了因强制亲和条件得不到满足时而“挂起”Pod的局面。</p>
<h3 id="Pod间的反亲和关系"><a href="#Pod间的反亲和关系" class="headerlink" title="Pod间的反亲和关系"></a>Pod间的反亲和关系</h3><p>Pod间的反亲和关系（podAntiAffinity）要实现的调度目标刚好与亲和关系相反，它的主要目标在于确保存在互斥关系的Pod对象不会运行在同一位置，或者确保仅需要在指定的位置配置单个代理程序（类似于DaemonSet确保每个节点仅运行单个某类Pod）等场景应用场景。因此，反亲和性调度一般用于分散同一类应用的Pod对象等，也包括把不同安全级别的Pod对象调度至不同的区域、机架或节点等。下面资源配置清单（pod-antiaffinity-required-demo.yaml）中定义了由同一Deployment创建但彼此基于节点位置互斥的Pod对象。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-antiaffinity-required</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctlr:</span> <span class="string">pod-antiaffinity-required</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctlr:</span> <span class="string">pod-antiaffinity-required</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">      <span class="attr">affinity:</span></span><br><span class="line">        <span class="attr">podAntiAffinity:</span></span><br><span class="line">          <span class="attr">requiredDuringSchedulingIgnoredDuringExecution:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">labelSelector:</span></span><br><span class="line">              <span class="attr">matchExpressions:</span></span><br><span class="line">              <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">app</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">&quot;demoapp&quot;</span>]&#125;</span><br><span class="line">              <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">ctlr</span></span><br><span class="line">                <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">                <span class="attr">values:</span> [<span class="string">&quot;pod-antiaffinity-required&quot;</span>]</span><br><span class="line">            <span class="attr">topologyKey:</span> <span class="string">kubernetes.io/hostname</span></span><br></pre></td></tr></table></figure>

<p>强制的反亲和约束下，deployment/pod-antiaffinity-required创建的4个Pod副本必须运行于不同的节点之上，但示例集群中一共只存在3个节点，因此，必然会有一个Pod对象处于Pending状态，如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l ctlr=pod-antiaffinity-required \</span></span><br><span class="line"><span class="language-bash">   -o custom-columns=NAME:.metadata.name,NODE:.spec.nodeName,STATUS:.status.phase</span></span><br><span class="line">NAME                                          NODE                    STATUS</span><br><span class="line">pod-antiaffinity-required-5745494d77-9g75r    k8s-node01.ilinux.io    Running</span><br><span class="line">pod-antiaffinity-required-5745494d77-rsj9d    k8s-node03.ilinux.io    Running</span><br><span class="line">pod-antiaffinity-required-5745494d77-xd446    k8s-node02.ilinux.io    Running</span><br><span class="line">pod-antiaffinity-required-5745494d77-zzz8s    &lt;none&gt;                  Pending</span><br></pre></td></tr></table></figure>

<p>类似地，Pod反亲和调度也支持使用柔性约束机制，调度器会尽量不把位置相斥的Pod对象调度到同一位置，但约束关系无法得到满足时，也可以违反约束规则进行调度，而非把Pod置于Pending状态。</p>
<h2 id="节点污点与Pod容忍度"><a href="#节点污点与Pod容忍度" class="headerlink" title="节点污点与Pod容忍度"></a>节点污点与Pod容忍度</h2><p>污点是定义在节点之上的键值型属性数据，用于让节点有能力主动拒绝调度器将Pod调度运行到节点上，除非该Pod对象具有接纳节点污点的容忍度。容忍度（tolerations）则是定义在Pod对象上的键值型属性数据，用于配置该Pod可容忍的节点污点。调度器插件TaintToleration负责确保仅那些可容忍节点污点的Pod对象可调度运行在上面。经典调度机制使用PodToleratesNodeTaints预选策略和TaintTolerationPriority优选函数完成该功能。节点污点和Pod容忍度在调度中的关系如图11-13所示。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220224154830268.png" alt="image-20220224154830268"></p>
<p>节点选择器（nodeSelector）和节点亲和性（nodeAffinity)两种调度方式都是通过在Pod对象上添加标签选择器来完成对特定类型节点标签的匹配，从而完成节点选择和绑定，相对而言，基于污点和容忍度的调度方式则是通过向节点添加污点信息来控制Pod对象的调度结果，从而给了节点控制何种Pod对象能够调度于其上的控制权。换句话说，节点亲和调度使得Pod对象被吸引到一类特定的节点，而污点的作用则相反，它为节点提供了排斥特定Pod对象的能力。</p>
<h3 id="污点与容忍度基础概念"><a href="#污点与容忍度基础概念" class="headerlink" title="污点与容忍度基础概念"></a>污点与容忍度基础概念</h3><p>污点定义在节点的nodeSpec中，而容忍度定义在Pod的podSpec中，它们都是键值型数据，但又都额外支持一个效用（effect）标识，语法格式为key=value:effect，其中key和value的用法及格式与资源注解信息相似，而污点上的效用标识则用于定义其对Pod对象的排斥等级，容忍度上的效用标识则用于定义其对污点的容忍级别。效用标识主要有以下3种类型。</p>
<ul>
<li>NoSchedule：不能容忍此污点的Pod对象不可调度至当前节点，属于强制型约束关系，但添加污点对节点上现存的Pod对象不产生影响。</li>
<li>PreferNoSchedule：NoSchedule的柔性约束版本，即调度器尽量确保不会将那些不能容忍此污点的Pod对象调度至当前节点，除非不存在其他任何能够容忍此污点的可用节点；添加该类效用的污点同样对节点上现存的Pod对象不产生影响。</li>
<li>NoExecute：不能容忍此污点的新Pod对象不可调度至当前节点，属于强制型约束关系，而且节点上现存的Pod对象因节点污点变动或Pod容忍度变动而不再满足匹配条件时，Pod对象将会被驱逐。</li>
</ul>
<p>在Pod对象上定义容忍度时，它支持两种操作符：一种是等值比较，表示容忍度与污点必须在key、value和effect三者之上完全匹配，另一种是存在性判断，表示二者的key和effect必须完全匹配，而容忍度中的value字段要使用空值。<br>一个节点可以配置使用多个污点，而一个Pod对象也可以有多个容忍度，将一个Pod对象的容忍度套用到特定节点的污点之上进行匹配度检测时将遵循如下逻辑。</p>
<blockquote>
<p>1）首先处理与容忍度匹配的污点。<br>2）对于不能匹配到容忍度的所有污点，若存在一个污点使用了NoSchedule效用标识，则拒绝调度当前Pod至该节点。<br>3）对于不能匹配到容忍度的所有污点，若都不具有NoSchedule效用标识，但至少有一个污点使用了PreferNoScheduler效用标识，则调度器会尽量避免将当前Pod对象调度至该节点。<br>4）如果至少有一个不能匹配容忍度的污点使用了NoExecute效用标识，节点将立即驱逐当前Pod对象，或者不允许该Pod调度至给定的节点；而且，即便容忍度匹配到使用了NoExecute效用标识的污点，若在Pod上定义容忍度时同时使用tolerationSeconds属性定义了容忍时限，则在超出时限后当前Pod也将会被节点驱逐。</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe nodes k8s-master01.ilinux.io</span></span><br><span class="line">Name:               k8s-master01.ilinux.io</span><br><span class="line">Roles:              master</span><br><span class="line">……</span><br><span class="line">Taints:             node-role.kubernetes.io/master:NoSchedule</span><br><span class="line">Unschedulable:      false</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>然而有些系统级应用也会在资源创建时被添加上相应的容忍度，以确保它们被DaemonSet控制器创建时能调度至Master节点运行一个实例，例如kube-proxy或者kube-flannel等。以任意一个kube-proxy实例为例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">POD=$(kubectl get pods -l k8s-app=kube-proxy -n kube-system \</span></span><br><span class="line"><span class="language-bash">     -o jsonpath=&#123;.items[0].metadata.name&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods <span class="variable">$POD</span> -n kube-system</span></span><br><span class="line">……</span><br><span class="line">Tolerations:     </span><br><span class="line">                 CriticalAddonsOnly</span><br><span class="line">                 node.kubernetes.io/disk-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io/memory-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io/network-unavailable:NoSchedule</span><br><span class="line">                 node.kubernetes.io/not-ready:NoExecute</span><br><span class="line">                 node.kubernetes.io/pid-pressure:NoSchedule</span><br><span class="line">                 node.kubernetes.io/unreachable:NoExecute</span><br><span class="line">                 node.kubernetes.io/unschedulable:NoSchedule</span><br></pre></td></tr></table></figure>

<p>运行着系统组件的Pod对象是构成Kubernetes系统的关键组成部分，因而它们通常被定义了更大的容忍度。从上面某kube-proxy实例的容忍度定义来看，这种容忍度还能容忍那些报告了存在磁盘压力、内存压力和PID压力的节点，以及那些未就绪的节点和不可达的节点等，以确保它们能在任何状态下正常调度至集群节点上运行。</p>
<h3 id="定义污点"><a href="#定义污点" class="headerlink" title="定义污点"></a>定义污点</h3><p>任何符合键值规范要求的字符串均可用于定义污点信息：可使用字母、数字、连接符、点号和下划线，且仅能以字母或数字开头，其中键名的长度上限为253个字符，值最长为63个字符。实践中，污点通常用于描述具体的部署规划，它们的键名形如node-type、node-role、node-project或node-geo等，而且一般还会在必要时带上域名以描述一些额外信息，例如node-type.ilinux.io等。kubectl taint命令可用于管理Node对象的污点信息，该命令的语法格式如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes &lt;node-name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt; …</span><br></pre></td></tr></table></figure>

<p>例如，定义节点k8s-node01.ilinux.io使用node-type=production:NoSchedule这一污点：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl taint nodes k8s-node01.ilinux.io node-type=production:NoSchedule</span></span><br><span class="line">node/k8s-node01.ilinux.io tainted</span><br></pre></td></tr></table></figure>

<p>node-type是具有NoScheduler效用标识的污点，它对k8s-node01上已有的Pod对象不产生影响，但对之后调度的Pod对象来说，不能容忍该污点则意味着无法调度至节点。类似下面的命令可以查看节点上的污点信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get nodes k8s-node01.ilinux.io -o jsonpath=&#123;.spec.taints&#125;</span></span><br><span class="line">[map[effect:NoSchedule key:node-type value:production]]</span><br></pre></td></tr></table></figure>

<p>需要注意的是，effect同样是污点的核心组成部分，即便键值数据相同但效用标识不同的污点也属于两个各自独立的污点信息。例如，将上面命令中的效用标识定义为PreferNoSchedule再添加一次：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl taint nodes k8s-node01.ilinux.io node-type=production:PreferNoSchedule</span><br><span class="line">node/k8s-node01.ilinux.io tainted</span><br></pre></td></tr></table></figure>

<p>删除节点上的污点仍旧可通过kubectl taint命令完成，但它要使用如下的命令格式，省略效用标识则表示删除使用指定键名的所有污点，否则只删除指定键名上的对应效用标识的污点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes &lt;node-name&gt; &lt;key&gt;[:&lt;effect&gt;]-</span><br></pre></td></tr></table></figure>

<p>例如，下面的命令可删除k8s-node01上node-type键的效用标识为NoSchedule的污点信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl taint nodes k8s-node01.ilinux.io node-type:NoSchedule-</span></span><br><span class="line">node/k8s-node01.ilinux.io untainted</span><br></pre></td></tr></table></figure>

<p>若要删除使用指定键名的所有污点，在删除命令中省略效用标识即能实现，例如下面的命令能删除k8s-node01上键名为node-type的所有污点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl taint nodes k8s-node01.ilinux.io node-type-</span></span><br><span class="line">node/k8s-node01.ilinux.io untainted</span><br></pre></td></tr></table></figure>

<p>若期望一次删除节点上的全部污点信息，通过kubectl patch命令直接将节点属性spec.taints的值置空即可，例如下面的命令可删除k8s-node01节点上的所有污点。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch nodes k8s-node01.ilinux.io -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;taints&quot;:[]&#125;&#125;&#x27;</span></span></span><br><span class="line">node/k8s-node01.ilinux.io patched</span><br></pre></td></tr></table></figure>

<p><font color="red">仅使用NoExecute标识的污点变动会影响节点上现有的Pod对象，其他两个效用标识都不会影响节点上的现有Pod对象。</font></p>
<h3 id="定义容忍度"><a href="#定义容忍度" class="headerlink" title="定义容忍度"></a>定义容忍度</h3><p>Pod对象的容忍度通过其spec.tolerations字段添加，根据使用的操作符不同，主要有两种可用形式：一种是与污点信息完全匹配的等值关系；另一种是判断污点信息存在性的匹配方式，它们分别使用Equal和Exists操作符表示。下面容忍度的定义示例使用了Equal操作符，其中tolerationSeconds用于定义延迟驱逐当前Pod对象的时长。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;production&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">  <span class="attr">tolerationSeconds:</span> <span class="number">3600</span></span><br></pre></td></tr></table></figure>

<p>下面的示例中定义了一个使用存在性判断机制的容忍度，它表示能够容忍以node-type为键名的、效用标识为NoExcute的污点。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-type&quot;</span></span><br><span class="line">  <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">  <span class="attr">effect:</span> <span class="string">&quot;NoExecute&quot;</span></span><br><span class="line">  <span class="attr">tolerationSeconds:</span> <span class="number">3600</span></span><br></pre></td></tr></table></figure>

<p>实践中，若集群中的一组机器专为运行非生产型的容器应用而设置，这些机器可能随时按需上下线，那么就应该为其添加污点信息，确保能容忍此污点的非生产型Pod对象可以调度其上。另外，有些有着特殊硬件的节点需要专用于运行一类有此类硬件资源需求的Pod对象时，例如有GPU设备的节点也应该添加污点信息，以排除其他的Pod对象。</p>
<h3 id="问题节点标识"><a href="#问题节点标识" class="headerlink" title="问题节点标识"></a>问题节点标识</h3><p>Kubernetes自v1.6版本起支持使用污点自动标识问题节点，它通过节点控制器在特定条件下自动为节点添加污点信息实现。它们都使用NoExecute效用标识，因此不能容忍此类污点的Pod对象也会遭到驱逐。目前，内置使用的此类污点有如下几个。</p>
<ul>
<li>node.kubernetes.io/not-ready：节点进入NotReady状态时被自动添加的污点。</li>
<li>node.alpha.kubernetes.io/unreachable：节点进入NotReachable状态时被自动添加的污点。</li>
<li>node.kubernetes.io/out-of-disk：节点进入OutOfDisk状态时被自动添加的污点。</li>
<li>node.kubernetes.io/memory-pressure：节点内存资源面临压力。</li>
<li>node.kubernetes.io/disk-pressure：节点磁盘资源面临压力。</li>
<li>node.kubernetes.io/network-unavailable：节点网络不可用。</li>
<li>node.cloudprovider.kubernetes.io/uninitialized：kubelet由外部的云环境程序启动时，它自动为节点添加此污点，待云控制器管理器中的控制器初始化此节点时再将污点删除。</li>
</ul>
<p>Kubernetes的核心组件通常都要容忍此类的污点，以确保相应的DaemonSet控制器能够无视此类污点在节点上部署相应的关键Pod对象，例如kube-proxy或kube-flannel等。</p>
<h2 id="拓扑分布式调度"><a href="#拓扑分布式调度" class="headerlink" title="拓扑分布式调度"></a>拓扑分布式调度</h2><p>根据指定的topologyKey将节点划分好拓扑结构是实现Pod间亲和与反亲和调度的关键所在，但Pod亲和调度仅能将相关的所有Pod分发到单个拓扑中，而反亲和调度则仅能在一个拓扑中部署单实例。这两种调度方式事实上是将Pod分布到拓扑结构中的两种特殊用例，更常规的用法是将一组Pod对象均匀地分布到拓扑中，这便是Kubernetes v1.16版引入的PodTopologySpread调度插件要实现的功能，该插件在Kubernetes v1.18版本进化至Beta版。经典调度策略使用EvenPodsSpread预选函数和EvenPodsSpreadPriority优选函数协同完成Pod的拓扑分布式调度。<br>Pod资源规范的拓扑分布约束嵌套定义在.spec.topologySpreadConstraints字段中，指示调度器如何基于集群中已有Pod放置待调度的Pod实例。</p>
<ul>
<li>topologyKey &lt;string&gt;：拓扑键，用来划分拓扑结构的节点标签，在指定的键上具有相同值的节点归属为同一拓扑；必选字段。</li>
<li>labelSelector &lt;Object&gt;：Pod标签选择器，用于定义该Pod需要针对哪类Pod对象的位置来确定自身可放置的位置。</li>
<li>maxSkew &lt;integer&gt;：允许Pod分布不均匀的最大程度，即可接受的当前拓扑中由labelSelector匹配到的Pod数量与所有拓扑中匹配到的最少Pod数量的最大差值，可简单用公式表示为max(count(current_topo(matched_pods))-min(topo(matched_pods)))，其中的topo是表示拓扑关系的伪函数名称。</li>
<li>whenUnsatisfiable &lt;string&gt;：拓扑无法满足maxSkew时采取的调度策略，默认值DoNotSchedule是一种强制约束，即不予调度至该区域；而另一可用值Schedule-Anyway则是柔性约束，无法满足约束关系时仍可将Pod放入该拓扑中。</li>
</ul>
<p>以如图11-14中的示例为例，假设foo和bar两个zone中的Pod均为由labelSelector匹配的Pod对象，于是Pod当前的分布模型为[2,1,0]。以foo区域为例，将图中的配置规范所属的Pod调度至该区域时，则当前区域可由labelSelector匹配到的Pod数量为3，而所有区域中可由同一个labelSelector匹配到的Pod数量最少的baz区域数量是0，因而Skew的值为3，这违反了maxSkew的定义，根据whenUnsatisfiable的默认值，该Pod将无法放入该拓扑。类似地，调度至bar区域时Skew的值为2，而调度至baz区域时Skew的值为0，因而此时仅baz一个区域可满足约束条件，这是该Pod唯一可放入的拓扑。显然，若maxSkew的值为2，则bar和baz都是该Pod能够放入的拓扑。<br><img src="/blog/2022/02/24/Pod%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6/image-20220223150232836.png" alt="image-20220223150232836">事实上，我们还可以在Pod规范上同时使用多个PodTopologySpread约束以实现Pod在多级拓扑间的均匀分布。例如，第一级约束负责将Pod分布在多个机架上，而第二级约束负责将Pod在每个机架上均匀分布到多个主机上等，甚至使用类似于region/zone/rack/host等多级约束等。<br>进一步地，在Pod上结合使用Node亲和调度（NodeSelector或NodeAffinity)，我们还能用PodTopologySpread让Pod在某一类型节点上的特定拓扑间均匀分布，而不再是默认的集群中的所有节点。例如，我们可选择将某工作负载分布于不具有GPU设备的节点所属的某特定拓扑中等。</p>
<h2 id="Pod优先级与抢占"><a href="#Pod优先级与抢占" class="headerlink" title="Pod优先级与抢占"></a>Pod优先级与抢占</h2><p>调度器框架内置的QueueSort扩展点允许注册调度器队列排序的插件，注册到该扩展点的内置插件是PrioritySort，它根据Pod资源规范中由spec. priorityClassName字段指定的PriorityClass所属的优先级进行排序，从而优先调度级别最高的Pod对象。对于优先级别相同Pod，则根据其进入队列的时间戳执行先进先出逻辑。<br>未能找到可满足待调度Pod运行要求的节点时，调度器会将该Pod转入Pending状态并为其启动“抢占”过程，调度器会在集群中尝试通过删除某节点上的一个或多个低优先级的Pod，让节点能够满足待调度Pod的运行条件，并将待调度Pod与该节点绑定。但是，若在等待驱逐完成的过程中出现了其他可用节点，则调度器将待调度Pod绑定至该可用节点。<br><font color="red">Pod优先级使用32位的非负整数表示，可用值范围为[0,1000000000]，值越大优先级越高，而大于1000000000的优先级预留给了系统级的关键类Pod，以防止这些Pod被驱逐。Kubernetes使用集群级别的API资源类型PriorityClass完成从优先级到名称的映射，并可由Pod在其规范中按名引用。</font>PriorityClass的资源规范及简要使用说明如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1</span>   <span class="comment"># 资源隶属的API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span>                <span class="comment"># 资源类别标识符</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>                    <span class="comment"># 资源名称</span></span><br><span class="line"><span class="string">value</span>  <span class="string">&lt;integer&gt;</span>                   <span class="comment"># 优先级，必选字段</span></span><br><span class="line"><span class="string">description</span>  <span class="string">&lt;string&gt;</span>              <span class="comment"># 该优先级描述信息</span></span><br><span class="line"><span class="string">globalDefault</span> <span class="string">&lt;boolean&gt;</span>            <span class="comment"># 是否为全局默认优先级</span></span><br><span class="line"><span class="string">preemptionPolicy</span>  <span class="string">&lt;string&gt;</span> <span class="comment"># 抢占策略，Never为禁用，默认为PreemptLowerPriority提示</span></span><br></pre></td></tr></table></figure>

<p><font color="red">若集群上存在多个设定了全局默认优先级的PriorityClass对象，仅优先级最小的会生效。</font><br>完整的Kubernetes集群除了API Server、Controller Manager、Scheduler和etcd等核心组件以外还有一些至关重要的组件，例如metrics-server、CoreDNS和Dashboard等，这些组件以常规Pod形式运行在集群节点上，以免于被驱逐。为此，Kubernetes默认直接附带了system-cluster-critical和system-node-critical两个特殊的PriorityClass以供这类Pod使用，前者的优先级为2000000000，而后者有着更高的优先级2000001000，它们都位于系统预留的优先级范围内。<br>下面的配置清单示例（priorityclass-demo.yaml）中定义了一个未禁用抢占机制的priorityclass/demoapp-priority资源，它仅用于为demoapp Service相关的Pod提供优先级配置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">scheduling.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PriorityClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-priority</span></span><br><span class="line"><span class="attr">value:</span> <span class="number">1000000</span></span><br><span class="line"><span class="attr">globalDefault:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">description:</span> <span class="string">&quot;Should be used for demoapp service pods only.&quot;</span></span><br><span class="line"><span class="attr">preemptionPolicy:</span> <span class="string">PreemptLowerPriority</span></span><br></pre></td></tr></table></figure>

<p>若期望全局禁用优先级抢占功能，需要编辑kube-scheduler的KubeSchedulerConfiguration配置，设定DisablePreemption参数的值为true。不过，Kubernetes自v1.15版本起也支持在单个PriorityClass对象上设定preemptionPolicy的值为Never来禁用资源级别的优先级抢占机制，但截至目前的v1.19版本，该功能仍处于alpha级别，需要在kube-scheduler启用NonPreemptingPriority功能才能被支持。当集群资源紧张时，关键Pod需要依赖调度程序抢占功能才能完成调度，所以不建议全局禁用抢占功能，而在PriorityClass级别的抢占禁止就显得格外有用了。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/" class="post-title-link" itemprop="url">网络模型与网络策略</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-02-23 10:31:33 / 修改时间：15:58:24" itemprop="dateCreated datePublished" datetime="2022-02-23T10:31:33+08:00">2022-02-23</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id><a href="#" class="headerlink" title></a></h1><p>Kubertnetes集群上运行的所有Pod资源默认都会从同一平面网络得到一个IP地址，无论是否处于同一名称空间，各Pod彼此之间都可使用各自的地址直接通信，Pod网络的管理由第三方项目以CNI插件方式完成。进一步来说，除了Pod网络管理，有相当一部分CNI网络插件还实现了网络策略，这些插件赋予管理员和用户通过自定义NetworkPolicy资源来管控Pod通信的能力。</p>
<h2 id="容器网络模型"><a href="#容器网络模型" class="headerlink" title="容器网络模型"></a>容器网络模型</h2><p>Network、IPC和UTS名称空间隔离技术是容器能够使用独立网络栈的根本，而操作系统的网络设备虚拟化技术是打通各容器间通信并构建起多样化网络拓扑的至关重要因素，在Linux系统上，这类的虚拟化设备类型有VETH、Bridge、VLAN、MAC VLAN、IP VLAN、VXLAN、MACTV和TAP/IPVTAP等。</p>
<h3 id="容器网络通信模式"><a href="#容器网络通信模式" class="headerlink" title="容器网络通信模式"></a>容器网络通信模式</h3><p>在Host模式中，各容器共享宿主机的根网络名称空间，它们使用同一个接口设备和网络协议栈，因此，用户必须精心管理共享同一网络端口空间容器的应用与宿主机应用，以避免端口冲突。<br>Bridge模式对host模式进行了一定程度的改进，在该模式中，容器从一个或多个专用网络（地址池）中获取IP地址，并将该IP地址配置在自己的网络名称空间中的网络端口设备上。于是，拥有独立、隔离的网络名称空间的各容器有自己独占的端口空间，而不必再担心各容器及宿主机间的端口冲突。<br>这里反复提到的Bridge是指Linux内核支持的虚拟网桥设备，它模拟的是物理网桥设备，工作于数据链路层，根据习得的MAC地址表向设备端口转发数据帧。虚拟以太网接口设备对（veth pair）是连接虚拟网桥和容器的网络媒介：一端插入到容器的网络栈中，表现为通信接口（例如eth0等），另一端则于宿主机上关联虚拟网桥并被降级为当前网桥的“从设备”，失去调用网络协议栈处理数据包的资格，从而表现为桥设备的一个端口，如图10-1所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140343118.png" alt="image-20220218140343118"></p>
<p>Linux网桥提供的是宿主机内部的网络，同一主机上的各容器可基于网桥和ARP协议完成本地通信。而在宿主机上，网桥表现为一个网络接口并可拥有IP地址，如图10-2中的docker0会在docker daemon进程启动后被自动配置172.17.0.1/16的地址。于是，由宿主机发出的网络包可通过此桥接口送往连接至同一个桥上的其他容器，如图10-1上的Container-1或Container-2，这些容器通常需要由某种地址分配组件（IPAM）自动配置一个相关网络（例如72.17.0.0/16）中的IP地址。<br>但此私有网络中的容器却无法直接与宿主机之外的其他主机或容器进行通信，通常作为请求方，这些容器需要由宿主机上的iptables借助SNAT机制实现报文转发，而作为服务方时，它们的服务需要宿主机借助于iptables的DNAT规则进行服务暴露。因而，总结起来，配置容器使用Bridge网络的步骤大体有如下几个：</p>
<blockquote>
<p>1）若不存在，则需要先在宿主机上添加一个虚拟网桥；<br>2）为每个容器配置一个独占的网络名称空间；<br>3）生成一对虚拟以太网接口（如veth pair），将一端插入容器网络名称空间，一端关联至宿主机上的网桥；<br>4）为容器分配IP地址，并按需生成必要的NAT规则。</p>
</blockquote>
<p>尽管Bridge模型下各容器使用独立且隔离的网络名称空间，且彼此间能够互连互通，但跨主机的容器间通信时，请求报文会首先由源宿主机进行一次SNAT（源地址转换）处理，而后由目标宿主机进行一次DNAT（目标地址转换）处理方可送到目标容器，如图10-2所示。这种复杂的NAT机制将会使得网络通信管理的复杂度随容器规模增呈成几何倍数上升，而且基于ipables实现的NAT规则，也限制了解决方案的规模和性能。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140433463.png" alt="image-20220218140433463"></p>
<p>Kubernetes系统依然面临着类似的问题，只不过，跨节点的容器通信问题变成了更抽象的Pod资源问题。我们知道，Kubernetes将具有亲密关系的容器整合成Pod作为基础单元，并设计了专用的网络模型来支撑Kubernetes组件间以及与其他应用程序的通信。这种网络模型基于扁平网络结构，无须将主机端口映射到容器端口便能完成分布式环境中的容器间通信，并负责解决4类通信需求：同一Pod内容器间的通信、Pod间的通信、Service到Pod间的通信以及集群外部与Service之间的通信。</p>
<h4 id="Pod内容器间通信"><a href="#Pod内容器间通信" class="headerlink" title="Pod内容器间通信"></a>Pod内容器间通信</h4><p>同一个Pod内运行的多个容器通过lo接口即可在本地内核协议栈上完成交互，如图10-3中的Pod P内的Container1和Container2之间的通信，这类似于同一主机上的多个进程间的本地通信。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140510175.png" alt="image-20220218140510175"></p>
<h4 id="分布式Pod间通信"><a href="#分布式Pod间通信" class="headerlink" title="分布式Pod间通信"></a>分布式Pod间通信</h4><p>各Pod对象需要运行在同一个平面网络中，每个Pod对象拥有一个虚拟网络接口和集群全局唯一的地址，该IP地址可用于直接与其他Pod进行通信，例如图10-4中的Pod P和Pod Q之间的通信。另外，运行Pod的各节点也会通过桥接设备等持有此平面网络中的一个IP地址，如图10-3中的cni0接口，这意味着Node到Pod间的通信也可直接在此网络进行。因此，Pod间的通信或Pod到Node间的通信类似于同一IP网络中的主机间进行的通信。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140600977.png" alt="image-20220218140600977"></p>
<p>Kubernetes设计了Pod通信模型。这些第三方插件要负责为各Pod设置虚拟网络接口、分配IP地址并将其接入到容器网络中等各种任务，以实现Pod间的直接通信。</p>
<h4 id="Service与Pod间的通信"><a href="#Service与Pod间的通信" class="headerlink" title="Service与Pod间的通信"></a>Service与Pod间的通信</h4><p>Service资源的专用网络也称为集群网络，需要在启动kube-apiserver时由–service-cluster-ip-range选项进行指定，例如默认的10.96.0.0/12，每个Service对象在此网络中拥有一个称为Cluster-IP的固定地址。管理员或用户对Service对象的创建或更改操作，会由API Server存储完成后触发各节点上的kube-proxy，并根据代理模式的不同将该Service对象定义为相应节点上的iptables规则或ipvs规则，Pod或节点客户端对Service对象的IP地址的访问请求将由这些iptables或ipvs规则进行调度和转发，从而完成Pod与Service之间的通信，如图10-4所示。</p>
<h4 id="集群外部客户端与Pod对象的通信"><a href="#集群外部客户端与Pod对象的通信" class="headerlink" title="集群外部客户端与Pod对象的通信"></a>集群外部客户端与Pod对象的通信</h4><p>引入集群外部流量到达Pod对象有4种方式，有两种是基于本地节点的端口（nodePort）或根网络名称空间（hostNetwork），另外两种则是基于工作在集群级别的NodePort或LoadBalancer类型的Service对象。不过，即便是四层代理的模式也要经由两级转发才能到达目标Pod资源：请求流量首先到达外部负载均衡器，由其调度至某个工作节点之上，而后再由工作节点的netfilter（kube-proxy）组件上的规则（iptables或ipvs）调度至某个目标Pod对象。<br>集群内的Pod间通信，即便通过Service进行“代理”和“调度”，但绝大部分都无须使用NAT，而是Pod间的直接通信。</p>
<h3 id="CNI网络插件基础"><a href="#CNI网络插件基础" class="headerlink" title="CNI网络插件基础"></a>CNI网络插件基础</h3><p>CNI是容器引擎与遵循该规范网络插件的中间层，专用于为容器配置网络子系统，目前由RKT、Docker、Kubernetes、OpenShift和Mesos等相关的容器运行时环境所运行。<br>通常，遵循CNI规范的网络插件是一个可执行程序文件，它们可由容器编排系统（例如Kubernetes等）调用，负责向容器的网络名称空间插入一个网络接口并在宿主机上执行必要的任务以完成虚拟网络配置，因而通常被称为网络管理插件，即NetPlugin。随后，NetPlugin还需要借助IPAM插件为容器的网络接口分配IP地址，这意味着CNI允许将核心网络管理功能与IP地址分配等功能相分离，并通过插件组合的方式堆叠出一个完整的解决方案。简单来说，目前的CNI规范主要由NetPlugin和IPAM两个插件API组成，如图10-5所示。<br><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140623865.png" alt="image-20220218140623865">以下是对两个插件的简要说明。</p>
<ul>
<li>网络插件也称Main插件，负责创建/删除网络以及向网络添加/删除容器，它专注于连通容器与容器之间以及容器与宿主机之间的通信，同容器相关的网络设备通常都由该类插件所创建，例如Bridge、IP VLAN、MAC VLAN、loopback、PTP、VETH以及VLAN等虚拟设备。</li>
<li>IPAM（IP Address Management），该类插件负责创建/删除地址池以及分配/回收容器的IP地址；目前，该类型插件的实现主要有host-local和dhcp两个，前一个基于预置的地址范围进行地址分配，而后一个通过DHCP协议获取地址。</li>
</ul>
<p>显然，NetPlugin是CNI中最重要的组成部分，它才是执行创建虚拟网络、为Pod生成网络接口设备，以及将Pod接入网络中等核心任务的插件。为了能够满足分布式Pod通信模型中要求的所有Pod必须在同一平面网络中的要求，<font color="red">NetPlugin目前常用的实现方案有Overlay网络（Overlay Network）和Underlay网络（Underlay Network）两类。</font></p>
<ul>
<li>Overlay网络借助VXLAN、UDP、IPIP或GRE等隧道协议，通过隧道协议报文封装Pod间的通信报文（IP报文或以太网帧）来构建虚拟网络。</li>
<li>Underlay网络通常使用direct routing（直接路由）技术在Pod的各子网间路由Pod的IP报文，或使用Bridge、MAC VLAN或IP VLAN等技术直接将容器暴露给外部网络。</li>
</ul>
<p><font color="red">其实，Overlay网络的底层网络也就是承载网络，因此，Underlay网络的解决方案也就是一类非借助隧道协议而构建的容器通信网络。相较于承载网络，Overlay网络由于存在额外的隧道报文封装，会存在一定程度的性能开销。然而，用户在不少场景中可能会希望创建跨越多个L2或L3的逻辑网络子网，这就只能借助Overlay封装协议实现。为Pod配置网络接口是NetPlugin的核心功能之一，但不同的容器虚拟化网络解决方案中，为Pod的网络名称空间创建虚拟接口设备的方式也会有所不同，目前，较为注流的实现方式有veth（虚拟以太网)设备、多路复用及硬件交换3种，如图10-6所示。</font></p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140640839.png" alt="image-20220218140640839"></p>
<ul>
<li>veth设备：创建一个网桥，并为每个容器创建一对虚拟以太网接口，一个接入容器内部，另一个留置于根名称空间内添加为Linux内核桥接功能或OpenvSwitch（OVS）网桥的从设备。</li>
<li>多路复用：多路复用可以由一个中间网络设备组成，它暴露多个虚拟接口，使用数据包转发规则来控制每个数据包转到的目标接口；MAC VLAN技术为每个虚拟接口配置一个MAC地址并基于此地址完成二层报文收发，IP VLAN则是分配一个IP地址并共享单个MAC，并根据目标IP完成容器报文转发。</li>
<li>硬件交换：现今市面上有相当数量的NIC都支持SR-IOV（单根I/O虚拟化），SR-IOV是创建虚拟设备的一种实现方式，每个虚拟设备自身表现为一个独立的PCI设备，并有着自己的VLAN及硬件强制关联的QoS；SR-IOV提供了接近硬件级别的性能。</li>
</ul>
<p>一般说来，基于VXLAN Overlay网络的虚拟容器网络中，NetPlugin会使用虚拟以太网内核模块为每个Pod创建一对虚拟网卡；基于MAC VLAN/IP VLAN Underlay网络的虚拟容器网络中，NetPlugin会基于多路复用模式中的MAC VLAN/IP VLAN内核模块为每个Pod创建虚拟网络接口设备；而基于IP报文路由技术的Underlay网络中，各Pod接口设备通常也是借助veth设备完成。<br>相比较来说，IPAM插件的功能则要简单得多，目前可用的实现方案中，host-local从本地主机可用的地址空间范围中分配IP地址，它没有地址租约，属于静态分配机制；而dhcp插件则需要一个特殊的客户端守护进程（通常是dhcp插件的子组件）运行在宿主机之上，它充当本地主机上各容器中的DHCP客户端与网络中的DHCP服务器之间的代理，并适当地续定租约。<br>Kubernetes借助CNI插件体系来组合需要的网络插件完成容器网络编排功能。每次初始倾化或删除Pod对象时，kubelet都会调用默认的CNI插件创建一个虚拟设备接口附加到相关的底层网络，为其设置IP地址、路由信息并将其映射到Pod对象的网络名称空间。具体过程是，kubelet首先在默认的/etc/cni/net.d/目录中查找JSON格式的CNI配置文件，接着基于该配置文件中各插件的type属性到/opt/cni/bin/中查找相关的插件二进制文件，由该二进制程序基于提供的配置信息完成相应的操作。<br>kubelet基于包含命令参数CNI_ARGS、CNI_COMMAND、CNI_IFNAME、CNI_NETNS、CNI_CONTAINERID、CNI_PATH的环境变量调用CNI插件，而被调用的插件同样使用JSON格式的文本信息进行响应，描述操作结果和状态。Pod对象的名称和名称空间将作为CNI_ARGS变量的一部分进行传递（例如K8S_POD_NAMESPACE=default; K8S_POD_NAME=myapp-6d9f48c5d9-n77qp;）。它可以定义每个Pod对象或Pod网络名称空间的网络配置（例如，将每个网络名称空间放在不同的子网中）。</p>
<h3 id="Overlay网络模型"><a href="#Overlay网络模型" class="headerlink" title="Overlay网络模型"></a>Overlay网络模型</h3><p>物理网络模型中，连通多个物理网桥上的主机的一个简单办法是通过媒介直接连接这些网桥设备，各个主机处于同一个局域网（LAN）之中，管理员只需要确保各个网桥上每个主机的IP地址不相互冲突即可。类似地，若能够直接连接宿主机上的虚拟网桥形成一个大的局域网，就能在数据链路层打通各宿主机上的内部网络，让容器可通过自有IP地址直接通信。为避免各容器间的IP地址冲突，一个常见的解决方案是将每个宿主机分配到同一网络中的不同子网，各主机基于自有子网向其容器分配IP地址。<br>显然，主机间的网络通信只能经由主机上可对外通信的网络接口进行，跨主机在数据链路层直接连接虚拟网桥的需求必然难以实现，除非借助宿主机间的通信网络构建的通信“隧道”进行数据帧转发。这种于某个通信网络之上构建出的另一个逻辑通信网络通常即10.1.2节提及的Overlay网络或Underlay网络。图10-7为Overlay网络功能示意图。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140706545.png" alt="image-20220218140706545"></p>
<p>隧道转发的本质是将容器双方的通信报文分别封装成各自宿主机之间的报文，借助宿主机的网络“隧道”完成数据交换。这种虚拟网络的基本要求是各宿主机只需支持隧道协议即可，对于底层网络没有特殊要求。<br>VXLAN协议是目前最流行的Overlay网络隧道协议之一，它也是由IETF定义的NVO3（Network Virtualization over Layer 3）标准技术之一，采用L2 over L4（MAC-in-UDP）的报文封装模式，将二层报文用三层协议进行封装，可实现二层网络在三层范围内进行扩展，将“二层域”突破规模限制形成“大二层域”。那么，同一大二层域就类似于传统网络中VLAN（虚拟局域网）的概念，只不过在VXLAN网络中，它被称作Bridge-Domain，以下简称为BD。类似于不同的VLAN需要通过VLAN ID进行区分，各BD要通过VNI加以标识。但是，为了确保VXLAN机制通信过程的正确性，涉及VXLAN通信的IP报文一律不能分片，这就要求物理网络的链路层实现中必须提供足够大的MTU值，或修改其MTU值以保证VXLAN报文的顺利传输。<br>VXLAN的显著的优势之一是对底层网络没有侵入性，管理员只需要在原有网络之上添加一些额外设备即可构建出虚拟的逻辑网络来。这个额外添加的设备称为VTEP（VXLAN Tunnel Endpoints），它工作于VXLAN网络的边缘，负责相关协议报文的封包和解包等操作，从作用来说相当于VXLAN隧道的出入口设备。<br>VTEP代表着一类支持VXLAN协议的交换机，而支持VXLAN协议的操作系统也可将一台主机模拟为VTEP，Linux内核自3.7版本开始通过vxlan内核模块原生支持此协议。于是，各主机上由虚拟网桥构建的LAN便可借助vxlan内核模块模拟的VTEP设备与其他主机上的VTEP设备进行对接，形成隧道网络。同一个二层域内的各VTEP之间都需要建立VXLAN隧道，因此跨主机的容器间直接进行二层通信的VXLAN隧道是各VTEP之间的点对点隧道，如图10-8所示。对于Flannel来说，这个VTEP设备就是各节点上生成flannel.1网络接口，其中的“1”是VXLAN中的BD标识VNI，因而同一Kubernetes集群上所有节点的VTEP设备属于VNI为1的同一个BD。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140722441.png" alt="image-20220218140722441"></p>
<p>类似VLAN的工作机制，相同VXLAN VNI在不同VTEP之间的通信要借助二层网关来完成，而不同VXLAN之间，或者VXLAN同非VXLAN之间的通信则需经由三层网关实现。VXLAN支持使用集中式和分布式两种形式的网关：前者支持流量的集中管理，配置和维护较为简单，但转发效率不高，且容易出现瓶颈和网关可用性问题；后者以各节点为二层或三层网关，消除了瓶颈。<br>然而，VXLAN网络中的容器在首次通信之前，源VTEP又如何得知目标服务器在哪一个VTEP，并选择正确的路径传输通信报文呢？常见的解决思路一般有两种：多播和控制中心。多播是指同一个BD内的各VTEP加入同一个多播域中，通过多播报文查询目标容器所在的目标VTEP。而控制中心则在某个共享的存储服务上保存所有容器子网及相关VTEP的映射信息，各主机上运行着相关的守护进程，并通过与控制中心的通信获取相关的映射信息。Flannel默认的VXLAN后端采用的是后一种方式，它把网络配置信息存储在etcd系统上。<br>Linux内核自3.7版本开始支持vxlan模块，此前的内核版本可以使用UDP、IPIP或GRE隧道技术。事实上，考虑到当今公有云底层网络的功能限制，Overlay网络反倒是一种最为可行的容器网络解决方案，仅那些更注重网络性能的场景才会选择Underlay网络。</p>
<h3 id="Underlay网络模型"><a href="#Underlay网络模型" class="headerlink" title="Underlay网络模型"></a>Underlay网络模型</h3><p>Underlay网络就是传统IT基础设施网络，由交换机和路由器等设备组成，借助以太网协议、路由协议和VLAN协议等驱动，它还是Overlay网络的底层网络，为Overlay网络提供数据通信服务。容器网络中的Underlay网络是指借助驱动程序将宿主机的底层网络接口直接暴露给容器使用的一种网络构建技术，较为常见的解决方案有MAC VLAN、IP VLAN和直接路由等。</p>
<h4 id="MAC-VLAN"><a href="#MAC-VLAN" class="headerlink" title="MAC VLAN"></a>MAC VLAN</h4><p>MAC VLAN支持在同一个以太网接口上虚拟出多个网络接口，每个虚拟接口都拥有唯一的MAC地址，并可按需配置IP地址。通常这类虚拟接口被网络工程师称作子接口，但在MAC VLAN中更常用上层或下层接口来表述。与Bridge模式相比，MAC VLAN不再依赖虚拟网桥、NAT和端口映射，它允许容器以虚拟接口方式直接连接物理接口。图10-9给出了Bridge与MAC VLAN网络对比示意图。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218140903448.png" alt="image-20220218140903448"></p>
<p>MAC VLAN有Private、VEPA、Bridge和Passthru几种工作模式，它们各自的工作特性如下。</p>
<ul>
<li>Private：禁止构建在同一物理接口上的多个MAC VLAN实例（容器接口）彼此间的通信，即便外部的物理交换机支持“发夹模式”也不行。</li>
<li>VPEA：允许构建在同一物理接口上的多个MAC VLAN实例（容器接口）彼此间的通信，但需要外部交换机启用发夹模式，或者存在报文转发功能的路由器设备。</li>
<li>Bridge：将物理接口配置为网桥，从而允许同一物理接口上的多个MAC VLAN实例基于此网桥直接通信，而无须依赖外部的物理交换机来交换报文；此为最常用的模式，甚至还是Docker容器唯一支持的模式。</li>
<li>Passthru：允许其中一个MAC VLAN实例直接连接物理接口。</li>
</ul>
<p>除了Passthru模式外的容器流量将被MAC VLAN过滤而无法与底层主机通信，从而将主机与其运行的容器完全隔离，其隔离级别甚至高于网桥式网络模型，这对于有多租户需求的场景尤为有用。由于各实例都有专用的MAC地址，因此MAC VLAN允许传输广播和多播流量，但它要求物理接口工作于混杂模式，很多公有云环境中并不允许使用混杂模式，这意味着MAC VLAN更适用于本地网络环境。<br>需要注意的是，MAC VLAN为每个容器使用一个唯一的MAC地址，这可能会导致具有安全策略以防止MAC欺骗的交换机出现问题，因为这类交换机的每个接口只允许连接一个MAC地址。另外，有些物理网卡存在可支撑的MAC地址数量上限。</p>
<h4 id="IP-VLAN"><a href="#IP-VLAN" class="headerlink" title="IP VLAN"></a>IP VLAN</h4><p>IP VLAN类似于MAC VLAN，它同样创建新的虚拟网络接口并为每个接口分配唯一的IP地址，不同之处在于，每个虚拟接口将共享使用物理接口的MAC地址，从而不再违反防止MAC欺骗的交换机的安全策略，且不要求在物理接口上启用混杂模式，如图10-10所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218141222681.png" alt="image-20220218141222681"></p>
<p>IP VLAN有L2和L3两种模型，其中IP VLAN L2的工作模式类似于MAC VLAN Bridge模式，上层接口（物理接口）被用作网桥或交换机，负责为下层接口交换报文；而IP VLAN L3模式中，上层接口扮演路由器的角色，负责为各下层接口路由报文，如图10-11所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218141321489.png" alt="image-20220218141321489"></p>
<p>IP VLAN L2模型与MAC VLAN Bridge模型都支持ARP协议和广播流量，它们拥有直接接入网桥设备的网络接口，能够通过802.1d数据包进行泛洪和MAC地址学习。但IP VLAN L3模式下，网络栈在容器内处理，不支持多播或广播流量，从这个意义上讲，它的运行模式与路由器的报文处理机制相同。<br>虽然支持多种网络模型，但MAC VLAN和IP VLAN不能同时在同一物理接口上使用。Linux内核文档中强调，MAC VLAN和IP VLAN具有较高的相似度，因此，通常仅在必须使用IP VLAN的场景中才不使用MAC VLAN。一般说来，强依赖于IP VLAN的场景有如下几个：</p>
<ul>
<li>Linux主机连接到的外部交换机或路由器启用了防止MAC地址欺骗的安全策略；</li>
<li>虚拟接口的需求数量超出物理接口能够支撑的容量上限，并且将接口置于混杂模式会给性能带来较大的负面影响；</li>
<li>将虚拟接口放入不受信任的网络名称空间中可能会导致恶意的滥用。<br>需要注意的是，Linux内核自4.2版本后才支持IP VLAN网络驱动，且在Linux主机上使用ip link命令创建的</li>
</ul>
<p>802.1q配置接口不具有持久性，因此需依赖管理员通过网络启动脚本保持配置。</p>
<h4 id="直接路由"><a href="#直接路由" class="headerlink" title="直接路由"></a>直接路由</h4><p>“直接路由”模型放弃了跨主机容器在L2的连通性，而专注于通过路由协议提供容器在L3的通信方案。这种解决方案因为更易于集成到现在的数据中心的基础设施之上，便捷地连接容器和主机，并在报文过滤和隔离方面有着更好的扩展能力及更精细的控制模型，因而成为容器化网络较为流行的解决方案之一。<br>一个常用的直接路由解决方案如图10-12所示，每个主机上的各容器在二层通过网桥连通，网关指向当前主机上的网桥接口地址。跨主机的容器间通信，需要依据主机上的路由表指示完成报文路由，因此每个主机的物理接口地址都有可能成为另一个主机路由报文中的“下一跳”，这就要求各主机的物理接口必须位于同一个L2网络中。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218141337314.png" alt="image-20220218141337314"></p>
<p>于是，在较大规模的主机集群中，问题的关键便转向如何更好地为每个主机维护路由表信息。常见的解决方案有：①Flannel host-gw使用存储总线etcd和工作在每个节点上的flanneld进程动态维护路由；②Calico使用BGP（Border Gateway Protocol）协议在主机集群中自动分发和学习路由信息。与Flannel不同的是，Calico并不会为容器在主机上使用网桥，而是仅为每个容器生成一对veth设备，留在主机上的那一端会在主机上生成目标地址，作为当前容器的路由条目，如图10-13所示。<br><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218141402977.png" alt="image-20220218141402977"></p>
<p>显然，较Overlay来说，无论是MAC VLAN、IP VLAN还是直接路由机制的Underlay网络模型的实现，它们因无须额外的报文开销而通常有着更好的性能表现，但对底层网络有着更多的限制条件。</p>
<h3 id="配置CNI插件"><a href="#配置CNI插件" class="headerlink" title="配置CNI插件"></a>配置CNI插件</h3><p>CNI具有很强的扩展性和灵活性，例如，如果用户对某个插件有特殊的需求，可以通过输入中的args和环境变量CNI_ARGS传递，然后在插件中实现自定义的功能，这大大增加了它的扩展性。CNI插件把main和ipam分开，为用户提供了自由组合它们的机制，甚至一个CNI插件也可以直接调用另外一个插件。<br>CNI项目中有两个代码仓库：一个是提供用于开发CNI网络插件的库文件libcni，以及命令行工具cnitool的containernetworking/cni；另一个是CNI内置的插件程序containernetworking/plugins，它目前附带了如下几类网络插件。<br>1）main类别中，各插件主要用于创建容器和容器接口，内置的实现有如下几个。</p>
<ul>
<li>bridge：创建一个虚拟网桥，并将宿主机和每个Pod接入该网桥。</li>
<li>ipvlan：向容器中添加一个IP VLAN网络接口。</li>
<li>macvlan：向容器中添加一个MAC VLAN网络接口，创建一个新MAC地址，并基于该地址向容器转发报文。</li>
<li>loopback：设置容器lo接口的状态。</li>
<li>ptp：创建一对veth设备。</li>
<li>vlan：分配一个VLAN设备。</li>
<li>host-device：将宿主机现有的某网络接口移入Pod中。</li>
</ul>
<p>2）ipam类别中，各插件用于为容器分配IP地址，内置的实现包括host-local、dhcp和static。</p>
<ul>
<li>dhcp：在每个节点上运行一个dhcp守护进程，它负责代理该节点上的所有容器中的dhcp客户端向dhcp服务发起请求。</li>
<li>host-local：基于本地的IP地址分配数据库，完成地址分配。</li>
</ul>
<p>static：为容器接口直接指定一个静态IP地址，仅应该用于调试目的。<br>3）meta类别的网络插件不实现任何网络功能，它们调用其他网络工具或插件完成管理功能，内置的实现有如下几个。</p>
<ul>
<li>flannel：根据Flannel配置文件生成网络接口。</li>
<li>tuning：调整现存某接口的sysctl参数值。</li>
<li>portmap：使用iptables将宿主机的端口映射至容器端口，实现hostPort功能。</li>
<li>bandwidth：基于流量控制工具tbf进行带宽限制。</li>
<li>sbr：为接口配置基于源IP地址的路由。</li>
<li>firewall：防火墙插件，使用iptables或firewalld规则管理进出的流量。</li>
</ul>
<p>具体操作方面，CNI网络插件通常应该支持添加（ADD）、删除（DEL）、检验（CHECK）和报告版本信息（VERSION）几个管理操作。除了VERSION外，其他3个操作通常都需要用到以下几个方面的配置信息。</p>
<ul>
<li>Container ID：容器标识，用于引用容器网络名称空间。</li>
<li>网络名称空间（netns）路径：即配置的目标网络名称空间的访问路径，例如/proc/[pid]/ns/net等；通常指定引用的容器ID后，其网络名称空间路径可通过容器的相关属性获取。</li>
<li>网络配置参数：一个JSON格式的配置文件，描述了配置容器网络的各相关参数，例如/etc/cin/net.d/10-mynet.json。</li>
<li>其他配置参数：用于在容器级别为每个容器提供一个简单的配置方式，以取代统一配置机制。</li>
</ul>
<p>容器内的网络接口名称：网络插件配置的目标接口，需要是遵循Linux系统网络插件命名规范的接口名称。<br>在含有网络配置参数的JSON格式的配置文件中，type属性用于指定要调用的网络插件的名称，调用者（例如Kubernetes或OpenShift等）可从预定义的目标列表中查找相关网络插件的可执行文件，并通过如下几个变量向其传递参数。</p>
<ul>
<li>CNI_COMMAND：需要执行的网络管理操作，例如ADD、DEL、CHECK或VERSION。</li>
<li>CNI_CONTAINERID：容器ID。</li>
<li>CNI_NETNS：网络名称空间相关的文件路径。</li>
<li>CNI_IFNAME：目标网络接口的名称，如果插件无法使用此接口，则必须返回错误。</li>
<li>CNI_ARGS：额外传入的参数。</li>
<li>CNI_PATH：搜索CNI插件时使用的目标路径列表。</li>
<li>CNI_CONF_NAME：使用的网络配置文件。</li>
</ul>
<p>插件的相关管理操作执行成功时以0为返回码，其中ADD操作成功时的返回结果是一个JSON格式的输出，它通常包含cniVersion、interfaces、ips、routes和dns几个数据段。<br>如前所述，kubelet中的CNI网络插件的配置文件以JSON格式表达，它可以以静态格式存储于磁盘上，也可以由容器管理系统从其他源动态生成，下面是配置文件中的常用字段：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">cniVersion</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># CNI配置文件的语义版本</span></span><br><span class="line"><span class="string">name</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 网络的名称，在当前主机上必须唯一</span></span><br><span class="line"><span class="string">type</span> <span class="string">&lt;string&gt;：</span>            <span class="comment"># CNI插件的可执行文件名</span></span><br><span class="line"><span class="string">args</span> <span class="string">&lt;map[string]string&gt;</span>   <span class="comment"># 由容器管理系统提供的附加参数，可选配置</span></span><br><span class="line"><span class="string">ipMasq</span>  <span class="string">&lt;Boolean&gt;</span>          <span class="comment"># 是否启用IP伪装，可选参数</span></span><br><span class="line"><span class="string">ipam</span> <span class="string">&lt;map[string]string&gt;</span>   <span class="comment"># IP地址分配插件，主要有host-local和dhcp</span></span><br><span class="line">  <span class="string">type</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 能够完成IP地址分配的插件的名称</span></span><br><span class="line">  <span class="string">subnet</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 分配IP地址时使用的子网地址</span></span><br><span class="line">  <span class="string">routes</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 路由信息</span></span><br><span class="line">    <span class="string">dst</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 目标主机或网络    </span></span><br><span class="line">    <span class="string">gw</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 网关地址</span></span><br><span class="line"><span class="string">dns</span> <span class="string">&lt;map[string]string&gt;</span>    <span class="comment"># 配置容器的DNS属性</span></span><br><span class="line">  <span class="string">nameservers</span> <span class="string">&lt;[]string&gt;</span>   <span class="comment"># DNS名称服务器列表，其值为ipv4或ipv5格式的地址</span></span><br><span class="line">  <span class="string">domain</span> <span class="string">&lt;[]string&gt;</span>        <span class="comment"># 用于短格式主机查找的本地域 </span></span><br><span class="line">  <span class="string">search</span> <span class="string">&lt;[]string&gt;</span>        <span class="comment"># 用于短格式主机查找的优先级排序的搜索域列表</span></span><br><span class="line">  <span class="string">options</span> <span class="string">&lt;[]string&gt;</span>       <span class="comment"># 传递给解析程序的选项列表</span></span><br></pre></td></tr></table></figure>

<p>作为基本功能的一个组成部分，CNI插件需要为接口分配和维护IP地址，并负责为IP地址生成必要的路由信息。这为CNI插件提供了极大的灵活性的同时也引入了较大负担，并且众多CNI插件可能需要重复提供相同的代码以完成此类功能。于是，IP地址分配通常由独立的IP地址管理（IPAM）插件负责，并由CNI插件进行调用以完成代码复用，常用的IP地址分配类型有host-local和dhcp两个，它们负责分配地址并将结果返回给调用者。<br>IPAM插件同CNI插件一样，都是通过运行相关的可执行文件进行调用，调用者在由CNI_PATH变量预定义的路径列表中搜索目标IPAM的可执行文件。IPAM插件必须接收所有传递给CNI插件的相同环境变量，类似于CNI插件，IPAM插件也通过标准输入（stdin）接收网络配置信息。<br>下面是一个示例配置，它使用Bridge插件，ipam调用类型为host-local，它通过在一个地址范围内挑选一个未使用的IP完成地址分配：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;cniVersion&quot;</span><span class="punctuation">:</span> <span class="string">&quot;0.4.0&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mynet&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;bridge&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="comment">// 插件类型专有的配置</span></span><br><span class="line">  <span class="attr">&quot;bridge&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cni0&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;ipam&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;host-local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="comment">// ipam专有的配置</span></span><br><span class="line">    <span class="attr">&quot;subnet&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.1.0.0/16&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;gateway&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.1.0.1&quot;</span></span><br><span class="line">  <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;dns&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;nameservers&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span> <span class="string">&quot;10.1.0.1&quot;</span> <span class="punctuation">]</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>CNI还支持使用plugins字段组合多个CNI网络插件依次进行网络配置，以实现将核心网络管理插件和meta插件等相组合，以堆叠出一个完整的解决方案。各插件以列表形式依次定义，前一个插件的配置结果将传递给后一个插件，直到列表中的所有插件都成功配置完成。下面是摘自Flannel自行提供给CNI的网络配置，它使用网络配置列表，分别调用了Flannel插件和PortMap插件来配置容器网络。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;cbr0&quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="attr">&quot;plugins&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;flannel&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;delegate&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;hairpinMode&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;isDefaultGateway&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;portmap&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;capabilities&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;portMappings&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>delegate是指将网络配置“委派”给某个指定的CNI内置插件来完成，对于Flannel插件来说，它通过delegate调用的插件是Bridge，因此容器网络配置实质上是由Bridge插件完成，Flannel不过是借助delegate向Bridge插件传递部分配置参数，例如网络地址10.244.0.0/16等信息。<br>另外，delegate配置段中的haripinMode参数用于定义是否启用发夹模式，在容器中的应用通过宿主机的端口映射（NAT）访问自己提供的服务时，此模式必须要置于启用状态，因为默认情况下，网桥设备不允许一个数据报文从同一端口进行收发操作，而发夹模式正是用于取消限制。例如，某Pod作为客户端访问自己所属Service对象又碰巧被算法调度回自身时，就必须要启用发夹模式。</p>
<h3 id="CNI插件与选型"><a href="#CNI插件与选型" class="headerlink" title="CNI插件与选型"></a>CNI插件与选型</h3><p>如前所述，CNI规范负责连接容器管理系统和网络插件两类组件，它们之间通过JSON格式的文件进行通信，以完成容器网络管理。具体的管理操作均由插件来实现，包括创建容器netns（网络名称空间）、关联网络接口到对应的netns，以及给网络接口分配IP等。CNI的基本思想是为容器运行时环境在创建容器时，先创建好netns，然后调用CNI插件为这个netns配置网络，而后启动容器内的进程。<br>下面是较为流行的部分网络插件项目。</p>
<ul>
<li>Flannel：由CoreOS提供的CNI网络插件，也是最简单、最受欢迎的网络插件；它使用VXLAN或UDP协议封装IP报文来创建Overlay网络，并借助etcd维护网络的分配信息，同一节点上的Pod间通信可基于本地虚拟网桥（cni0）进行，而跨节点的Pod间通信则要由flanneld守护进程封装隧道协议报文后，通过查询etcd路由到目的地；Flannel也支持host-gw路由模型。</li>
<li>Calico：同Flannel一样广为流行的CNI网络插件，以灵活、良好的性能和网络策略所著称。Calico是路由型CNI网络插件，它在每台机器上运行一个vRouter，并基于BGP路由协议在节点之间路由数据包。Calico支持网络策略，它借助iptables实现访问控制功能。另外，Calico也支持IPIP型的Overlay网络。</li>
<li>Canal：由Flannel和Calico联合发布的一款统一网络插件，它试图将二者的功能集成在一起，由前者提供CNI网络插件，由后者提供网络策略。</li>
<li>WeaveNet：由Weaveworks提供的CNI网络插件，支持网络策略。WeaveNet需要在每个节点上部署vRouter路由组件以构建起一个网格化的TCP连接，并通过Gossip协议来同步控制信息。在数据平面上，WeaveNet通过UDP封装实现L2隧道报文，报文封装支持两种模式：一种是运行在用户空间的sleeve（套筒）模式，另一种是运行在内核空间的fastpath（快速路径）模式，当网络拓扑不适合fastpath模式时，Weave将自动切换至sleeve模式。</li>
<li>Multus CNI：多CNI插件，实现了CNI规范的所有参考类插件（例如Flannel、MAC VLAN、IPVLAN和DHCP等）和第三方插件（例如Calico、Weave和Contiv等），也支持Kubernetes中的SR-IOV、DPDK、OVS-DPDK和VPP工作负载，以及Kubernetes中的云原生应用程序和基于NFV的应用程序，是需要为Pod创建多网络接口时的常用选择。</li>
<li>Antrea：一款致力于成为Kubernetes原生网络解决方案的CNI网络插件，它使用OpenvSwitch构建数据平面，基于Overlay网络模型完成Pod间的报文交换，支持网络策略，支持使用IPSec ESP加密GRE隧道流量。</li>
<li>DAMM：由诺基亚发布的电信级的CNI网络插件，支持具有高级功能的IP VLAN模式，内置IPAM模块，可管理多个集群范围内的不连续三层网络；支持通过CNI meta插件将网络管理功能委派给任何其他网络插件。</li>
<li>kube-router：kube-router是Kubernetes网络的一体化解决方案，它可取代kube-proxy实现基于ipvs的Service，能为Pod提供网络，支持网络策略以及拥有完美兼容BGP协议的高级特性。</li>
</ul>
<p>通常来说，选择网络插件时应该基于底层系统环境限制、容器网络的功能需求和性能需求3个重要的评估标准来衡量插件的适用性。</p>
<ul>
<li>底层系统环境限制：公有云环境多有自己专有的实现，例如Google GCE、Azure CNI、AWS VPC CNI和Aliyun Terway等，它们通常是相应环境上较佳的选择。若虚拟化环境限制较多，除Overlay网络模型别无选择，则可用的方案有Flannel VXLAN、Calico IPIP、Weave和Antrea等。物理机环境几乎支持任何类型的网络插件，此时一般应该选择性能较好的Calico BGP、Flannel host-gw或DAMM IP VLAN等。</li>
<li>容器网络功能需求：支持NetworkPolicy的解决方案以Calico、WeaveNet和Antrea为代表，而且后两个支持节点到节点间的通信加密。而大量Pod需要与集群外部资源互联互通时，应该选择Underlay网络模型一类的解决方案。</li>
<li>容器网络性能需求：Overlay网络中的协议报文有隧道开销，性能略差，而Underlay网络则几乎不存这方面的问题，但Overlay或Underlay路由模型的网络插件支持较快的Pod创建速度，而Underlay模型中的IP VLAN或MAC VLAN模式则较慢。</li>
</ul>
<h2 id="Flannel网络插件"><a href="#Flannel网络插件" class="headerlink" title="Flannel网络插件"></a>Flannel网络插件</h2><p>Flannel是用于解决容器跨节点通信问题的解决方案，兼容CNI插件API，支持Kubernetes、OpenShift、Cloud Foundry、Mesos、Amazon ECS、Singularity和OpenSVC等平台。它使用“虚拟网桥和veth设备”的方式为Pod创建虚拟网络接口，通过可配置的“后端”定义Pod间的通信网络，支持基于VXLAN和UDP的Overlay网络，以及基于三层路由的Underlay网络。在IP地址分配方面，它将预留的一个专用网络（默认为10.244.0.0/16）切分成多个子网后作为每个节点的Pod CIDR，而后由节点以IPAM插件的host-local形式进行地址分配，并将子网分配信息保存于etcd之中。</p>
<h3 id="Flannel配置基础"><a href="#Flannel配置基础" class="headerlink" title="Flannel配置基础"></a>Flannel配置基础</h3><p>Flannel在每个主机上运行一个名为flanneld的二进制代理程序，它负责从预留的网络中按照指定或默认的掩码长度为当前节点申请分配一个子网，并将网络配置、已分配的子网和辅助数据（例如主机的公网IP等）存储在Kubernetes API或etcd之中。Flannel使用称为后端的容器网络机制转发跨节点的Pod报文，它目前支持的主流后端如下。</p>
<ul>
<li>vxlan：使用Linux内核中的vxlan模块封装隧道报文，以Overlay网络模型支持跨节点的Pod间互联互通；同时，该后端类型支持直接路由模式，在该模式下，位于同一二层网络内节点之上的Pod间通信可通过路由模式直接发送，而跨网络的节点之上的Pod间通信仍要使用VXLAN隧道协议转发；因而，VXLAN隶属于Overlay网络模型，或混合网络模型；vxlan后端模式中，flanneld监听UDP的8472端口发送的封装数据包。</li>
<li>host-gw：即Host GateWay，它类似于VXLAN中的直接路由模式，但不支持跨网络的节点，因此这种方式强制要求各节点本身必须在同一个二层网络中，不太适用于较大的网络规模；host-gw有着较好的转发性能，且易于设定，推荐对报文转发性能要求较高的场景使用。</li>
<li>udp：使用常规UDP报文封装完成隧道转发，性能较前两种方式低很多，它仅在不支持前两种后端的环境中使用；UDP后端模式中，flanneld监听UDP的8285端口发送的封装报文。</li>
</ul>
<p>除了这3种后端之外，Flannel还实验性地支持IPIP、IPSec、AliVPC、AWS VPC、Alloc和GCE几种后端。<br>为了跟踪各子网分配信息等，Flannel使用etcd来存储虚拟IP和主机IP之间的映射，每个节点上运行的flanneld守护进程负责监视etcd中的信息并完成报文路由。默认情况下，Flannel的配置信息保存在etcd存储系统的键名/coreos.com/network/config之下，我们可以使用etcd服务的客户端工具来设定或修改其可用的相关配置。config的值是一个JSON格式的字典数据结构，它可以使用的键包含以下几个。</p>
<blockquote>
<p>1）Network：Flannel在全局使用CIDR格式的IPv4网络，字符串格式，此为必选键，余下的均为可选。<br>2）SubnetLen：为全局使用的IPv4网络基于多少位的掩码切割供各节点使用的子网，在全局网络的掩码小于24（例如16）时默认为24位。<br>3）SubnetMin：分配给节点使用的起始子网，默认为切分完成后的第一个子网；字符串格式。<br>4）SubnetMax：分配给节点使用的最大子网，默认为切分完成后的最大一个子网；字符串格式。<br>5）Backend：Flannel要使用的后端类型，以及后端相关的配置，字典格式；不同的后端通常会有专用的配置参数。</p>
</blockquote>
<p>Flannel项目官方给出的在线配置清单中默认使用的VXLAN后端，相关的配置定义在kube-system名称空间ConfigMap资源kube-flannel-cfg中，配置内容如下所示。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net-conf.json<span class="punctuation">:</span> |</span><br><span class="line">  <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Network&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.244.0.0/16&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Backend&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;VxLAN&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>上面的配置示例可以看出，Flannel预留使用的网络为默认的10.244.0.0/16，默认使用24位长度的子网掩码为各节点分配切分的子网，因而，它将有10.244.0.0/24～ 10.244.255.0/24范围内的256个子网可用，每个节点最多支持为254个Pod对象各分配一个IP地址。它使用的后端是VXLAN类型，flanneld将监听UDP的8472端口。</p>
<h3 id="VXLAN后端"><a href="#VXLAN后端" class="headerlink" title="VXLAN后端"></a>VXLAN后端</h3><p>Flannel会在集群中每个运行flanneld的节点之上创建一个名为flannel.1的虚拟网桥作为本节点隧道出入口的VTEP设备，其中的1表示VNI，因而所有节点上的VTEP均属于同一VXLAN，或者属于同一个大二层域（BD），它们依赖于二层网关进行通信。Flannel采用了分布式的网关模型，它把每个节点都视为到达该节点Pod子网的二层网关，相应的路由信息由flanneld自动生成。<br>Flannel需要在每个节点运行一个flanneld守护进程，启动时，该进程从etcd加载JSON格式的网络配置等信息，它会基于网络配置获取适用于当前节点的子网租约，还要根据其他节点的租约生成路由信息，以正确地路由数据报文等。与Kubernetes结合使用时，flanneld也可托管给集群之上的DeamonSet控制器。Flannel项目仓库中的在线配置清单通过名为kube-flannel-ds的DaemonSet控制器资源，在每个节点运行一个Flannel相关的Pod对象，Pod模板中使用hostNetwork: true进行网络配置，让每个节点上的Pod资源直接共享节点的网络名称空间，因而配置结果直接在节点的根网络名称空间生效。<br>在VXLAN模式下，flanneld从etcd获取子网并配置了后端之后会生成一个环境变量文件（默认为/run/flannel/subnet.env），其中包含本节点使用的子网，以及为了承载隧道报文而设置的MTU的定义等，如下面的配置示例所示。随后，flanneld还将持续监视etcd中相应配置租约信息的变动，并实时反映到本地路由信息之上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FLANNEL_NETWORK=10.244.0.0/16</span><br><span class="line">FLANNEL_SUBNET=10.244.1.1/24</span><br><span class="line">FLANNEL_MTU=1450</span><br><span class="line">FLANNEL_IPMASQ=true</span><br></pre></td></tr></table></figure>

<p>为了确保VXLAN机制通信过程的正确性，通常涉及VXLAN通信的IP报文一律不能分片，这就要求物理网络的链路层实现中必须提供足够大的MTU值，或修改各节点的MTU值以保证VXLAN报文的顺利传输，如上面配置示例中使用的1450字节。降低默认MTU值，以及额外的头部开销，必然会影响到报文传输过程中的数据交换效率。</p>
<h3 id="Flannel-VXLAN后端"><a href="#Flannel-VXLAN后端" class="headerlink" title="Flannel VXLAN后端"></a>Flannel VXLAN后端</h3><p>下面的路由信息取自k8s-node01.ilinux.io节点，它由该节点上的flanneld根据集群中各节点获得的子网信息生成。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink </span><br><span class="line">10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1 </span><br><span class="line">10.244.2.0/24 via 10.244.2.0 dev flannel.1 onlink </span><br><span class="line">10.244.3.0/24 via 10.244.3.0 dev flannel.1 onlink</span><br></pre></td></tr></table></figure>

<p>其中，10.244.0.0/24由k8s-master01使用，10.244.1.0/24由k8s-node01节点使用，10.244.2.0/ 24由k8s-node02节点使用，10.244.3.0/24由k8s-node03节点使用。这些路由条目恰恰反映了同节点Pod间通信时经由cni0虚拟网桥转发，而跨节点Pod间通信时，报文将经由当前节点（k8s-node01）的flannel.1隧道入口（VTEP设备）外发，隧道出口由“下一跳”信息指定，例如到达10.244.2.0/24网络的报文隧道出口是10.244.2.0指向的接口，它配置在k8s-node02的flannel.1接口之上，该接口正是k8s-node02上的隧道出入口（VTEP设备）。<br>VXLAN网络将各VTEP设备作为同一个二层网络上的接口，这些接口设备组成一个虚拟的二层网络。因而，图10-11中的Pod-1发往Pod-4的IP报文将在流经其所在节点的flannel.1接口时封装成数据帧，源MAC是k8s-node01节点上的flannel.1接口的MAC地址，而目标MAC则是k8s-node02节点上flannel.1接口的MAC地址。但Flannel并非依赖ARP进行MAC地址学习，而是由节点上的flanneld进程启动时将本地flannel.1接口IP与MAC地址的映射信息上报到etcd中，并由其他各节点上的flanneld来动态生成相应的解析记录。下面的解析记录取自k8s-node01节点，它们分别指明了集群中的其他节点上的flannel.1接口各自对应的MAC地址，PERMANENT属性表明这些记录均永久有效。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ip neighbour show | awk &#x27;$3==&quot;flannel.1&quot;&#123;print $0&#125;&#x27;</span><br><span class="line">10.244.2.0 dev flannel.1 lladdr be:f8:5a:a5:6e:d3 PERMANENT</span><br><span class="line">10.244.0.0 dev flannel.1 lladdr 52:2b:52:42:dc:ed PERMANENT</span><br><span class="line">10.244.3.0 dev flannel.1 lladdr 32:d3:60:46:93:47 PERMANENT</span><br></pre></td></tr></table></figure>

<p>VXLAN协议使用UDP报文封装隧道内层数据帧，Pod发出的报文经隧道入口flannel.1封装成数据帧，再由flanneld进程（客户端）封装成UDP报文，之后发往目标Pod对象所在节点的flanneld进程（服务端）。该UDP报文就是所谓的VXLAN隧道，它会在已经生成的帧报文之外再封装一组协议头部，如图10-15所示为VXLAN头部、外层UDP头部、外层IP头部和外层帧头部。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218142922007.png" alt="image-20220218142922007"></p>
<p>该UDP报文的IP头部中，源地址为当前节点某接口的IP地址，目标地址应该为目标Pod所在节点的某接口的IP地址。但本地节点之上并没有任何路由信息帮助指向目标节点，由flanneld生成的路由中仅指明了到达目标Pod时的隧道出口的flannel.1接口的IP地址。事实上，Flannel把flannel.1接口也作为网桥设备使用，该设备上附加了一个同样由flanneld维护的、称为FDB（Forwarding Database）的转发数据库。该数据库指明了到达目标节点flannel.1接口需要经由的下一跳IP，该IP是目标Pod所在节点的IP地址，即外部IP头部中的目标IP。下面的转发条目取自k8s-node01节点，各条目的功能做了简单注释，这些条目分别指明了到达集群中不同的节点的flannel.1接口时需要经过的下一跳IP地址。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# bridge fdb show flannel.1 | awk &#x27;$3==&quot;flannel.1&quot;&#123;print $0&#125;&#x27;</span><br><span class="line">32:d3:60:46:93:47 dev flannel.1 dst 172.29.9.13 self permanent   # 转发至k8s-node03节点</span><br><span class="line">be:f8:5a:a5:6e:d3 dev flannel.1 dst 172.29.9.12 self permanent   # 转发至k8s-node02节点</span><br><span class="line">52:2b:52:42:dc:ed dev flannel.1 dst 172.29.9.1 self permanent    # 转发至k8s-master01节点</span><br></pre></td></tr></table></figure>

<p>假设图10-14中的Pod-4运行在demoapp应用，下面的命令运行在k8s-node01之上，它抓取了Pod-1通过HTTP协议访问Pod-4中由demoapp运行的Web服务的一次请求/响应事务。其中的MAC地址52:54:00:66:b9:c1与1a:bd:6f:c5:1e:42分别是k8s-node01的ens3和flannel.1接口的地址，而52:54:00:08:99:ed与be:f8:5a:a5:6e:d3分别是k8s-node02的ens3和flannel.1接口的地址。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# tcpdump -i ens3 -en udp port 8472</span><br><span class="line">14:21:47.194643 52:54:00:66:b9:c1 &gt; 52:54:00:08:99:ed, ethertype IPv4 (0x0800), length 190: 172.29.9.11.36529 &gt; 172.29.9.12.8472: OTV, flags [I] (0x08), overlay 0, instance 1  #请求报文隧道头部</span><br><span class="line">1a:bd:6f:c5:1e:42 &gt; be:f8:5a:a5:6e:d3, ethertype IPv4 (0x0800), length 140: 10.244.1.20.40854 &gt; 10.244.2.16.80: Flags [P.], seq 1:75, ack 1, win 507, options [nop,nop,TS val 3194580517 ecr 2849996851], length 74: HTTP: GET / HTTP/1.1  # 请求报文内层头部</span><br><span class="line">……</span><br><span class="line">14:21:47.198184 52:54:00:08:99:ed &gt; 52:54:00:66:b9:c1, ethertype IPv4 (0x0800), length 133: 172.29.9.12.52397 &gt; 172.29.9.11.8472: OTV, flags [I] (0x08), overlay 0, instance 1 # 响应报文隧道头部</span><br><span class="line">be:f8:5a:a5:6e:d3 &gt; 1a:bd:6f:c5:1e:42, ethertype IPv4 (0x0800), length 83: 10.244.2.16.80 &gt; 10.244.1.20.40854: Flags [P.], seq 1:18, ack 75, win 502, options [nop,nop,TS val 2849996855 ecr 3194580517], length 17: HTTP: HTTP/1.0 200 OK  # 响应报文内层头部</span><br></pre></td></tr></table></figure>

<p>这种外层封装后的报文就是常规的UDP报文，只是为了避免数据帧超过标准的MTU大小，内层数据帧不得不减小至1450字节。因此，VXLAN Overlay网络可正常运行在任何能够传输常规UDP报文的环境中，包括存在很多底层限制的公有云环境。代价是，牺牲了网络报文的一小部分载荷能力，降低了性能。<br>我们也不难想到，依赖于flanneld维护的、由各VTEP设备flannel.1接口组成的二层网络中的各设备的ARP解析记录，flannel.1虚拟网桥上的FDB转发数据库，甚至不在同一IP网络中的集群各节点，只要它们彼此间经由路由互相可达，这种外层转发依然能够成功达成。于是，VXLAN Overlay网络并不要求所有节点都处于同一个二层网络，这有利于在更复杂的网络环境下组建Kubernetes集群。<br>另外，VXLAN后端的可用配置参数除了Type之外还有如下几个，它们都有默认值，用户可以按需进行自定义配置。</p>
<ul>
<li>VNI：VXLAN的标识符，默认为1；数值型数据。</li>
<li>Port：用于发送封装的报文的UDP端口，默认为8472；数值型数据。</li>
<li>GBP：全称为Group Based Policy，配置是否启用VXLAN的基于组的策略机制，默认为否；布尔型数据。</li>
<li>DirectRouting：是否为同一个二层网络中的节点启用直接路由机制，类似于host-gw后端的功能；此种场景下，VXLAN仅为不在同一个二层网络中的节点封装并转发VXLAN隧道报文；布尔型数据。</li>
</ul>
<p>其中，直接路由参数能够配置Flannel实现三层转发式的容器网关，该网关能够以直接路由方式在Pod间转发通信报文。</p>
<h3 id="直接路由-1"><a href="#直接路由-1" class="headerlink" title="直接路由"></a>直接路由</h3><p>为了提升性能，Flannel的VXLAN后端还支持DirectRouting模式，即在集群中的各节点上添加必要的路由信息，让Pod间的IP报文通过节点的二层网络直接传送，如图10-16所示。仅在通信双方的Pod对象所在的节点跨IP网络时，才启用传统的VXLAN隧道方式转发通信流量。若Kubernetes集群节点全部位于单个二层网络中，则DirectRouting模式下的Pod间通信流量基本接近于直接使用二层网络。即便节点分布在有限的几个可互相通信的网络中的Kubernetes集群来说，合理的应用部署拓扑也能省去相当一部分的隧道开销。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218143359115.png" alt="image-20220218143359115"></p>
<p>对于托管部署在Kubernetes上的Flannel来说，修改kube-system名称空间下的configmaps/kube-flannel-cfg资源，为VXLAN后端添加DirectRouting子键，并设置其值为true即可，如下面的配置示例。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">net-conf.json<span class="punctuation">:</span> |</span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Network&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.244.0.0/16&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Backend&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;VxLAN&quot;</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">&quot;Directrouting&quot;</span><span class="punctuation">:</span> <span class="keyword">true</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>我们可直接编辑活动状态的configmaps/kube-flannel-cfg资源，也可基于配置清单修改后再次应用到集群上。修改完成后，还需要以某种策略让各节点上的Flannel Pod重载生效新配置，比如手动删除以触发Pod重建的方式进行滚动更新等。更新完成后，节点上的路由规则也会相应发生变动，到达与本地节点位于同一二层网络中的其他节点，Pod子网的下一跳地址由对端flannel.1接口地址变为了宿主机物理接口的地址（如图10-13中的ens3接口），本地用于发出报文的接口从flannel.1变成了本地的物理接口。仍然以k8s-node01节点为例，修改VXLAN后端支持DirectRouting模式，则该节点上的路由信息变动为如下结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ip route show</span><br><span class="line">10.244.0.0/24 via 172.29.9.1 dev ens3 </span><br><span class="line">10.244.1.0/24 dev cni0 proto kernel scope link src 10.244.1.1 </span><br><span class="line">10.244.2.0/24 via 172.29.9.12 dev ens3 </span><br><span class="line">10.244.3.0/24 via 172.29.9.13 dev ens3</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>Pod与节点通常不在同一网络。Pod间的通信报文需要经由宿主机的物理接口发出，必然会经过iptables/netfilter的forward钩子，为了避免该类报文被防火墙拦截，Flannel必须为其设定必要的放行规则。本书示例集群中的每个节点上iptables filter表的FORWARD链上都会生成如下两条转发规则，以确保由物理接口接收或发送的目标地址或源地址为10.244.0.0/16网络的所有报文能够正常通过。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">target     prot opt source               destination</span><br><span class="line">ACCEPT     all  --  10.244.0.0/16        0.0.0.0/0</span><br><span class="line">ACCEPT     all  --  0.0.0.0/0            10.244.0.0/16</span><br></pre></td></tr></table></figure>

<p>假设图10-13中的Pod-4运行在demoapp应用，下面的命令运行在k8s-node01之上，它抓取了Pod-1通过HTTP协议访问Pod-4中由demoapp运行的Web服务的一次请求/响应事务。命令结果显示：跨节点的Pod-1和Pod-4借助内核中的路由规则正常完成了通信过程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">tcpdump -i ens3 -en tcp port 80</span></span><br><span class="line">17:16:44.883691 52:54:00:66:b9:c1 &gt; 52:54:00:08:99:ed, ethertype IPv4 (0x0800), length 140: 10.244.1.20.53456 &gt; 10.244.2.16.80: Flags [P.], seq 1:75, ack 1, win 507, options [nop,nop,TS val 3205078281 ecr 2860494615], length 74: HTTP: GET / HTTP/1.1</span><br><span class="line">……</span><br><span class="line">17:16:44.884831 52:54:00:08:99:ed &gt; 52:54:00:66:b9:c1, ethertype IPv4 (0x0800), length 83: 10.244.2.16.80 &gt; 10.244.1.20.53456: Flags [P.], seq 1:18, ack 75, win 502, options [nop,nop,TS val 2860494616 ecr 3205078281], length 17: HTTP: HTTP/1.0 200 OK</span><br></pre></td></tr></table></figure>

<p>显然，这种路由规则无法表达跨二层网络的节点上Pod间通信的诉求，因为到达目标网络（某Pod子网）的下一跳地址无法指向另一个网络中的节点地址。因而，集群中的每个节点上依然保留有VXLAN隧道相关的flannel.1设备，以支持那些跨IP网络的节点上的Pod间通信。</p>
<h3 id="host-gw后端"><a href="#host-gw后端" class="headerlink" title="host-gw后端"></a>host-gw后端</h3><p>Flannel的host-gw后端通过添加必要的路由信息，并使用节点的二层网络直接发送Pod间的通信报文，其工作方式类似于VXLAN后端中的直接路由功能，但不包括该后端支持的隧道转发能力，这意味着host-gw后端要求各节点必须位于同一个二层网络中。其工作模型示意图如图10-17所示。因完全不会用到VXLAN隧道，所以使用了host-gw后端的Flannel网络也就无须用到VTEP设备flannel.1。<br><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220222210712817.png" alt="image-20220222210712817">host-host-gw后端没有多余的配置参数，直接设定配置文件中的Backend.Type键的值为host-gw关键字即可。同样，直接修改kube-system名称空间中的configmaps/kube-flannel.cfg配置文件，类似下面配置示例中的内容即可。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net-conf.json<span class="punctuation">:</span> |</span><br><span class="line">    <span class="punctuation">&#123;</span></span><br><span class="line">      <span class="attr">&quot;Network&quot;</span><span class="punctuation">:</span> <span class="string">&quot;10.244.0.0/16&quot;</span><span class="punctuation">,</span></span><br><span class="line">      <span class="attr">&quot;Backend&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;Type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;host-gw&quot;</span></span><br><span class="line">      <span class="punctuation">&#125;</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>

<p>配置完成后，集群中的各节点会生成类似VXLAN后端的DirectRouting路由及iptables规则，以转发Pod网络的通信报文，它完全省去了隧道转发模式的额外开销。代价是，对于非同一个二层网络的报文转发，host-gw完全无能为力。相对而言，VXLAN的DirectRouting后端转发模式兼具VXLAN后端和host-gw后端的优势，既保证了传输性能，又具备跨二层网络转发报文的能力。<br>当Kubernetes集群规模较大时，其路由信息的规模也将变得庞大且不易维护。相比较来说，Calico通过BGP协议自动维护路由条目，较之Flannel以etcd为总线以上报、查询和更新配置的工作逻辑更加高效和易于维护，因而更适用于大型网络。</p>
<h2 id="Calico网络插件"><a href="#Calico网络插件" class="headerlink" title="Calico网络插件"></a>Calico网络插件</h2><p>与Flannel相比，Calico的一个显著优势是对网络策略的支持，它允许用户动态定义访问控制规则以管控进出容器的数据报文，从而为Pod间通信按需施加安全策略。<br>Calico是一个三层的虚拟网络解决方案，它把每个节点都当作虚拟路由器（vRouter），并把每个节点上的Pod都当作是“节点路由器”后的一个终端设备并为其分配一个IP地址。各节点路由器通过BGP协议学习生成路由规则，从而实现不同节点上Pod间的互联互通，如图10-18所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218164035327.png" alt="image-20220218164035327"></p>
<p>BGP是互联网上一个核心的去中心化自治路由协议，它通过维护IP路由表或“前缀”表来实现自治系统（AS）之间的可达性，通常作为大规模数据中心维护不同的自治系统之间路由信息的矢量路由协议。Linux内核原生支持BGP，因而我们可轻易把一台Linux主机配置成为边界网关。<br>Calico把Kubernetes集群环境中的每个节点上的Pod所组成的网络视为一个自治系统，而每个节点也就自然由各自的Pod对象组成虚拟网络，进而形成自治系统的边界网关。各节点间通过BGP协议交换路由信息并生成路由规则。但考虑到并非所有网络都能支持BGP，而且BGP路由模型要求所有节点必须要位于同一个二层网络，所以Calico还支持基于IPIP和VXLAN的Overlay网络模型，它们的工作模式与Flannel的VXLAN和IPIP模型并无显著不同。<br>类似Flannel在VXLAN后端启用DirectRouting时的网络模型，Calico也支持混合使用路由和Overlay网络模型，BGP路由模型用于二层网络的高性能通信，IP-IP或VXLAN用于跨子网的节点间报文转发，如图10-19所示。<font color="red">IP-IP协议包头非常小，理论上它的速度要比VXLAN稍快一点，但安全性更差。</font></p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218164057815.png" alt="image-20220218164057815"></p>
<p>需要注意的是，Calico网络提供的在线部署清单中默认使用的是IPIP隧道网络，而非BGP或者混合模型，因为它假设节点的底层网络不支持BGP协议。明确需要使用BGP或混合模型时，需要事先将清单下载至本地，按需修改后方可部署在Kubernetes集群之上。</p>
<h3 id="Calico架构"><a href="#Calico架构" class="headerlink" title="Calico架构"></a>Calico架构</h3><p>Calico的系统组件主要有Felix、BGP路由反射器、编排系统插件、BIRD和etcd存储系统等，各组件间的关系如图10-20所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218164115486.png" alt="image-20220218164115486"></p>
<p>BGP模式下的Calico所承载的各Pod资源直接基于vRouter经由基础网络进行互联，它非叠加、无隧道、不使用VRF表，也不依赖于NAT，因此每个工作负载都可以直接配置使用公网IP接入互联网，当然，也可以按需使用网络策略控制它的网络连通性。<br>（1）Felix<br>Felix是运行于各节点上守护进程，它主要负责完成接口管理、路由规划、ACL规划和状态报告几个核心任务，从而为各端点（VM或Container）生成连接机制。</p>
<blockquote>
<p>1）接口管理，负责创建网络接口、生成必要信息并送往内核，以确保内核能正确处理各端点的流量，尤其是要确保目标节点MAC能响应当前节点上各工作负载的MAC地址的ARP请求，以及为Felix管理的接口打开转发功能。另外，接口管理还要监控各接口的变动以确保规则能得到正确应用。<br>2）路由规划，负责为当前节点上运行的各端点在内核FIB（Forwarding Information Base）中生成路由信息，以保证到达当前节点的报文可正确转发给端点。<br>3）ACL规划，负责在Linux内核中生成ACL，实现仅放行端点间的合规流量，并确保流量不能绕过Calico等安全措施。<br>4）状态报告，负责提供网络健康状态的相关数据，尤其是报告由Felix管理的节点上的错误和问题。这些报告数据会存储在etcd，供其他组件或网络管理员使用。</p>
</blockquote>
<p>（2）编排系统插件<br>编排系统插件的主要功能是将Calico整合进所在的编排系统中，例如Kubernetes或OpenStack等。它主要负责完成API转换，从而让管理员和用户能够无差别地使用Calico的网络功能。换句话说，编排系统通常有自己的网络管理API，相应的插件要负责将对这些API的调用转换为Calico的数据模型，并存储到Calico的存储系统中。因而，编排插件的具体实现依赖于底层编排系统，不同的编排系统有各自专用的插件。<br>（3）etcd存储系统<br>利用etcd，Calico网络可实现为有明确状态（正常或故障）的系统，且易于通过扩展应对访问压力的提升，避免自身成为系统瓶颈。另外，etcd也是Calico各组件的通信总线。<br>（4）BGP客户端<br>Calico要求在每个运行着Felix的节点上同时运行一个称为BIRD的守护进程，它是BGP协议的客户端，负责将Felix生成的路由信息载入内核并通告给整个网络中。<br>（5）BGP路由反射器<br>Calico在每一个计算节点利用Linux内核实现了一个高效的vRouter（虚拟路由器）进行报文转发。每个vRouter通过BGP协议将自身所属节点运行的Pod资源的IP地址信息，基于节点上的专用代理程序（Felix）生成路由规则向整个Calico网络内传播。尽管小规模部署能够直接使用BGP网格模型，但随着节点数量（假设为N）的增加，这些连接的数量就会以N2的规模快速增长，从而给集群网络带来巨大的压力。因此，一般建议大规模的节点网络使用BGP路由反射器进行路由学习，BGP的点到点通信也就转为与中心点的单路通信模型。另外，出于冗余考虑，生产实践中应该部署多个BGP路由反射器。而对于Calico来说，BGP客户端程序除了作为客户端使用外，也可以配置为路由反射器。<br>另外，Calico可将关键配置抽象成资源类型，并允许用户按需定义资源对象以完成系统配置，这些资源对象保存在Datastore中，Datastore可以是独立管理的etcd存储系统，也可以是Kubernetes API封装的集群状态存储系统（即Kubernetes使用的etcd存储系统）。Calico专有的资源类型有十几种，包括IPPool（IP地址池）、NetworkPolicy（网络策略）、BGPConfiguration（BGP配置参数）和FelixConfiguration（Felix配置参数）等。类似于Kubernetes API资源的定义，这些资源的配置格式同样以JSON使用apiVersion、kind、metadata和spec等一级字段进行定义，并能够使用calicoctl客户端工具进行管理，也支持由kublet借助CRD进行这类资源的管理。<br>以Kubernetes API为Datastore的部署场景中，Calico还需将这些资源类型相应定义为Kubernetes上的CRD。</p>
<h3 id="Calico配置基础"><a href="#Calico配置基础" class="headerlink" title="Calico配置基础"></a>Calico配置基础</h3><p>与Kubernetes集群整合时，Calico需要配置calico-node和calico-kube-controllers两个重要组件，如图10-21所示，各组件通过Datastore读取与自身相关的资源定义完成配置。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218164145074.png" alt="image-20220218164145074"></p>
<ul>
<li>calico/node：Calico在Kubernetes集群每个节点运行的节点代理，负责提供felix、bird4、bird6和confd等守护进程。</li>
<li>calico/kube-controllers：Calico运行在Kubernetes之上的自定义控制器，也是Calico协同Kubernetes的插件。</li>
</ul>
<p>Calico有两种部署方式：一种是让calico/node独立运行在Kubernetes集群之外，但calico/kube-controllers依然需要以Pod资源形式运行在集群之上；另一种是以CNI插件方式配置Calico，使Calico完全托管运行在Kubernetes集群之上。对于后一种方式，Calico提供了在线的部署清单，它分别为50节点及以下规模和50节点以上规模的Kubernetes集群使用Kubernetes API作为Dabastore提供了不同的配置清单，也为使用独立的etcd集群提供了专用配置清单。但这3种类型的配置清单中，Calico默认启用的是基于IPIP隧道的Overlay网络，因而它会在所有流量上使用IPIP隧道而不是BGP路由。以下配置定义在部署清单中DaemonSet/calico-node资源的Pod模板中的calico-node容器之上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置在IPv4类型的地址池上启用的IP-IP及其类型，支持3种可用值</span></span><br><span class="line"><span class="comment"># Always（全局流量）、Cross-SubNet（跨子网流量）和Never</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CALICO_IPV4POOL_IPIP</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;Always&quot;</span></span><br><span class="line"><span class="comment"># 是否在IPV4地址池上启用VXLAN隧道协议，取值及意义与Flannel的VXLAN后端相同，</span></span><br><span class="line"><span class="comment"># 但在全局流量启用VXLAN时将完全不再需要BGP网络，建议将相关的组件禁用</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CALICO_IPV4POOL_VXLAN</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;Never&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们可以将环境变量CALICO_IPV4POOL_IPIP的值设置为Cross-SubNet（不区分大小写）来启用混合网络模型，它将启用BGP路由网络，且仅会在跨节点子网的流量间启用隧道封装。想要启用VXLAN隧道，只需要把环境变量CALICO_IPV4POOL_VXLAN的值设置为Always或Cross-SubNet即可，但在全局流量上使用VXLAN隧道时建议将ConfigMap/calico-node中calico-backend键的值设置为vxlan以禁用BIRD，并在DaemonSet/calico-node资源的Pod模型中禁用calico-node容器的存活探针和就绪探针对bird的检测，相关的配置要点如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">livenessProbe:</span></span><br><span class="line">  <span class="attr">exec:</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/calico-node</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-felix-live</span></span><br><span class="line">    <span class="comment"># - -bird-live</span></span><br><span class="line">  <span class="attr">readinessProbe:</span></span><br><span class="line">    <span class="attr">exec:</span></span><br><span class="line">    <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/calico-node</span></span><br><span class="line">    <span class="comment"># - -bird-ready</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-felix-ready</span></span><br></pre></td></tr></table></figure>

<p>需要注意的是，Calico分配的地址池需要与Kubernetes集群的Pod网络的定义保持一致。Pod网络通常由kubeadm init初始化集群时使用–pod-network-cidr选项指定的，而Calico在其默认的配置清单中默认使用192.168.0.0/16作为Pod网络，因而部署Kubernetes集群时应该规划好要使用的网络地址，并设定此二者相匹配。对使用了Flannel的10.244.0.0/16网络环境而言，可以修改资源清单中的定义，从而将其修改为其他网络地址。以下配置片段取自Calico的部署清单，它定义在DaemonSet/calico-node资源的Pod模板中的calico-node容器之上。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># IPV4地址池的定义，value值需要与kube-controller-manager的--cluster-network</span></span><br><span class="line"><span class="comment"># 选项的值保持一致，以下环境变量默认处于注释状态</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CALICO_IPV4POOL_CIDR</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;192.168.0.0/16&quot;</span></span><br><span class="line"><span class="comment"># Calico默认以26位子网掩码切分地址池并将各子网配置给集群中的节点，若需要使用其他</span></span><br><span class="line"><span class="comment"># 的掩码长度，则需要定义如下环境变量</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">CALICO_IPV4POOL_BLOCK_SIZE</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;24&quot;</span></span><br><span class="line"><span class="comment"># Calico默认并不会从Node.Spec.PodCIDR中分配地址，但可通过将如下变量</span></span><br><span class="line"><span class="comment"># 设置为true并结合host-local这一IPAM插件来强制从PodCIDR中分配地址</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">USE_POD_CIDR</span></span><br><span class="line">  <span class="attr">value:</span> <span class="string">&quot;false&quot;</span></span><br></pre></td></tr></table></figure>

<p>不过，目前版本的Calico已经能够自动检测由kubeadm部署的Kubernetes集群中的Pod网络，并自动将类似上面配置清单中CALICO_IPV4POOL_CIDR和CALICO_IPV4POOL_BLOCK_SIZE环境变量的值适配到该Pod网络，但其他方式部署的Kubernetes集群仍需管理员自行核验这种适配机制是否能得以满足。<br>在地址分配方面，Calico在JSON格式的CNI插件配置文件中使用专有的calico-ipam插件，该插件并不会使用Node.Spec.PodCIDR中定义的子网作为节点本地为Pod分配地址的地址池，而是根据Calico插件为各节点配置的地址池进行地址分配。若期望为节点真正使用地址池，吻合PodCIDR的定义，则需要将部署清单中DaemonSet/calico-node资源的Pod模板的calico-node容器的USE_POD_CIDR环境变量值设置为true，并修改ConfigMap/calico-config资源中cni_network_config键的plugins.ipam.type值为host-local，且使用podCIDR为子网，具体配置如下所示。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;ipam&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;type&quot;</span><span class="punctuation">:</span> <span class="string">&quot;host-local&quot;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;subnet&quot;</span><span class="punctuation">:</span> <span class="string">&quot;usePodCidr&quot;</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br></pre></td></tr></table></figure>

<p>下面以自我管理的Kubernetes集群为例来说明Calico IPIP和BGP网络的基本应用。</p>
<h3 id="IPIP隧道网络"><a href="#IPIP隧道网络" class="headerlink" title="IPIP隧道网络"></a>IPIP隧道网络</h3><p>kubenet通过/etc/cni/net.d/目录下的CNI配置文件加载要使用的网络插件完成Pod网络配置，为了避免冲突，通常不应该也没必要同时提供多个CNI解决方案。因此，在部署Calico之前，需要先移除此前使用的Flannel插件，最便捷的方式是基于部署清单完成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</span></span><br></pre></td></tr></table></figure>

<p>Calico 3目前仅支持Kubernetes 1.8及其以上版本，并且它要求使用一个能够被各组件访问的键值存储系统，在Kubernetes环境中，可用的选择有etcd v3或Kubernetes API数据存储。本部署示例会把Kubernetes API作为Calico的数据存储取代etcd，这也是截至本书写作时最新稳定版本Calico 3中推荐的配置。若无须改动默认配置，则直接基于在线资源清单创建相关资源即可。但我们这里为了吻合此Flannel的使用习惯，需要自定义设置Pod网络为10.244.0.0/16，切分子网时的掩码长度为24，并设置在PodCIDR中为工作负载分配IP地址，但在全局流量上默认使用的网络模型是IPIP隧道。下面首先将在线资源清单下载至本地。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl https://docs.projectcalico.org/manifests/calico.yaml -O</span></span><br></pre></td></tr></table></figure>

<p><font color="red">使用host-local IPAM插件时，Calico的部分功能将变得不可用。例如，以节点或名称空间为组，分别从不同地址池分配IP地址等。</font><br>而后修改calico.yaml文件，修改资源定义使得其符合Flannel的使用习惯，具体设置方式请参考3.2节的说明。配置完成后，使用如下命令将资源部署到集群之上即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f calico.yaml</span></span><br></pre></td></tr></table></figure>

<p>该资源清单将Calico的所有资源部署在kube-system名称空间之中，待calico-node与kube-controllers相关的Pod进入就绪状态之后即可验证和使用相应的网络功能，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -n kube-system -o wide | awk <span class="string">&#x27;/^calico-(node|kube-controller)/&#123;print&#125;&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>工作在IPIP模式的Calico会在每个节点上创建一个tunl0接口作为隧道出入口来封装IPIP隧道报文。Calico会为每一个Pod资源创建一对veth设备，其中一端作为Pod的网络接口，另一端（名称以cali为前缀，后跟随机字串）留置在节点的根网络名称空间，它未使用风格模式，因而并未关联成为任何虚拟网桥设备的从接口，如图10-22所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218164726217.png" alt="image-20220218164726217"></p>
<p>IPIP隧道网络仍需依赖于BGP维护节点间的可达性。部署完成后，Calico会通过BGP协议在每个节点上生成到达Kubernetes集群中其他各节点的Pod子网路由信息。下面的路由条件截取自k8s-node01主机，它们是由各节点上的BIRD以点对点的方式（node-to-node mesh）向网络中的其他节点进行通告并学习其他节点的通告而得。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.244.0.0/24 via 172.29.9.1 dev tunl0 proto bird onlink </span><br><span class="line">blackhole 10.244.1.0/24 proto bird </span><br><span class="line">10.244.2.0/24 via 172.29.9.12 dev tunl0 proto bird onlink </span><br><span class="line">10.244.3.0/24 via 172.29.9.13 dev tunl0 proto bird onlink</span><br></pre></td></tr></table></figure>

<p>对于创建的每个常规Pod资源，Calico CNI插件需要在节点的根网络名称空间中生成一个专用路由条目，用于确保以Pod IP为目标地址的报文能够经由相应的留置在根网络名称空间中的一端设备送达，相关的路由条目格式类似如下所示。这是因为Calico没有在节点上为本地的所有Pod资源使用一个虚拟网桥进行报文转发所致。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">10.244.1.2 dev cali584bb1c9fa8 scope link  # 到达10.244.1.2的报文经cali584bb1c9fa8接口送达；</span><br><span class="line">10.244.1.3 dev cali8747614b74b scope link  # </span><br><span class="line">10.244.1.4 dev cali8a15ed22215 scope link  #</span><br></pre></td></tr></table></figure>

<p>在集群中部署一些Pod资源即可完成集群网络连接测试。假设在k8s-node01上存在一个IP地址为10.244.1.3的Pod A，以及在k8s-node02上存在一个IP地址为10.244.2.2且运行有demoapp应用的Pod B。通过curl命令在Pod A的交互式接口对Pod B发起HTTP请求，而后在k8s-node01的物理接口上抓取通信报文即可分析IPIP隧道报文通信格式，相关命令及截取的一次通信的往返结果示例如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">tcpdump -i ens3 -nn ip host 172.29.9.11 and host 172.29.9.12</span></span><br><span class="line">14:28:30.881146 IP 172.29.9.11 &gt; 172.29.9.12: IP 10.244.1.3.37576 &gt; 10.244.2.2.80: Flags [P.], seq 1:75, ack 1, win 504, options [nop,nop,TS val 3996121466 ecr 257294088], length 74: HTTP: GET / HTTP/1.1 (ipip-proto-4)</span><br><span class="line">14:28:30.882290 IP 172.29.9.12 &gt; 172.29.9.11: IP 10.244.2.2.80 &gt; 10.244.1.3.37576: Flags [P.], seq 1:18, ack 75, win 510, options [nop,nop,TS val 257294089 ecr 3996121466], length 17: HTTP: HTTP/1.0 200 OK (ipip-proto-4)</span><br></pre></td></tr></table></figure>

<p>命令结果显示出，跨节点Pod间通信经由IPIP协议的三层隧道转发，外层IP首部中的IP地址为通信双方的节点IP（172.29.9.11和172.29.9.12），内层IP头部为通信双方的Pod IP（10.244.1.3和10.244.2.2）。需要注意的是，Calico CNI设置的tunl0接口的MTU默认为1440，这种设置主要是为适配Google的GCE环境，非GCE的物理环境中，其最佳值为1480。部署前，修改配置清单中ConfigMap/calico-config资源的veith_mtu键的值为1480即可。<br>另外，对50个节点以上规模的集群来说，所有Calico节点基于Kubernetes API存取数据会给API Server带去不小的通信压力，解决办法是使用calico-typha进程将所有Calico的通信集中起来，统一与API Server进行交互。Calico为该应用场景提供了专用的在线配置清单<a target="_blank" rel="noopener" href="https://docs.projectcalico.org/manifests/calico-typha.yaml%EF%BC%8C%E5%AE%83%E4%B8%BB%E8%A6%81%E6%B7%BB%E5%8A%A0%E4%BA%86Deployment/calico-typha%E5%92%8CService/calico-typha%E4%B8%A4%E4%B8%AA%E8%B5%84%E6%BA%90%E3%80%82%E9%9C%80%E8%A6%81%E8%87%AA%E5%AE%9A%E4%B9%89%E7%9A%84%E8%AF%9D%EF%BC%8C%E5%9F%BA%E6%9C%AC%E8%AF%84%E4%BC%B0%E6%A0%87%E5%87%86%E6%98%AF%E6%AF%8F%E4%B8%AAcalico-typha">https://docs.projectcalico.org/manifests/calico-typha.yaml，它主要添加了Deployment/calico-typha和Service/calico-typha两个资源。需要自定义的话，基本评估标准是每个calico-typha</a> Pod资源可承载100～200个（上限）Calico Node的连接请求，而整个集群中的calico-typha Pod资源总数尽量不要超过20个。</p>
<h3 id="客户端工具calicoctl"><a href="#客户端工具calicoctl" class="headerlink" title="客户端工具calicoctl"></a>客户端工具calicoctl</h3><p>calicoctl能够直接与Calico Datastore进行交互，用于管理Calico系统抽象出的各种资源，通过资源管理实现查看、修改或配置Calico系统特性。我们可以基于特定的Pod来提供calicoctl工具程序，也可直接将相关的二进制程序部署在管理节点之上，例如管理员运行kubectl工具的主机等。下面在k8s-master01上直接下载编译后的calicoctl文件，并将其保存在/usr/bin/目录中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">wget https://github.com/projectcalico/calicoctl/releases/download/v3.14.1/calicoctl</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo <span class="built_in">mv</span> calicoctl /usr/bin/</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo <span class="built_in">chmod</span> +x /usr/bin/calicoctl</span></span><br></pre></td></tr></table></figure>

<p>calicoctl成功认证到Calico的数据存储系统（Datastore）上之后才能查看或进行各类管理操作，所需要的认证方式也就取决于Datastore的类型。以Kubernetes API为数据存储时，calicoctl需要使用类似kubectl的认证信息完成认证，常用的实现方式有环境变量和配置文件两种。环境变量DATASTORE_TYPE用于指定存储类型，而KUBECONFIG则用于指定配置文件kubeconfig的认证文件路径，例如以如下命令格式运行calicoctl命令，测试读取Calico系统的节点信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes -o wide</span></span><br><span class="line">NAME                ASN       IPV4          PV6   </span><br><span class="line">k8s-master01.ilinux.io    (64512)   172.29.9.1/16 </span><br><span class="line">k8s-node01.ilinux.io     (64512)   172.29.9.11/16 </span><br><span class="line">k8s-node02.ilinux.io     (64512)   172.29.9.12/16</span><br><span class="line">k8s-node03.ilinux.io     (64512)   172.29.9.13/16</span><br></pre></td></tr></table></figure>

<p>为了更方便使用，我们也可以直接将认证信息等保存在配置文件中，calicoctl默认加载的配置文件是/etc/calico/calicoctl.cfg，配置信息以YAML格式进行组织，语法格式类似于Kubernetes的资源配置清单。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CalicoAPIConfig</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">datastoreType:</span> <span class="string">&quot;kubernetes&quot;</span></span><br><span class="line">  <span class="attr">kubeconfig:</span> <span class="string">&quot;/path/to/.kube/config&quot;</span></span><br></pre></td></tr></table></figure>

<p>将上面示例配置中的/PATH/TO路径修改为相应的用户主目录即可，例如/home/ik8s/。当然，也可以是用户自定义的其他kubeconfig配置文件的存放路径。<br>calicoctl的通用语法格式为calicoctl [options] &lt;command&gt; [&lt;args&gt;…]。它支持apply、delete、get、patch、replace、node和ipam等子命令，分别用于增、删、改、查相应的资源配置或打印相关状态信息等。<br>例如，下面命令列出Datastore中所有的ipPool资源对象。ipPool是常用的资源类型之一，它代表当前Calico系统可用的地址池资源。默认部署生成的地址池资源名称为default-ipv4-ippool。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl get ipPool</span></span><br><span class="line">NAME                  CIDR            SELECTOR   </span><br><span class="line">default-ipv4-ippool   10.244.0.0/16   all()</span><br></pre></td></tr></table></figure>

<p>calicoctl同样支持资源的多种输出格式，例如yaml、json、wide、go-template和custom- columns等，其功能完全类似kubectl中的用法。例如，下面的命令以YAML格式输出了默认地址池的详细定义。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">calicoctl</span> <span class="string">get</span> <span class="string">ipPool</span> <span class="string">default-ipv4-ippool</span> <span class="string">-o</span> <span class="string">yaml</span> </span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">IPPool</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-ipv4-ippool</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">blockSize:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">cidr:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">  <span class="attr">ipipMode:</span> <span class="string">Always</span></span><br><span class="line">  <span class="attr">natOutgoing:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeSelector:</span> <span class="string">all()</span></span><br><span class="line">  <span class="attr">vxlanMode:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure>

<p>我们可将上面命令输出的结果保存于本地文件中，修改其特定属性值后，再重新应用（apply）到Datastore从而完成配置更新，例如添加disabled: true以禁用指定的地址池等；也可同时修改资源名称和特定属性值后再应用到Datastore上以创建新的资源。<br>再如，下面的命令打印了地址池中相关地址块与IP地址的分配状态，包括地址池及各地址块中的IP总数、已分配数量和可用数量等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl ipam show --show-blocks</span></span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165047187.png" alt="image-20220218165047187"></p>
<p><font color="red">直接以ectd为Datastore的场景中，calicoctl则要使用由etcd信任的CA所签发的数字证书认证到etcd，主流的配置方式同样有环境变量和配置文件两种。</font></p>
<h3 id="BGP网络与BGP-Reflector"><a href="#BGP网络与BGP-Reflector" class="headerlink" title="BGP网络与BGP Reflector"></a>BGP网络与BGP Reflector</h3><p>一般来说，仅在那些不支持用户自定义BGP配置的网络中才会完全使用IPIP或VXLAN隧道网络，对于自主可控且规模较大的网络环境，非常有必要启用BGP降低网络开销以提升传输性能。对于Calico来说，修改ipPool属性相应的配置便可调整使用的网络类型。以此前部署的Calico系统默认使用的地址池default-ipv4-ippool为例，获取该资源的配置清单并保存为本地文件，修改ipipMode（或vxlanMode）的属性值为CrossSubnet或Never便能启用直接路由网络。<br>下面的配置清单示例（default-ipv4-ippool.yaml）将spec.ipipMode的属性值从Never修改为CrossSubnet，表示仅在跨IP网络节点上的Pod间通信才使用IPIP隧道，同一网络节点上的Pod间通信则使用路由方式直接进行。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">IPPool</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default-ipv4-ippool</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">blockSize:</span> <span class="number">24</span></span><br><span class="line">  <span class="attr">cidr:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/16</span></span><br><span class="line">  <span class="attr">ipipMode:</span> <span class="string">CrossSubnet</span></span><br><span class="line">  <span class="attr">natOutgoing:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">nodeSelector:</span> <span class="string">all()</span></span><br><span class="line">  <span class="attr">vxlanMode:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure>

<p>将上面配置清单的定义使用calicoctl apply命令重新应用到Calico Datastore上后便可立即生效。显然，这种变动会影响现有的通信流量，不建议在生产环境中随意变动。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl apply -f default-ipv4-ippool.yaml</span> </span><br><span class="line">Successfully applied 1 &#x27;IPPool&#x27; resource(s)</span><br></pre></td></tr></table></figure>

<p>随后，等BGP信息传播完成后，节点将同一网络内其他节点相关的路由条目经由IPIP模型的tunl0接口传输，变为节点上的某物理接口，如ens3等。下面的路由信息片段截取自k8s-node01主机之上。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">10.244.0.0/24 via 172.29.9.1 dev ens3 proto bird </span><br><span class="line">blackhole 10.244.1.0/24 proto bird</span><br><span class="line">10.244.2.0/24 via 172.29.9.12 dev ens3 proto bird </span><br><span class="line">10.244.3.0/24 via 172.29.9.13 dev ens3 proto bird</span><br></pre></td></tr></table></figure>

<p>在集群中部署一些Pod资源即可完成集群网络连接测试。假设在k8s-node01上存在一个IP地址为10.244.1.3的Pod A，以及在k8s-node02上存在一个IP地址为10.244.2.2的运行有demoapp应用的Pod B。通过curl命令在Pod A的交互式接口对Pod B发起HTTP请求，而后在k8s-node01的物理接口上抓取通信报文即可分析IPIP隧道报文通信格式，相关命令及截取的一次通信的往返结果示例如下，它显示出Pod之间直接基于底层网络完成了彼此间的通信。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# tcpdump -i ens3 -nn tcp port 80</span><br><span class="line">19:22:00.940398 IP 10.244.1.3.41522 &gt; 10.244.2.2.80: Flags [P.], seq 1:75, ack 1, win 504, options [nop,nop,TS val 1533673122 ecr 1404645777], length 74: HTTP: GET / HTTP/1.1</span><br><span class="line">19:22:00.943548 IP 10.244.2.2.80 &gt; 10.244.1.3.41522: Flags [P.], seq 1:18, ack 75, win 510, options [nop,nop,TS val 1404645781 ecr 1533673122], length 17: HTTP: HTTP/1.0 200 OK</span><br></pre></td></tr></table></figure>

<p>默认情况下，Calico的BGP网络工作在节点网格（node-to-node mesh）模型下，各节点间以对等方式广播路由，它仅适用于规模较小的集群环境。下面命令的结果显示的便是当前节点（k8s-master01）要对等广播路由的其他q节点，各节点打印的结果都会有所不同。随着节点数量的增多，这种对等广播的规模和数量将以指数级别上升。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~$ sudo calicoctl node status</span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165208447.png" alt="image-20220218165208447"></p>
<p>中级集群环境应该使用全局对等BGP（global BGP peers）模型，通过在同一个二层网络中使用一个或一组BGP反射器构建BGP网络环境，大型集群环境甚至可以使用每节点对等BGP模型（per-node BGP peers），即分布式BGP反射器模型。Calico的节点代理calico/node自身就能够充当BGP路由反射器，我们可以在Kubernetes集群外部的专用主机上部署calico/node作为路由反射器，也可以在集群中选择专用的几个节点进行配置。<br>下面我们仅出于测试目的，将集群中的k8s-master01部署为集群中的路由反射器，来说明其配置过程。通常来说，配置集群节点成为路由反射器大体有3个步骤：配置选定的Node作为BGP路由反射器、配置所有节点作为BGP对等节点（BGPPeer）向路由反射器发送路由信息，以及禁用节点网格。</p>
<ul>
<li>（1）配置路由反射器</li>
</ul>
<p>下面的配置清单示例（reflector-node.yaml）定义Calico Node资源对象k8s-master01.ilinux.io成为路由反射器，其中的spec.bgp.routeReflectorClusterID字段以IP地址格式的值为BGP路由器集群提供标识符，而特地添加的route-reflector标签则于配置BGPPeer时筛选节点。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Node</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">route-reflector:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">k8s-master01.ilinux.io</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">bgp:</span></span><br><span class="line">    <span class="attr">ipv4Address:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.1</span><span class="string">/16</span></span><br><span class="line">    <span class="attr">ipv4IPIPTunnelAddr:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">    <span class="attr">routeReflectorClusterID:</span> <span class="number">1.1</span><span class="number">.1</span><span class="number">.1</span></span><br></pre></td></tr></table></figure>

<p>运行如下命令将配置清单示例中的定义的资源配置应用（打补丁）到Datastore之上以完成路由反射器的配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl apply -f reflector-node.yaml</span></span><br><span class="line">Successfully applied 1 &#x27;Node&#x27; resource(s)</span><br></pre></td></tr></table></figure>

<ul>
<li>（2）配置BGP对等节点</li>
</ul>
<p>同一BGP路由器集群中的各节点都需要成为Reflector的BGP对等节点以交换路由信息。k8s-master01.ilinux.io自身同样运行于Pod资源，因而它自身同样需样成为Reflector的BGP对等节点。下面的配置清单示例（bgppeer-demo.yaml）定义集群所有节点同符合标签选择器route-reflector==”true”节点（路由反射器）进行“对等”。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">BGPPeer</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">bgppeer-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeSelector:</span> <span class="string">all()</span></span><br><span class="line">  <span class="attr">peerSelector:</span> <span class="string">route-reflector==&quot;true&quot;</span></span><br></pre></td></tr></table></figure>

<p>基于如下命令将BGPPeer/bgppeer-demo资源创建到Datastore之上，相关的“对等”关系便当即生效。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl apply -f bgppeer-demo.yaml</span></span><br><span class="line">Successfully applied 1 &#x27;BGPPeer&#x27; resource(s)</span><br></pre></td></tr></table></figure>

<p>新的BGPPeer资源定义了并非Node-to-Node的对等关系，因而在路由反射器节点和其他节点所看到的结果相差较大，因为路由反射器同集群中的所有节点对等，但其他节点仅会同路由反射器节点对等。下面命令运行在路由反射器（k8s-master01）之上，它已然能够与集群中的节点建立对等关系（自我对等的关系不会显示在命令结果中）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~ $ sudo calicoctl node status</span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165347240.png" alt="image-20220218165347240"></p>
<p>其中，输出结果中的PEER TYPE显示了对等通信的类型，常见的值有node-to-node mesh、node specific和global等，node specific表示与特定的节点对等，而省略node和nodeSelector字段时出现的global则表示全局对等。</p>
<ul>
<li>（3）禁用节点网格</li>
</ul>
<p>因原有的Node-to-Node网络的对等关系尚未禁用，上面命令的输出结果显示，BGP的路由传播仍然以对等网格的方式进行。下面的资源清单（default-bgpconfiguration.yaml）定义的BGPConfiguration/default资源就用于禁用这种BGP网格。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: projectcalico.org/v3</span><br><span class="line">kind: BGPConfiguration</span><br><span class="line">metadata:</span><br><span class="line">  name: default</span><br><span class="line">spec:</span><br><span class="line">  logSeverityScreen: Info</span><br><span class="line">  nodeToNodeMeshEnabled: false</span><br><span class="line">  asNumber: 63400  # BGP对等通信时使用的默认AS号</span><br></pre></td></tr></table></figure>

<p>需要特别说明的，禁用BGP网格的配置参数nodeToNodeMeshEnabled，以及BGP会话中使用的默认AS号码仅能够定义在名为default的全局BGPConfiguration资源中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">calicoctl apply -f bgpconfiguration-demo.yaml</span></span><br><span class="line">Successfully applied 1 &#x27;BGPConfiguration&#x27; resource(s)</span><br></pre></td></tr></table></figure>

<p>随后，由BGPPeer/bgppeer-demo资源定义的各Calico/Node与BGP路由反射器对等关系便会生效，下面的命令仍然是在路由反射器节点之上运行。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo calicoctl node status</span></span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165525797.png" alt="image-20220218165525797"></p>
<p>命令结果显示，由BGPPeer/bgppeer-demo资源定义的对等关系已然生效。我们可人为地关闭一个节点来模拟BGP对等节点故障，以验证其动态路由的管理能力。首先，我们关闭k8s-node03.ilinux.io主机，随后在路由反射器节点上重新获取对等节点的状态，可看到相应的故障信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~ $ sudo calicoctl node status</span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165542731.png" alt="image-20220218165542731">随后，我们可以在任意节点上验证与k8s-node03.ilinux.io相关的路由条目被移除的结果。若该节点恢复后，相关路由条目会重新添加回来，则证明BGP路由反射器能够正确工作。考虑到可用性，建议在生产环境中配置多个路由反射器节点。</p>
<h2 id="网络策略"><a href="#网络策略" class="headerlink" title="网络策略"></a>网络策略</h2><p>网络策略是控制Pod资源组间以及与其他网络端点间如何进行通信的规范，它使用标签来分组Pod，并在该组Pod之上定义规则来管控其流量，从而为Kubernetes提供更为精细的流量控制以及租户隔离机制。NetworkPolicy资源是Kubernetes API的一等公民，管理员或用户可使用NetworkPolicy这一标准资源类型按需定义网络访问控制策略。</p>
<h3 id="网络策略与配置基础"><a href="#网络策略与配置基础" class="headerlink" title="网络策略与配置基础"></a>网络策略与配置基础</h3><p>Kubernetes自身仅实现了NetworkPolicy API的规范，具体的策略实施要靠CNI网络插件完成，例如，Calico、Antrea、Canal和Weave等，但Flannel并不支持。<br>Calico的calico/kube-controllers是该项目中用于将用户定义的网络策略予以实现的组件，它主要依赖于在节点上构建iptables规则实现访问控制功能，如图10-28所示。其他支持网络策略的插件也有类似的将网络策略加以实现的“策略控制器”或“策略引擎”，它们通过API监听创建Pod时生成的新端点，并负责按需为其附加相关的网络策略。<br><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165659812.png" alt="image-20220218165659812"></p>
<p>Kubernetes默认并未对Pod之上的流量作为任何限制，Pod对象能够与集群上的其他任何Pod通信，也能够与集群外部的网络端点交互。NetworkPolicy是名称空间级别的资源，允许用户使用标签选择器在筛选出的一组Pod对象上分别管理Ingress和Egress流量。一旦将Network Policy引入到名称空间中，则被标签选择器“选中”的Pod将默认拒绝所有流量，而仅放行由特定的NetworlPolicy资源明确“允许”的流量。然而，未被任何NetworkPolicy资源的标签选择器选中的Pod对象的流量则不受影响。<br>换句话说，NetworkPolicy就是定义在一组Pod资源上的Ingress规则或Egress规则，或二者的组合定义，具体生效的范围则由“策略类型”（policyType)进行指定。Ingress和Egress规则的基本配置要素如图10-29所示。</p>
<p><img src="/blog/2022/02/23/%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%8E%E7%BD%91%E7%BB%9C%E7%AD%96%E7%95%A5/image-20220218165721382.png" alt="image-20220218165721382"></p>
<p>我们知道，NetworkPolicy是Kubernetes API中标准的资源类型，它同样由apiVersion、kind、metadata和spec等字段所定义，下面给出了其基本配置框架和简要注释信息。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span> <span class="comment"># 资源隶属的API群组及版本号</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span>              <span class="comment"># 资源类型的名称</span></span><br><span class="line"><span class="attr">metadata:</span>                        <span class="comment"># 资源元数据</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>                  <span class="comment"># 资源名称标识</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># NetworkPolicy是名称空间级别的资源</span></span><br><span class="line"><span class="attr">spec:</span>                            <span class="comment"># 期望的状态</span></span><br><span class="line">  <span class="string">podSelector</span> <span class="string">&lt;Object&gt;</span>  <span class="comment"># 当前规则生效的同一名称空间中的一组目标Pod对象，必选字段</span></span><br><span class="line">                                 <span class="comment"># 空值表示当前名称空间中的所有Pod资源</span></span><br><span class="line">  <span class="string">policyTypes</span> <span class="string">&lt;[]string&gt;</span>         <span class="comment"># Ingress表示生效ingress字段；Egress表示生效</span></span><br><span class="line"><span class="comment"># egress字段，同时提供表示二者均有效</span></span><br><span class="line"><span class="string">ingress</span> <span class="string">&lt;[]Object&gt;</span>               <span class="comment"># 入站流量源端点对象列表，即白名单，空值表示“所有”</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">from</span> <span class="string">&lt;[]Object&gt;</span>                <span class="comment"># 具体的端点对象列表，空值表示所有合法端点</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">ipBlock</span>  <span class="string">&lt;Object&gt;</span>            <span class="comment"># IP地址块范围内的端点，不能与另外两个字段同时使用</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">namespaceSelector</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># 匹配的名称空间内的端点</span></span><br><span class="line">     <span class="string">podSelector</span> <span class="string">&lt;Object&gt;</span>        <span class="comment"># 由Pod标签选择器匹配到的端点，空值表示&lt;none&gt;</span></span><br><span class="line">  <span class="string">ports</span> <span class="string">&lt;[]Object&gt;</span>      <span class="comment"># 具体的端口对象列表，空值表示所有合法端口</span></span><br><span class="line"><span class="string">engress</span> <span class="string">&lt;[]Object&gt;</span>      <span class="comment"># 出站流量目标端点对象列表，即白名单，空值表示“所有”</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">to</span> <span class="string">&lt;[]Object&gt;</span>                  <span class="comment"># 具体的端点对象列表，空值表示所有合法端点，格式同ingres.from；</span></span><br><span class="line">  <span class="string">ports</span> <span class="string">&lt;[]Object&gt;</span>               <span class="comment"># 具体的端口对象列表，空值表示所有合法端口</span></span><br></pre></td></tr></table></figure>

<p>为了方便描述NetworkPolicy资源及其功能，我们会时常用到以下几个术语。</p>
<ul>
<li>Pod组：由NetworkPolicy资源通过Pod标签选择器（spec.podSelector）动态选出的一组Pod资源集合，它们也是该网络策略规则管控的目标，可通过macthLabel或matchExpression类型的标签选择器选定。</li>
<li>Egress规则：出站流量的相关规则，负责管控由选定的Pod组发往其他网络端点的流量，可由流量的目标网络端点（spec.egress.to）和端口（spec.egress.ports）来定义。</li>
<li>Ingress规则：入站流量的相关规则，负责管控可由选定的Pod组所接收的流量，它能够由流量发出的源端点（spec.ingress.from）和流量的目标端口（spec.ingress.ports）来定义。</li>
<li>对端端点（to, from）：与选定的Pod组交互的对端主机，它可由CIDR格式的IP地址块（ipBlock）、网络名称空间选择器（namespaceSelector）来匹配名称空间内的所有Pod对象，甚至也可以是由Pod选择器（podSelector）在指定名称空间中选出的一组特定Pod对象等。</li>
</ul>
<p>在Ingress规则中，由from指定的网络端点也称为“源端点”；而在Egress规则中，网络端点也称为“目标端点”，它们用to字段标识。对于未启用Ingress或Egress规则的Pod组，流量方向默认均为“允许”，即默认为非隔离状态。而一旦在networkpolicy.spec中明确给出了ingress或egress字段，则它们的from或to字段的值就成了白名单列表；空值意味着选定所有端点，即允许相应方向上的所有流量通过，此时ingress和egress字段作用与未启用流量方向设置时相同。<br>Ingress或Egress规则的生效机制略复杂，以Ingress为例，明确定义spec.policyType为Ingress，但却未定义spec.ingress字段，则它无法匹配任何流量，因而选出的Pod组将不接受任何端点的访问，而使用了空值的spec.ingress字段或者spec.ingress.from字段，表示匹配所有合法端点，因而选出的Pod组可被任意端点访问。另一方面，即便Egress规则拒绝了所有流量，但由Ingress规则放行的请求流量的响应报文依然能够正常出站，它并不受限于Egress规则的定义，反之亦然。<br>尽管功能上日渐丰富，但NetworkPolicy资源仍然具有相当的局限性，例如它没有明确的拒绝规则、缺乏对选择器高级表达式的支持、不支持应用层规则，以及没有集群范围的网络策略等。为了解决这些限制，Calico等提供了自有的策略CRD，包括NetworkPolicy和GlobalNetworkPolicy等，其中的NetworkPolicy CRD比Kubernetes NetworkPolicy API提供了更大的功能集，包括拒绝规则、规则解析以及应用层规则等，但相关的规则需要由calicoctl创建。<br>Calico项目既能独立地为Kubernetes集群提供网络插件和网络策略，也能与Flannel结合在一起，由Flannel提供网络解决方案，而Calico仅用于提供网络策略，这种解决方案就是独立的Canal项目。不过，Canal目前直接使用Calico和Flannel项目，代码本身并没有任何修改，因此Canal仅是一种部署模式，用于安装和配置项目，从用户和编排系统的角度无缝地作为单一网络解决方案协同工作。接下来对网络策略话题的讲解将在10.3节部署的Calico环境基础上进行。</p>
<h3 id="管控入站流量"><a href="#管控入站流量" class="headerlink" title="管控入站流量"></a>管控入站流量</h3><p>服务类型的Pod对象通常是流量请求的目标对象，但它们的服务未必应该公开给所有网络端点访问，这就有必要对它们的访问许可施加控制。在待管控流量Pod对象所处的名称空间创建一个NetworkPolicy对象，使用spec.podSelector选中这组Pod，并在spec.ingress字段中嵌套管理规则，便能定向放行入站的访问流量。<br>ingress字段可嵌套使用的from和ports均为可选字段，空值意味着授权任意端点访问本地Pod组的任意端口，即放行所有入站流量。当仅定义了from字段时会隐含本地Pod组上的所有端口，而仅定义ports则隐含所有的源端点。from（源端点）和ports（目标端口）定义在同一个列表项中会隐含“逻辑与”关系，它匹配那些同时满足from和ports定义的入站流量。<br>（1）ingress.from字段<br>from字段的值是一个对象列表，用于界定访问目标Pod组的一到多个流量来源，可嵌套使用ipBlock、namespaceSelector和podSelector这3个可选字段。这3个字段匹配Pod资源的方式各有不同，且ipBloc与另外两个字段互斥，而同时使用namespaceSelector和podSelector字段时隐含“逻辑与”关系，而多个列表项彼此间隐含“逻辑或”关系。</p>
<ul>
<li>ipBlock &lt;Object&gt;：根据IP地址或网络地址块匹配流量源端点。</li>
<li>namespaceSelector &lt;Object&gt;：使用标签选择器挑选名称空间，它将匹配由此标签选择器选出的相关名称空间内的所有Pod对象；空值表示匹配所有的名称空间，即源端点可为集群上的任意Pod对象。</li>
<li>podSelector &lt;Object&gt;：于NetworkPolicy资源所在的当前名称空间内基于标签选择器挑选Pod对象，空值表示挑选当前名称空间内的所有Pod对象；与namespaceSelector字段同时使用时，作用域为挑选出的名称空间，而非当前名称空间。</li>
</ul>
<p>（2）ingress.ports字段<br>ports字段的值也是一个对象列表，用于界定可被源端点访问的目标端口，它嵌套port和protocol来定义流量的目标端口，即由NetworkPolicy资源匹配到的当前名称空间内的Pod组上的端口。</p>
<ul>
<li><p>port &lt;string&gt;：端口号或在container上定义的端口名称，未定义时匹配所有端口。</p>
</li>
<li><p>protocol &lt;string&gt;：传输层协议名称，TCP或UDP，默认为TCP。<br>来看一个管控入站流量的示例。示例代码（netpol-dev-demoapp-ingress.yaml）中的NetworkPolicy资源将dev名称空间中满足标签选择器app=demoapp的所有Pod对象定义为Pod组，通过Ingress规则定义了该Pod组上入站流量规则。</p>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-ingress</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span>           <span class="comment"># 网络策略生效的名称空间</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span>             <span class="comment"># 定义本地Pod组的标签选择器</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">policyTypes:</span> [<span class="string">&quot;Ingress&quot;</span>] <span class="comment"># 仅生效Ingress规则</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span>   <span class="comment"># 规则1：可访问Pod组上任意端口的流量源</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span>   <span class="comment"># 流量源之一：指定名称空间中的所有端点</span></span><br><span class="line">        <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">name</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span> [<span class="string">dev</span>, <span class="string">kube-system</span>, <span class="string">logs</span>, <span class="string">monitoring</span>, <span class="string">kubernetes-dashboard</span>]</span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ipBlock:</span>             <span class="comment"># 流量源之二：指定网络地址范围内的所有端点</span></span><br><span class="line">        <span class="attr">cidr:</span> <span class="number">10.244</span><span class="number">.0</span><span class="number">.0</span><span class="string">/24</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span>                  <span class="comment"># 规则2：可访问Pod组的80端口的流量源</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span>   <span class="comment"># 流量源，除default名称空间之外的其他所有名称空间中的端点</span></span><br><span class="line">        <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">name</span>, <span class="attr">operator:</span> <span class="string">NotIn</span>, <span class="attr">values:</span> [<span class="string">default</span>]&#125;</span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>将上面清单中的NetworlPolicy/demoapp-ingress创建之前请确保dev名称空间正常存在，否则，需要事先创建它。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f netpol-dev-demoapp-ingress.yaml</span></span><br><span class="line">networkpolicy.networking.k8s.io/demoapp-ingress created</span><br></pre></td></tr></table></figure>

<p>随后，为了测试Ingress规则的访问控制效果，我们需要先在dev名称空间中创建出满足标签选择器的本地Pod资源。下面命令创建了Deployment/demoapp资源，它会自动为Pod添加app=demoapp标签，该标签又被Service/demoapp作为对应后端端点的过滤条件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create deployment demoapp --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n dev</span></span><br><span class="line">deployment.apps/demoapp created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create service nodeport demoapp --tcp=80 -n dev</span></span><br><span class="line">service/demoapp created</span><br></pre></td></tr></table></figure>

<p>我们仅对该服务的80端口的相关规则进行测试，首先在default名称空间对dev名称空间中的service/demoapp发起访问请求，测试其是否会被拒绝。这里需要先确保default名称空间有name=default标签，否则需要事先为其设定该标签。<font color="red">任何期望能够以标签选择器匹配的名称空间都需要事先规划并完成标签的添加，例如示例中通过name键筛选的default、kube-system、kubernetes-dashboard、logs和monitoring等。</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/default name=default</span></span><br><span class="line">namespace/default labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-<span class="variable">$RANDOM</span> --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n default \</span></span><br><span class="line"><span class="language-bash">           --<span class="built_in">rm</span> -it --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@client-17773 /]# curl --connect-timeout 5 demoapp.dev.svc.cluster.local.</span><br><span class="line">curl: (28) Connection timed out after 5001 milliseconds</span><br></pre></td></tr></table></figure>

<p>上面的命令结果显示，default名称空间中的Pod对象发往dev名称空间特定Pod组的请求因未明确设置放行规则而被拒绝。接着，我们再换到未限定端口的名称空间列表中的某一个进行测试，例如prod，以确保该请求能被第2个规则所匹配。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create namespace prod</span></span><br><span class="line">namespace/prod created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-<span class="variable">$RANDOM</span> --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n prod \</span></span><br><span class="line"><span class="language-bash">          --<span class="built_in">rm</span> -it --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@client-12528 /]#  curl --connect-timeout 5 demoapp.dev.svc.cluster.local.</span><br><span class="line">…… ClientIP: 10.244.2.16, ServerName: demoapp-6c5d545684-fshkq, ServerIP: 10.244.3.5!</span><br></pre></td></tr></table></figure>

<p>命令结果显示请求被允许通过，这完全符合我们的规则限定。最后，在集群之外通过NodePort向目标服务发起请求，以测试非名称空间中端点的请求放行状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">NODEPORT=$(kubectl get service/demoapp -n dev -o jsonpath=<span class="string">&#x27;&#123;.spec.ports[0].nodePort&#125;&#x27;</span>)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl --connect-timeout 5 http://k8s-node03.ilinux.io:<span class="variable">$NODEPORT</span></span></span><br><span class="line">curl: (28) Connection timed out after 5001 milliseconds</span><br></pre></td></tr></table></figure>

<p>因没有任何规则可匹配到集群外部的端点，因而请求会被拒绝。这也表明，在namespaceSelector中使用排除法时，最后的限定是集群上被排除的端点之外的其他端点。若要放行集群外部的端点，我们应该使用没有任何限制的流量源，例如from: {}。</p>
<h3 id="管控出站流量"><a href="#管控出站流量" class="headerlink" title="管控出站流量"></a>管控出站流量</h3><p>除非是在当前名称空间中即可完成所有目标功能，否则大多数情况下，一个名称空间中的Pod资源总是有对外请求的需求，例如向CoreDNS请求解析名称等。因此，通常应该将出站流量的默认策略设置为准许通过。但如果要对流量实施精细管理，仅放行有对外请求必要的Pod对象的出站流量，可使用同Ingress规则相似的逻辑来定义Egress规则。<br>networkpolicy.spec中嵌套的egress字段用于定义出站流量规则，就特定的Pod集合来说，出站流量一样默认处于放行状态，但只要有一个NetworkPolicy资源的标签选择器可以匹配到该Pod集合，则默认策略将转为拒绝。与Ingress规则不同的之外在于，egress字段嵌套使用to和ports字段，前者用于定义本地Pod组的请求流量可发往的目标端点，其格式与逻辑都与ingres.from相同，后者同样用于限定可访问的目标端口，不过是指被访问的对端端点上的服务端口，如图10-26所示。<br>下面配置清单示例（netpol-dev-demoapp-egress.yaml）的NetworkPolicy资源为匹配标签app=demoapp的Pod组通过Egress规则限制了可外发请求流量的白名单，它仅能访问指定端点的特定服务端口。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-egress</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span>           <span class="comment"># 定义本地Pod组的标签选择器</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">policyTypes:</span> [<span class="string">&quot;Egress&quot;</span>]<span class="comment"># 仅生效Egress规则</span></span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>                  <span class="comment"># 规则1：仅生效于UDP协议的53号端口；不限制流量目标</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>                  <span class="comment"># 规则2：仅生效于TCP协议的6379端口</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span>       <span class="comment"># 流量目标：当前名称空间中匹配指定标签的Pod对象</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">app:</span> <span class="string">redis</span>     <span class="comment"># 访问redis数据存储服务</span></span><br><span class="line">    <span class="attr">ports:</span>  </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>                  <span class="comment"># 规则3：仅生效于TCP协议的80端口</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">podSelector:</span>       <span class="comment"># 流量目标：当前名称空间中匹配指定标签的Pod对象</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">app:</span> <span class="string">demoapp</span>   <span class="comment"># 同一组Pod内彼此间可互相访问</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>我们将配置清单示例中的NetworkPolicy/demoapp-egress资源创建到集群上的dev名称空间中，以便于后续的测试操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f netpol-dev-demoapp-egress.yaml</span></span><br><span class="line">networkpolicy.networking.k8s.io/demoapp-egress created</span><br></pre></td></tr></table></figure>

<p>下面将dev名称空间中的Deployment/demoapp资源的Pod副本数调整为多个，来测试该组Pod内Pod间互相访问的结果状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale deployments/demoapp --replicas=3 -n dev</span></span><br><span class="line">deployment.apps/demoapp scaled</span><br></pre></td></tr></table></figure>

<p>在Service对象demoapp.dev.svc关联到的任意一个Pod之上对该服务对象自身发起访问请求，即可同时测试DNS名称解析服务请求和组内demoapp应用服务的请求结果状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">POD=$(kubectl get pods -l app=demoapp -o jsonpath=<span class="string">&#x27;&#123;.items[0].metadata.name&#125;&#x27;</span> -n dev)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> <span class="variable">$POD</span> -n dev -- curl -s demoapp.dev.svc</span></span><br><span class="line">……ClientIP: 10.244.1.11, ServerName: demoapp-6c5d545684-bw59t, ServerIP: 10.244.2.18!</span><br></pre></td></tr></table></figure>

<p>但该组Pod无法访问Egress放行白名单之外的其他任何服务，例如下面测试访问kubernetes-dashboard名称空间中曾经部署过的kubernetes-dashboard服务的最后结果就是因请求超时而退出。而移除dev名称空间中的NetworkPolicy/demoapp-egress资源，该请求就会成功完成，感兴趣的读者可自行测试其效果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> <span class="variable">$POD</span> -n dev -- \</span></span><br><span class="line"><span class="language-bash">    curl -s -k https://kubernetes-dashboard.kubernetes-dashboard.svc</span></span><br><span class="line">command terminated with exit code 28</span><br></pre></td></tr></table></figure>

<p>事实上，同一组Pod上的Ingress和Egress规则通常应该定义在同一个NetworkPolicy资源之上，我们前面只是为了分开说明其用法而刻意放置在了不同的资源之上进行定义。另外需要再次说明的是，Ingress规则放行的请求响应报文不受Egress规则的限制，同理，Egress规则放行的出站请求而得到的入站响应报文也不受Ingress规则的限制。</p>
<h3 id="隔离名称空间"><a href="#隔离名称空间" class="headerlink" title="隔离名称空间"></a>隔离名称空间</h3><p>实践中，以名称空间分隔的多租户甚至是多项目的Kubernetes集群上，通常应该设定彼此间的通信隔离，以提升系统整体安全性。但这些名称空间通常应该允许内部各Pod间的通信，以及允许来自集群上管理类应用专用名称空间的请求，包括kube-system和kubernetes-dashboard，以及集群式日志收集系统专用的名称空间（例如logs）和监控系统专用的名称空间（例如monitoring）等。同时，这些名称空间通常会请求DNS服务，以及Kubernetes的API等。下面的配置清单示例（netpol-stage-default.yaml）为stage名称空间创建了一个名为default的NetworkPolicy资源，它大体实现了上述要求。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">networking.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">NetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">stage</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">podSelector:</span> &#123;&#125;                      <span class="comment"># 当前名称空间中的所有Pod对象</span></span><br><span class="line">  <span class="attr">policyTypes:</span> [<span class="string">&quot;Ingress&quot;</span>, <span class="string">&quot;Egress&quot;</span>]   <span class="comment"># Ingress和Egress规则同时生效</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">from:</span>                              <span class="comment"># 入站规则1：开放所有端口</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span>               <span class="comment"># 流量源：来自指定名称空间中的所有源端点</span></span><br><span class="line">        <span class="attr">matchExpressions:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">name</span></span><br><span class="line">          <span class="attr">operator:</span> <span class="string">In</span></span><br><span class="line">          <span class="attr">values:</span> [<span class="string">stage</span>,<span class="string">kube-system</span>,<span class="string">logs</span>,<span class="string">monitoring</span>,<span class="string">kubernetes-dashboard</span>]</span><br><span class="line">  <span class="attr">egress:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>  <span class="comment"># 出站规则1：开放对任意外部端点上UDP协议53端口的访问</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">UDP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">53</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>                                <span class="comment"># 出站规则2：仅生效于TCP协议的443端口</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span>               <span class="comment"># 流量目标：指定名称空间内的指定Pod对象</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">kube-system</span></span><br><span class="line">      <span class="attr">podSelector:</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">component:</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="attr">ports:</span>                             <span class="comment"># 端口列表</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">to:</span>                                <span class="comment"># 出站规则3：生效的所有端口</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">namespaceSelector:</span>               <span class="comment"># 流量目标：当前名称空间中的所有端点</span></span><br><span class="line">        <span class="attr">matchLabels:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">stage</span></span><br></pre></td></tr></table></figure>

<p>若不希望完全放行当前名称空间中所有Pod对象彼此间的流量，可以从Ingress和Egress规则中将适配当前名称空间的部分移除，而后由其他规则显式放行必要的内部流量。<br>显然，每个名称空间都需要以当前名称空间为中心设置如上NetworkPolicy资源才能完成彼此间隔离，但Kubernetes不支持集群级别的NetworkPolicy，因而只能逐个名称空间进行定义，且需要确保各名称空间中的用户不能轻易删除该NetworkPolicy资源。</p>
<h3 id="Calico的网络策略"><a href="#Calico的网络策略" class="headerlink" title="Calico的网络策略"></a>Calico的网络策略</h3><p>Calico支持GlobalNetworkPolicy和NetworkPolicy两种资源，前者用于定义集群全局网络策略，而后者大致可看作Kubernetes NetworkPolicy的一个超集。<br>GlobalNetworkPolicy支持使用selector、serviceAccountSelector或namespaceSelector来选定网络策略的生效范围，默认为all()，即集群上的所有端点。下面的配置清单示例（globalnetworkpolicy-demo.yaml）为非系统类名称空间定义了一个通用的网络策略。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">projectcalico.org/v3</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">GlobalNetworkPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">namespaces-default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">order:</span> <span class="number">0.0</span>   <span class="comment"># 策略叠加时的应用次序，数字越小越先应用，冲突时，后者会覆盖前者</span></span><br><span class="line">  <span class="comment"># 策略应用目标为非指定名称空间中的所有端点</span></span><br><span class="line"><span class="attr">namespaceSelector:</span> <span class="string">name</span> <span class="string">not</span> <span class="string">in</span> </span><br><span class="line">&#123;<span class="string">&quot;kube-system&quot;</span>,<span class="string">&quot;kubernetes-dashboard&quot;</span>,<span class="string">&quot;logs&quot;</span>,<span class="string">&quot;monitoring&quot;</span>&#125;</span><br><span class="line">  <span class="attr">types:</span> [<span class="string">&quot;Ingress&quot;</span>, <span class="string">&quot;Egress&quot;</span>]</span><br><span class="line">  <span class="attr">ingress:</span>      <span class="comment"># 入站流量规则</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">Allow</span>   <span class="comment"># 白名单</span></span><br><span class="line">    <span class="attr">source:</span></span><br><span class="line">      <span class="comment"># 可由下面系统名称空间中每个源端点访问策略生效目标中端点的任意端口</span></span><br><span class="line">      <span class="attr">namespaceSelector:</span> <span class="string">name</span> <span class="string">in</span> </span><br><span class="line">      &#123;<span class="string">&quot;kube-system&quot;</span>,<span class="string">&quot;kubernetes-dashboard&quot;</span>,<span class="string">&quot;logs&quot;</span>,<span class="string">&quot;monitoring&quot;</span>&#125;</span><br><span class="line">  <span class="attr">egress:</span>           <span class="comment"># 出站流量规则</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">action:</span> <span class="string">Allow</span>   <span class="comment"># 允许所有</span></span><br></pre></td></tr></table></figure>

<p>示例中，非系统名称空间中的Pod资源不允许非系统名称空间中的任何端点访问，包括同一名称空间中的其他端点。以dev为例，指定的4个系统名称空间中的端点可访问dev内的任何端点，但dev内的各端点彼此间并不能互相访问，也不能访问其他非系统名称空间中的端点。但各名称空间内的出站流量不受任何限制。确保示例中显式引用的几个名称空间拥有相应的标签后，将配置清单示例中的资源创建到Calico Datastore之中便能即时生效。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">calicoctl apply -f globalnetworkpolicy-demo.yaml</span></span><br><span class="line">Successfully applied 1 &#x27;GlobalNetworkPolicy&#x27; resource(s)</span><br></pre></td></tr></table></figure>

<p>策略生效后，我们可以多方验证其访问控制效果。下面以dev和logs名称空间为例进行简单检验。我们先删除dev下此前创建的所有NetworkPolicy资源，以精确测试全局网络策略的效果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete networkpolicy --all -n dev</span></span><br><span class="line">networkpolicy.networking.k8s.io &quot;demoapp-egress&quot; deleted</span><br><span class="line">networkpolicy.networking.k8s.io &quot;demoapp-ingress&quot; deleted</span><br></pre></td></tr></table></figure>

<p>按照全局网络策略的定义，dev名称空间的网络端点可以外发任何请求，包括请求kube-system中的kube-dns服务等，但这些网络端点彼此间无法访问，也不允许其他非系统名称空间中的端点访问。<br>1）以此前创建的Deployment/demoapp中任意一个Pod作为客户端进行测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">POD=$(kubectl get pods -l app=demoapp -o jsonpath=<span class="string">&#x27;&#123;.items[0].metadata.name&#125;&#x27;</span> -n dev)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it <span class="variable">$POD</span> -n dev -- /bin/sh</span></span><br></pre></td></tr></table></figure>

<p>2）测试DNS名称解析，成功完成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@demoapp-6c5d545684-6nqbv /]# host -t A demoapp.dev.svc</span><br><span class="line">demoapp.dev.svc.cluster.local has address 10.100.76.85</span><br></pre></td></tr></table></figure>

<p>3）测试访问当前名称空间中的服务，失败。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@demoapp-6c5d545684-6nqbv /]# curl --connect-timeout 5 demoapp.dev.svc</span><br><span class="line">curl: (28) Connection timed out after 5000 milliseconds</span><br></pre></td></tr></table></figure>

<p>4）测试访问集群外部服务，成功。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@demoapp-6c5d545684-6nqbv /]# curl -I http://ilinux.io</span><br><span class="line">HTTP/1.1 200 OK</span><br></pre></td></tr></table></figure>

<p>另一方面，dev名称空间中的各Pod允许接收指定的4个系统名称空间中任意端点发来的请求，并接受除此之外的其他名称空间中端点的访问请求。下面以monitoring和default名称空间为例，使用其内部的端点向dev名称空间中的demoapp服务进行请求测试。<br>1）若不存在，则先创建名称空间，并为其打上标签。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create namespace monitoring</span></span><br><span class="line">namespace/monitoring created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespace monitoring name=monitoring</span></span><br><span class="line">namespace/monitoring labeled</span><br></pre></td></tr></table></figure>

<p>2）在monitoring名称空间中创建一个Pod，以之为客户端进行测试，可成功完成访问。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-<span class="variable">$RANDOM</span> --image=<span class="string">&quot;ikubernetes/admin-toolbox:v1.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">           --<span class="built_in">rm</span> -it --<span class="built_in">command</span> -n monitoring -- /bin/sh</span></span><br><span class="line">[root@client-26148 /]# curl demoapp.dev.svc</span><br><span class="line">……ClientIP: 10.244.1.20, ServerName: demoapp-6c5d545684-bw59t, ServerIP: 10.244.2.18!</span><br></pre></td></tr></table></figure>

<p>3）在default名称空间中创建一个Pod，以之为客户端进行测试，则请求会失败。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-<span class="variable">$RANDOM</span> --image=<span class="string">&quot;ikubernetes/admin-toolbox:v1.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">           --<span class="built_in">rm</span> -it --<span class="built_in">command</span> -n default -- /bin/sh</span></span><br><span class="line">[root@client-5583 /]# curl --connect-timeout 5 demoapp.dev.svc</span><br><span class="line">curl: (28) Connection timed out after 5001 milliseconds</span><br></pre></td></tr></table></figure>

<p>定义好使用的全局网络策略，名称空间管理员便可按需使用NetworkPolicy资源组合定义本地入站流量的白名单，来设置端点的访问控制机制。GlobalNetworkPolicy和NetworkPolicy更详细的用法，请读者参考Calico的文档。<br>另外部署Calico时，GlobalNetworkPolicy和NetworkPolicy都以CRD的形式分别映射到了Kubernetes API之上，只不过它们隶属于自定义的crd.projectcalico.org/v1这一API群组和版本，管理员亦可使用该CRD来定义Calico的全局网络策略和名称空间级别的网络策略，其格式和意义基本与原生格式相同，这里不再给出相关的示例。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/" class="post-title-link" itemprop="url">认证、授权与准入控制</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-21 21:13:08" itemprop="dateCreated datePublished" datetime="2022-02-21T21:13:08+08:00">2022-02-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-23 15:55:56" itemprop="dateModified" datetime="2022-02-23T15:55:56+08:00">2022-02-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="Kubernetes访问控制"><a href="#Kubernetes访问控制" class="headerlink" title="Kubernetes访问控制"></a>Kubernetes访问控制</h2><p>API Server作为Kubernetes集群系统的网关，是访问及管理资源对象的唯一入口，通过HTTPS协议暴露了一个RESTful风格的接口。所有需要访问集群资源的集群组件或客户端，包括kube-controller-manager、kube-scheduler、kubelet和kube-proxy等集群基础组件，CoreDNS等集群的附加组件，以及此前使用的kubectl命令等都必须要经此网关请求与集群进行通信。所有客户端均要经由API Server访问或改变集群状态以及完成数据存储，并且API Server会对每一次的访问请求进行合法性检验，包括用户身份鉴别、操作权限验证以及操作是否符合全局规范的约束等。所有检查均正常完成且对象配置信息合法性检验无误之后才能访问或存入数据到后端存储系统etcd中，如图9-1所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218174842501.png" alt="image-20220218174842501"></p>
<p>客户端认证操作由API Server配置的一到多个认证插件完成。收到请求后，API Server依次调用配置的认证插件来校验客户端身份，直到其中一个插件可以识别出请求者的身份为止。授权操作则由一到多个授权插件完成，这些插件负责确定通过认证的用户是否有权限执行发出的资源操作请求，该类操作包括创建、读取、删除或修改指定的对象等。随后，通过授权检测的用户请求修改相关的操作还要经由一到多个准入控制插件的遍历式检测，例如使用默认值补足要创建的目标资源对象中未定义的各字段、检查目标Namespace资源对象是否存在、检查请求创建的Pod对象是否违反系统资源限制等，而其中任何的检查失败都可能会导致写入操作失败。</p>
<h3 id="用户账户与用户组"><a href="#用户账户与用户组" class="headerlink" title="用户账户与用户组"></a>用户账户与用户组</h3><p>Kubernetes系统上的用户账户及用户组的实现机制与常规应用略有不同。Kubernetes集群将那些通过命令行工具kubectl、客户端库或者直接使用RESTful接口向API Server发起请求的客户端上的请求主体分为两个不同的类别：现实中的“人”和Pod对象，它们的用户身份分别对应用户账户（User Account，也称为普通用户）和服务账户（Service Account，简称SA）。<br>1）用户账户：其使用主体往往是“人”，<font color="red">一般由外部的用户管理系统存储和管理，Kubernetes本身并不维护这一类的任何用户账户信息，它们不会存储到API Server之上，仅仅用于检验用户是否有权限执行其所请求的操作。</font><br>2）服务账户：其使用主体是“应用程序”，专用于为Pod资源中的服务进程提供访问Kubernetes API时的身份标识（identity）；<font color="red">ServiceAccount资源通常要绑定到特定的名称空间，它们由API Server自动创建或通过API调用，由管理员手动创建，通常附带着一组访问API Server的认证凭据——Secret，可由同一名称的Pod应用访问API Server时使用。</font><br>用户账户通常用于复杂的业务逻辑管控，作用于系统全局，因而名称必须全局唯一。Kubernetes并不会存储由认证插件从客户端请求中提取的用户及所属的组信息，因而也就没有办法对普通用户进行身份认证，它们仅仅用于检验该操作主体是否有权限执行其所请求的操作。相比较来说，服务账户则隶属于名称空间级别，仅用于实现某些特定操作任务，因此功能上要轻量得多。这两类账户都可以隶属于一个或多个用户组。<br>用户组只是用户账户的逻辑集合，它本身没有执行系统操作的能力，但附加于组上的权限可由其内部的所有用户继承，以实现高效的授权管理机制。Kubernetes有以下几个内置用于特殊目的的组。</p>
<ul>
<li>system:unauthenticated：未能通过任何一个授权插件检验的账户的、所有未通过认证测试的用户统一隶属的用户组。</li>
<li>system:authenticated：认证成功后的用户自动加入的一个专用组，用于快捷引用所有正常通过认证的用户账户。</li>
<li>system:serviceaccounts：所有名称空间中的所有ServiceAccount对象。</li>
<li>system:serviceaccounts:&lt;namespace&gt;：特定名称空间内所有的ServiceAccount对象。<br>对API Server来说，来自客户端的请求要么与用户账户绑定，要么以某个服务账户的身份进行，要么被视为匿名请求。群集内部或外部的每个进程，包括由人类用户使用kubectl，以及各节点上运行的kubelet进程，再到控制平面的成员组件，必须在向API Server发出请求时进行身份验证，否则即被视为匿名用户。</li>
</ul>
<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><p>Kubernetes使用身份验证插件对API请求进行身份验证，它允许管理员自定义服务账户和用户账户要启用或禁用的插件，并支持各自同时启用多种认证机制。具体设定时，至少应该为服务账户和用户账户各自启用一个认证插件。<br>如果启用了多种认证机制，账号认证过程由认证插件以串行方式进行，直到其中一种认证机制成功完成即结束。若认证失败，服务器则响应以401状态码，反之，请求者就会被Kubernetes识别为某个具体的用户（以其用户名进行标识），并且该连接上随后的操作都会以此用户身份进行。API Server对于接收到的每个访问请求会调用认证插件，尝试将以下属性与访问请求相关联。</p>
<ul>
<li>Username：用户名，例如kubernetes-admin等。</li>
<li>UID：用户的数字标签符，用于确保用户身份的唯一性。</li>
<li>Groups：用户所属的组，用于权限指派和继承。</li>
<li>Extra：键值数据类型的字符串，用于提供认证需要用到的额外信息。</li>
</ul>
<p>API Server支持以下几种具体的认证方式，其中所有的令牌认证机制通常被统称为“承载令牌认证”。</p>
<blockquote>
<ul>
<li>1）静态密码文件认证：将用户名和密码等信息以明文形式存储在CSV格式的文件中，由kube-apiserver在启动时通过–basic-auth-file选项予以加载，添加或删除用户都需要重启API Server；客户端通过在HTTP Basic认证（Authorization: Basic <a href="base64-encoded-username:password">base64-encoded-username:password</a>标头）方式中将用户名和密码编码后对该文件进行认证；显然，该认证方式应该在非生产性的环境中使用。<font color="red"> 静态密码文件认证插件自Kubernetes v1.20版本中预以弃用，因而，后面9.1.3节中使用该插件的测试操作部分在v1.20及之后的版本上不可用，但在v1.19及之前的版本中，仍然可用。</font></li>
<li>2）静态令牌文件认证：即保存用于认证的令牌信息的静态文件，由kube-apiserver的命令行选项–token-auth-file加载，且API Sever进程启动后不可更改；HTTP协议的客户端能基于承载令牌（Authorization: Bearer <token>标头）对静态令牌文件进行身份验证，它将令牌编码后通过请求报文中的Authorization头部承载并传递给API Server即可；建议仅将该认证方式用于非生产性环境中。</token></li>
<li>3）X509客户端证书认证：客户端在请求报文中携带X.509格式的数字证书用于认证，其认证过程类似于HTTPS协议通信模型；认证通过后，证书中的主体标识（Subject）将被识别为用户标识，其中的字段CN（Common Name）的值是用户名，字段O（Organization）的值是用户所属的组。例如/CN=ilinux/O=opmasters/O=admin中，用户名为ilinux，它属于opmasters和admin两个组；该认证方式可通过–client-ca-file=SOMEFILE选项启用。</li>
<li>4）引导令牌（Bootstrap Token）认证：一种动态管理承载令牌进行身份认证的方式，常用于简化组建新Kubernetes集群时将节点加入集群的认证过程，需要由kube-apiserver通过–experimental-bootstrap-token-auth选项启用；新的工作节点首次加入时，Master使用引导令牌确认节点身份的合法性之后自动为其签署数字证书以用于后续的安全通信，kubeadm初始化的集群也是这种认证方式；这些令牌作为Secrets存储在kube-system命名空间中，可以动态管理和创建它们，并由TokenCleaner控制器负责删除过期的引导令牌。</li>
<li>5）ServiceAccount令牌认证：该认证方式会由kube-apiserver程序自动启用，它同样使用签名的承载令牌来验证请求；该认证方式还支持通过可选项–service-account-key-file加载签署承载令牌的密钥文件，未指定时将使用API Server自己的TLS私钥；ServiceAccount通常由API Server自动创建，并通过ServiceAccount准入控制器将其注入Pod对象，包括ServiceAccount上的承载令牌，容器中的应用程序请求API Server的服务时以此完成身份认证。</li>
<li>6）OpenID Connect令牌认证：简称为OIDC，是OAuth 2协议的一种扩展，由Azure AD、Salesforce和Google Accounts等OAuth 2服务商所支持，协议的主要扩展是返回的附加字段，其中的访问令牌也称为ID令牌；它属于JSON Web令牌（JWT）类型，有服务器签名过的常用字段，例如email等；kube-apiserver启用这种认证功能的相关选项较多。</li>
<li>7）Webhook令牌认证：Webhook身份认证是用于验证承载令牌的钩子；HTTP协议的身份验证允许将服务器的URL注册为Webhook，并接收带有承载令牌的POST请求进行身份认证；客户端使用kubeconfig格式的配置文件，在文件中，users指的是API Server的Webhook，而clusters则指的是API Server。</li>
<li>8）代理认证：API Server支持从请求头部的值中识别用户，例如常用的X-Remote-User、X-Remote-Group和几个以X-Remote-Extra-开头的头部，它旨在与身份验证代理服务相结合，由该代理设置相应的请求头部；为了防止头欺骗，在检查请求标头之前，需要身份认证代理服务向API Server提供有效的客户端证书，以验证指定CA（由选项–requestheader-client-ca-file等进行指定）的代理服务是否合法。</li>
</ul>
</blockquote>
<p>未能被任何验证插件明确拒绝的请求中的用户即为匿名用户，该类用户会被冠以system:anonymous用户名，隶属于system:unauthenticated用户组。若API Server启用了除AlwaysAllow以外的认证机制，则匿名用户处于启用状态。但是，出于安全因素的考虑，建议管理员通过–anonymous-auth=false选项将其禁用。<br>API Server还允许用户通过模拟头部冒充另一个用户，这些请求可以以手动方式覆盖请求中用于身份验证的用户信息。例如，管理员可以使用此功能临时模拟其他用户来查看请求是否被拒绝，以进行授权策略调试。<br>除了身份信息，请求报文还需要提供操作方法及其目标对象，例如针对某Pod资源对象进行的创建、查看、修改或删除操作等。具体来说，它包含如下信息。</p>
<ul>
<li>API：用于定义请求的目标是否为一个API资源。</li>
<li>Request path：请求的非资源型路径，例如/api或/healthz。</li>
<li>API group：要访问的API组，仅对资源型请求有效；默认为core API group。</li>
<li>Namespace：目标资源所属的名称空间，仅对隶属于名称空间类型的资源有效。</li>
<li>API request verb：API请求类的操作，即资源型请求（对资源执行的操作），包括get、list、create、update、patch、watch、proxy、redirect、delete和deletecollection等。</li>
<li>HTTP request verb：HTTP请求类的操作，即非资源型请求要执行的操作，如get、post、put和delete。</li>
<li>Resource：请求的目标资源的ID或名称。</li>
<li>Subresource：请求的子资源。</li>
</ul>
<p>为了核验用户的操作许可，成功通过身份认证后的操作请求还需要转交给授权插件进行许可权限检查，以确保其拥有执行相应操作的许可。API Server主要支持使用4类内置的授权插件来定义用户的操作权限。</p>
<ul>
<li>Node：基于Pod资源的目标调度节点来实现对kubelet的访问控制。</li>
<li>ABAC：Attribute-based access control，基于属性的访问控制。</li>
<li>RBAC：Role-based access control，基于角色的访问控制。</li>
<li>Webhook：基于HTTP回调机制实现外部REST服务检查，确认用户授权的访问控制。</li>
</ul>
<p>还有AlwaysDeny和AlwaysAllow两个特殊的授权插件，其中AlwaysDeny（总是拒绝）仅用于测试，而AlwaysAllow（总是允许）则用于不期望进行授权检查时直接在授权检查阶段放行所有的操作请求。–authorization-mode选项用于定义API Server要启用的授权机制，多个选项值彼此间以逗号进行分隔。<br>而准入控制器（admission controller）则用于在客户端请求经过身份验证和授权检查之后，将对象持久化存储到etcd之前拦截请求，从而实现在资源的创建、更新和删除操作期间强制执行对象的语义验证等功能，而读取资源信息的操作请求则不会经由准入控制器检查。API Server内置了许多准入控制器，常用的包含下面列出的几种。不过，其中的个别控制器仅在较新版本的Kubernetes中才被支持。</p>
<blockquote>
<p>1）AlwaysAdmit和AlwaysDeny：前者允许所有请求，后者则拒绝所有请求。<br>2）AlwaysPullImages：总是下载镜像，即每次创建Pod对象之前都要去下载镜像，常用于多租户环境中，以确保私有镜像仅能够由拥有权限的用户使用。<br>3）NamespaceLifecycle：拒绝在不存在的名称空间中创建资源，而删除名称空间则会级联删除其下的所有其他资源。<br>4）LimitRanger：可用资源范围界定，用于对设置了LimitRange的对象所发出的所有请求进行监控，以确保其资源请求不会超限。<br>5）ServiceAccount：用于实现服务账户管控机制的自动化，实现创建Pod对象时自动为其附加相关的Service Account对象。<br>6）PersistentVolumeLabel：为那些由云计算服务商提供的PV自动附加region或zone标签，以确保这些存储卷能正确关联且仅能关联到所属的region或zone。<br>7）DefaultStorageClass：监控所有创建PVC对象的请求，以保证那些没有附加任何专用StorageClass的请求会被自动设定一个默认值。<br>8）ResourceQuota：用于为名称空间设置可用资源上限，并确保当其中创建的任何设置了资源限额的对象时，不会超出名称空间的资源配额。<br>9）DefaultTolerationSeconds：如果Pod对象上不存在污点宽容期限，则为它们设置默认的宽容期，以宽容notready:NoExecute和unreachable:NoExecute类的污点5分钟时间。<br>10）ValidatingAdmissionWebhook：并行调用匹配当前请求的所有验证类的Webhook，任何一个校验失败，请求即失败。<br>11）MutatingAdmissionWebhook：串行调用匹配当前请求的所有变异类的Webhook，每个调用都可能会更改对象。<br>早期的准入控制器代码需要由管理员编译进kube-apiserver中才能使用，实现方式缺乏灵活性。于是，Kubernetes自v1.7版本引入了Initializers和External Admission Webhooks来尝试突破此限制，而且自v1.9版本起，External Admission Webhooks被分为Mutating-AdmissionWebhook和ValidatingAdmissionWebhook两种类型，分别用于在API中执行对象配置的变异和验证操作。检查期间，仅那些顺利通过所有准入控制器检查的资源操作请求的结果才能保存到etcd中，而任何一个准入控制器的拒绝都将导致写入请求失败。</p>
</blockquote>
<h3 id="测试使用API-Server的访问控制机制"><a href="#测试使用API-Server的访问控制机制" class="headerlink" title="测试使用API Server的访问控制机制"></a>测试使用API Server的访问控制机制</h3><p> 如前所述，认证、授权和准入控制功能都以API Server插件形式存在，它们都可由kube-apiserver相应的选项进行启用和配置。由kubeadm部署的Kubernetes集群的API Server以静态Pod形式运行，相关配置清单（/etc/kubernetes/manifests/kube-apiserver.yaml）中配置kube-apiserver如下的一部分选项：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-apiserver</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--secure-port=6443</span>  <span class="comment"># API Server监听的安全端口（HTTPS协议）</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--insecure-port=0</span>   <span class="comment"># API Server监听的非安全端口（HTTP协议），0表示禁用</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--client-ca-file=/etc/kubernetes/pki/ca.crt</span> <span class="comment"># 启用X509数字证书认证</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--authorization-mode=Node,RBAC</span>              <span class="comment"># 启用Node和RBAC授权插件</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--enable-admission-plugins=NodeRestriction</span>  <span class="comment"># 额外启用的准入控制器列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--enable-bootstrap-token-auth=true</span>          <span class="comment"># 启用Bootstrap Token认证</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--requestheader-extra-headers-prefix=X-Remote-Extra-</span> <span class="comment">#代理认证相关的配置</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--requestheader-group-headers=X-Remote-Group</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--requestheader-username-headers=X-Remote-User</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="string">--service-account-key-file=/etc/kubernetes/pki/sa.pub</span></span><br></pre></td></tr></table></figure>

<p>API Server的几乎每个认证插件都有自己专用的一至多个配置选项，例如上面配置中启用的X509证书认证、Bootstrap Token认证和代理认证，Server Account认证默认处于启用状态，–service-account-key-file选项仅指定用于签署承载令牌的密钥文件，未配置时默认使用API Server的TLS私钥。<font color="red">认证操作由各插件以“短路”方式进行，客户端请求会依次经由认证插件进行检查，任何一个插件认证成功即终止认证过程。</font><br>启用的授权插件需要显式指定，它们需要以列表值的格式统一定义在–authorization-mode选项之上，例如上面配置中启用了Node和RBAC授权插件。启用的各授权插件同样以“短路”机制运行，任何一个授权插件鉴权成功即可结束授权检查过程。<br>API Server默认会启用一部分准入控制器，额外需要启用的准入控制器需要以列表值的形式定义在–enable-admission-plugins选项之上，需要显式禁用准入控制器以列表值的形式定义在–disable-admission-plugins选项之上。与认证和授权不同的是，一个请求必须要成功通过所有准入控制器的检查，否则即会被拒绝。提示<br>不同版本的Kubernetes默认启用的准入控制器可能存在区别，要了解当前使用的版本上默认启用的准入控制器，可在kube-system名称空间中API Server相关的静态Pod内部运行“kube-apiserver -h | grep ‘enable-admission-plugins’”命令获取。<br>测试或研发环境中，使用静态密码文件认证是添加普通用户的快捷途径。API Server的静态密码认证文件遵循CSV格式，每一行存储一个用户账户信息，格式为password,user,uid,”group1,group2,group3”，用户组一段可以省略，而多个用户组也以逗号分隔，但需要使用双引号将所有用户组进行整体引用。例如，我们可以在Master节点上的/etc/kubernetes/authfiles目录中创建拥有类似如下内容的文件passwd.csv，它提供了ilinux和ik8s两个用户，二者都属于kubeusers用户组。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ilinux@MageEdu,ilinux,1009,&quot;kubeusers,defaultadmin&quot;</span><br><span class="line">ik8s@MageEdu,ik8s,1010,&quot;kubeusers,defaultadmin&quot;</span><br></pre></td></tr></table></figure>

<p>API Server的–basic-auth-file选项要通过本地路径加载该文件以启用静态密码文件认证方式，这里采用hostPath存储卷的方式将宿主机目录/etc/kubernetes/authfiles/关联到静态Pod资源kube-apiserver的相同目录下。修改Master节点上的/etc/kubernetes/manifests/kube-apiserver.yaml文件，添加hostPath存储卷及卷挂载配置，需要改动的配置部分如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">command:</span>       <span class="comment"># 在command中为kube-apiserver添加--basic-auth-file选项</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kube-apiserver</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">--basic-auth-file=/etc/kubernetes/authfiles/passwd.csv</span></span><br><span class="line">  <span class="string">……</span></span><br><span class="line">  <span class="attr">volumeMounts:</span>  <span class="comment"># 为容器指定额外多挂载的hostPath存储卷，存储有静态密码文件</span></span><br><span class="line">  <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/etc/kubernetes/authfiles</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">static-auth-files</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">hostPath:</span>    <span class="comment"># 将宿主机上存储有静态密码文件的目录作为hostPath存储卷</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/etc/kubernetes/authfiles</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">static-auth-files</span></span><br></pre></td></tr></table></figure>

<p>kubelet的调谐循环会监视节点上指定的用于加载静态Pod配置清单的目录，默认为/etc/kubernetes/manifests。该目录中的任何清单发生变动时，都会由kubelet自动做出相应的处理，例如重新创建相关的Pod资源，因而配置清单修改完成后很快就会由kubelet重新创建kube-apiserver相关的静态Pod对象，从而启动静态密码文件认证机制。<font color="red">安全起见，passwd.csv文件仅应该让必要用户拥有访问权限，并拒绝其他一切用户的任何操作，即将该文件属主和属组设置为root，并设定其权限模型为0400。</font><br>API Server默认监听TCP的6443端口，未指定用户身份并以curl命令向Master节点的该端口上HTTPS服务的根路径发起请求测试，会被识别为匿名用户，但会出现该用户不具有访问相应资源权限的错误信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -k https://k8s-master01.ilinux.io:6443/ | grep message</span>                    </span><br><span class="line">  &quot;message&quot;: &quot;forbidden: User \&quot;system:anonymous\&quot; cannot get path \&quot;/\&quot;&quot;,</span><br></pre></td></tr></table></figure>

<p>但以静态密码文件passwd.csv中的某一个用户名及其密码发起访问请求，则会由API Server标识出成功认证并识别的用户名，例如下面的测试命令及响应结果所示，但该用户同样因不具有指定资源的访问权限而被拒绝。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -u ilinux:ilinux@MageEdu -k https://172.29.9.1:6443/ | grep message</span></span><br><span class="line">  &quot;message&quot;: &quot;forbidden: User \&quot;ilinux\&quot; cannot get path \&quot;/\&quot;&quot;,</span><br></pre></td></tr></table></figure>

<p><font color="red">使用来自HTTP/HTTPS客户端的基本身份验证时，API Server需要客户端提供一个特定标头Authorization，它的值采用Basic BASE64ENCODED (USER:PASSWORD)格式。</font><br>为了让ilinux用户获取资源操作权限，我们可以将API Server的–authorization-mode的选项值直接修改为AlwaysAllow，但这样也会将所有权限开放给任意用户，包括匿名用户，这将为系统引入无法预料的风险。因而，我们接下来基于RBAC授权插件，将默认的集群角色（ClusterRole）admin通过角色绑定（RoleBinding）机制，为ilinux用户授权default名称空间的管理权限来测试认证用户的授权功能。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create rolebinding default-ns-admin --clusterrole=admin --user=ilinux -n default</span>       </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/default-ns-admin created</span><br></pre></td></tr></table></figure>

<p>此时，ilinux用户拥有default名称空间及其内部资源对象的管理权限，我们可以通过该用户尝试访问该名称空间，以及该名称空间下的所有资源、指定类型的资源或指定的资源对象，例如下面的命令获取到了default名称空间的资源规范。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -u ilinux:ilinux@MageEdu -k https://172.29.9.1:6443/api/v1/namespaces/default/</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Namespace&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">  ……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>但同样能够认证通过的ik8s用户因未被授权，便不具有访问default名称空间的权限，如下面的测试命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -u ik8s:ik8s@MageEdu -k https://k8s-master01.ilinux.io:6443/api/v1/namespaces/default/</span></span><br><span class="line">……</span><br><span class="line">&quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">&quot;message&quot;: &quot;namespaces \&quot;default\&quot; is forbidden: User \&quot;ik8s\&quot; cannot get resource \&quot;namespaces\&quot; in API group \&quot;\&quot; in the namespace \&quot;default\&quot;&quot;,</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>另外，静态令牌文件同样是CSV格式的文件，它与静态密码文件的不同之处仅在于第一个字段提供的是令牌而非密码字符串，该令牌采用[a-z0-9]{6}.[a-z0-9]{16}的格式，第一部分代表令牌ID，第二部分则是令牌密钥。客户端认证时，直接在HTTP中以Authorization: Bearer &lt;token&gt;标头完成认证。我们可以在前面静态密码文件认证配置的基础上完成静态令牌文件认证的测试。</p>
<ul>
<li>步骤1：执行类似如下命令，在/etc/kubernetes/authfiles目录下创建token.csv文件，生成相关的用户配置。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo <span class="built_in">echo</span> <span class="string">&quot;<span class="subst">$(openssl rand -hex 3)</span>.<span class="subst">$(openssl rand -hex 8)</span>,ilinux,1009,\&quot;kubeusers,defaultadmin\&quot;&quot;</span> \</span></span><br><span class="line"><span class="language-bash">&gt;&gt; /etc/kubernetes/authfiles/token.csv</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo <span class="built_in">echo</span> <span class="string">&quot;<span class="subst">$(openssl rand -hex 3)</span>.<span class="subst">$(openssl rand -hex 8)</span>,ik8s,1010,\&quot;kubeusers,defaultadmin\&quot;&quot;</span> \</span></span><br><span class="line"><span class="language-bash">&gt;&gt; /etc/kubernetes/authfiles/token.csv</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo <span class="built_in">chmod</span> 400 /etc/kubernetes/authfiles/token.csv</span></span><br></pre></td></tr></table></figure>

<ul>
<li>步骤2：编辑/etc/kubernetes/manifests/kube-apiserver.yaml配置清单，为kube-apiserver添加配置选项–token-auth-file=/etc/kubernetes/authfiles/token.csv。</li>
<li>步骤3：以ilinux用户承载令牌认证的方式向API Server发起资源操作请求进行访问测试，由下面的命令及结果可知，认证和授权均得以成功完成。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">TOKEN=$(sudo awk -F <span class="string">&quot;,&quot;</span> <span class="string">&#x27;$2==&quot;ilinux&quot;&#123;print $1&#125;&#x27;</span> /etc/kubernetes/authfiles/token.csv)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -H <span class="string">&quot;Authorization: Bearer <span class="variable">$TOKEN</span>&quot;</span> -k \</span></span><br><span class="line"><span class="language-bash">      https://172.29.9.1:6443/api/v1/namespaces/default/</span></span><br><span class="line">&#123;</span><br><span class="line">    &quot;kind&quot;: &quot;Namespace&quot;,</span><br><span class="line">    &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">    &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由此可见，静态密码文件认证及静态令牌文件认证配置简单，很适合学习和试用Kubernetes的访问控制功能，但它们包含明文密码或令牌信息，且改动配置需要重启API Server，添加用户代价较大，不安全且不灵活，因此不建议用于生产环境。</p>
<h2 id="ServiceAccount及认证"><a href="#ServiceAccount及认证" class="headerlink" title="ServiceAccount及认证"></a>ServiceAccount及认证</h2><p>Kubernetes原生的程序一般能够直接与API Server进行交互（见图9-2），并进行资源状态的查询或更新，例如Flannel和CoreDNS等。API Server同样需要对这类来自Pod资源中的客户端程序进行身份验证，服务账户也是专用于这类场景的账号。ServiceAccount资源一般由用户身份信息及保存了认证信息的Secret对象组成。</p>
<h3 id="ServiceAccount自动化"><a href="#ServiceAccount自动化" class="headerlink" title="ServiceAccount自动化"></a><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218175532065.png" alt="image-20220218175532065">ServiceAccount自动化</h3><p>此前创建的每个Pod资源都自动关联了一个Secret存储卷，并由其容器挂载至/var/run/secrets/kubernetes.io/serviceaccount目录，下面的信息取自某Pod对象详细描述信息中自动挂载的存储卷相关的片段。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Containers:</span></span><br><span class="line"><span class="string">……</span></span><br><span class="line">    <span class="attr">Mounts:</span></span><br><span class="line">      <span class="string">/var/run/secrets/kubernetes.io/serviceaccount</span> <span class="string">from</span> <span class="string">default-token-</span> <span class="string">w54hg</span> <span class="string">(ro)</span></span><br><span class="line"><span class="string">……</span></span><br><span class="line"><span class="attr">Volumes:</span></span><br><span class="line">  <span class="attr">default-token-bq6zc:</span></span><br><span class="line">    <span class="attr">Type:</span>        <span class="string">Secret</span> <span class="string">(a</span> <span class="string">volume</span> <span class="string">populated</span> <span class="string">by</span> <span class="string">a</span> <span class="string">Secret)</span></span><br><span class="line">    <span class="attr">SecretName:</span>  <span class="string">default-token-</span> <span class="string">w54hg</span></span><br><span class="line">    <span class="attr">Optional:</span>    <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p><font color="red">该挂载点目录中通常存在3个文件：ca.crt、namespace和token，其中，token文件保存了ServiceAccount的认证令牌，容器中的进程使用该账户认证到API Server，进而由认证插件完成用户认证并将其用户名传递给授权插件。</font><br>每个Pod对象只有一个服务账户，若创建Pod资源时未予明确指定，则ServiceAccount准入控制器会为其自动附加当前名称空间中默认的服务账户，其名称通常为default。下面的命令显示了default这个服务账户的详细信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe serviceaccounts/default -n default</span></span><br><span class="line">Name:                default</span><br><span class="line">……</span><br><span class="line">Mountable secrets:   default-token-w54hg</span><br><span class="line">Tokens:              default-token-w54hg</span><br><span class="line">Events:              &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>Kubernetes系统通过3个独立的组件间相互协作实现了上面描述的Pod对象服务账户的自动化过程：<font color="red">ServiceAccount准入控制器、令牌控制器和ServiceAccount控制器。ServiceAccount控制器负责为名称空间管理相应的资源对象，它需要确保每个名称空间中都存在一个名为default的服务账户对象。ServiceAccount准入控制器内置在API Server中，负责在创建或更新Pod时按需进行ServiceAccount资源对象相关信息的修改，包括如下操作。</font></p>
<ul>
<li>若Pod没有显式定义使用的ServiceAccount对象，则将其设置为default。</li>
<li>若Pod显式引用了ServiceAccount，则负责检查被引用的对象是否存在，不存在时将拒绝Pod资源的创建请求。</li>
<li>若Pod中不包含ImagePullSecerts，则把ServiceAccount的ImagePullSecrets附加其上。</li>
<li>为带有访问API的令牌的Pod对象添加一个存储卷。</li>
<li>为Pod对象中的每个容器添加一个volumeMounts，将ServiceAccount的存储卷挂载至/var/run/secrets/kubernetes.io/serviceaccount。</li>
</ul>
<p>令牌控制器是控制平面组件Controller Manager中的一个专用控制器，它工作于异步模式，负责完成如下任务：</p>
<ul>
<li>监控ServiceAccount的创建操作，并为其添加用于访问API的Secret对象；</li>
<li>监控ServiceAccount的删除操作，并删除其相关的所有ServiceAccount令牌密钥；</li>
<li>监控Secret对象的添加操作，确保其引用的ServiceAccount存在，并在必要时为Secret对象添加认证令牌；</li>
<li>监控Secret对象的删除操作，以确保删除每个ServiceAccount对此Secret的引用。</li>
</ul>
<p>为确保完整性等，必须为kube-controller-manager使用–service-account-private-key-file选项指定一个私钥文件，用于对生成的ServiceAccount令牌进行签名，该私钥文件必须是PEM格式。同时，要使用–service-account-key-file为kube-apiserver指定与前面的私钥配对的公钥文件，实现在认证期间对认证令牌进行校验。</p>
<h3 id="ServiceAccount基础应用"><a href="#ServiceAccount基础应用" class="headerlink" title="ServiceAccount基础应用"></a>ServiceAccount基础应用</h3><p>ServiceAccount是Kubernetes API上的一种资源类型，它属于名称空间级别，用于让Pod对象内部的应用程序在与API Server通信时完成身份认证。如前所述，同样名为ServiceAccount的准入控制器实现了服务账户自动化，该准入控制器为每个名称空间都自动生成了一个名为default的默认资源对象。<br>每个Pod对象可附加其所属名称空间中的一个ServiceAccount资源，且只能附加一个。不过，一个ServiceAccount资源可由其所属名称空间中的多个Pod对象共享使用。创建Pod资源时，用户可使用spec.serviceAccountName属性直接指定要使用的ServiceAccount对象，或者省略此字段而由准入控制器自动附加当前名称空间中默认的ServiceAccount，以确保每个Pod对象至少基于该服务账户有权限读取当前名称空间中其他资源对象的元数据信息。<br>Kubernetes也支持用户按需创建ServiceAccount资源并将其指定到特定应用的Pod对象之上，结合集群启用的授权机制为该ServiceAccount资源赋予所需要的更多权限，从而构建出更加灵活的权限委派模型。</p>
<h4 id="命令式ServiceAccount资源创建"><a href="#命令式ServiceAccount资源创建" class="headerlink" title="命令式ServiceAccount资源创建"></a>命令式ServiceAccount资源创建</h4><p>kubectl create serviceaccount命令能够快速创建自定义的ServiceAccount资源，我们仅需要在命令后给出目标ServiceAccount资源的名称。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create serviceaccount my-service-account</span></span><br><span class="line">serviceaccount/my-service-account created</span><br></pre></td></tr></table></figure>

<p>Kubernetes会为创建的ServiceAccount资源自动生成并附加一个Secret对象，该对象以ServiceAccount资源名称为前缀，如下面命令的执行结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get serviceaccounts/my-service-account -o jsonpath=&#123;.secrets[0].name&#125;</span></span><br><span class="line">my-service-account-token-zjbxb</span><br></pre></td></tr></table></figure>

<p>该Secret对象属于特殊的kubernetes.io/service-account-token类型，它包含ca.crt、namespace和token这3个数据项，它们分别包含Kubernetes Root CA证书、Secret对象所属的名称空间和访问API Server的令牌。由Pod对象以Secret存储卷的方式将该类型的Secret对象挂载至/var/run/secrets/kubernetes.io/serviceaccount目录后，这3个数据项映射为同名的3个文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secrets/my-service-account-token-zjbxb -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5REN……</span><br><span class="line">  namespace: ZGVmYXVsdA==</span><br><span class="line">  token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJbXRwWkNJNklsUldZWFp……</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/service-account.name: my-service-account</span><br><span class="line">    kubernetes.io/service-account.uid: 0a7937c3-d4fd-4d1a-b685-3f775b3c1a21</span><br><span class="line">  ……</span><br><span class="line">type: kubernetes.io/service-account-token</span><br></pre></td></tr></table></figure>

<h4 id="ServiceAccount资源清单"><a href="#ServiceAccount资源清单" class="headerlink" title="ServiceAccount资源清单"></a>ServiceAccount资源清单</h4><p>事实上，以资源规范形式创建Secret对象时，以类似如上命令结果的形式，为Secret对象使用资源注解kubernetes.io/service-account.name引用一个现存的ServiceAccount对象，并指定资源类型为特定的kubernetes.io/service-account-token，我们便可以将指定的ServiceAccount对象引用的Secret对象予以置换，该Secret对象同样会自动生成固定的3个数据项。<br>更完善地创建ServiceAccount资源的方式是使用资源规范，该规范比较简单，它没有spec字段，而是将几个关键定义直接通过一级字段给出，具体的规范格式如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span>               <span class="comment"># ServiceAccount所属的API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span>         <span class="comment"># 资源类型标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 资源名称</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># ServiceAccount是名称空间级别的资源</span></span><br><span class="line"><span class="string">automountServiceAccountToken</span> <span class="string">&lt;boolean&gt;</span>   <span class="comment"># 是否让Pod自动挂载API令牌</span></span><br><span class="line"><span class="string">secrets</span> <span class="string">&lt;[]Object&gt;</span>           <span class="comment"># 以该SA运行的Pod要使用的Secret对象所组成的列表</span></span><br><span class="line">  <span class="string">apiVersion</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 引用的Secret对象所属的API群组及版本，可省略</span></span><br><span class="line">  <span class="string">kind</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 引用的资源类型，这里是指Secret，可省略</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 引用的Secret对象的名称，通常仅给出该字段即可</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 引用的Secret对象所属的名称空间</span></span><br><span class="line">  <span class="string">uid</span>  <span class="string">&lt;string&gt;</span>              <span class="comment"># 引用的Secret对象的标识符</span></span><br><span class="line"><span class="string">imagePullSecrets</span> <span class="string">&lt;[]Object&gt;</span>  <span class="comment"># 引用的用于下载Pod中容器镜像的Secret对象列表</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># docker-registry类型的Secret资源名称</span></span><br></pre></td></tr></table></figure>

<p>下面的配置清单是一个ServiceAccount资源示例，它位于serviceaccount-demo.yaml文件中，它仅指定了资源名称，以及允许Pod对象将其自动挂载为存储卷，引用的Secret对象则交由系统自动生成。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">namespace-admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">automountServiceAccountToken:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>将配置清单中定义的default-ns-admin资源创建到集群上，ServiceAccount控制器会自动为其附加以该资源名称为前缀的Secret对象，如下面的命令结果所示。随后，用户便可以在创建的Pod对象上引用该ServiceAccount对象，以借助权限管理机制实现自主控制Pod对象资源访问权限。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f serviceaccount-demo.yaml</span> </span><br><span class="line">serviceaccount/namespace-admin created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get serviceaccount/namespace-admin -o jsonpath=&#123;.secrets[0].name&#125;</span></span><br><span class="line">namespace-admin-token-mhdbn</span><br></pre></td></tr></table></figure>

<p>另外，ServiceAccount资源还可以基于spec.imagePullSecret字段附带一个由下载镜像专用的Secret资源组成的列表，让Pod对象在创建容器时且从私有镜像仓库下载镜像文件之前完成身份认证。下面的示例定义了一个从本地私有镜像仓库Harbor下载镜像文件时的Secret对象信息的ServiceAccount。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eshop-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">eshop</span></span><br><span class="line"><span class="attr">imagePullSecrets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">local-harbor-secret</span></span><br></pre></td></tr></table></figure>

<p>其中，local-harbor-secret是docker-registry类型的Secret对象，包含目标Docker Registry的服务入口、用户名、密码及用户的电子邮件等信息，它必须要由用户提前手动创建。该Pod资源所在节点上的kubelet进程使用Secret对象中的令牌认证到目标Docker Registry，以下载运行容器所需要的镜像文件。</p>
<h3 id="Pod资源上的服务账户"><a href="#Pod资源上的服务账户" class="headerlink" title="Pod资源上的服务账户"></a>Pod资源上的服务账户</h3><p>借助权限分配模型，按需应用“最小权限法则”将不同的资源操作权限配置给不同的账户，是有效降低安全风险的法则之一。有相当一部分Kubernetes原生应用程序依赖的权限都会大于从Pod默认ServiceAccount继承到的权限，且彼此间各有不同，为这类应用定制一个专用的ServiceAccount并授予所需的全部权限是主流的解决方案。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-with-sa</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">adminbox</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/admin-toolbox:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">serviceAccountName:</span> <span class="string">namespace-admin</span></span><br></pre></td></tr></table></figure>

<p>该Pod资源创建完成后会以Secret存储卷的形式自动挂载serviceaccounts/default-ns-admin的Secret对象，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-with-serviceaccount.yaml</span> </span><br><span class="line">pod/pod-with-sa created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/pod-with-sa -o jsonpath=<span class="string">&#x27;&#123;range .spec.volumes[*]&#125;&#123;.name&#125;&#123;end&#125;&#x27;</span></span></span><br><span class="line">namespace-admin-token-mhdbn</span><br></pre></td></tr></table></figure>

<p>Secret对象默认的挂载路径是/var/run/secrets/kubernetes.io/serviceaccount。与API Server交互时，工作负载进程会使用该目录下的ca.crt证书文件验证API Server的服务器证书是否为自己信任的证书颁发机构（所在集群的kubernetes-ca）所签发；验证服务器身份成功通过后，工作负载向API Server请求操作namespace文件指定的名称空间中的资源时，会将token文件中的令牌以承载令牌的认证方式提交给API Server进行验证，权限校验则由授权插件完成。我们可在pods/pod-with-sa的交互式接口中进行访问测试。</p>
<ul>
<li>1）切换到pods/pod-with-sa的adminbox容器的Secret对象的挂载点为工作目录以便于加载所需要的文件：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it pod-with-sa -- /bin/sh</span></span><br><span class="line">[root@pod-with-sa /]# cd /var/run/secrets/kubernetes.io/serviceaccount/</span><br></pre></td></tr></table></figure>

<ul>
<li>2）在容器中使用curl命令向API Server发起访问请求，–cacert选项用于指定验证服务器端的CA证书，而-H选项用于自定义头部，它指定了使用的承载令牌；下面的命令使用了“命令引用”机制来加载token和namespace文件的内容，其结果显示容器进程使用指定的ServiceAccount进行身份认证成功。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@pod-with-sa …]# curl --cacert ./ca.crt -H &quot;Authorization: Bearer $(cat ./token)&quot; \</span><br><span class="line">              https://kubernetes/api/v1/namespaces/$(cat ./namespace)/</span><br><span class="line">  ……</span><br><span class="line">  &quot;status&quot;: &quot;Failure&quot;,</span><br><span class="line">  &quot;message&quot;: &quot;……forbidden: User \&quot;system:serviceaccount:default:namespace-</span><br><span class="line">  admin\&quot;……&quot;</span><br><span class="line">  &quot;reason&quot;: &quot;Forbidden&quot;,</span><br><span class="line">  ……</span><br></pre></td></tr></table></figure>

<p>接下来，单独向serviceaccount/namespace-admin授予default名称空间的管理权限，pods/pod-with-sa中的进程便能借助该ServiceAccount的身份管理相应名称空间下的资源，这可以使用类似于向普通用户ilinux授权的方式进行，感兴趣的读者可自行进行测试。</p>
<ul>
<li>1）切换至kubectl管理终端运行如下资源创建命令：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create rolebinding namespace-admin-binding-admin --clusterrole=admin \</span></span><br><span class="line"><span class="language-bash">     --serviceaccount=default:namespace-admin -n default</span></span><br><span class="line">rolebinding.rbac.authorization.k8s.io/namespace-admin-binding-admin created</span><br></pre></td></tr></table></figure>

<ul>
<li>2）回到pods/pod-with-sa的adminbox容器中再次运行访问测试命令即可验证授权结果，如下命令表示namespace-admin用户已然有权限访问default名称空间。事实上，它拥有该名称空间中所有资源的CRUD权限。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@pod-with-sa /run/secrets/kubernetes.io/serviceaccount]# curl --cacert ./ca.crt \</span><br><span class="line">    -H &quot;Authorization: Bearer $(cat ./token)&quot; \</span><br><span class="line">    https://kubernetes/api/v1/namespaces/$(cat ./namespace)/</span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;Namespace&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;v1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;name&quot;: &quot;default&quot;,</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>但是，default名称空间引用了serviceaccounts/default资源中Pod的容器进程却不具有如上权限，因为它们并未获得相应的授权。事实上，kube-system名称空间中的许多应用都使用了专用的ServiceAccount资源，例如Flannel、CoreDNS、kube-proxy以及多种控制器等，感兴趣的读者可自行通过命令了解相应的ServiceAccount资源信息。</p>
<h2 id="X509数字证书认证"><a href="#X509数字证书认证" class="headerlink" title="X509数字证书认证"></a>X509数字证书认证</h2><p>X509数字证书认证常用的方式有“单向认证”和“双向认证”。SSL / TLS最常见的应用场景是将X.509数字证书与服务器端关联，但客户端不使用证书。单向认证是客户端能够验证服务端的身份，但服务端无法验证客户端的身份，至少不能通过SSL / TLS协议进行。之所以如此，是因为SSL / TLS安全性最初是为互联网应用开发，保护客户端是高优先级的需求，它可以让客户端确保目标服务器不会被冒名顶替，如图9-3所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183109261.png" alt="image-20220218183109261"></p>
<p>基于其他机制（如HTTP基本认证）验证客户端身份可能更容易些，而且这些机制没有生成和分发X.509数字证书的高昂开销。但安全性要求较高的场景中，使用组织私有的证书分发系统也一样能够借助数字证书完成客户端认证。图9-4展示了服务端与客户端的双向认证机制。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183126472.png" alt="image-20220218183126472"></p>
<p>双向认证的场景中，服务端与客户端需各自配备一套数字证书，并拥有信任的签证机构的证书列表。使用私有签证机构颁发的数字证书时，除了证书管理和分发，通常还要依赖用户手动将此私有签证机构的证书添加到信任的签证机构列表中。X509数字证书认证是Kubernetes默认使用的认证机制，采用双向认证模式。</p>
<h3 id="Kubernetes的X509数字证书认证体系"><a href="#Kubernetes的X509数字证书认证体系" class="headerlink" title="Kubernetes的X509数字证书认证体系"></a>Kubernetes的X509数字证书认证体系</h3><p>构建安全基础通信环境的Kubernetes集群时，需要用到PKI基础设施以完成独立HTTPS安全通信及X509数字证书认证的场景有多种，如图9-5所示。API Server是整个Kubernetes集群的通信网关，controller-manager、scheduler、kubelet及kube-proxy等API Server的客户端均需要经由API Server与etcd通信，完成资源状态信息获取及更新等。同样出于安全通信的目的，Master的各组件（API Server、controller-manager和scheduler）需要基于SSL/TLS向外提供服务，而且与集群内部组件间通信时（主要是各节点上的kubelet和kube-proxy）还需要进行双向身份验证。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183202775.png" alt="image-20220218183202775"></p>
<p>Kubernetes集群中存在3个需要独立完成X509数字证书认证和HTTPS通信的体系：一是etcd集群成员、服务器及其客户端；二是API Server及其客户端，以及kubelet API及其客户端；三是Kubernetes认证代理体系中的服务器和客户端。这3个独立的体系各自需要一个独立证书颁发机构为体系内的服务器和客户端颁发证书，完成体系内的组件身份认证同时又彼此隔离，如表9-1所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183221066.png" alt="image-20220218183221066"></p>
<p>（1）etcd集群CA及相关的数字证书</p>
<p>Kubernetes的API Server将集群的状态数据存储到集群存储服务etcd中，包括含有敏感数据的Secret资源对象。出于提升服务可用性、数据冗余及安全性等目的，生产环境通常应该配置有3、5或7个节点的etcd集群，集群内各节点间基于HTTPS协议进行通信，它们使用Peer类型的数字证书进行通信时的身份认证。而且，各etcd节点提供Server类型的数字证书与客户端建立安全连接，并验证其客户端Client类型的数字证书。Kubernetes集群各组件中，kube-apiserver是唯一一个可直接与集群存储通信的组件，它是etcd服务的客户端。<br>（2）Kubernetes集群CA及相关的数字证书<br>我们知道，Kubernetes集群的其他各组件均需要通过kube-apiserver访问集群资源，同样出于安全性等目的，API Server也要借助HTTPS协议与其客户端通信，而X509双向数字证书认证仅是API Server支持的认证方式中的一种，客户端也可能会使用HTTP Basic或Bearer Token认证方式接入到API Server。<br>另外，kubelet也通过HTTPS端点暴露了一组API，这些API提供了多个不同级别的敏感数据接口，并支持来自客户端的请求在节点和容器上执行不同级别的操作。默认情况下，匿名请求将自动隶属于system:unauthenticated用户组，其用户名为system:anonymous。不过，kubelet可使用–anonymous-auth=false选项拒绝匿名访问，并通过–client-ca-file选项指定CA方式验证客户端身份。kubelet可直接使用kubernetes-ca，同时应该为kube-apiserver使用–kubelet-client-certificate和–kubelet-client-key选项指定认证到kubelet的客户端证书与私钥。<br>（3）认证代理服务体系CA及相关的数字证书<br>API Server支持将认证功能交由外部的其他认证服务代为完成，这些服务通过特定的响应头部返回身份验证的结果状态，API Server扩展服务就是认证代理的最常见应用场景之一。<br>除了API Server提供的核心API，Kubernetes还支持通过聚合层（aggregation layer）对其进行扩展。简单来说，聚合层允许管理员在群集中部署使用其他Kubernetes风格的API，例如service catalog或用户自定义的API Server等。聚合层本身打包在kube-apiserver程序中，并作为进程的一部分运行，但仅在管理员通过指定的APIService对象注册扩展资源之后，它才会代理转发相应的请求。而APIService则会由运行在Kubernetes集群上的Pod中的extention-apiserver实现。<br>创建一个APIService资源时，作为注册和发现过程的一部分，kube-aggregator控制器（位于kube-apiserver内部）将与extention-apiserver的HTTP2连接，而后将经过身份验证的用户请求经由此连接代理到extention-apiserver上，于是，kube-aggregator被设置为执行RequestHeader客户端认证。<br>不过，只有kube-apiserver在启动时使用了如下选项时，才能启用其内置的聚合层：</p>
<ul>
<li>–requestheader-client-ca-file=&lt;path to aggregator CA cert&gt;</li>
<li>–requestheader-allowed-names=front-proxy-client</li>
<li>–requestheader-extra-headers-prefix=X-Remote-Extra-</li>
<li>–requestheader-group-headers=X-Remote-Group</li>
<li>–requestheader-username-headers=X-Remote-User</li>
<li>–proxy-client-cert-file=&lt;path to aggregator proxy cert&gt;</li>
<li>–proxy-client-key-file=&lt;path to aggregator proxy key&gt;</li>
</ul>
<p>proxy-client-cert-file和proxy-client-key-file包含kube-aggregator执行客户端证书身份验证的证书/密钥对，它使用requestheader-client-ca-file中指定的CA文件对聚合器证书进行签名。requestheader-allowed-names包含允许充当伪装前端代理的身份/名称列表（客户端证书中使用的CN），而requestheader-username-headers、requestheader-group-headers和requestheader-extraheaders-prefix携带一个HTTP头的列表，用于携带远程用户信息。<br>完整运行的Kubernetes系统需要为etcd、API Server及前端代理（front proxy）生成多个数字证书，如表9-2所示。<br>表9-2　Kubernetes上的数字证书</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183239124.png" alt="image-20220218183239124"></p>
<p>另外，其他集群上运行的应用（Pod）同其客户端的通信经由不可信的网络传输时也可能需要用到TLS/SSL协议，例如Nginx Pod与其客户端间的通信，客户端来自于互联网时，此处通常需配置一个公信的服务端证书。<br>显然，普通用户使用这种认证方式的前提是，它们各自拥有自己的数字证书，证书中的CN和O属性分别提供了准确的用户标识和用户组。API Server可接受或拒绝这些证书，评估标准在于证书是否由API Server信任的客户端证书CA（由选项–client-ca-file指定，默认为kubernetes-ca）所签发，但API Server自身并不了解这些证书，因此也不了解各个用户，它仅知道负责为各个客户端颁发证书的CA。因此，相较于静态密码文件认证和静态令牌文件认证来说，X509数字证书认证实现了用户管理与Kubernetes集群的分离，且有着更好的安全性。<br>X509数字证书认证因其可不依赖第三方服务、有着更好的安全性以及与API Server相分离等优势，成为Kubernetes系统内部默认使用的认证方式。但是，将X509数字证书用于普通用户认证的缺陷也是显而易见的，它主要表现在如下两个方面。</p>
<ul>
<li>证书的到期时间在颁发时设定，其生命周期往往很长（数月甚至数年），且事实上的身份验证功能也是在颁发时完成，若撤销用户的可用身份只能使用证书吊销功能完成。</li>
<li>现实使用中，证书通常由一些通用的签证机构签发，而API Server需要信任该CA；显然，获得该CA使用权限的用户便能够授予自己可认证到的Kubernetes的任意凭据或身份，因而集群管理员必须自行集中管理证书，这任务往往并不轻松。<br>对于大型组织来说，Kubernetes系统用户量大且变动频繁，静态密码文件和静态令牌文件认证方式动辄需要重启API Server，而X509认证中的证书维护开销较高且无法灵活变动凭据生效期限，因此这些认证方式都非理想选择。实践中，人们通常使用ID Token进行Kubernetes的普通用户身份认证，API Server的OpenID Connect令牌认证插件即用于该场景。</li>
</ul>
<h3 id="TLS-Bootstrapping机制"><a href="#TLS-Bootstrapping机制" class="headerlink" title="TLS Bootstrapping机制"></a>TLS Bootstrapping机制</h3><p>TLS Bootstrapping机制有什么用途呢？Kubernetes采用了由kubelet自行生成私钥和证书签署请求，而后发送给集群上的证书签署进程（CA），由管理员审核后予以签署或直接由控制器进程自动统一签署。这种方式就是kubelet TLS Bootstrapping机制。<br>然而，一旦开启TLS Bootstrapping功能，任何kubelet进程都可以向API Server发起验证请求并加入到集群中，包括那些非计划或非授权主机，这必将增大管理验证操作时的审核工作量。为此，API Server设计了可经由–enable-bootstrap-token-auth选项启用的Bootstrap Token（引导令牌）认证插件。该插件用于加强TLS Bootstrapping机制，仅那些通过Bootstrap Token认证的请求才可以使用TLS Bootstrapping发送证书签署请求给控制平面，并由相应的审批控制器（approval controller）完成证书签署和分发。<br>kubeadm启用了节点加入集群时的证书自动签署功能，因此加入过程在kubeadm join命令成功后即完成。<br>Kubelet会把签署后的证书及配对的私钥存储到–cert-dir选项指定的目录下，并以之生成kubeconfig格式的配置文件，该文件的保存路径以–kubeconfig选项指定，它保存有API Server的地址以及认证凭据。若指定的kubeocnfig配置文件不存在，kubelet会转而使用Bootstrap Token，从API Server自动请求完成TLS Bootstrapping过程。<br>kube-controller-manager内部有一个用于证书颁发的控制循环，它采用了类似于cfssl签证器格式的自动签证器，颁发的所有证书默认具有一年有效期限。正常使用中的Kubernetes集群需要在证书过期之前完成更新，以免集群服务不可用。较新版本的kubeadm部署工具已经能够自动完成更新，如下第一条命令用于检测证书有效期限，在接近过期时间的情况下，即可使用第二条命令进行更新。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">kubeadm alpha certs check-expiration</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash">kubeadm alpha certs renew all</span></span><br></pre></td></tr></table></figure>

<p>Kubernetes 1.8之后的版本中使用的csrapproving审批控制器内置于kube-controller-manager，并且默认为启用状态。此审批控制器使用SubjectAccessview API确认给定的用户是否有权限请求CSR（证书签署请求），而后根据授权结果判定是否予以签署。不过，为了避免同其他审批器冲突，内置的审批器并不显式拒绝CSR，而只是忽略它们。</p>
<h2 id="kubeconfig配置文件"><a href="#kubeconfig配置文件" class="headerlink" title="kubeconfig配置文件"></a>kubeconfig配置文件</h2><p>  Kubernetes设计了一种称为kubeconfig的配置文件，它保存有接入一到多个Kubernetes集群的相关配置信息，并允许管理员按需在各配置间灵活切换，如图9-6所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183729156.png" alt="image-20220218183729156"></p>
<p>客户端程序可通过默认路径、–kubeconfig选项或者KUBECONFIG环境变量自定义要加载的kubeconfig文件，从而能够在每次的访问请求中可认证到目标API Server。</p>
<h3 id="kubeconfig文件格式"><a href="#kubeconfig文件格式" class="headerlink" title="kubeconfig文件格式"></a>kubeconfig文件格式</h3><p>kubeconfig文件中，各集群的接入端点以列表形式定义在clusters配置段中，每个列表项代表一个Kubernetes集群，并拥有名称标识；各身份认证信息（credentials）定义在users配置段中，每个列表项代表一个能够认证到某Kubernetes集群的凭据。将身份凭据与集群分开定义以便复用，具体使用时还要以context（上下文）在二者之间按需建立映射关系，各context以列表形式定义在contexts配置段中，而当前使用的映射关系则定义在current-context配置段中。kubeconfig文件的格式如图9-7所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218183747264.png" alt="image-20220218183747264"></p>
<p>使用kubeadm初始化Kubernetes集群过程中，在Master节点上生成的/etc/kubernetes/admin.conf文件就是一个kubeconfig格式的文件，它由kubeadm init命令自动生成，可由kubectl加载后接入当前集群的API Server。kubectl加载kubeconfig文件的默认路径为$HOME/.kube/config，在kubeadm init命令初始化集群过程中有一个步骤便是将/etc/kubernetes/admin.conf复制为该默认搜索路径上的文件。当然，我们也可以通过–kubeconfig选项或KUBECONFIG环境变量将其修改为其他路径。<br>kubectl config view命令能打印kubeconfig文件的内容，下面的命令结果显示了默认路径下的文件配置，包括集群列表、用户列表、上下文列表以及当前使用的上下文（current-context）等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config view</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: DATA+OMITTED</span><br><span class="line">    server: https://k8s-api.ilinux.io:6443</span><br><span class="line">  name: kubernetescontexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kubernetes</span><br><span class="line">    user: kubernetes-admin</span><br><span class="line">  name: kubernetes-admin@kubernetescurrent-context: kubernetes-admin@kubernetesusers:</span><br><span class="line">- name: kubernetes-admin</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: REDACTED</span><br><span class="line">    client-key-data: REDACTED</span><br></pre></td></tr></table></figure>

<p>用户也可以在kubeconfig配置文件中按需自定义相关的配置信息，以实现使用不同的用户账户接入集群等功能。kubeconfig是一个文本文件，尽管可以使用文本处理工具直接编辑它，但强烈建议用户使用kubectl config及其子命令进行该文件的设定，以便利用其他自动进行语法检测等额外功能。kubectl config的常用子命令有如下几项。</p>
<ul>
<li>view：打印kubeconfig文件内容。</li>
<li>set-cluster：设定新的集群信息，以单独的列表项保存于clusters配置段。</li>
<li>set-credentials：设置认证凭据，保存为users配置段的一个列表项。</li>
<li>set-context：设置新的上下文信息，保存为contexts配置段的一个列表项。</li>
<li>use-context：设定current-context配置段，确定当前以哪个用户的身份接入到哪个集群之中。</li>
<li>delete-cluster：删除clusters中指定的列表项。</li>
<li>delete-context：删除contexts中指定的列表项。</li>
<li>get-clusters：获取clusters中定义的集群列表。</li>
<li>get-contexts：获取contexts中定义的上下文列表。</li>
</ul>
<p>kubectl config命令的相关操作将针对加载的单个kubeconfig文件进行，它根据其优先级由高到低，依次搜索–kubeconfig选项指定的文件、KUBECONFIG环境变量指定的文件和默认的.HOME/.kube/config文件，以其中任何一种方式加载到配置文件后即可终止搜索过程。不过，kubectl config命令支持同时使用多个kubeconfig文件，以及将多个配置文件合并为一个。</p>
<h3 id="自定义kubeconfig文件"><a href="#自定义kubeconfig文件" class="headerlink" title="自定义kubeconfig文件"></a>自定义kubeconfig文件</h3><p>通常，一个完整kubeconfig配置的定义至少应该包括集群、身份凭据、上下文及当前上下文4项，但在保存有集群和身份凭据的现有kubeconfig文件基础上添加新的上下文时，可能只需要提供身份凭据而复用已有的集群定义，具体的操作步骤要按实际情况进行判定。<br>例如，我们下面尝试创建一个新的kubeconfig文件，设定它使用此前定义的基于静态密码文件认证的ilinux用户接入到现有的Kubernetes集群，该集群API Server的网络端点为<a href="https://k8s-api.ilinux.io:6443，相关的CA证书保存在Master节点上的/etc/kubernetes/pki/ca.crt文件中，而配置结果则使用--kubeconfig选项保存在当前用户主目录下的.kube/kube-dev.config文件中。">https://k8s-api.ilinux.io:6443，相关的CA证书保存在Master节点上的/etc/kubernetes/pki/ca.crt文件中，而配置结果则使用--kubeconfig选项保存在当前用户主目录下的.kube/kube-dev.config文件中。</a><br>步骤1：添加集群配置，包括集群名称、API Server URL和信任的CA的证书；clusters配置段中的各列表项名称需要唯一。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-cluster kube-dev --embed-certs=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">    --certificate-authority=/etc/kubernetes/pki/ca.crt \</span></span><br><span class="line"><span class="language-bash">    --server=<span class="string">&quot;https://k8s-api.ilinux.io:6443&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Cluster &quot;kube-dev&quot; set.</span><br></pre></td></tr></table></figure>

<p>步骤2：添加身份凭据，使用静态密码文件认证的客户端提供用户名和密码即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-credentials ilinux \</span></span><br><span class="line"><span class="language-bash">    --username=ilinux --password=ilinux@MageEdu \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">User &quot;ilinux&quot; set.</span><br></pre></td></tr></table></figure>

<p>步骤3：以用户ilinux的身份凭据与kube-dev集群建立映射关系。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-context ilinux@kube-dev \</span></span><br><span class="line"><span class="language-bash">    --cluster=kube-dev --user=ilinux \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Context &quot;ilinux@kube-dev&quot; created.</span><br></pre></td></tr></table></figure>

<p>步骤4：设置当前上下文为ilinux@kube-dev。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config use-context ilinux@kube-dev --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Switched to context &quot;ilinux@kube-dev&quot;.</span><br></pre></td></tr></table></figure>

<p>步骤5：预览kube-dev.config文件，确认其配置信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config view --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br></pre></td></tr></table></figure>

<p>步骤6：使用该kubeconfig中的当前上下文进行测试访问；该用户仅被授权了default名称空间的所有权限，因而不具有列出集群级别资源的权限，但能查看default名称空间的状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Error from server (Forbidden): namespaces is forbidden: User &quot;ilinux&quot; cannot list resource &quot;namespaces&quot; in API group &quot;&quot; at the cluster scope</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces/default --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">NAME       STATUS     AGE</span><br><span class="line">default    Active     3d</span><br></pre></td></tr></table></figure>

<p>上面的第6步确认了自定义配置中的ilinux用户有效可用，它被API Server借助静态密码文件认证插件完成认证并标识为ilinux用户，从而拥有该用户的资源操作权限。为了进一步测试并了解kubeconfig的使用格式，下面把基于令牌文件认证的ik8s用户添加进同一个kubeconfig文件中。ik8s用户同ilinux用户位于同一集群上，因此，我们可省略添加集群的步骤而直接复用它。<br>步骤7：添加身份凭据，使用静态令牌文件认证的客户端认证时只需要提供静态令牌信息；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">TOKEN=$(sudo awk -F <span class="string">&quot;,&quot;</span> <span class="string">&#x27;$2==&quot;ik8s&quot;&#123;print $1&#125;&#x27;</span> /etc/kubernetes/authfiles/token.csv)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-credentials ik8s --token=<span class="string">&quot;<span class="variable">$TOKEN</span>&quot;</span> \</span></span><br><span class="line"><span class="language-bash">      --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">User &quot;ik8s&quot; set.</span><br></pre></td></tr></table></figure>

<p>步骤8：为用户ik8s的身份凭据与kube-dev集群建立映射关系。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-context ik8s@kube-dev \</span></span><br><span class="line"><span class="language-bash">    --cluster=kube-dev --user=ik8s \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Context &quot;ik8s@kube-dev&quot; created.</span><br></pre></td></tr></table></figure>

<p>步骤9：将当前上下文切换为ik8s@kube-dev。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config use-context ik8s@kube-dev --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">Switched to context &quot;ik8s@kube-dev&quot;.</span><br></pre></td></tr></table></figure>

<p>步骤10：预览kube-dev.config文件，确认ik8s用户相关的各项配置信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config view --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br></pre></td></tr></table></figure>

<p>步骤11：依旧使用当前上下文发起集群访问测试，ik8s用户未获得任何授权，但它能够被系统识别为ik8s用户，这表示身份认证请求成功返回；我们这次使用kubectl的whoami插件进行测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl whoami --kubeconfig=$HOME/.kube/kube-dev.config</span><br><span class="line">ik8s</span><br></pre></td></tr></table></figure>

<p><font color="red">除了静态令牌，客户端认证到API Server的各种令牌都可以使用前面的这种方式添加到kubeconfig文件中，包括ServiceAccount令牌、OpenID Connnect令牌和Bootstrap令牌等。</font><br>当kubectl引用了拥有两个及以上context的kubeconfig文件时，可随时通过kubectl config use-context命令在不同上下文之间切换，它们可能使用不同的身份凭据接入相同的集群或不同的集群之上。如下命令结果表示当前加载的配置文件中共有两个context，而拥有星号标识的是当前使用的context，即current-context。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config get-contexts --kubeconfig=<span class="variable">$HOME</span>/.kube/kube-dev.config</span></span><br><span class="line">CURRENT   NAME               CLUSTER    AUTHINFO   NAMESPACE</span><br><span class="line">*         ik8s@kube-dev      kube-dev   ik8s       </span><br><span class="line">          ilinux@kube-dev    kube-dev   ilinux</span><br></pre></td></tr></table></figure>

<p>实践中，API Server支持的X509数字证书认证和OpenID Connect令牌认证才是客户端使用最多的认证方式，我们后面使用一节的篇幅来专门介绍它们的配置。</p>
<h3 id="X509数字证书身份凭据"><a href="#X509数字证书身份凭据" class="headerlink" title="X509数字证书身份凭据"></a>X509数字证书身份凭据</h3><p>kubeadm部署Kubernetes集群的过程中会自动生成多个kubeconfig文件，它们是默认位于/etc/kubernetes目录下以.conf为后缀名的文件，前缀名称代表了它的适用场景，其中的admin.conf中保存了以X509数字证书格式提供身份凭据的kubernetes-admin用户，该用户能够以管理员的身份对当前集群发起资源操作请求。<br>由kubeadm初始化的Kubernetes集群上，kube-apiserver默认信任的CA就是集群自己的kubernetes-ca，该CA的数字证书是Master节点之上的/etc/kubernetes/pki/ca.crt文件。于是，客户端按需生成证书签署请求，再由管理员通过kubernetes-ca为客户端签署证书，便可让客户端以其证书中的CN为用户名认证到API Server上。为了便于说明问题，下面将客户端生成私钥和证书签署请求，服务器签署该请求，以及客户端将证书配置为kubeconfig文件的步骤统一进行说明，所有操作都在Master节点上运行。<br>步骤1：以客户端的身份，生成目标用户账号mason的私钥及证书签署请求，保存在用户主目录下的.certs目录中。<br>① 生成私钥文件，注意其权限应该为600以阻止其他用户读取。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">mkdir</span> <span class="variable">$HOME</span>/.certs</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">(<span class="built_in">umask</span> 077; openssl genrsa -out <span class="variable">$HOME</span>/.certs/mason.key 2048)</span></span><br></pre></td></tr></table></figure>

<p>② 创建证书签署请求，-subj选项中CN的值将被API Server识别为用户名，O的值将被识别为用户组。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl req -new -key <span class="variable">$HOME</span>/.certs/mason.key \</span></span><br><span class="line"><span class="language-bash">-out <span class="variable">$HOME</span>/.certs/mason.csr \</span></span><br><span class="line"><span class="language-bash">-subj <span class="string">&quot;/CN=mason/O=developers&quot;</span></span></span><br></pre></td></tr></table></figure>

<p>步骤2：以kubernetes-ca的身份签署ikubernetes的证书请求，这里直接读取相关的CSR文件，并将签署后的证书仍然保存在当前系统用户主目录下的.certs中。<br>① 基于kubernetes-ca签署证书，并为其设置合理的生效时长，例如365天。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">sudo openssl x509 -req -days 365 -CA /etc/kubernetes/pki/ca.crt \</span></span><br><span class="line"><span class="language-bash">-CAkey /etc/kubernetes/pki/ca.key -CAcreateserial \</span></span><br><span class="line"><span class="language-bash">-<span class="keyword">in</span> <span class="variable">$HOME</span>/.certs/mason.csr -out <span class="variable">$HOME</span>/.certs/mason.crt</span></span><br><span class="line">Signature ok</span><br><span class="line">subject=CN = mason, O = developers</span><br><span class="line">Getting CA Private Key</span><br></pre></td></tr></table></figure>

<p>② 必要时，还可以验证生成的数字证书的相关信息（可选）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl x509 -<span class="keyword">in</span> <span class="variable">$HOME</span>/.certs/mason.crt -text -noout</span></span><br></pre></td></tr></table></figure>

<p>步骤3：以ikubernetes的身份凭据生成kubeconfig配置，将其保存在kubectl默认搜索路径指向的$HOME/.kube/config文件中。另外，因指向当前集群的配置项已经存在，即位于clusters配置段中的kubernetes，这里直接复用该集群定义。<br>① 根据X509数字证书及私钥创建身份凭据，列表项名称同目标用户名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-credentials mason --embed-certs=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">  --client-certificate=<span class="variable">$HOME</span>/.certs/mason.crt \</span></span><br><span class="line"><span class="language-bash">  --client-key=<span class="variable">$HOME</span>/.certs/mason.key</span>          </span><br><span class="line">User &quot;mason&quot; set.</span><br></pre></td></tr></table></figure>

<p>② 配置context，以mason的身份凭据访问已定义的Kubernetes集群，该context的名称为</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mason@kubernetes。</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-context mason@kubernetes --cluster=kubernetes --user=mason</span></span><br><span class="line">Context &quot;mason@kubernetes&quot; created.</span><br></pre></td></tr></table></figure>

<p>③ 将当前上下文切换为mason@kubernetes，或直接在kubectl命令上使用“–context= ‘mason@kubernetes’”以完成该用户的认证测试，下面的命令选择了以第二种方式进行认证，虽然提示权限错误，但mason用户已被API Server正确识别；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces/default --context=<span class="string">&#x27;mason@kubernetes&#x27;</span></span></span><br><span class="line">Error from server (Forbidden): namespaces &quot;default&quot; is forbidden: User &quot;mason&quot; cannot get resource &quot;namespaces&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</span><br></pre></td></tr></table></figure>

<p>以上，我们通过创建自定义的数字证书，实现了将mason用户认证到API Server，并将该用户的身份凭据保存至kubeconfig文件中。</p>
<h3 id="多kubeconfig文件与合并"><a href="#多kubeconfig文件与合并" class="headerlink" title="多kubeconfig文件与合并"></a>多kubeconfig文件与合并</h3><p>至此，我们可以看到kubectl config一次仅能使用单个kubeconfig文件。事实上，若将两个文件路径以冒号分隔并赋值给KUBECONFIG环境变量，也能够让kubectl config一次加载多个文件信息，优先级由高到低为各文件自左而右的次序。下面两条命令的结果显示，左侧文件拥有较高的优先级，因而其配置的current-context成为默认使用的context。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">export</span> KUBECONFIG=<span class="string">&quot;<span class="variable">$HOME</span>/.kube/config:<span class="variable">$HOME</span>/.kube/kube-dev.config&quot;</span></span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config get-contexts</span></span><br><span class="line">CURRENT   NAME             CLUSTER         AUTHINFO     NAMESPACE</span><br><span class="line">      ik8s@kube-dev                 kube-dev     ik8s               </span><br><span class="line">      ilinux@kube-dev               kube-dev     ilinux             </span><br><span class="line">*     kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   </span><br><span class="line">      mason@kubernetes              kubernetes   mason</span><br></pre></td></tr></table></figure>

<p>联合使用多个kubeconfig时，同样可以按需调整当前使用的context，其实现方式同使用单个kubeconfig文件并没有不同之处。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config use-context mason@kubernetes</span></span><br><span class="line">Switched to context &quot;mason@kubernetes&quot;.</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config get-contexts</span>                                       </span><br><span class="line">CURRENT   NAME         CLUSTER      AUTHINFO      NAMESPACE</span><br><span class="line">          ik8s@kube-dev                 kube-dev     ik8s               </span><br><span class="line">          ilinux@kube-dev               kube-dev     ilinux             </span><br><span class="line">          kubernetes-admin@kubernetes   kubernetes   kubernetes-admin   </span><br><span class="line">*         mason@kubernetes              kubernetes   mason</span><br></pre></td></tr></table></figure>

<p>kubectl config view命令会将多个配置文件的内容按给定的次序连接并输出，其风格类似于Linux系统的cat命令。但我们也能够在view命令中将加载的多个配置文件展平为单个配置文件的格式予以输出，并将结果保存在指定路径下便能将多个kubeconfig文件合并为一。例如，下面的命令便将KUBECONFIG环境变量中指定的两个kubeconfig文件合并成了单个配置文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config view --merge --flatten  &gt; <span class="variable">$HOME</span>/.kube/kube.config</span></span><br></pre></td></tr></table></figure>

<p>此时，切换kubectl config加载新生成的kubeconfig配置文件，它便直接拥有了此前两个文件中定义的所有信息，且current-context亦遵循此前命令中的设定，即mason@kubernetes。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">KUBECONFIG=<span class="string">&quot;<span class="variable">$HOME</span>/.kube/kube.config&quot;</span></span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config get-contexts</span></span><br></pre></td></tr></table></figure>

<p>后面章节会继续用到ilinux、ik8s和mason等多个用户来测试授权结果，因而这里直接把$HOME/.kube目录下合并生成的kube.config覆盖到默认的config，以便能随时切换到各用户。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">unset</span> KUBECONFIG</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">cp</span> <span class="variable">$HOME</span>/.kube/kube.config <span class="variable">$HOME</span>/.kube/config</span></span><br></pre></td></tr></table></figure>

<p>为了不影响后续的操作需求，我们这里还把context切换回集群管理员kubernetes-admin@kubernetes。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-context kubernetes-admin@kubernetes</span></span><br><span class="line">Switched to context &quot;kubernetes-admin@kubernetes&quot;.</span><br></pre></td></tr></table></figure>

<h2 id="基于角色的访问控制：RBAC"><a href="#基于角色的访问控制：RBAC" class="headerlink" title="基于角色的访问控制：RBAC"></a>基于角色的访问控制：RBAC</h2><p>DAC（自主访问控制）、MAC（强制访问控制）、RBAC（基于角色的访问控制）和ABAC（基于属性的访问控制）这4种主流的权限管理模型中，Kubernetes支持使用后两种完成普通账户和服务账户的权限管理，另外支持的权限管理模型还有Node和Webhook两种。<br>RBAC是一种新型、灵活且使用广泛的访问控制机制，它将权限授予角色，通过让“用户”扮演一到多个“角色”完成灵活的权限管理，这有别于传统访问控制机制中将权限直接赋予使用者的方式。相对于Kubernetes支持的ABAC和Webhook等授权机制，RBAC具有如下优势。</p>
<ul>
<li>对集群中的资源和非资源型URL的权限实现了完整覆盖。</li>
<li>整个RBAC完全由少数几个API对象实现，而且与其他API对象一样可以用kubectl或API调用进行操作。</li>
<li>支持权限的运行时调整，无须重新启动API Server。提示</li>
</ul>
<h3 id="RBAC授权模型"><a href="#RBAC授权模型" class="headerlink" title="RBAC授权模型"></a>RBAC授权模型</h3><p>RBAC是一种特定的权限管理模型，它把可以施加在“资源对象”上的“动作”称为“许可权限”，这些许可权限能够按需组合在一起构建出“角色”及其职能，并通过为“用户账户或组账户”分配一到多个角色完成权限委派。这些能够发出动作的用户在RBAC中也称为“主体”。图9-8展现了RBAC中用户、角色与权限之间的关系。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218184626677.png" alt="image-20220218184626677">RBAC就是一种访问控制模型，它以角色为中心界定“谁”（subject）能够“操作”（verb）哪个或哪类“对象”（object）。动作的发出者即“主体”，通常以“账号”为载体，在Kubernetes系统上，它可以是普通账户，也可以是服务账户。“动作”用于表明要执行的具体操作，包括创建、删除、修改和查看等行为，对于API Server来说，即PUT、POST、DELETE和GET等请求方法。而“对象”则是指管理操作能够施加的目标实体，对Kubernetes API来说主要指各类资源对象以及非资源型URL。<br>API Server是RESTful风格的API，各类客户端由认证插件完成身份验证，而后通过HTTP协议的请求方法指定对目标对象的操作请求，并由授权插件进行授权检查，而操作的对象则是URL路径指定的REST资源。表9-3给出了HTTP方法和API Server资源操作的对应关系。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218184647841.png" alt="image-20220218184647841">Kubernetes系统上的普通账户或服务账户向API Server发起资源操作请求，并以相应HTTP方法承载，如图9-9所示，由运行在API Server之上的授权插件RBAC进行鉴权。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218184704051.png" alt="image-20220218184704051"><font color="red">Kubernetes系统的RBAC授权插件将角色分为Role和ClusterRole两类，它们都是Kubernetes内置支持的API资源类型，其中Role作用于名称空间级别，用于承载名称空间内的资源权限集合，而ClusterRole则能够同时承载名称空间和集群级别的资源权限集合。Role无法承载集群级别的资源类型的操作权限，这类的资源包括集群级别的资源（例如Nodes）、非资源类型的端点（例如/healthz），以及作用于所有名称空间的资源（例如跨名称空间获取任何资源的权限)等。</font><br>利用Role和ClusterRole两类角色进行赋权时，需要用到另外两种资源RoleBinding和ClusterRoleBinding，它们同样是由API Server内置支持的资源类型。RoleBinding用于将Role绑定到一个或一组用户之上，它隶属于且仅能作用于其所在的单个名称空间。RoleBinding可以引用同一名称中的Role，也可以引用集群级别的ClusterRole，但引用ClusterRole的许可权限会降级到仅能在RoleBinding所在的名称空间生效。而ClusterRoleBinding则用于将ClusterRole绑定到用户或组，它作用于集群全局，且仅能够引用ClusterRole。四者之间的关系如图9-10所示。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218184720792.png" alt="image-20220218184720792"></p>
<p>图9-10中，全局作用范围的User2因通过A名称空间中的RoleBinding关联至Role-A上，因而它仅能在NamespaceA名称空间中发挥作用。名称空间B中的ServiceAccount1通过RoleBinding关联至集群级别的ClusterRole-M上，对该账户来说，ClusterRole-M上的操作权限也仅限于该名称空间。全局级别的用户User1通过ClusterRoleBindig关联到ClusterRole-M，因而，该用户将在集群级别行使该角色的权限。<br>通常，我们可以把Kubernetes集群用户大体规划为集群管理员、名称空间管理员和用户（通常为开发人员）3类。</p>
<ul>
<li>集群管理员可以创建、读取、更新和删除任何策略对象，能够创建命名空间并将其分配给名称空间管理员；此角色适用于在整个集群中管理所有租户或项目的管理员。</li>
<li>名称空间管理员可以管理其名称空间中的用户，此角色适用于特定单一租户或项目的管理员。</li>
<li>开发者用户可以创建、读取、更新和删除名称空间内的非策略对象，如Pod、Job和Ingress等，但只在它们有权访问的名称空间中拥有这些权限。</li>
</ul>
<h3 id="Role与ClusterRole"><a href="#Role与ClusterRole" class="headerlink" title="Role与ClusterRole"></a>Role与ClusterRole</h3><p>Role和ClusterRole是API Server内置的两种资源类型，它们在本质上都只是一组许可权限的集合。Role和ClusterRole的资源规范完全相同，该规范没有使用spec字段，而是直接使用rules字段嵌套授权规则列表。规则的基本要素是动作（verb）和相关的目标资源，后者支持指定一个或多个资源类型、特定资源类型下的单个或多个具体的资源，以及非资源类型的URL等。在Role和ClusterRole资源上定义的rules也称为PolicyRule，即策略规则，它可以内嵌的字段有如下几个。</p>
<blockquote>
<p>1）apiGroups &lt;[]string&gt;：目标资源的API群组名称，支持列表格式指定多个组，空值（””）表示核心群组。<br>2）resources &lt;[]string&gt;：规则应用的目标资源类型，例如pods、services、deployments和daemonsets等，未同时使用resourceNames字段时，表示指定类型下的所有资源。ResourceAll表示所有资源。<br>3）resourceNames &lt;[]string&gt;：可选字段，指定操作适用的具体目标资源名称。<br>4）nonResourceURLs &lt;[]string&gt;：用于定义用户有权限访问的网址列表，它并非名称空间级别的资源，因此只能应用于ClusterRole，Role支持此字段仅是为了格式上的兼容；该字段在一条规则中与resources和resourceNames互斥。<br>5）verbs &lt;[]string&gt;：可应用在此规则匹配到的所有资源类型的操作列表，可用选项有get、list、create、update、patch、watch、proxy、redirect、delete和deletecollection；此为必选字段。</p>
</blockquote>
<p>下面的配置清单示例（pods-reader-rbac.yaml）在default名称空间中定义了一个名称为Role的资源，它设定了读取、列出及监视pods和services资源，以及pods/log子资源的许可权限。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pods-reader</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]   <span class="comment"># &quot;&quot; 表示核心API群组</span></span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;pods&quot;</span>, <span class="string">&quot;services&quot;</span>, <span class="string">pods/log&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>绝大多数资源可通过其资源类型的名称引用，例如pods或services等，这些名称与它们在API endpoint中的形式相同。另外，有些资源类型支持子资源，例如Pod对象的/log，Node对象的/status等，它们在API Server上的URL形如下面的表示格式。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/api/v1/namespaces/&#123;namespace&#125;/pods/&#123;name&#125;/log</span><br></pre></td></tr></table></figure>

<p>RBAC角色引用这种类型的子资源时需要使用resource/subresource的格式，例如上面示例规则中的pods/log。另外，还可以通过直接给定资源名称（resourceName）来引用特定的资源，但此时仅支持get、delete、update和patch等。<br>ClusterRole资源隶属于集群级别，它引用名称空间级别的资源意味着相关的操作权限能够在所有名称空间生效，同时，它也能够引用Role所不支持的集群级别的资源类型，例如nodes和persistentvolumes等。下面的清单示例（nodes-admin-rbac.yaml）定义了ClusterRole资源nodes-admin，它拥有管理集群节点信息的权限。ClusterRole不属于名称空间，所以其配置不能够使用metadata.namespace字段。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nodes-admin</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;*&quot;</span>]</span><br></pre></td></tr></table></figure>

<p>将上面两个清单中分别定义的Role和ClusterRole资源创建到集群上，以便按需调用并验证其权限。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pods-reader-rbac.yaml -f nodes-admin-rbac.yaml</span> </span><br><span class="line">role.rbac.authorization.k8s.io/pods-reader created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/nodes-admin created</span><br></pre></td></tr></table></figure>

<p>Role或ClusterRole资源的详细描述能够以比较直观的方式打印相关的规则定义，图9-11就是由kubectl describe roles/pods-reader clusterroles/nodes-admin命令输出的规则定义。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218190623307.png" alt="image-20220218190623307"></p>
<p>另外，kubectl命令也分别提供了创建Role和ClusterRole资源的命令式命令，create role和create clusterrole，它们支持如下几个关键选项。</p>
<ul>
<li>–verb：指定可施加于目标资源的动作，支持以逗号分隔的列表值，也支持重复使用该选项分别指定不同的动作，例如–verb=get,list,watch，或者–verb=get –verb=list –verb=watch。<br>+</li>
<li>–resource：指定目标资源类型，使用格式类似于–verb选项。</li>
<li>–resource-name：指定目标资源，使用格式类似于–verb选项。</li>
<li>–non-resource-url：指定非资源类型的URL，使用格式类似于–verb选项，但仅适用于clusterrole资源。</li>
</ul>
<p>例如，下面的第一条命令创建了dev名称空间，第二条命令在该名称空间创建了一个具有所有资源管理权限的roles/admin资源，第三条命令则创建了一个有PVC和PV资源管理权限的clusterroles/pv-admin资源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create namespace dev</span> </span><br><span class="line">namespace/dev created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create role admin -n dev --resource=<span class="string">&quot;*.*&quot;</span> \</span></span><br><span class="line"><span class="language-bash">         --verb=<span class="string">&quot;get,list,watch,create,delete,deletecollection,patch,update&quot;</span></span></span><br><span class="line">role.rbac.authorization.k8s.io/admin created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create clusterrole pv-admin --verb=<span class="string">&quot;*&quot;</span> \</span></span><br><span class="line"><span class="language-bash">         --resource=<span class="string">&quot;persistentvolumeclaims,persistentvolumes&quot;</span></span></span><br><span class="line">clusterrole.rbac.authorization.k8s.io/pv-admin created</span><br></pre></td></tr></table></figure>

<p>但是，Role或ClusterRole对象本身并不能作为动作的执行主体，它们需要“绑定”到主体（例如User、Group或Service Account）之上完成赋权，而后由相应主体执行资源操作。</p>
<h3 id="RoleBinding与ClusterRoleBinding"><a href="#RoleBinding与ClusterRoleBinding" class="headerlink" title="RoleBinding与ClusterRoleBinding"></a>RoleBinding与ClusterRoleBinding</h3><p>RoleBinding负责在名称空间级别向普通账户、服务账户或组分配Role或ClusterRole，而ClusterRoleBinding则只能用于在集群级别分配ClusterRole。但二者的配置规范格式完全相同，它们没有spec字段，直接使用subjects和roleRef两个嵌套的字段。其中，subjects的值是一个对象列表，用于给出要绑定的主体，而roleRef的值是单个对象，用于指定要绑定的Role或ClusterRole资源。subjects字段的可嵌套字段如下。</p>
<ul>
<li>apiGroup &lt;string&gt;：要引用的主体所属的API群组，对于ServiceAccount类的主体来说默认为””，而User和Group类主体的默认值为”rbac.authorization.k8s.io”。</li>
<li>kind &lt;string&gt;：要引用的资源对象（主体）所属的类别，可用值为User、Group和ServiceAccount，必选字段。</li>
<li>name &lt;string&gt;：引用的主体的名称，必选字段。</li>
<li>namespace &lt;string&gt;：引用的主体所属的名称空间，对于非名称空间类型的主体，例如User和Group，其值必须为空，否则授权插件将返回错误信息。<br>roleRef的可嵌套字段如下。</li>
<li>apiGroup &lt;string&gt;：引用的资源（Role或ClusterRole）所属的API群组，必选字段。</li>
<li>kind &lt;string&gt;：引用的资源所属的类别，可用值为Role或ClusterRole，必选字段。</li>
<li>name &lt;string&gt;：引用的资源（Role或ClusterRole）的名称。</li>
</ul>
<p>需要注意的是，RoleBinding仅能够引用同一名称空间中的Role资源，例如下面配置清单中的RoleBindings在dev名称空间中把admin角色分配给用户mason，从而mason拥有了此角色之上的所有许可授权。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mason-admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mason</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Role</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br></pre></td></tr></table></figure>

<p>把示例中的RoleBinding资源mason-admin创建到集群上，便能够以该用户的身份测试其继承而来的权限是否已然生效。下面以–context选项临时将用户切换为mason@kubernetes进行资源管理，测试命令及结果显示，mason已然具有dev名称空间下的资源操作权限。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run demoapp --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n dev --context=<span class="string">&quot;mason@kubernetes&quot;</span></span></span><br><span class="line">pod/demoapp created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get all -n dev --context=<span class="string">&quot;mason@kubernetes&quot;</span></span>  </span><br><span class="line">NAME         READY    STATUS    RESTARTS      AGE</span><br><span class="line">pod/demoapp   1/1     Running      0          52s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods/demoapp -n dev --context=<span class="string">&quot;mason@kubernetes&quot;</span></span></span><br><span class="line">pod &quot;demoapp&quot; deleted</span><br></pre></td></tr></table></figure>

<p>RoleBinding也能够为主体分配集群角色，但它仅能赋予主体访问RoleBinding资源本身所在的名称空间之内的、由ClusterRole所持有的权限。例如，对于具有PVC和PV管理权限的clusterroles/pv-admin来说，在dev名称空间中使用RoleBinding将其分配给用户mason，意味着mason仅对dev名称空间下的PVC资源具有管理权限，它无法继承clusterroles/pv-admin除dev名称空间之外的其他名称空间中的PVC管理权限，更不能继承集群级别资源PV的任何权限。<br>一种高效分配权限的做法是，由集群管理员在集群范围预先定义好一组具有名称空间级别资源权限的ClusterRole资源，而后由RoleBinding分别在不同名称空间中引用它们，从而在多个名称空间向不同用户授予RoleBinding所有名称空间下的相同权限。<br>由此可见，Role和RoleBinding是名称空间级别的资源，它们仅能用于完成单个名称空间内的访问控制，需要赋予某主体多个名称空间中的访问权限时就不得不在各名称空间分别进行。若需要完成集群全局的资源管理授权，或者希望资源操作能够针对Nodes、Namespaces和PersistentVolumes等集群级别的资源进行，或者针对/api、/apis、/healthz或/version等非资源型URL路径进行，就需要使用ClusterRoleBinding。提示<br>nonResourceURLs资源仅支持get访问权限。<br>下面的配置清单示例（rolebinding-and-clusterrolebinding-rbac.yaml）中，rolebinding/mason-pvc-admin资源位于dev名称空间，它使用RoleBinding为用户mason分配了pv-admin这一集群角色，而clusterrolebinding/ik8s-pv-admin隶属集群级别，它使用ClusterRoleBinding为ik8s分配了pv-admin这一集群。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">RoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mason-pvc-admin</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mason</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-admin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ik8s-pv-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">User</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">ik8s</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-admin</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s</span></span><br></pre></td></tr></table></figure>

<p>将示例中的两个资源创建到集群之上，即可通过对比测试RoleBinding和ClusterRole-Binding为用户分配集群角色在功能上的不同之处。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f rolebinding-and-clusterrolebinding-rbac.yaml</span> </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/mason-pvc-admin created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/ik8s-pv-admin created</span><br></pre></td></tr></table></figure>

<p>首先，我们使用mason用户进行测试，它仅能访问dev名称空间下的名称空间级别的PVC资源，且无法通过RoleBinding从clusterroles/pv-admin继承指定名称空间之外的任何权限，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc -n dev --context=<span class="string">&quot;mason@kubernetes&quot;</span></span></span><br><span class="line">No resources found in dev namespace.</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc -n default --context=<span class="string">&quot;mason@kubernetes&quot;</span></span></span><br><span class="line">Error from server (Forbidden): persistentvolumeclaims is forbidden: User &quot;mason&quot; cannot list resource &quot;persistentvolumeclaims&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pv --context=<span class="string">&quot;mason@kubernetes&quot;</span></span>         </span><br><span class="line">Error from server (Forbidden): persistentvolumes is forbidden: User &quot;mason&quot; cannot list resource &quot;persistentvolumes&quot; in API group &quot;&quot; at the cluster scope</span><br></pre></td></tr></table></figure>

<p>然后，我们使用ik8s用户进行测试，它通过ClusterRoleBinding从clusterroles/pv-admin继承了该集群角色的所有授权，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc -n default --context=<span class="string">&quot;ik8s@kube-dev&quot;</span></span>   </span><br><span class="line">No resources found in default namespace.</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc -n dev --context=<span class="string">&quot;ik8s@kube-dev&quot;</span></span>    </span><br><span class="line">No resources found in dev namespace.</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pv --context=<span class="string">&quot;ik8s@kube-dev&quot;</span></span>        </span><br><span class="line">No resources found in default namespace.</span><br></pre></td></tr></table></figure>

<p>另外，kubectl也提供了分别创建RoleBinding和ClusterRoleBinding资源的命令式命令：create rolebinding和create clusterrolebinding，它们使用的选项基本相同，常用的选项如下。<br>▪–role=””：绑定的角色，仅RoleBinding支持。<br>▪–clusterrole=””：绑定的集群角色，RoleBinding和ClusterRoleBinding均支持。<br>▪–group=[]：绑定的组，支持逗号分隔的列表格式。<br>▪–user=[]：绑定的普通账户，支持逗号分隔的列表格式。<br>▪–serviceaccount=[]：绑定的服务账户，支持逗号分隔的列表格式。<br>例如，下面的命令为用户组kubeusers分配了集群角色nodes-admin，从而该组的所有用户均自动继承该角色上的所有许可权限。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create clusterrolebinding kubeusers-nodes-admin \</span></span><br><span class="line"><span class="language-bash">      --clusterrole=<span class="string">&#x27;nodes-admin&#x27;</span> --group=<span class="string">&#x27;kubeusers&#x27;</span></span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/kubeusers-nodes-admin created</span><br></pre></td></tr></table></figure>

<p>kubectl命令的rolesum和rbac-view等插件能辅助使用RBAC及相关的组件，感兴趣的读者可自行测试其用法。</p>
<h3 id="聚合型ClusterRole"><a href="#聚合型ClusterRole" class="headerlink" title="聚合型ClusterRole"></a>聚合型ClusterRole</h3><p>Kubernetes自1.9版本开始支持在ClusterRole的rules字段中嵌套aggregationRule字段来整合其他ClusterRole资源的规则，这种类型的ClusterRole对象的实际可用权限受控于控制器，具体许可授权由所有被标签选择器匹配到的ClusterRole的聚合授权规则合并生成。<br>下面的配置清单中首先定义了两个拥有标签的集群角色global-resources-view和global-resources-edit，而后在第三个集群角色资源global-resources-admin上使用聚合规则的标签选择器来匹配前两个资源的标签，因此，集群角色global-resources-admin的权限将由匹配到的其他ClusterRole资源的规则列表自动聚合而成。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">global-resources-view</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.ilinux.io/aggregate-to-global-admin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>, <span class="string">&quot;namespaces&quot;</span>, <span class="string">&quot;persistentvolumes&quot;</span>, <span class="string">&quot;clusterroles&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;get&quot;</span>, <span class="string">&quot;list&quot;</span>, <span class="string">&quot;watch&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">global-resources-edit</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">rbac.ilinux.io/aggregate-to-global-admin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&quot;&quot;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&quot;nodes&quot;</span>, <span class="string">&quot;namespaces&quot;</span>, <span class="string">&quot;persistentvolumes&quot;</span>]</span><br><span class="line">  <span class="attr">verbs:</span> [<span class="string">&quot;create&quot;</span>, <span class="string">&quot;delete&quot;</span>, <span class="string">&quot;deletecollection&quot;</span>, <span class="string">&quot;patch&quot;</span>, <span class="string">&quot;update&quot;</span>]</span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">global-resources-admin</span></span><br><span class="line"><span class="attr">aggregationRule:</span></span><br><span class="line">  <span class="attr">clusterRoleSelectors:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">rbac.ilinux.io/aggregate-to-global-admin:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line"><span class="attr">rules:</span> []  <span class="comment"># 该规则列表为空，它将由控制器自动聚合生成</span></span><br></pre></td></tr></table></figure>

<p>任何能够被示例中clusterrole/globa-resources-admin资源的标签选择器匹配到的Cluster-Role资源的相关规则将一同合并为它的授权规则，并且相关作用域内的任何ClusterRole资源的变动都将实时反馈到聚合资源之上。因而，聚合型ClusterRole的规则会随着标签选择器的匹配结果动态变化。<br>事实上，Kubernetes系统上面向用户的内置ClusterRole admin和edit也是聚合型的ClusterRole对象，因为这可以使得默认角色中包含自定义资源的相关规则，例如由CustomResourceDefinitions或Aggregated API服务器提供的规则等。</p>
<h3 id="面向用户的内置ClusterRole"><a href="#面向用户的内置ClusterRole" class="headerlink" title="面向用户的内置ClusterRole"></a>面向用户的内置ClusterRole</h3><p>API Server内置了一组默认的ClusterRole和ClusterRoleBinding资源预留给系统使用，其中大多数都以system:为前缀。另外有一些不以system:为前缀的默认的ClusterRole资源是为面向用户的需求而设计，包括集群管理员角色cluster-admin，以及专用于授予特定名称空间级别权限的集群角色admin、edit和view，如图9-12所示。掌握这些默认的内置ClusterRole资源有助于按需创建用户并分配相应权限。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218191548207.png" alt="image-20220218191548207">内置的clusterroles/cluster-admin资源拥有管理集群所有资源的权限，而内置的clusterrolebindings/cluster-admn将该角色分配给了system:masters用户组，这意味着所有隶属于该组的用户都将自动具有集群的超级管理权限。kubeadm安装设置集群时，自动创建的配置文件/etc/kubernetes/admin.conf中定义的用户kubernetes-admin使用证书文件/etc/kubernetes/pki/apiserver-kubelet-client.crt向API Server进行验证。而该数字证书的Subject属性值为/O=system:masters，API Server会在成功验证该用户的身份之后将其识别为system: master用户组的成员。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl x509 -<span class="keyword">in</span> /etc/kubernetes/pki/apiserver-kubelet-client.crt  -noout -subject</span>     </span><br><span class="line">subject=O = system:masters, CN = kube-apiserver-kubelet-client</span><br></pre></td></tr></table></figure>

<p>于是，为Kubernetes集群自定义超级管理员的方法至少有两种：一是将用户归入system:masters组，二是通过ClusterRoleBinding直接将用户绑定至内置的集群角色cluster-admin上。具体实现方法可参照9.5.4节和本节中的内容略加变通实现。<br>另外，在多租户、多项目或多环境等使用场景中，用户通常应该获得名称空间级别绝大多数资源的管理（admin）、只读（view）或编辑（edit)权限，可通过在指定的名称空间中创建RoleBinding资源引用内置的ClusterRole资源进行这类权限的快速授予。例如，在名称空间dev中创建一个RoleBinding资源，为ik8s用户分配集群角色admin，将使得该用户具有管理dev名称空间中除了名称空间本身及资源配额之外的所有资源的权限。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create rolebinding ik8s-admin --clusterrole=admin --user=ik8s -n dev</span>           </span><br><span class="line">rolebinding.rbac.authorization.k8s.io/ik8s-admin created</span><br></pre></td></tr></table></figure>

<p>若仅需要授予编辑或只读权限，在创建RoleBinding时引用ClusterRole的edit或view便能实现。表9-4总结了典型的面向用户的内置ClusterRole及其功用。<br>表9-4　面向用户的内置ClusterRole资源<br>另外，API Server默认创建的以system:为前缀的大多数ClusterRole和ClusterRoleBinding专为Kubernetes系统的基础架构而设计，修改这些资源可能会导致集群功能不正常。例如，若修改了为kubelet赋权的system:node将会导致kubelet无法正常工作。所有默认的ClusterRole和ClusterRoleBinding都打上了kubernetes.io/bootstrapping=rbac-defaults标签。<br>每次启动时，API Server都会自动为所有默认的ClusterRole重新赋予缺失的权限，同时为默认的ClusterRoleBinding绑定缺失的主体。这种机制给了集群从意外修改中自动恢复的能力，以及升级版本后自动将ClusterRole和ClusterRoleBinding升级到满足新版本需求的能力。提示<br>必要时，在默认的ClusterRole或ClusterRoleBinding上设置annnotation中的rbac.authorization.kubernetes.io/autoupdate属性的值为false，即可禁止这种自动恢复功能。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218191725068.png" alt="image-20220218191725068"></p>
<p>另外，启用RBAC后，Kubernetes系统的各核心组件、附加组件，以及由controller-manager运行的核心控制器等，几乎都要依赖于合理的授权才能正常运行。因而，RBAC权限模型为这些组件内置了可获得最小化的资源访问授权的ClusterRole和ClusterRoleBinding，例如system:kube-sheduler、system:kube-controller-manager、system:node、system:node-proxier和system:kube-dns等，其中大多数组件都可以做到见名知义，这里不再逐一给出说明。</p>
<h2 id="认证与权限应用案例：Dashboard"><a href="#认证与权限应用案例：Dashboard" class="headerlink" title="认证与权限应用案例：Dashboard"></a>认证与权限应用案例：Dashboard</h2><p>Kubernetes Dashboard项目为Kubernetes集群提供了一个基于Web的通用UI，支持集群管理、应用管理及应用排障等功能，截至本书编写时最新的版本为2.x系列。Dashboard项目包含前端和后端两个组件，如图9-13所示。前端运行于客户端浏览器中，由TypeScript编写，它使用标准的HTTP方法将请求发送到后端并从后端获取业务数据；后端是使用Go语言编写的HTTP服务器，它负责接收前端的请求、将数据请求发送到适配的远程后端（例如Kubernetes API Server等）或实现业务逻辑等。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218191931776.png" alt="image-20220218191931776">Dashboard依赖Metrics Server完成指标数据的采集和可视化，因而在部署该组件之前，Dashboard的部分功能将处于不可用状态。<br>Dashboard 1.7（不含）之前的版本在部署时直接赋予管理权限，这种方式可能存在安全风险，因此1.7及之后的版本默认在部署时仅定义运行Dashboard所需要的最小权限，并且只有在Master主机上通过kubectl proxy命令创建代理后，才能在本机进行访问。</p>
<h3 id="部署Dashboard"><a href="#部署Dashboard" class="headerlink" title="部署Dashboard"></a>部署Dashboard</h3><p>出于安全因素的考虑，Dashboard在其项目仓库中推荐的默认部署清单（recommended.yaml）中仅定义了运行自身所需要的最小权限，并且强制要求远程访问必须要基于HTTPS通信，否则应该通过kubectl proxy以代理方式进行。因而，若需要绕过kubectl proxy代理直接访问Dashboard，必须要为其HTTP服务进程提供用于建立HTTPS连接的服务器端证书。<br>推荐的部署清单默认便会在内存中生成自签证书，并以之生成名为kubernetes-dashboard-certs的Secret对象，Dashboard Pod将从该Secret中加载证书（tls.crt）和私钥（tls.key)。若需要使用自定义的证书，则应该在执行如下部署命令之前先把准备好的证书与私钥文件分别以tls.crt和tls.key为键名，创建成kubernetes-dashboard名称空间下名为kubernetes-dashboard-certs的Secret对象，需要用到时，在Dashboard部署之前参考Secret对象的管理方式完成创建即可。下面的命令未自定义Secret，它直接使用Dashboard项目master分支中的配置清单完成应用部署：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/aio/deploy/recommended.yaml</span></span><br></pre></td></tr></table></figure>

<p>部署完成的Dashboard支持多种不同的访问方式，例如kubectl proxy、kubectl port-forward、节点端口、Ingress或API Server等，这里重点介绍节点端口。默认创建的Service对象（kubernetes-dashboard）类型为ClusterIP，它仅能在Pod客户端中访问，若需要在集群外使用浏览器访问Dashboard，可将该Service对象类型修改为NodePort后，通过节点端口进行访问。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch svc kubernetes-dashboard -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&#x27;</span> -n kubernetes-dashboard</span></span><br></pre></td></tr></table></figure>

<p>未显式指定的NodePort属性值将会由Service控制器随机分配，下面获取该端口号以便在集群外通过浏览器访问。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get services/kubernetes-dashboard -n kubernetes-dashboard \</span></span><br><span class="line"><span class="language-bash">      -o jsonpath=<span class="string">&#x27;&#123;.spec.ports[0].nodePort&#125;&#x27;</span></span></span><br><span class="line">30272</span><br></pre></td></tr></table></figure>

<p>图9-14显示了Dashboard的默认登录页面，它支持直接通过目标Service Account的令牌加载身份凭据，或者以该令牌为身份凭据生成专用的kubeconfig文件，并通过指定的文件路径向Dashboard提交认证信息。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218191947673.png" alt="image-20220218191947673">Dashboard的资源访问权限继承自登录时的Service Account用户，我们可以向相关的Service Account分配特定的角色或集群角色完成Dashboard用户权限模型的构建。</p>
<p>例如，为登录的用户授权集群级别管理权限时，可直接使用ClusterRoleBinding为Service Account分配内置的集群角色cluster-admin，而授权名称空间级别的管理权限时，可在目标名称空间上向Service Account分配内置的集群角色admin。当然，也可以直接自定义RBAC的角色或集群角色，以完成特殊需求的权限委派。</p>
<h3 id="认证与授权"><a href="#认证与授权" class="headerlink" title="认证与授权"></a>认证与授权</h3><p>Kubernetes Dashboard自身并不进行任何形式的身份验证和鉴权，它仅是把用户提交的身份凭据转发至后端的API Server完成验证，资源操作请求及权限检查同样会提交至后端的API Server进行。从某种意义上讲，Dashboard更像是用户访问Kubernetes的代理程序，发送给API Server的身份认证及资源操作请求都是由Dashboard应用程序完成，因而用户提交的身份凭据需要关联至某个Service Account。<br>集群全局的资源管理操作依赖于集群管理员权限，因而需要为专用于访问Dashboard的Service Account分配内置的cluster-admin集群角色。随后，将相应Service Account的令牌信息提交给Dashboard并认证到API Server，便可使得Dashboard继承了该账户的所有管理权限。例如，下面在kubernetes-dashboard名称空间创建一个名为dashboard-admin的Service Account完成该目标。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create serviceaccount admin-user -n kubernetes-dashboard</span></span><br><span class="line">serviceaccount/admin-user created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create clusterrolebinding admin-user --clusterrole=cluster-admin \</span></span><br><span class="line"><span class="language-bash">      --serviceaccount=kubernetes-dashboard:admin-user</span></span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/admin-user created</span><br></pre></td></tr></table></figure>

<p>随后，获取到服务账户kubernetes-dashboard:admin-user关联的Secret对象中的令牌信息，提交给Dashboard即可完成认证。下面第一个命令检索到该服务账户的Secret对象名称并保存到变量中，第二个命令则从该Secret中获取经过Base64编码后的令牌信息，并打印出解码后的令牌信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">ADMIN_SECRET=$(kubectl -n kubernetes-dashboard get secret | awk <span class="string">&#x27;/^admin-user/&#123;print $1&#125;&#x27;</span>)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">ADMIN_TOKEN=$(kubectl get secrets <span class="variable">$ADMIN_SECRET</span> -n kubernetes-dashboard \</span></span><br><span class="line"><span class="language-bash">       -o jsonpath=<span class="string">&#x27;&#123;.data.token&#125;&#x27;</span> | base64 -d)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">echo</span> <span class="variable">$ADMIN_TOKEN</span></span></span><br><span class="line">eyJhbGciOiJSUzI1NiIsImtpZCI6IlRWYXZxTGE5UE9SY0JZUHV0NUdRN1hIeXBMa……</span><br></pre></td></tr></table></figure>

<p>将上面打印出的令牌信息复制到Dashboard的登录界面便可完成认证，随后Dashboard便会打开类似图9-15所示的主面板页面。</p>
<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218192221358.png" alt="image-20220218192221358"></p>
<p>显然，每次访问Dashboard之前都要先通过如上命令获取相应的令牌是件相当烦琐的事情，更简便的办法是依该身份凭据创建出一个专用的kubeconfig文件并存储到客户端，随后登录时在浏览器中通过本地路径加载该kubeconfig文件即可完成认证，更加安全和便捷。<br>创建kubeconfig文件的方法在9.4节中已经有过详细介绍，下面仅给出相关步骤，实现为服务账户kubernetes-dashboard:admin-user创建相关的配置文件。<br>1）添加集群配置，包括集群名称、API Server URL和信任的CA的证书；clusters配置段中的各列表项名称需要唯一。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-cluster kubernetes --embed-certs=<span class="literal">true</span> \</span></span><br><span class="line"><span class="language-bash">    --certificate-authority=/etc/kubernetes/pki/ca.crt \</span></span><br><span class="line"><span class="language-bash">    --server=<span class="string">&quot;https://k8s-api.ilinux.io:6443&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/admin-user.config</span></span><br></pre></td></tr></table></figure>

<p>2）添加身份凭据，可使用静态密码文件认证的客户端提供用户名和密码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-credentials admin-user --token=<span class="variable">$ADMIN_TOKEN</span> \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/admin-user.config</span></span><br></pre></td></tr></table></figure>

<p>3）以用户admin-user的身份凭据与Kubernetes集群建立映射关系。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config set-context admin-user@kubernetes --cluster=kubernetes \</span></span><br><span class="line"><span class="language-bash">    --user=admin-user --kubeconfig=<span class="variable">$HOME</span>/.kube/admin-user.config</span></span><br></pre></td></tr></table></figure>

<p>4）设置当前上下文为admin-user@kubernetes。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl config use-context admin-user@kubernetes \</span></span><br><span class="line"><span class="language-bash">    --kubeconfig=<span class="variable">$HOME</span>/.kube/admin-user.config</span></span><br></pre></td></tr></table></figure>

<p>至此为止，一个用于Dashboard登录认证的、拥有管理员权限的kubeconfig配置文件设置完成，把文件复制到远程客户端上即可用于Dashboard kubeconfig类型的登录认证。<br>另外，若需要设定的用户仅具有某个名称空间的管理权限，或者仅拥有集群或名称空间级别的资源读取权限，都能够通过RBAC权限管理模型来实现。这类用户的设定过程与前述步骤中的关键不同之处仅在于角色分配步骤。例如，在名称空间kubernetes-dashboard中创建服务账户monitor-user，并通过ClusterRoleBinding为其分配默认的集群角色view，便可创建一个仅具有全局读取权限的Dashboard用户，所需步骤如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create serviceaccount monitor -n kubernetes-dashboard</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create clusterrolebinding monitor --clusterrole=view \</span></span><br><span class="line"><span class="language-bash">      --serviceaccount=kubernetes-dashboard:monitor</span></span><br></pre></td></tr></table></figure>

<p>或者在名称空间dev中创建一个服务账户dev-ns-admin，并通过RoleBinding为其分配默认的集群角色admin，就能创建一个仅具有dev名称空间管理权限的Dashboard用户，所需要的步骤如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create serviceaccount ns-admin -n dev</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create rolebinding ns-admin --clusterrole=admin --serviceaccount=dev:ns-admin</span></span><br></pre></td></tr></table></figure>

<p>无论分配了何种集群角色或拥有ServiceAccount账户的特定角色，它们的认证信息提取及使用kubeconfig配置文件的方式都是相同的，这里不再给出具体的步骤，感兴的读者可自行测试。另外，Dashboard能大大简化kubectl命令行里的各种操作，但二者的核心功能相似之处甚多，读者朋友们根据界面提示信息很快就能掌握其使用方法。</p>
<h2 id="准入控制器"><a href="#准入控制器" class="headerlink" title="准入控制器"></a>准入控制器</h2><p>API Server中的准入控制器同样以插件形式存在，它们会拦截所有已完成认证的，且与资源创建、更新和删除操作相关的请求，以强制实现控制器中定义的功能，包括执行对象的语义验证和设置缺失字段的默认值等，具体功能取决于API Server启用的插件。目前，Kubernetes内置了30多个准入控制器。</p>
<h3 id="准入控制器概述"><a href="#准入控制器概述" class="headerlink" title="准入控制器概述"></a>准入控制器概述</h3><p>Kubernetes自1.7版本引入了Initializers和External Admission Webhooks来尝试突破此限制，而且自1.9版本起，External Admission Webhooks又被分为MutatingAdmissionWebhook和ValidatingAdmissionWebhook两种类型，分别用于在API中执行对象配置的“变异”和“验证”操作，前一种类型的控制器会“改动”和“验证”资源规范，而后一种类型仅“验证”资源规范是否合规。<br>在具体的代码实现上，一个准入控制器可以是验证型、变异型或兼具此两项功能。例如，LimitRanger准入控制器可以使用默认资源请求和限制（变异阶段）来扩展Pod，也能够校验有着显式资源需求定义的Pod是否超出LimitRange对象（验证阶段）的定义。而在具体运行时，准入控制也会根据准入控制器类型分阶段运行，第一个阶段串行运行各变异型控制器，第二阶段则串行运行各验证型控制器，如图9-16所示。在此过程中，任何控制器拒绝请求都将导致整个请求被即刻拒绝，并将错误信息返回给客户端。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220221114926133.png" alt="image-20220221114926133"></p>
<p>Kubernetes集群内置功能的某些方面实际上就是由准入控制器控制的，例如，删除名称空间并进入Terminating状态时，NamespaceLifecycle准入控制器将会阻止在该名称空间中创建任何新的资源对象。甚至于，必须启用准入控制器才能使用Kubernetes集群的某些更高级的安全功能，例如在整个命名空间上强制实施安全配置基线的Pod安全策略等。<br>API Server默认便会启用部分准入控制器，它也支持通过–enable-admission-plugins选项显式指定要加载的准入控制器，使用–disable-admission-plugins选项显式指定要禁用的准入控制器。提示<br>Kubernetes内置支持的所有准入控制器及其功能说明请参考官方文档中的说明，具体地址为<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/%E3%80%82">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/。</a><br>Kubernetes正是依赖LimitRange资源和相应的LimitRanger准入控制器、ResourceQuota资源和同名的准入控制器，以及PodSecurityPolicy资源和同名的准入控制器为多租户或多项目的集群环境提供了基础的安全策略框架。</p>
<h3 id="LimitRange"><a href="#LimitRange" class="headerlink" title="LimitRange"></a>LimitRange</h3><p>LimitRange支持在Pod级别与容器级别分别设置CPU和内存两种计算资源的可用范围，它们对应的资源范围限制类型分别为Pod和Container。一旦在名称空间上启用LimitRange，该名称空间中的Pod或容器的requests和limits的各项属性值必须在对应的可用资源范围内，否则将会被拒绝，这是验证型准入控制器的功能。以Pod级别的CPU资源为例，若某个LimitRange资源为其设定了[0.5,4]的区间，则相应名称空间下任何Pod资源的requests.cpu的属性值必须要大于等于500m，同时，limits.cpu的属性值也必须要小于等于4。而未显式指定request和limit属性的容器，将会从LimitRange资源上分别自动继承相应的默认设置，这是变异型准入控制器的功能，如图9-17所示。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220221121230803.png" alt="image-20220221121230803">另外，LimitRange也支持在PersistentVolumeClaim资源级别设定存储空间的范围限制，它用于限制相应名称空间中创建的PVC对象请求使用的存储空间不能逾越指定的范围。未指定requests和limits属性的PVC规范，将在创建时自动继承LimitRange上配置的默认值。<br>下面的资源清单（limitrange-demo.yaml)分别为dev名称空间中的Pod、Container和PersistentVolumeClaim资源定义了各自的资源范围，并为后两者指定了相应可用资源规范的limits和requests属性上的默认值。其中用到的各配置属性中，default用于定义limits的默认值，defaultRequest定义requests的默认值，min定义最小资源用量，而最大资源用量可以使用max给出固定值，也可以使用maxLimitRequestRatio设定最小用量的指定倍数，同时定义二者时，其意义要相符。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">LimitRange</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">resource-limits</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Pod</span></span><br><span class="line">      <span class="attr">max:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;4&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;4Gi&quot;</span> </span><br><span class="line">      <span class="attr">min:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;16Mi&quot;</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Container</span></span><br><span class="line">      <span class="attr">max:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;4&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;1Gi&quot;</span> </span><br><span class="line">      <span class="attr">min:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;100m&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;4Mi&quot;</span> </span><br><span class="line">      <span class="attr">default:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;2&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;512Mi&quot;</span> </span><br><span class="line">      <span class="attr">defaultRequest:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span> </span><br><span class="line">      <span class="attr">maxLimitRequestRatio:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;4&quot;</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line">      <span class="attr">max:</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">&quot;10Gi&quot;</span></span><br><span class="line">      <span class="attr">min:</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">&quot;1Gi&quot;</span></span><br><span class="line">      <span class="attr">default:</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">&quot;5Gi&quot;</span></span><br><span class="line">      <span class="attr">defaultRequest:</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">&quot;1Gi&quot;</span></span><br><span class="line">      <span class="attr">maxLimitRequestRatio:</span></span><br><span class="line">        <span class="attr">storage:</span> <span class="string">&quot;5&quot;</span></span><br></pre></td></tr></table></figure>

<p><font color="red">LimitRange仅在Container资源类型上可为CPU与内存设置default（limits属性的默认值）和defaultrequest（requests属性的默认值），Pod资源类型不支持。</font><br>LimitRange资源的详细描述会以非常直观、清晰的方式输出相关的资源限制及默认值的定义，将如上配置清单中的LimitRange资源resource-limits创建到集群上，而后便可使用describe命令查看：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f limitrange-demo.yaml</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe limitranges/resource-limits -m dev</span></span><br></pre></td></tr></table></figure>

<p>输出结果类似图9-18。<br><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218192802902.png" alt="image-20220218192802902">我们可以通过在dev名称空间中创建Pod对象与PVC对象对各限制的边界和默认值的效果进行多维度测试。先创建一个仅包含一个容器且没有默认系统资源需求和限制的Pod对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run testpod-1 --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n dev</span></span><br></pre></td></tr></table></figure>

<p>Pod对象testpod-1资源规范中被自动添加了CPU和内存资源的requests和limits属性，它的值来自limitranges/resource-limits中的定义，如下面的命令及截取的结果片段所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pod/testpod-1 -n dev -o yaml</span></span><br><span class="line">……</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - image: ikubernetes/demoapp:v1.0</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: testpod-1</span><br><span class="line">    resources:</span><br><span class="line">      limits:</span><br><span class="line">        cpu: &quot;2&quot;</span><br><span class="line">        memory: 512Mi</span><br><span class="line">      requests:</span><br><span class="line">        cpu: 500m</span><br><span class="line">        memory: 64Mi</span><br><span class="line">  ……</span><br></pre></td></tr></table></figure>

<p>若Pod对象设定的CPU或内存的requests属性值小于LimitRange中相应资源的下限，或limits属性值大于设定的相应资源的上限，就会触发LimitRanger准入控制器拒绝相关的请求。例如下面创建Pod的命令中，仅requests.memory一个属性值违反了limitrange/resource-limits中的定义，但请求同样会被拒绝。</p>
   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run testpod-2 --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> -n dev \</span></span><br><span class="line"><span class="language-bash">      --limits=<span class="string">&#x27;cpu=2,memory=1Gi&#x27;</span> --requests=<span class="string">&#x27;cpu=1,memory=8Mi&#x27;</span></span> </span><br><span class="line">Error from server (Forbidden): pods &quot;testpod-2&quot; is forbidden: minimum memory usage per Pod is 16Mi, but request is 8388608</span><br></pre></td></tr></table></figure>

<p>类似地，在dev名称空间中创建的PVC对象的可用存储空间也将受到LimitRange资源中定义的限制，鉴于篇幅有限，这里不再给出具体的过程，感兴趣的读者朋友可自行完成测试。<br>需要注意的是，LimitRange生效于名称空间级别，它需要定义在每个名称空间之上；另外，定义的限制仅对该资源创建后的Pod和PVC资源创建请求有效，对之前已然存在的资源无效；再者，不建议在生效于同一名称空间的多个LimitRange资源上，对同一个计算资源限制进行分别定义，以免产生歧义或导致冲突。</p>
<h3 id="ResourceQuota"><a href="#ResourceQuota" class="headerlink" title="ResourceQuota"></a>ResourceQuota</h3><p>。ResourceQuota资源能够定义名称空间级别的资源配额，从而在名称空间上限制聚合资源消耗的边界，它支持以资源类型来限制用户可在本地名称空间中创建的相关资源的对象数量，以及这些对象可消耗的计算资源总量等。</p>
<p>  而同名的ResourceQuota准入控制器负责观察传入的请求，并确保它没有违反相应名称空间中ResourceQuota资源定义的任何约束。ResourceQuota准入控制器属于“验证”类型的控制器，用户创建或更新资源的操作违反配额约束时将会被拒绝，API Server会响应以HTTP状态代码403 FORBIDDEN，并显示一条消息以提示违反的约束条件。<br><font color="red">    ResourceQuota资源可限制名称空间中处于非终止状态的所有Pod对象的计算资源需求及计算资源限制总量。 </font></p>
<ul>
<li>cpu或requests.cpu：CPU资源相关请求的总量限额。</li>
<li>memory或requests.cpu：内存资源相关请求的总量限额。</li>
<li>limits.cpu：CPU资源相关限制的总量限额。</li>
<li>limits.memory：内存资源相关限制的总量限额。</li>
</ul>
<p>ResourceQuota资源还支持为本地名称空间中的PVC存储资源的需求总量和限制总量设置限额，它能够分别从名称空间中的全部PVC、隶属于特定存储类的PVC以及基于本地临时存储的PVC分别进行定义。</p>
<ul>
<li><p>requests.storage：所有PVC存储需求的总量限额。</p>
</li>
<li><p>persistentvolumeclaims：可以创建的PVC总数限额。</p>
</li>
<li><p>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/requests.storage：特定存储类上可使用的所有PVC存储需求的总量限额。</p>
</li>
<li><p>&lt;storage-class-name&gt;.storageclass.storage.k8s.io/persistentvolumeclaims：特定存储类上可使用的PVC总数限额。</p>
</li>
<li><p>requests.ephemeral-storage：所有Pod可以使用的本地临时存储资源的requets总量。</p>
</li>
<li><p>limits.ephemeral-storage：所有Pod可用的本地临时存储资源的limits总量。</p>
</li>
</ul>
<p>自v1.9版本起开始支持以count/&lt;resource&gt;.&lt;group&gt;的格式对所有资源类型对象的计数配额，例如count/deployments.apps、count/deployments.extensions和count/services等。<br>下面的资源清单（resourcequota-demo.yaml）在dev名称空间中定义了一个ResourceQuota资源对象，它定义了计算资源与存储资源分别在requests和limits维度的限额，也定义了部署资源类型中的可用对象数量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ResourceQuota</span><br><span class="line">metadata:</span><br><span class="line">  name: resourcequota-demo</span><br><span class="line">  namespace: dev</span><br><span class="line">spec:</span><br><span class="line">  hard:</span><br><span class="line">    pods: &quot;5&quot;</span><br><span class="line">    count/services: &quot;5&quot;</span><br><span class="line">    count/configmaps: &quot;5&quot;</span><br><span class="line">    count/secrets: &quot;5&quot;</span><br><span class="line">    count/cronjobs.batch: &quot;2&quot;</span><br><span class="line">    requests.cpu: &quot;2&quot;</span><br><span class="line">    requests.memory: &quot;4Gi&quot;</span><br><span class="line">    limits.cpu: &quot;4&quot;</span><br><span class="line">    limits.memory: &quot;8Gi&quot;</span><br><span class="line">    count/deployments.apps: &quot;2&quot;</span><br><span class="line">    count/statefulsets.apps: &quot;2&quot;</span><br><span class="line">    persistentvolumeclaims: &quot;6&quot;</span><br><span class="line">    requests.storage: &quot;20Gi&quot;</span><br><span class="line">    fast-rbd.storageclass.storage.k8s.io/requests.storage: &quot;20Gi&quot;</span><br><span class="line">    fast-rbd.storageclass.storage.k8s.io/persistentvolumeclaims: &quot;6&quot;</span><br></pre></td></tr></table></figure>

<p>与LimitRange不同的是，ResourceQuota会计入指定范围内，先前的资源对象对系统资源和资源对象的限额占用情况，因此将resourceqouta-demo创建到集群上之后，dev名称空间中现有的资源会立即分去限额内的一部分可用空间，这在ResourceQuota资源的详细描述中会有直观展示，如图9-19所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f resourcequota-demo.yaml</span>             </span><br><span class="line">resourcequota/resourcequota-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe resourcequotas/resourcequota-demo -n dev</span></span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/21/%E8%AE%A4%E8%AF%81%E3%80%81%E6%8E%88%E6%9D%83%E4%B8%8E%E5%87%86%E5%85%A5%E6%8E%A7%E5%88%B6/image-20220218193332606.png" alt="image-20220218193332606"></p>
<p>上面第二条命令结果显示，dev名称空间下的Pod资源限额已被先前的自主式Pod对象消耗了1/5，与此同时，计算资源请求和限制也各占用了一部分配额。随后，在dev名称空间中创建Pod资源时，requests.cpu、requests.memroy、limits.cpu、limits.memory和pods等任何一个限额的超出都将致使创建操作失败，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run testpod-2 --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">      --requests=<span class="string">&quot;cpu=2,memory=1Gi&quot;</span> --limits=<span class="string">&quot;cpu=2,memory=1Gi&quot;</span> -n dev</span> </span><br><span class="line">Error from server (Forbidden): pods &quot;testpod-2&quot; is forbidden: exceeded quota: resourcequota-demo, requested: requests.cpu=2, used: requests.cpu=500m, limited: requests.cpu=2</span><br></pre></td></tr></table></figure>

<p>每个ResourceQuota资源对象上还支持定义一组作用域，用于定义资源上的配额仅生效于这组作用域交集范围内的对象，目前适用范围包括Terminating、NotTerminating、BestEffort和NotBestEffort。</p>
<ul>
<li><p>Terminating：匹配.spec.activeDeadlineSeconds的属性值大于等于0的所有Pod对象。</p>
</li>
<li><p>NotTerminating：匹配.spec.activeDeadlineSeconds的属性值为空的所有Pod对象。</p>
</li>
<li><p>BestEffort：匹配所有位于BestEffort QoS类别的Pod对象。</p>
</li>
<li><p>NotBestEffort：匹配所有非BestEffort QoS类别的Pod对象。</p>
</li>
</ul>
<p>另外，Kubernetes自v1.8版本起支持管理员设置不同的优先级类别（PriorityClass）创建Pod对象，而且自v1.11版本起还支持对每个PriorityClass对象分别设定资源限额。于是，管理员还可以在ResourceQuota资源上使用scopeSelector字段定义其生效的作用域，它支持基于Pod对象的优先级来控制Pod对系统资源的消耗。</p>
<h3 id="PodSecurityPolicy"><a href="#PodSecurityPolicy" class="headerlink" title="PodSecurityPolicy"></a>PodSecurityPolicy</h3><p>PSP资源就是集群全局范围内定义的Pod资源可用的安全上下文策略。同名的PodSecurityPolicy准入控制器负责观察集群范围内的Pod资源的运行属性，并确保它没有违反PodSecurityPolicy资源定义的约束条件。<br>PSP准入控制器会根据显式定义的PSP资源中的安全策略判定允许何种Pod资源的创建操作，若无任何可用的安全策略，它将阻止创建任何Pod资源。Kubernetes集群默认并不会自动生成任何PSP资源，因而该准入控制器默认处于禁用状态。PSP资源的API接口（policy/v1beta1/podsecuritypolicy）独立于PSP准入控制器，因此管理员可以先定义好必要的Pod安全策略，再设置kube-apiserver启用PSP准入控制器。不当的Pod安全策略可能会产生难以预料的副作用，因此请确保添加的任何PSP对象都经过了充分测试。<br>PodSecurityPolicy是标准的API资源类型，它隶属于policy群组，在spec字段中嵌套多种安全规则来定义期望的目标，资源规范及简要的使用说明如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span>   <span class="comment"># PSP资源所属的API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span>   <span class="comment"># 资源类型标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 资源名称</span></span><br><span class="line"><span class="attr">spec:</span>  </span><br><span class="line">  <span class="string">allowPrivilegeEscalation</span>  <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否允许权限升级</span></span><br><span class="line">  <span class="string">allowedCSIDrivers</span> <span class="string">&lt;[]Object&gt;</span>        <span class="comment">#内联CSI驱动程序列表，必须在Pod规范中显式定义</span></span><br><span class="line">  <span class="string">allowedCapabilities</span> <span class="string">&lt;[]string&gt;</span>      <span class="comment"># 允许使用的内核能力列表，“*”表示all</span></span><br><span class="line">  <span class="string">allowedFlexVolumes</span> <span class="string">&lt;[]Object&gt;</span>       <span class="comment"># 允许使用的Flexvolume列表，空值表示all</span></span><br><span class="line">  <span class="string">allowedHostPaths</span> <span class="string">&lt;[]Object&gt;</span>         <span class="comment"># 允许使用的主机路径列表，空值表示all</span></span><br><span class="line">  <span class="string">allowedProcMountTypes</span> <span class="string">&lt;[]string&gt;</span>    <span class="comment"># 允许使用的ProcMountType列表，空值表示默认</span></span><br><span class="line">  <span class="string">allowedUnsafeSysctls</span> <span class="string">&lt;[]string&gt;</span>     <span class="comment"># 允许使用的非安全sysctl参数，空值表示不允许</span></span><br><span class="line">  <span class="string">defaultAddCapabilities</span>  <span class="string">&lt;[]string&gt;</span>  <span class="comment"># 默认添加到Pod对象的内核能力，可被drop</span></span><br><span class="line">  <span class="string">defaultAllowPrivilegeEscalation</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否默认允许内核权限升级</span></span><br><span class="line">  <span class="string">forbiddenSysctls</span>  <span class="string">&lt;[]string&gt;</span>        <span class="comment"># 禁止使用的sysctl参数，空表示不禁用</span></span><br><span class="line">  <span class="string">fsGroup</span> <span class="string">&lt;Object&gt;</span>    <span class="comment"># 允许在SecurityContext中使用的fsgroup，必选字段</span></span><br><span class="line">    <span class="string">rule</span> <span class="string">&lt;string&gt;</span>     <span class="comment"># 允许使用的FSGroup规则，支持RunAsAny和MustRunAs</span></span><br><span class="line">    <span class="string">ranges</span> <span class="string">&lt;[]Object&gt;</span> <span class="comment"># 允许使用的组ID范围，需要与MustRunAs规则一同使用</span></span><br><span class="line">      <span class="string">max</span>  <span class="string">&lt;integer&gt;</span>                  <span class="comment"># 最大组ID号</span></span><br><span class="line">      <span class="string">min</span>  <span class="string">&lt;integer&gt;</span>                  <span class="comment"># 最小组ID号</span></span><br><span class="line">  <span class="string">hostIPC</span> <span class="string">&lt;boolean&gt;</span>                   <span class="comment"># 是否允许Pod使用hostIPC</span></span><br><span class="line">  <span class="string">hostNetwork</span> <span class="string">&lt;boolean&gt;</span>               <span class="comment"># 是否允许Pod使用hostNetwork</span></span><br><span class="line">  <span class="string">hostPID</span> <span class="string">&lt;boolean&gt;</span>                   <span class="comment"># 是否允许Pod使用hostPID</span></span><br><span class="line">  <span class="string">hostPorts</span> <span class="string">&lt;[]Object&gt;</span>                <span class="comment"># 允许Pod使用的主机端口暴露其服务的范围</span></span><br><span class="line">    <span class="string">max</span>  <span class="string">&lt;integer&gt;</span>                    <span class="comment"># 最大端口号，必选字段</span></span><br><span class="line">    <span class="string">min</span>  <span class="string">&lt;integer&gt;</span>                    <span class="comment"># 最小端口号，必选字段</span></span><br><span class="line">  <span class="string">privileged</span>  <span class="string">&lt;boolean&gt;</span>               <span class="comment"># 是否允许运行特权Pod</span></span><br><span class="line">  <span class="string">readOnlyRootFilesystem</span>  <span class="string">&lt;boolean&gt;</span>   <span class="comment"># 是否设定容器的根文件系统为“只读”</span></span><br><span class="line">  <span class="string">requiredDropCapabilities</span> <span class="string">&lt;[]string&gt;</span> <span class="comment"># 必须要禁用的内核能力列表</span></span><br><span class="line">  <span class="string">runAsGroup</span>  <span class="string">&lt;Object&gt;</span> <span class="comment"># 允许Pod在runAsGroup中使用的值列表，未定义表示不限制</span></span><br><span class="line">  <span class="string">runAsUser</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># 允许Pod在runAsUser中使用的值列表，必选字段</span></span><br><span class="line">    <span class="string">rule</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 支持RunAsAny、MustRunAs和MustRunAsNonRoot</span></span><br><span class="line">    <span class="string">ranges</span> <span class="string">&lt;[]Object&gt;</span>  <span class="comment"># 允许使用的组ID范围，需要跟MustRunAs规则一同使用</span></span><br><span class="line">      <span class="string">max</span>  <span class="string">&lt;integer&gt;</span>                  <span class="comment"># 最大组ID号</span></span><br><span class="line">      <span class="string">min</span>  <span class="string">&lt;integer&gt;</span>                  <span class="comment"># 最小组ID号</span></span><br><span class="line">  <span class="string">runtimeClass</span> <span class="string">&lt;Object&gt;</span>               <span class="comment"># 允许Pod使用的运行类，未定义表示不限制</span></span><br><span class="line">    <span class="string">allowedRuntimeClassNames</span> <span class="string">&lt;[]string&gt;</span> <span class="comment"># 可使用的runtimeClass列表，“*”表示all</span></span><br><span class="line">    <span class="string">defaultRuntimeClassName</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 默认使用的runtimeClass</span></span><br><span class="line">  <span class="string">seLinux</span> <span class="string">&lt;Object&gt;</span>                    <span class="comment"># 允许Pod使用的selinux标签，必选字段</span></span><br><span class="line">    <span class="string">rule</span> <span class="string">&lt;string&gt;</span> <span class="comment"># MustRunAs表示使用seLinuxOptions定义的值；RunAsAny表示可使用任意值</span></span><br><span class="line">    <span class="string">seLinuxOptions</span>  <span class="string">&lt;Object&gt;</span>   <span class="comment"># 自定义seLinux选项对象，与MustRunAs协作生效</span></span><br><span class="line">  <span class="string">supplementalGroups</span>  <span class="string">&lt;Object&gt;</span> <span class="comment"># 允许Pod在SecurityContext中使用附加组，必选字段  </span></span><br><span class="line"><span class="string">volumes</span> <span class="string">&lt;[]string&gt;</span>            <span class="comment"># 允许Pod使用的存储卷插件列表，空表示禁用，“*”表示all</span></span><br></pre></td></tr></table></figure>

<p>启用PSP准入控制器后要部署任何Pod对象，相关的User Account及Service Account必须全部获得了恰当的Pod安全策略授权。以常规用户的身份直接创建Pod对象时，PSP准入控制器将根据该账户被授权使用的Pod安全策略验证其凭据，若无任何安全策略约束该Pod对象的安全性，则创建操作将会被拒绝。基于控制器（例如Deployment）创建Pod对象时，PSP准入控制器会根据Pod对象的Service Account被授权使用的Pod安全策略验证其凭据，若不存在支持该Pod对象的安全性要求的安全策略，则Pod控制器资源自身能成功创建，但Pod对象不能。<br>然而，即便在启用了PSP准入控制器的情况下，PSP对象依然不会生效，管理员还需要借助授权插件（例如RBAC）将use权限授权给特定的Role或ClusterRole，再为相关的User Account或Service Account分配这些角色才能让PSP策略真正生效。下面简单说明为Kubernetes集群设定的能支撑集群自身运行的框架性的Pod安全策略，以及允许非管理员使用的Pod安全策略，而后启用PSP准入控制器中使这些策略生效的方法。</p>
<h4 id="设置特权及受限的PSP对象"><a href="#设置特权及受限的PSP对象" class="headerlink" title="设置特权及受限的PSP对象"></a>设置特权及受限的PSP对象</h4><p>通常，system:masters组内的管理员账户、system:node组内的kubelet账户，以及kube-system名称空间中的所有服务账户需要拥有创建各类Pod对象的权限，包括创建特权Pod对象。因此，启用PSP准入控制器之前需要先创建一个特权PSP资源，并将该资源的使用权赋予各类管理员账户以确保Kubernetes集群的基础服务可以正常运行。一个示例性的特权PSP资源清单（psp-privileged.yaml）如下，它启用了几乎所有的安全配置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">privileged</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">seccomp.security.alpha.kubernetes.io/allowedProfileNames:</span> <span class="string">&#x27;*&#x27;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">allowedCapabilities:</span> [<span class="string">&#x27;*&#x27;</span>]</span><br><span class="line">  <span class="attr">allowedUnsafeSysctls:</span> [<span class="string">&#x27;*&#x27;</span>]</span><br><span class="line">  <span class="attr">volumes:</span> [<span class="string">&#x27;*&#x27;</span>]</span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPorts:</span> </span><br><span class="line">  <span class="bullet">-</span> <span class="attr">min:</span> <span class="number">0</span></span><br><span class="line">    <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">  <span class="attr">hostIPC:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br><span class="line">  <span class="attr">runAsGroup:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br></pre></td></tr></table></figure>

<p>出于安全加强的需要，除了有特权需求的系统级应用程序及集群管理员账户之外，其他应用或普通账户默认不应该允许使用与安全上下文相关的任何配置。因而，系统内置的特殊组之外的其他普通账户或服务账户，绝大多数都不必使用安全配置，它们仅可使用受限的安全策略。下面的资源清单（psp-restrict.yaml）定义了一个完全受限的安全策略，它禁止了几乎所有的特权操作。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodSecurityPolicy</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">restricted</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">seccomp.security.alpha.kubernetes.io/allowedProfileNames:</span> <span class="string">&#x27;docker/default&#x27;</span></span><br><span class="line">    <span class="attr">apparmor.security.beta.kubernetes.io/allowedProfileNames:</span> <span class="string">&#x27;runtime/default&#x27;</span></span><br><span class="line">    <span class="attr">seccomp.security.alpha.kubernetes.io/defaultProfileName:</span>  <span class="string">&#x27;docker/default&#x27;</span></span><br><span class="line">    <span class="attr">apparmor.security.beta.kubernetes.io/defaultProfileName:</span>  <span class="string">&#x27;runtime/default&#x27;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">privileged:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">allowPrivilegeEscalation:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">allowedUnsafeSysctls:</span> []</span><br><span class="line">  <span class="attr">requiredDropCapabilities:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ALL</span></span><br><span class="line">  <span class="comment"># 允许使用的核心存储卷类型</span></span><br><span class="line">  <span class="attr">volumes:</span> [<span class="string">&#x27;configMap&#x27;</span>, <span class="string">&#x27;emptyDir&#x27;</span>, <span class="string">&#x27;projected&#x27;</span>, <span class="string">&#x27;secret&#x27;</span>, <span class="string">&#x27;secret&#x27;</span>, </span><br><span class="line">  <span class="string">&#x27;persistentVolumeClaim&#x27;</span>]</span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">hostIPC:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">runAsUser:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;MustRunAsNonRoot&#x27;</span></span><br><span class="line">  <span class="attr">seLinux:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;RunAsAny&#x27;</span></span><br><span class="line">  <span class="attr">supplementalGroups:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;MustRunAs&#x27;</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">      <span class="comment"># Forbid adding the root group.</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">  <span class="attr">fsGroup:</span></span><br><span class="line">    <span class="attr">rule:</span> <span class="string">&#x27;MustRunAs&#x27;</span></span><br><span class="line">    <span class="attr">ranges:</span></span><br><span class="line">      <span class="comment"># Forbid adding the root group.</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">min:</span> <span class="number">1</span></span><br><span class="line">        <span class="attr">max:</span> <span class="number">65535</span></span><br><span class="line">  <span class="attr">readOnlyRootFilesystem:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>将上面两个资源清单中定义的PSP资源提交并创建到集群之上，随后便可授权特定的Role或ClusterRole资源通过use调用它们。PSP资源创建完成后才能授权特定的Role或ClusterRole资源通过use进行调用。我们这里首先使用如下命令将上面配置清单中定义的资源创建到集群之上，调用的方式将在后面一小节中进行说明。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f psp-privileged  -f psp-restricted.yaml</span></span><br><span class="line">podsecuritypolicy.policy/privileged created</span><br><span class="line">podsecuritypolicy.policy/restricted created</span><br></pre></td></tr></table></figure>

<h4 id="创建ClusterRole并完成账户绑定"><a href="#创建ClusterRole并完成账户绑定" class="headerlink" title="创建ClusterRole并完成账户绑定"></a>创建ClusterRole并完成账户绑定</h4><p>启用PodSecurityPolicy准入控制器后，仅被授权使用PSP资源的账户才能够在该资源中定义的策略框架下行使账户权限范围内的资源管理操作。因此，这里还需要显式授予system:masters、system:nodes和system:serviceaccounts:kube-system组内的用户可以使用podsecuritypolicy/privileged资源，其他成功认证后的用户能够使用podsecuritypolicy/restricted资源。RBAC权限模型中，任何Subject都不能直接获得权限，它们需要借助分配到的角色获得权限。因此，下面先创建两个分别能使用podsecuritypolicy/privileged和podsecuritypolicy/restricted资源的ClusterRole。<br>下面的资源清单（clusterrole-with-psp.yaml）中创建了两个ClusterRole资源，授权psp-privileged可以使用名为privileged的安全策略，psp-restricted可以使用名为restricted的安全策略。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-restricted</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&#x27;policy&#x27;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&#x27;podsecuritypolicies&#x27;</span>]</span><br><span class="line">  <span class="attr">verbs:</span>  [<span class="string">&#x27;use&#x27;</span>]</span><br><span class="line">  <span class="attr">resourceNames:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">restricted</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-privileged</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroups:</span> [<span class="string">&#x27;policy&#x27;</span>]</span><br><span class="line">  <span class="attr">resources:</span> [<span class="string">&#x27;podsecuritypolicies&#x27;</span>]</span><br><span class="line">  <span class="attr">verbs:</span>  [<span class="string">&#x27;use&#x27;</span>]</span><br><span class="line">  <span class="attr">resourceNames:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">privileged</span></span><br></pre></td></tr></table></figure>

<p>下面的资源清单（clusterrolebinding-with-psp.yaml）定义了两个ClusterRoleBinding对象：前一个为system:masters、system:node和system:serviceaccounts:kube-system组的账户分配集群角色psp-privileged，从而能够使用任何安全配置；后一个为system: authenticated组内的账户分配集群角色psp-restricted，以禁止它们在Pod和容器上使用任何安全配置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">privileged-psp-user</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-privileged</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:masters</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:node</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:serviceaccounts:kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">restricted-psp-user</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line">  <span class="attr">kind:</span> <span class="string">ClusterRole</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">psp-restricted</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">kind:</span> <span class="string">Group</span></span><br><span class="line">  <span class="attr">apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">system:authenticated</span></span><br></pre></td></tr></table></figure>

<p>将上面两个资源清单中定义的ClusterRole和ClusterRoleBinding资源创建到集群上，即可为API Server启用PodSecurityPolicy准入控制器。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f clusterrole-with-psp.yaml -f clusterrolebinding-with-psp.yaml</span> </span><br><span class="line">clusterrole.rbac.authorization.k8s.io/psp-restricted created</span><br><span class="line">clusterrole.rbac.authorization.k8s.io/psp-privileged created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/privileged-psp-user created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/restricted-psp-user created</span><br></pre></td></tr></table></figure>

<h4 id="启用PSP准入控制器"><a href="#启用PSP准入控制器" class="headerlink" title="启用PSP准入控制器"></a>启用PSP准入控制器</h4><p>API Server的应用程kube-apiserver使用–enable-admission-plugins选项显式指定要加载的准入控制器列表，因此在该选项的列表中添加PodSecurityPolicy条目，并重启kube-apiserver程序便能启用PSP准入控制器。对于使用kubeadm部署的Kubernetes集群来说，编辑Master节点上的/etc/kubernetes/manifests/kube-apiserver.yaml配置清单，直接修改–enable-admission-plugins选项的值，并添加PodSecurityPolicy列表项即可，各列表项以逗号分隔。kubelet监控到/etc/kubernetes/manifests目录下的任何资源清单的改变时都会自动重建相关的Pod对象，因此编辑并保存kube-apiserver.yaml资源清单后，kubelet会通过重建相关的静态Pod而自动生效。<br>待kube-apiserver重启完成后，可通过监测API Server程序的运行状态及相关日志来判定PodSecurityPolicy准入控制器是否成功启用。以静态Pod运行kube-apiserver的日志同样可使用kubectl logs命令获取。如下面的命令及截取的结果所示，PodSecurityPolicy准入控制器已然成功加载。若Kubernetes的各系统类Pod资源运行状态正常，即表示安全策略已然成功启用。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl logs kube-apiserver-k8s-master01.ilinux.io -n kube-system</span></span><br><span class="line">……</span><br><span class="line">plugins.go:158] Loaded 13 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,…,PodSecurityPolicy,…</span><br><span class="line">plugins.go:161] Loaded 11 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurityPolicy,…</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p><font color="red">尽管PSP对已处于运行状态的Pod或容器没有影响，但对于正常运行中的Kubernetes集群来说，中途启用PodSecurityPolicy仍然可能会导致诸多难以预料的错误，尤其是没有事先为用到安全配置的Pod资源准备好可用的PSP资源时，这些Pod资源一旦重启便会因触发PSP策略而被阻止。</font><br>接下来，我们可通过能成功认证的普通账户测试其创建Pod资源时是否受限于restricted安全策略，以验证PodSecurityPolicy资源的生效状态。下面的命令尝试以dev名称空间的管理员mason用户创建一个使用了主机端口（hostPort）的Pod资源，但该操作被PodSecurityPolicy拒绝。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run pod-with-hostport --image=<span class="string">&quot;ikubernetes/demoapp:v1.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">      --port=80 --hostport=32080 -n dev --context=<span class="string">&#x27;mason@kubernetes&#x27;</span></span></span><br><span class="line">Error from server (Forbidden): pods &quot;pod-with-hostport&quot; is forbidden: unable to validate against any pod security policy: [spec.containers[0].hostPort: Invalid value: 32080: Host port 32080 is not allowed to be used. Allowed ports: []]</span><br></pre></td></tr></table></figure>

<p>因为mason用户由API Server成功认证后，将被自动归类到system:authenticated组和它所属的developers组中，但仅前一个组有权使用restricted安全策略。移除命令中的–hostport选项再次执行创建操作即可成功完成，感兴趣的读者可自行测试。另外，我们也可按此方式授权特定的用户拥有特定类型的Pod对象创建权限，但策略冲突时可能会导致意料不到的结果，因此将任何Pod安全策略应用到生产环境之前请务必做到充分测试。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/" class="post-title-link" itemprop="url">应用编排与管理(控制器)</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-10 10:31:33" itemprop="dateCreated datePublished" datetime="2022-02-10T10:31:33+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-18 13:43:33" itemprop="dateModified" datetime="2022-02-18T13:43:33+08:00">2022-02-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="应用编排与管理"><a href="#应用编排与管理" class="headerlink" title="应用编排与管理"></a>应用编排与管理</h1><p>应用程序版本升级时的在线应用更新操作也在实践中形成了灰度更新、蓝绿部署和金丝雀部署等解决方案。但这类的编排任务由传统的人工或工具化编排进化到了由ReplicaSet、Deployment或ReplicaSet控制器实现的半自动化编排机制，而HPA（Horizontal Pod Autoscaler）和VPA（Vertical Pod Autoscaler）控制器更是让这类任务彻底走向完全自动化。本章着重于介绍无状态应用控制器ReplicaSet和Deployment、系统应用控制器DaemonSet、单次任务控制器Job和定时作业控制器CronJob等。</p>
<h2 id="Kubernetes控制器基础"><a href="#Kubernetes控制器基础" class="headerlink" title="Kubernetes控制器基础"></a>Kubernetes控制器基础</h2><p>我们可以把API Server想象成存储Kubernetes资源对象的数据库系统，它仅支持预置的数据存储方案，每个方案对应于一种资源类型，客户端将API创建的、符合数据存储方案的数据项称为资源对象。但这些基于数据方案创建并存储于API Server中的仅是对象的定义。例如，一个Pod对象的定义并不代表某个以容器形式运行的应用，它仅停留在“纸面上”，我们还需要某个程序以特定的步骤调用容器运行时接口，按照Pod对象的定义创建出具体的应用容器来。这一类负责把API Server上存储的对象定义实例化到集群上的程序就是控制器。控制器需要运行为守护进程：一方面，注册监视API Server上隶属该控制器类型的对象定义（spec）的变动，及时将变动反映到集群中的对象实例之上；另一方面，通过控制循环（control loop，也可称为控制回路）持续监视集群上由其负责管控的对象实例的实际状态，在因故障、更新或其他原因导致当前状态（Status）发生变化而与期望状态（spec）时，通过实时运行相应的程序代码尝试让对象的真实状态向期望状态迁移和逼近。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210103349273.png" alt="image-20220210103349273"></p>
<h3 id="控制器与Pod资源"><a href="#控制器与Pod资源" class="headerlink" title="控制器与Pod资源"></a>控制器与Pod资源</h3><p>本质上讲，Kubernetes的核心就是控制理论，控制器中实现的控制回路是一种闭环（反馈）控制系统，该类型的控制系统基于反馈回路将目标系统的当前状态与预定义的期望状态相比较，二者之间的差异作为误差信号产生一个控制输出作为控制器的输入，以减少或消除目标系统当前状态与期望状态的误差，如图。这种控制循环在Kubernetes上也称为调谐循环（reconciliation loop）。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210112506987.png" alt="image-20220210112506987"></p>
<p>对Kubernetes来说，无论控制器的具体实现有多么简单或多么复杂，它基本都是通过定期重复执行如下3个步骤来完成控制任务。</p>
<blockquote>
<p>1）从API Server读取资源对象的期望状态和当前状态。<br>2）比较二者的差异，而后运行控制器中的必要代码操作现实中的资源对象，将资源对象的真实状态修正为Spec中定义的期望状态，例如创建或删除Pod对象，以及发起一个云服务API请求等。<br>3）变动操作执行成功后，将结果状态存储在API Server上的目标资源对象的status字段中。<br>图8-3给出了Kubernetes控制循环工作示意图。</p>
</blockquote>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210112550902.png" alt="image-20220210112550902"></p>
<p>任务繁重的Kubernetes集群上同时运行着数量巨大的控制循环，每个循环都有一组特定的任务要处理，为了避免API Server被请求淹没，需设定控制回路以较低的频率运行，默认每5分钟一次。同时，为了能及时触发由客户端提交的期望状态的更改，控制器向API Server注册监视受控资源对象，这些资源对象期望状态的任何变动都会由Informer组件通知给控制器立即执行而无须等到下一轮的控制循环。控制器使用工作队列将需要运行的控制循环进行排队，从而确保在受控对象众多或资源对象变动频繁的场景中尽量少地错过控制任务。<br>出于简化管理的目的，Kubernetes将数十种内置的控制器程序整合成了名为kube-controller-manager的单个应用程序，并运行为独立的单体守护进程，它是控制平面的重要组件，也整个Kubernetes集群的控制中心。提示<br>Kubernetes可用的控制器有attachdetach、bootstrapsigner、clusterrole-aggregation、cronjob、csrapproving、csrcleaner、csrsigning、daemonset、deployment、disruption、endpoint、garbagecollector、horizontalpodautoscaling、job、namespace、node、persistentvolume-binder、persistentvolume-expander、podgc、pvc-protection、replicaset、replicationcontroller、resourcequota、route、service、serviceaccount、serviceaccount-token、statefulset、tokencleaner和ttl等数十种。<br>工作负载范畴的控制器资源类型包括ReplicationController、ReplicaSet、Deployment、DaemonSet、StatefulSet、Job和CronJob等，它们各自代表一种类型的Pod控制器资源，分别实现不同的应用编排机制。<br>通常，一个工作负载控制器资源通常应该包含3个基本的组成部分。</p>
<ul>
<li>标签选择器：匹配并关联Pod对象，并据此完成受其管控的Pod对象的计数。</li>
<li>期望的副本数：期望在集群中精确运行受控的Pod对象数量。</li>
<li>Pod模板：用于新建Pod对象使用的模板资源。注意</li>
</ul>
<p>DaemonSet控制器用于确保集群中每个工作节点或符合条件的每个节点上都运行着一个Pod副本，而非某个预设的精确数量值，因而不具有上面组成部分中的第二项。<br>例如，如图8-4所示的Deployment控制器eshop-deploy对象使用app=eshop为标签选择器，以过滤当前名称空间中的Pod对象，它期望能够匹配到的Pod对象副本数量精确为4个。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210112622509.png" alt="image-20220210112622509"></p>
<p>将eshop-deploy对象创建到集群上之后，Deployment控制器将根据该对象的定义标签选择器过滤符合条件的Pod对象并对其进行计数，少于指定数量的缺失部分将由控制器通过Pod模板予以创建，而多出的副本也将由控制器请求终止及删除。<br>通常，对于那些以Deployment、DaemonSet或StatefulSet控制器编排的且需要长期运行的容器应用，其应用更新、回滚和扩缩容也是编排操作的核心任务。但这类任务所导致的Pod对象的变动势必会影响透过Service来访问应用服务的客户端，如图8-5所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210125851052.png" alt="image-20220210125851052"></p>
<p>显然，生产环境中的应用编排过程通常不能影响或过度影响当前正在获取服务的用户体验，达成该类目标也是应用程序控制器的核心功能之一，事实上，Deployment等甚至允许用户自定义更新策略来自定义应用升级过程。</p>
<h3 id="Pod模板资源"><a href="#Pod模板资源" class="headerlink" title="Pod模板资源"></a>Pod模板资源</h3><p>Pod模板资源是Kubernetes API的常用资源类型，常用于为控制器指定自动创建Pod资源对象时所需的配置信息。内嵌于控制器的Pod模板的配置信息中不需要apiVersion和kind字段，除此之外的其他内容跟定义自主式Pod对象所支持的字段几乎完全相同，这包括metadata和spec及其内嵌的其他各字段。<br>工作负载控制器类的资源的spec字段通常都要内嵌replicas、selector和template字段，其中template便是用于定义Pod模板。下面是一个定义在ReplicaSet资源中的模板资源示例，它基于ikubernetes/demoapp:v1.0镜像简单定义了一个应用，并同时配置了存活探针和就绪探针。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">replicaset-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minReadySeconds:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">stable</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">release:</span> <span class="string">stable</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br></pre></td></tr></table></figure>

<p>如上示例中，spec.template字段在定义时仅给出了metadata和spec两个字段，它的使用方法与自主式Pod资源几乎完全相同。一个特别的建议是，生产环境中运行的Pod对象务必要添加存活探针和就绪探针，否则Kubernetes无法准确判定应用的存活状态和就绪状态，而只能把处于运行中的容器进程一律视为在健康运行，而健康运行的容器进程则一律视为就绪。显然，定义在Pod模板中的存储卷资源将由当前模板创建出的所有Pod实例共享使用，因此定义时务必要确保该存储卷允许多路客户端同时访问，以及多路写操作时的数据安全性。<br>应用编排是Kubernetes的核心功能，因而控制器资源及其使用的Pod模板也随之成为最常用的两种资源类型。</p>
<h2 id="ReplicaSet控制器"><a href="#ReplicaSet控制器" class="headerlink" title="ReplicaSet控制器"></a>ReplicaSet控制器</h2><p>Kubernetes较早期的版本中仅有ReplicationController一种类型的Pod控制器，后来又陆续引入了更多的控制器实现，这其中就包括用来取代ReplicationController的新一代实现ReplicaSet。事实上，ReplicaSet除了支持基于集合的标签选择器，以及它的滚动更新（RollingUpdate）机制要基于更高级的Deployment控制器实现之外，目前ReplicaSet的其余功能基本与ReplicationController相同。考虑到Kubernetes强烈推荐使用ReplicaSet控制器，且表示ReplicationController不久后即将废弃，因而本节重点介绍ReplicaSet控制器。</p>
<h3 id="功能分析"><a href="#功能分析" class="headerlink" title="功能分析"></a>功能分析</h3><p>ReplicaSet（简称RS）是工作负载控制器类型的一种实现，隶属于名称空间级别，主要用于编排无状态应用，核心目标在于确保集群上运行有指定数量的、符合其标签选择器的Pod副本。ReplicaSet规范由标签选择器、期望的副本数和Pod模板3个主要因素所定义，它在控制循环中持续监视同一名称空间中运行的Pod对象，并在每个循环中将标签选择器筛选出的Pod数量与期望的数量相比较，通过删除多余的Pod副本或借助于模板创建出新的Pod来确保该类Pod对象数量能始终吻合所期望的数量。<br>标签选择器是ReplicaSet判断一个Pod对象是否处于其作用域的唯一标准，Pod模板仅在补足缺失数量的Pod对象时使用，这意味着由其他Pod规范所创建的Pod对象也存在进入某个ReplicaSet作用域的可能性。因而，我们要精心设计同一名称空间中使用的标签选择器，以竭力避免它们以相同的条件出现在不同的控制器对象之上，这种原则同样交叉适用于其他类型的控制器对象。<br>ReplicaSet规范中的副本数量、标签选择器，甚至是Pod模板都可以在对象创建后随时按需进行修改。降低期望的Pod副本数量会导致删除现有的Pod对象，而增加该数量值会促使ReplicaSet控制器根据模板创建出新的Pod对象。修改标签选择器会导致ReplicaSet在当前名称空间中匹配Pod标签，这可能会让它无法再匹配到现有Pod副本的标签，进而触发必要的删除或创建操作。另外，ReplicaSet不会关注筛选到的现存Pod对象或者由其自身创建的Pod对象中的实际内容，因此Pod模板的改动也仅会对后来新建的Pod副本有影响。事实上，ReplicaSet所支持的更新机制也正是建立在Pod模板更新后以“删除后的自动重建”机制之上。<br>相较于手动创建和管理Pod对象来说，ReplicaSet控制器能够实现以下功能。</p>
<ul>
<li>确保Pod对象的数量精确反映期望期：ReplicaSet对象需要确保由其控制运行的Pod副本数量精确吻合配置中定义的期望值，否则会自动补足所缺或终止所余。</li>
<li>确保Pod健康运行：探测到由其管控的Pod对象健康状态检查失败或因其所在的工作节点故障而不可用时，自动请求控制平面在其他工作节点创建缺失的Pod副本。</li>
<li>弹性伸缩：应用程序业务规模因各种原因时常存在明显波动，如波峰或波谷期间，可以通过改动ReplicaSet控制器规范中的副本数量动态调整相关Pod资源对象的数量，甚至是借助HPA控制器实现Pod资源规模的自动伸缩。</li>
</ul>
<p>但ReplicaSet并非是用户使用无状态应用控制器的最终形态，Deployment控制器基于ReplicaSet实现了滚动更新、自动回滚、金丝雀部署甚至是蓝绿部署等更为高级和自动化的任务编排功能，因而成为用户在编排无状态应用时更高级的选择。</p>
<h3 id="ReplicaSet基础应用"><a href="#ReplicaSet基础应用" class="headerlink" title="ReplicaSet基础应用"></a>ReplicaSet基础应用</h3><p>ReplicaSet由kind、apiVersion、metadata、spec和status这5个一级字段组成，它的基本配置框架如下面的配置规范所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">…</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">minReadySeconds</span> <span class="string">&lt;integer&gt;</span>  <span class="comment"># Pod就绪后多少秒内，任一容器无崩溃方可视为“就绪”</span></span><br><span class="line">  <span class="string">replicas</span> <span class="string">&lt;integer&gt;</span> <span class="comment"># 期望的Pod副本数，默认为1</span></span><br><span class="line">  <span class="attr">selector:</span>    <span class="comment"># 标签选择器，必须匹配template字段中Pod模板中的标签</span></span><br><span class="line">    <span class="string">matchExpressions</span> <span class="string">&lt;[]Object&gt;</span>     <span class="comment"># 标签选择器表达式列表，多个列表项之间为“与”关系</span></span><br><span class="line">    <span class="string">matchLabels</span> <span class="string">&lt;map[string]string&gt;</span> <span class="comment"># map格式的标签选择器</span></span><br><span class="line">  <span class="attr">template:</span>    <span class="comment"># Pod模板对象</span></span><br><span class="line">    <span class="attr">metadata:</span>  <span class="comment"># Pod对象元数据</span></span><br><span class="line">      <span class="attr">labels:</span>  <span class="comment"># 由模板创建出的Pod对象所拥有的标签，必须要能够匹配前面定义的标签选择器</span></span><br><span class="line">    <span class="attr">spec:</span>      <span class="comment"># Pod规范，格式同自主式Pod</span></span><br><span class="line">      <span class="string">……</span></span><br></pre></td></tr></table></figure>

<p>ReplicaSet规范中用于定义标签选择器的selector字段为必先字段，它支持matchLabels和matchExpressions两种表示格式。前者使用字符串映射格式，以key: value形式表达要匹配的标签；后者支持复杂的表达式格式，支持基于“等值（运算符=和!=）”和基于“集合”（运算符为in和notin等）的表示方法，同时定义二者时的内生逻辑为“与”关系。Pod模板中定义的标签必须要能匹配到其所属ReplicaSet对象的标签选择器，否则，ReplicaSet将因始终不具有足额的Pod副本数而无限创建下去，这相当于程序代码中无终止条件的死循环。另外，minReadySeconds字段用于指定在Pod对象启动后的多长时间内其容器未发生崩溃等异常情况即被视为“就绪”，默认为值0秒，表示一旦就绪性探测成功，即被视作可用。<br>将“Pod模板资源”一节中的示例保存于资源清单文件中，例如replicaset-demo.yaml，而后即可使用类似如下命令将其创建到集群上来观察其运行特性。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f replicaset-demo.yaml</span> </span><br><span class="line">replicaset.apps/replicaset-demo created</span><br></pre></td></tr></table></figure>

<p>ReplicaSet对象的详细描述信息会输出对象的重点信息，例如标签选择器、Pod状态、Pod模板和相关的事件等。ReplicaSet控制器会追踪作用域的各Pod的运行状态，并把它们归类到Running、Waiting、Succeeded和Failed这4种状态之中。default名称空间中并未存在使用app: replicaset-demo这一标签的Pod对象，因此replicaset-demo需要根据指定的Pod模板创建出replicas字段指定数量的Pod实例，它们的名称以其所属的ReplicaSet对象的名称为前缀。如下的命令输出中可知，replicaset-demo成功创建出的两个Pod实例均处于健康运行状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe replicasets/replicaset-demo</span></span><br><span class="line">Name:         replicaset-demo</span><br><span class="line">Namespace:    default</span><br><span class="line">Selector:     app=demoapp,release=stable</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  Replicas:  2 current / 2 desired</span><br><span class="line">Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  app=replicaset-demo</span><br><span class="line">  Containers:</span><br><span class="line">   demoapp:</span><br><span class="line">    Image:        ikubernetes/demoapp:v1.0</span><br><span class="line">    Port:         80/TCP</span><br><span class="line">    Host Port:    0/TCP</span><br><span class="line">    Liveness:   http-get http://:80/livez delay=5s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Readiness:  http-get http://:80/readyz delay=15s timeout=1s period=10s #success=1 #failure=3</span><br><span class="line">    Environment:  &lt;none&gt;</span><br><span class="line">    Mounts:       &lt;none&gt;</span><br><span class="line">  Volumes:        &lt;none&gt;</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age   From                   Message</span><br><span class="line">  ----    ------            ----  ----                   -------</span><br><span class="line">  Normal  SuccessfulCreate  36s   replicaset-controller  Created pod: replicaset-demo-z6bqt</span><br><span class="line">  Normal  SuccessfulCreate  36s   replicaset-controller  Created pod: replicaset-demo-vwb5g</span><br></pre></td></tr></table></figure>

<p>我们也可以单独打印ReplicaSet对象的简要及扩展信息来了解其运行状态，例如期望的Pod副本数（DESIRED）、当前副本数（CURRENT）和就绪的副本数（READY），以及使用的镜像和标签选择器等，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets/replicaset-demo -o wide</span></span><br><span class="line">NAME  DESIRED  CURRENT  READY  AGE  CONTAINERS  IMAGES  SELECTOR</span><br><span class="line">replicaset-demo   2      2         2      77s   demoapp     ……   app=demoapp,…</span><br></pre></td></tr></table></figure>

<p>通常，就绪的副本数量与期望的副本数量相同便意味着该ReplicaSet控制器以符合期望的状态运行于集群之上，由其编排的容器应用可正常借助专用的Service对象向客户端提供服务。<br>长期运行中的Kubernetes系统环境存在着不少导致Pod对象数目与期望值不符合的可能因素，例如作用域内Pod对象的意外删除、Pod对象标签的变动、ReplicaSet控制器的标签选择器变动，甚至是作用域内Pod对象所在的工作节点故障等。ReplicaSet控制器的调谐循环能实时监控到这类异常，并及时启动调谐操作。<br>任何原因导致的标签选择器匹配Pod对象缺失，都会由ReplicaSet控制器自动补足，我们可通过手动修改replicaset-demo标签选择器作用域内任一现有Pod对象标签，使得匹配失败，则该控制器的标签选择器会触发控制器的Pod对象副本缺失补足机制，其操作步骤如下。</p>
<ul>
<li>步骤1：获取replicaset-demo标签选择器作用域内的一个Pod对象：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">pod=$(kubectl get pods -l app=demoapp,release=stable -o jsonpath=&#123;.items[1].metadata.name&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">echo</span> <span class="variable">$pod</span></span></span><br><span class="line">replicaset-demo-z6bqt</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤2：删除该Pod对象的任意一个标签，例如app，使其无法再匹配到replicaset-demo的标签选择器：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label pod <span class="variable">$pod</span> app-</span></span><br><span class="line">pod/replicaset-demo-z6bqt labeled</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤3：验证replicaset-demo是否将此前的Pod对象替换为了新建的Pod对象：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable</span></span><br><span class="line">NAME            READY   STATUS   RESTARTS   AGE</span><br><span class="line">replicaset-demo-fcrkl      0/1     Running   0          4s</span><br><span class="line">replicaset-demo-vwb5g      1/1     Running   0          5m49s</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤4：可以看到此前的Pod对象依然存在，但它成为自主式Pod对象，而代表上级引用关系的metadata.ownerReferences字段变成空值，于是下面的命令便不再有返回值。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods <span class="variable">$pod</span> -o jsonpath=&#123;.metadata.ownerReferences&#125;</span></span><br></pre></td></tr></table></figure>

<p>由此可见，通过修改Pod资源的标签即可将其从控制器的管控之下移出，若修改后的标签又能被其他控制器资源的标签选择器命中，则该Pod对象又成为隶属另一控制器的副本。若修改其标签后的Pod对象不再隶属于任何控制器，它就成了自主式Pod。<br>另一方面，一旦被标签选择器匹配到的Pod对象数量因任何原因超出期望值，多余的部分也将被控制器自动删除。例如，我们可以为此前移出作用域的Pod对象重新添加app标签，让其能够再次匹配到replicaset-demo的标签选择器，这将触发控制器删除多余的Pod对象，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label pods <span class="variable">$pod</span> app=demoapp</span></span><br><span class="line">pod/replicaset-demo-z6bqt labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable</span></span><br><span class="line">NAME                    READY    STATUS        RESTARTS   AGE</span><br><span class="line">replicaset-demo-fcrkl    1/1     Terminating   0          2m17s</span><br><span class="line">replicaset-demo-vwb5g    1/1     Running       0          8m2s</span><br><span class="line">replicaset-demo-z6bqt    1/1     Running       0          8m2s</span><br></pre></td></tr></table></figure>

<h3 id="应用更新与回滚"><a href="#应用更新与回滚" class="headerlink" title="应用更新与回滚"></a>应用更新与回滚</h3><p>ReplicaSet不会校验作用域内处于活动状态的Pod对象的内容，改动Pod模板的定义对已经创建完成的活动对象无效，但在用户手动删除其旧版本的Pod对象后能够自动以新代旧，实现控制器下的应用更新。通过修改Pod中某容器的镜像文件版本进行应用程序的版本升级是最常见的应用更新场景。<br><font color="red">尽管ReplicaSet资源的Pod模板可随时按需修改，但它仅影响其后新建的Pod对象，对已有的Pod副本不产生作用，因此，ReplicaSet自身并不会自动触发更新机制，它依赖于用户的手动触发机制。Deployment控制器是建立在ReplicaSet之上的，专用于支持声明式更新功能的更高级实现。</font><br>在配置清单replicaset-demo.yaml中定义的replicaset-demo资源中，修改Pod模板中的容器使用的镜像文件为更高的版本，例如下面示例性配置片段中的ikubernetes/demoapp: v1.1，而后将变动的配置清单重新应用到集群上便可完成ReplicaSet控制器资源的更新。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.1</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>将更新后的配置清单应用到集群之中，可以发现，现有各Pod对象中demoapp容器的镜像版本与replicaset-demo的Pod模板中的镜像版本存在差异。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f replicaset-demo.yaml</span></span><br><span class="line">replicaset.apps/replicaset-demo configured</span><br></pre></td></tr></table></figure>

<p>首先，我们可以使用如下命令获取活动对象replicaset-demo的Pod模板中定义的镜像文件及版本信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets/replicaset-demo -o jsonpath=&#123;.spec.template.spec.containers[0].image&#125;</span></span><br><span class="line">ikubernetes/demoapp:v1.1</span><br></pre></td></tr></table></figure>

<p>接着，通过如下命令获取replicaset-demo控制下的所有Pod对象中的demoapp容器的镜像文件及版本信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable \</span></span><br><span class="line"><span class="language-bash">-o jsonpath=<span class="string">&#x27;&#123;range .items[*]&#125;[&#123;.metadata.name&#125;, &#123;.spec.containers[0].image&#125;]&#123;&quot;\n&quot;&#125;&#123;end&#125;&#x27;</span></span></span><br><span class="line">[replicaset-demo-vwb5g, ikubernetes/demoapp:v1.0]</span><br><span class="line">[replicaset-demo-z6bqt, ikubernetes/demoapp:v1.0]</span><br></pre></td></tr></table></figure>

<p>上面两个命令及返回结果证实了“更新Pod模板不会对现在的Pod对象产生实质影响”的结论。另一方面，Pod中定义的容器及镜像的字段是不可变字段，我们无法在Pod创建完成后动态更新其容器镜像，因而接下来只有手动将replicaset-demo的现有Pod对象移出其标签选择器作用域（修改标签或删除Pod对象）来触发基于新的Pod模板新建Pod对象，以完成应用的版本更新。</p>
<h4 id="常见更新机制"><a href="#常见更新机制" class="headerlink" title="常见更新机制"></a>常见更新机制</h4><p>常见的更新机制有如下两种。</p>
<h5 id="1）单批次替换"><a href="#1）单批次替换" class="headerlink" title="1）单批次替换"></a>1）单批次替换</h5><p>一次性替换所有Pod对象（见图8-6）：也称为重建式更新（recreate），是最为简单、高效的更新方式，但会导致相应的服务在一段时间内（至少一个Pod对象更新完成并就绪）完全不可用，因而一般不会用在对服务可用性有较高要求的生产环境中。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210130435715.png" alt="image-20220210130435715"></p>
<h5 id="2）多批次替换"><a href="#2）多批次替换" class="headerlink" title="2）多批次替换"></a>2）多批次替换</h5><p>一次仅替换一批Pod对象（见图8-7）：也称为滚动更新，是一种略复杂的更新方式，需要根据实时业务量和Pod对象的总体承载力做好批次规划，而后待一批Pod对象就绪后再更新另一批，直到全部完成为止；该策略实现了不间断服务的目标，但更新过程中会出现不同的应用版本并存且同时提供服务的状况。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210130506818.png" alt="image-20220210130506818"></p>
<p>接下来，我们在replicaset-demo之上分别进行更新测试来验证这两种方式的更新效果。我们先为replicaset-demo作用域内的各Pod对象创建一个ClusterIP类型的Service对象，以方便客户端在更新过程中进行请求测试，以下配置保存于service-for-replicaset-demo.yaml清单文件中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">    <span class="attr">release:</span> <span class="string">stable</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>接下来，将上面配置清单中的Service对象demoapp通过如下命令创建到集群之上，随后的应用测试将以之作为访问入口。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f service-for-replicaset-demo.yaml</span> </span><br><span class="line">service/demoapp created</span><br></pre></td></tr></table></figure>

<h4 id="重建式更新测试"><a href="#重建式更新测试" class="headerlink" title="重建式更新测试"></a>重建式更新测试</h4><ul>
<li>步骤1：在管理节点上打开一个新的终端，创建一个临时的客户端Pod并发起持续性的请求测试，以验证单批次更新过程中是否会发生服务中断。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run pod-<span class="variable">$RANDOM</span> --image=ikubernetes/admin-toolbox:v1.0 -it \</span></span><br><span class="line"><span class="language-bash">          --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@pod-28426 /]#</span><br></pre></td></tr></table></figure>

<p>此时，在临时Pod的交互式接口中运行如下循环进行请求测试，立即可以看到v1.0版本的demoapp的响应结果；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@pod-28426 /]# while true; do curl --connect-timeout 1 \</span><br><span class="line">         demoapp.default.svc; sleep 1; done</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤2：删除replicaset-demo作用域内的所有Pod对象，而后观察其更新结果。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods -l app=demoapp,release=stable</span></span><br><span class="line">pod &quot;replicaset-demo-vwb5g&quot; deleted</span><br><span class="line">pod &quot;replicaset-demo-z6bqt&quot; deleted</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤3：使用如下命令查看是否生成具有同样标签的新Pod对象。在如下命令结果中的任何一个新Pod对象就绪之前，curl命令返回结果会出现一定数量的请求超时，这是单批次更新的必然结果；验证完成后，应该停止测试循环。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable</span>      </span><br><span class="line">NAME                   READY    STATUS    RESTARTS   AGE</span><br><span class="line">replicaset-demo-mjc5x   0/1     Running   0          10s</span><br><span class="line">replicaset-demo-w5lxw   0/1     Running   0          10s</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤4：验证这些新的Pod对象中demoapp容器是否更新为指定的新镜像文件及版本。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable \</span></span><br><span class="line"><span class="language-bash">    -o jsonpath=<span class="string">&#x27;&#123;range .items[*]&#125;[&#123;.metadata.name&#125;, &#123;.spec.containers[0].</span></span></span><br><span class="line">    image&#125;]&#123;&quot;\n&quot;&#125;&#123;end&#125;&#x27;</span><br><span class="line">[replicaset-demo-mjc5x, ikubernetes/demoapp:v1.1]</span><br><span class="line">[replicaset-demo-w5lxw, ikubernetes/demoapp:v1.1]</span><br></pre></td></tr></table></figure>

<p>事实上，修改Pod模板时，不仅能替换镜像文件的版本，甚至可以将其替换为其他应用程序的镜像，只不过此类需求并不多见。若同时改动的还有Pod模板中的其他字段，在新旧更替的过程中，它们也将随之被应用。</p>
<h4 id="滚动式更新测试"><a href="#滚动式更新测试" class="headerlink" title="滚动式更新测试"></a>滚动式更新测试</h4><ul>
<li>步骤1：同前一节中的测试方式相似，我们需要在管理节点上打开一个新的终端，创建一个临时的客户端Pod以发起持续性的请求测试，以验证滚动更新过程中是否会发生服务中断。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run pod-<span class="variable">$RANDOM</span> --image=ikubernetes/admin-toolbox:v1.0 -it \</span></span><br><span class="line"><span class="language-bash">          --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@pod-10196 /]#</span><br></pre></td></tr></table></figure>

<p>此时，在临时Pod的交互式接口中运行如下循环进行请求测试，立即可以看到v1.0版本的demoapp的响应结果；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@pod-10196 /]# while true; do curl --connect-timeout 1 \</span><br><span class="line">         demoapp.default.svc; sleep 1; done</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤2：更新replicaset-demo的Pod模板中demoapp容器使用ikubernetes/demoapp:v1.2镜像。本次，我们使用更便捷的kubectl set image命令。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">set</span> image replicasets/replicaset-demo demoapp=<span class="string">&quot;ikubernetes/demoapp:v1.2&quot;</span></span></span><br><span class="line">replicaset.apps/replicaset-demo image updated</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤3：将replicaset-demo作用域的仅有的两个Pod对象分成两个批次进行更新。为了便于识别待删除对象，下面的命令获取现有的相关两个Pod对象的名称保存在数组中，并打印出相关的Pod对象名称。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">pods=($(kubectl get pods -l app=demoapp,release=stable  \</span></span><br><span class="line"><span class="language-bash">    -o jsonpath=<span class="string">&quot;&#123;range .items[*]&#125;&#123;.metadata.name&#125;&#123;&#x27;\t&#x27;&#125;&#123;end&#125;&quot;</span>))</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">echo</span> <span class="variable">$&#123;pods[@]&#125;</span></span></span><br><span class="line">replicaset-demo-l857r replicaset-demo-r9t8f</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤4：尝试删除一个Pod对象，以触发启动更新操作。随后，立即运行一个交互式的监视命令持续监视replicaset-demo作用域内各Pod对象的状态变动，可以看到旧版本Pod对象的删除及新Pod创建过程中的事件。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods <span class="variable">$&#123;pods[1]&#125;</span></span></span><br><span class="line">pod &quot;replicaset-demo-r9t8f&quot; deleted</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable -w</span></span><br><span class="line">NAME                    READY   STATUS         RESTARTS   AGE</span><br><span class="line">replicaset-demo-l857r   1/1     Running        0          143m</span><br><span class="line">replicaset-demo-r9t8f   1/1     Terminating    0          143m</span><br><span class="line">replicaset-demo-zxsh7   0/1     Running        0          8s</span><br><span class="line">replicaset-demo-zxsh7   1/1     Running        0          20s</span><br></pre></td></tr></table></figure>

<p>在删除命令执行后的新建Pod对象replicaset-demo-zxsh7就绪之前，客户端持续发出访问请求的所有响应均应该来自未删除的旧版本Pod对象。新Pod就绪后才能由相应的Service对象demoapp识别为Ready状态的后端端点，并路由请求报文至该端点，此时响应报文来自一新一旧两个版本的Pod对象，下面的内容就截取自相关测试命令的返回结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iKubernetes demoapp v1.2 !!……, ServerName: replicaset-demo-zxsh7, ServerIP: 10.244.3.16!</span><br><span class="line">iKubernetes demoapp v1.1 !!……, ServerName: replicaset-demo-l857r, ServerIP: 10.244.1.41!</span><br></pre></td></tr></table></figure>

<ul>
<li>步骤5：再删除另一个旧版本的Pod对象，待替换的新Pod就绪后，测试命令的响应内容均来自于新版本的Pod对象，滚动更新也就全部完成了。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods <span class="variable">$&#123;pods[0]&#125;</span></span></span><br><span class="line">pod &quot;replicaset-demo-l857r&quot; deleted</span><br></pre></td></tr></table></figure>

<p>由以上测试过程可知，滚动更新过程不会导致服务中断，唯一的问题在于两个版本有短暂的共存期，若两个版本使用了不同的数据库格式，则需要禁 止新版本执行写操作，以免数据异常。必要时，用户还可以将Pod模板改回旧的版本进行应用的“降级”或“回滚”，它的操作过程与上述过程类似，不同之处仅是将镜像文件改为过去曾使用过的历史版本。</p>
<h3 id="应用扩容与缩容"><a href="#应用扩容与缩容" class="headerlink" title="应用扩容与缩容"></a>应用扩容与缩容</h3><p>改动ReplicaSet控制器对象配置中期望的Pod副本数量（replicas字段）会由控制器实时做出响应，从而实现应用规模的水平伸缩。replicas的修改及应用方式同Pod模板，不过，kubectl提供了一个专用的子命令scale用于实现应用规模的伸缩，它支持从资源清单文件中获取新的目标副本数量，也可以直接在命令行通过–replicas选项读取，例如将replicaset-demo控制器的Pod副本数量提升至4个：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale replicasets/replicaset-demo --replicas=4</span></span><br><span class="line">replicaset.apps/replicaset-demo scaled</span><br></pre></td></tr></table></figure>

<p>由下面显示的rs-example资源的状态可以看出，将其Pod资源副本扩展至5个的操作已经成功完成：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets/replicaset-demo</span></span><br><span class="line">NAME              DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset-demo   4         4         4       3h</span><br></pre></td></tr></table></figure>

<p>ReplicaSet缩容的方式与扩容方式相同，我们只需要明确指定目标副本数量即可。例如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale replicasets/replicaset-demo --replicas=1</span></span><br><span class="line">replicaset.apps/replicaset-demo scaled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets/replicaset-demo</span>               </span><br><span class="line">NAME              DESIRED   CURRENT   READY   AGE</span><br><span class="line">replicaset-demo   1         1         1       3h</span><br></pre></td></tr></table></figure>

<p>另外，kubectl scale命令还支持在现有Pod副本数量符合指定值时才执行扩展操作，这仅需要为命令使用–current-replicas选项即可。例如，下面的命令表示如果replicaset-demo目前的Pod副本数量为2，就将其扩展至3个：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale replicasets/replicaset-demo --current-replicas=2 --replicas=3</span></span><br><span class="line">error: Expected replicas to be 2, was 1</span><br></pre></td></tr></table></figure>

<p>但由于replicaset-demo控制器现存的副本数量是1个，上面的扩容操作不会真正执行，而是仅返回了错误提示。</p>
<h3 id="高级更新策略"><a href="#高级更新策略" class="headerlink" title="高级更新策略"></a>高级更新策略</h3><p>除联合使用多个ReplicaSet外，我们还能为应用更新功能模拟实现更加灵活和更易于维护的滚动更新、金丝雀部署和蓝绿部署等。1. 滚动更新<br>ReplicaSet上的应用更新也能够不改变现有资源（简称为rs-old）的定义，而是借助创建一个有着新版本Pod模板的新ReplicaSet资源（简称为rs-new）实现。新旧版本的ReplicaSet使用了不同的标签选择器，它们筛选相同的Pod标签，但至少会有一个标签匹配到不同的值，余下的标签各自匹配相同值，相关的Service对象的标签选择器会匹配这些拥有相同值的标签。<br>我们可以设计用rs-old和rs-new共同筛选app、release和version标签，其中app和release分别匹配相同值，例如app=demoapp、release=stable，而version则匹配不同值，如rs-old匹配version=v1.0，而rs-new匹配version=v1.1。同时，Service的标签选择器则筛选app=demoapp和release=stable，以便能匹配到更新期间两个不同ReplicaSet作用域内不同版本的Pod对象。具体如图8-8所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210132418459.png" alt="image-20220210132418459"></p>
<p>rs-new的初始副本数为0，在更新过程中，我们以特定的分批（每个批次简称1个单位或步长）策略逐步增加rs-new的replicas字段值，并同步降低rs-old的replicas字段值，直到rs-new副本数为期望的数量，而rs-old的副本数为0时更新过程结束，如图8-9所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210132457826.png" alt="image-20220210132457826"></p>
<p>具体操作时，我们可以采取如下3种不同的策略进行整个滚动更新过程：</p>
<ul>
<li>先于rs-new上增加1个单位的Pod副本，待全部就绪后再于rs-old上降低1个单位的副本数，待所有旧Pod成功终止后进行下一批次；</li>
<li>先于rs-old上降低1个单位的Pod副本，待所有旧Pod成功终止后再于rs-new上增加1个单位的副本数，再待所有新Pod对象就绪后进行下一批次；</li>
<li>以同步的方式进行，rs-new上新增1个单位的Pod对象，与此同时，rs-old上降低1个单位的副本数，等新Pod全部就绪且旧Pod全部成功终止后进行下一批次。</li>
</ul>
<p>由此可见，第一和第三种策略会导致更新过程中，新旧两个ReplicaSet资源作用域内的Pod对象总和超出用户期望的副本数，而第二种和第三种策略会使得更新过程中Service的可用后端端点数缺少1个单位，但第三种策略能够更快地完成更新过程。因而，选择更新策略就存在两种重要的判断标准：一是Kubernetes集群资源是否可承载短时间内Pod数量的增加；另一个是支撑相应服务请求总量所依赖的Pod实例数。<br>无论采取哪种滚动策略，我们都可以让更新过程在完成第一批次后暂停一段时长，根据新版本发现的问题以及路由到新版本应用上的用户体验和反馈，来判断是继续完成余下批次的更新操作，还是撤回此前一个批次的更新操作。显然，这种方式能够降低更新过程中的风险，它通过放出的一只“金丝雀”（canary）避免了更大范围的更新故障。<br>另外，我们可保留最近一个范围内的副本数为0的旧版本的ReplicaSet资源于更新历史中，以便按需对比历史更新中所做出的改动，随时按需以类似于“回滚”的更新策略应用至历史中的任一版本。但显然上述的这些操作步骤过于烦琐，以手动方式操作极易出错，幸运的是，更高级别的Pod控制器Deployment能自动实现滚动更新和回滚，并为用户提供了自定义更新策略的接口，这些内容我们将在8.3节中展开说明。2. 蓝绿部署<br>滚动更新过程中，会存在两个不同版本的应用同时向客户端提供服务，且更新和回滚过程耗时较长。另一种更为妥帖的更新方式是，在旧版本ReplicaSet资源运行的同时直接创建一个全Pod副本的新版本ReplicaSet，待所有的新Pod就绪后一次性地将客户端流量全部迁至新版本之上，这种更新策略也称为蓝绿部署（Blue-Green Deployment）。<br>显然，为了避免更新过程中新旧版本ReplicaSet资源的Pod完全并存时Service将流量发往不同版本的Pod对象，我们需要设定Service使用的标签选择器仅能匹配到其中一个版本的Pod对象。最简单的实现方式是让Service与ReplicaSet使用完全相同的标签选择器，但每次更新过程中，在新版本所有Pod就绪之后，修改其标签选择器与新版本的ReplicaSet的标签选择器相同，如图8-10所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210132515650.png" alt="image-20220210132515650"></p>
<p>Service将所有客户端流量代理至新版本的Pod上运行一段时长之后，若确定运行正常，即可将旧版本ReplicaSet的副本数置零后保存到历史版本序列中。相较于滚动更新来说，蓝绿部署实现步骤要简单很多，用户完全能够以手动方式完成。<br>例如，下面的配置清单通过环境变量的方式定义了一个可复用的ReplicaSet资源规范，其中的DEPLOY代表部署类型blue或green，而VERSION则用于表示demoapp的程序版本号，它保存在replicaset-blue-green.yaml文件中。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ReplicaSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">rs-$&#123;DEPLOY&#125;</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">minReadySeconds:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">ctr:</span> <span class="string">rs-$&#123;DEPLOY&#125;</span></span><br><span class="line">      <span class="attr">version:</span> <span class="string">$&#123;VERSION&#125;</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">ctr:</span> <span class="string">rs-$&#123;DEPLOY&#125;</span></span><br><span class="line">        <span class="attr">version:</span> <span class="string">$&#123;VERSION&#125;</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:$&#123;VERSION&#125;</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>而下面的这个配置清单以类似的方式定义了一个方便复用的Service资源规范，其中的环境变量的作用与前一个配置清单中的环境变量相同，该配置保存在service-blue-green.yaml文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: demoapp-svc</span><br><span class="line">  namespace: default</span><br><span class="line">spec:</span><br><span class="line">  type: ClusterIP</span><br><span class="line">  selector:</span><br><span class="line">    app: demoapp</span><br><span class="line">    ctr: rs-$&#123;DEPLOY&#125;</span><br><span class="line">    version: $&#123;VERSION&#125;</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br><span class="line">    targetPort: 80</span><br></pre></td></tr></table></figure>

<p>为了测试蓝绿部署的效果，我们先将replicaset-blue-green.yaml配置清单中的ReplicaSet资源以rs-blue的名称部署为待更新的老版本，它使用1.0的demoapp镜像，创建的Pod对象名称均以rs-blue为前缀。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">DEPLOY=<span class="string">&#x27;blue&#x27;</span> VERSION=<span class="string">&#x27;v1.0&#x27;</span> envsubst &lt; replicaset-blue-green.yaml | kubectl apply -f -</span></span><br><span class="line">replicaset.apps/rs-blue created</span><br></pre></td></tr></table></figure>

<p>提示<br>envsubst是一个shell命令，能够从标准输入接收文本，完成环境变量替换。<br>接下来，将service-blue-green.yaml配置清单中的Service资源demoapp-svc部署到集群上，它使用同rs-blue对象相同的标签选择器，等rs-blue作用域内的至少一个Pod就绪后即可接受客户端请求。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">DEPLOY=<span class="string">&#x27;blue&#x27;</span> VERSION=<span class="string">&#x27;v1.0&#x27;</span> envsubst &lt; service-blue-green.yaml | kubectl apply -f -</span></span><br><span class="line">service/demoapp-svc created</span><br></pre></td></tr></table></figure>

<p>随后，在新终端中启动一个用于测试的临时Pod对象，在其接口使用curl命令发起持续性访问请求。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run pod-<span class="variable">$RANDOM</span> --image=ikubernetes/admin-toolbox:v1.0 -it \</span></span><br><span class="line"><span class="language-bash">    --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@pod-30411 /]#</span><br><span class="line">[root@pod-30411 /]# while true; do curl --connect-timeout 1 demoapp-svc; sleep 1; done</span><br></pre></td></tr></table></figure>

<p>待rs-blue期望的两个Pod对象均能正常提供服务后，即可假设需要更新到新的demoapp版本。此时，我们需要先基于replicaset-blue-green.yaml配置清单创建名为rs-green的新版本ReplicaSet。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">DEPLOY=<span class="string">&#x27;green&#x27;</span> VERSION=<span class="string">&#x27;v1.1&#x27;</span> envsubst &lt; replicaset-blue-green.yaml | kubectl apply -f -</span>                        </span><br><span class="line">replicaset.apps/rs-green created</span><br></pre></td></tr></table></figure>

<p>随后，等到rs-green的两个Pod均就绪后，将Service对象demoapp-svc的标签选择器修改为匹配新版本ReplicaSet对象rs-green作用域内的所有Pod，可通过如下命令完成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">DEPLOY=<span class="string">&#x27;green&#x27;</span> VERSION=<span class="string">&#x27;v1.1&#x27;</span> envsubst &lt; service-blue-green.yaml | kubectl apply -f -</span>      </span><br><span class="line">service/demoapp-svc configured</span><br></pre></td></tr></table></figure>

<p>这时，我们可以在专用于发起请求测试的终端上看到所有的响应报文均来自新版本的Pod中的容器应用demoapp。最后，将rs-blue的Pod副本数设置为0即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale replicasets/rs-blue --replicas=0</span></span><br><span class="line">replicaset.apps/rs-blue scaled</span><br></pre></td></tr></table></figure>

<p>显然，由于蓝绿部署要求两个及以上版本应用的Pod同时在线，对于应用规模较大而集群资源较为紧张的场景就成为“不可能”任务，而滚动更新则不具有这方面的问题。下面我们将着力介绍可用于声明式更新功能的Deployment控制器。</p>
<h2 id="Deployment控制器"><a href="#Deployment控制器" class="headerlink" title="Deployment控制器"></a>Deployment控制器</h2><p>Deployment（简写为deploy）是Kubernetes控制器的一种高级别实现，它构建于ReplicaSet控制器之上，如图8-11所示。它可用于为Pod和ReplicaSet资源提供声明式更新，并能够以自动方式实现8.2节中介绍的跨多个ReplicaSet对象的滚动更新功能。相比较来说，Pod和ReplicaSet是较低级别的资源，以至于很少被直接使用。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210132948513.png" alt="image-20220210132948513"></p>
<p>Deployment控制器资源的主要职责同样是为了保证Pod资源健康运行，其大部分功能通过调用ReplicaSet控制器实现，并增添了部分特性。</p>
<ul>
<li>事件和状态查看：必要时可以查看Deployment对象的更新进度和状态。</li>
<li>版本记录：将Deployment对象的更新操作予以保存，以便后续可能执行的回滚操作使用。</li>
<li>回滚：更新操作启动后的任一时刻（包括完成后）发现问题，都可以通过回滚机制将应用返回到前一个或由用户指定的历史记录中的版本。</li>
<li>暂停和启动：更新过程中能够随时暂停和继续完成后面的步骤。</li>
<li>多种更新方案：一是Recreate，即重建更新机制，单批次更新所有Pod对象；另一个是RollingUpdate，即滚动更新机制，多批次逐步替换旧有的Pod至新的版本。</li>
</ul>
<p>Deployment资源的扩缩容机制与ReplicaSet相同，修改.spec.replicas即能实时触发其规模变动操作。另外，kubectl scale是专用于扩展特定控制器类型的应用规模的命令，包括Deployment、ReplicaSet和StatefulSet等。</p>
<h3 id="Deployment基础应用"><a href="#Deployment基础应用" class="headerlink" title="Deployment基础应用"></a>Deployment基础应用</h3><p>Deployment是标准的API资源类型，它以ReplicaSet资源为基础资源进行应用编排，并能够自动实现策略式滚动更新或单批次重建式更新，因而它的spec字段中嵌套使用的字段包含了ReplicaSet控制器支持的所有字段，而Deployment也正是利用这些信息完成其二级资源ReplicaSet对象的创建。另外，Deployment还支持几个专用于定义部署及相关策略的字段，具体使用说明如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span>           <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span>              <span class="comment"># 资源类型特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 名称空间；Deployment隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">minReadySeconds</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># Pod就绪后多少秒内任一容器无崩溃方可视为“就绪”</span></span><br><span class="line">  <span class="string">replicas</span> <span class="string">&lt;integer&gt;</span>          <span class="comment"># 期望的Pod副本数，默认为1</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;object&gt;</span>           <span class="comment"># 标签选择器，必须匹配template字段中Pod模板的标签</span></span><br><span class="line">  <span class="string">template</span> <span class="string">&lt;object&gt;</span>           <span class="comment"># Pod模板对象</span></span><br><span class="line">  <span class="string">revisionHistoryLimit</span> <span class="string">&lt;integer&gt;</span> <span class="comment"># 滚动更新历史记录数量，默认为10</span></span><br><span class="line">  <span class="string">strategy</span> <span class="string">&lt;Object&gt;</span>           <span class="comment"># 滚动更新策略</span></span><br><span class="line">    <span class="string">type</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># 滚动更新类型，可用值有Recreate和Rollingupdate</span></span><br><span class="line">    <span class="string">rollingUpdate</span> <span class="string">&lt;Object&gt;</span>    <span class="comment"># 滚动更新参数，专用于RollingUpdate类型</span></span><br><span class="line">      <span class="string">maxSurge</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 更新期间可比期望的Pod数量多出的数量或比例</span></span><br><span class="line">      <span class="string">maxUnavailable</span> <span class="string">&lt;string&gt;</span> <span class="comment"># 更新期间可比期望的Pod数量缺少的数量或比例</span></span><br><span class="line">  <span class="string">progressDeadlineSeconds</span> <span class="string">&lt;integer&gt;</span> <span class="comment"># 滚动更新故障超时时长，默认为600秒</span></span><br><span class="line">  <span class="string">paused</span> <span class="string">&lt;boolean&gt;</span>            <span class="comment"># 是否暂停部署过程</span></span><br></pre></td></tr></table></figure>

<p>若无须自定义更新策略等相关配置，除了资源类型之外，Deployment资源的基础配置格式几乎与ReplicaSet完全相同。下面是一个配置清单示例，它定了一个名为deployment-demo的Deployment资源，为了便于复用，我们把镜像标签以环境变量VERSION进行标识。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">deployment-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">4</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">stable</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">release:</span> <span class="string">stable</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:$&#123;VERSION&#125;</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br></pre></td></tr></table></figure>

<p>同其他类型资源的创建方式类似，Deployment资源规范同样使用kubectl apply或kubectl create命令进行创建，但为了真正、全面地体现Deployment的声明式配置功能，建议统一使用声明式的管理机制创建和更新Deployment资源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">VERSION=<span class="string">&#x27;v1.0&#x27;</span> envsubst &lt; deployment-demo.yaml | kubectl apply --record -f -</span></span><br><span class="line">deployment.apps/deployment-demo created</span><br></pre></td></tr></table></figure>

<p>kubectl get deployments命令可以列出创建的Deployment对象的简要状态信息，下面命令结果显示出的字段中，UP-TO-DATE表示已经满足期望状态的Pod副本数量，而AVAILABLE则表示当前处于就绪状态并已然可向客户端提供服务的副本数量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get deployments/deployment-demo</span></span><br><span class="line">NAME             READY   UP-TO-DATE     AVAILABLE       AGE</span><br><span class="line">deployment-demo   4/4        4              4           36s</span><br></pre></td></tr></table></figure>

<p>Deployment资源会由控制器自动创建下级ReplicaSet资源，并自动为其生成一个遵循[DEPLOYMENT-NAME]-[POD-TEMPLATE-HASH-VALUE]格式的名称，其中的hash值由Deployment控制器根据Pod模板计算生成。另外，Deployment还会将用户定义在Pod模板上的标签应用到下级ReplicaSet资源之上，并附加一个pod-template-hash的标签，标签值即Pod模板的hash值。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets -l app=demoapp,release=stable --show-labels</span></span><br><span class="line">NAME            DESIRED   CURRENT   READY   AGE     LABELS</span><br><span class="line">deployment-demo-b479b6f9f   ……  app=demoapp,pod-template-hash=b479b6f9f, release=stable</span><br></pre></td></tr></table></figure>

<p>Pod对象则使用同上级ReplicaSet资源一样的标签，包括pod-template-hash，而各Pod对象的名称同样遵循ReplicaSet对象对Pod命名的格式，它以ReplicaSet对象的名称为前缀，后跟5位随机字符。下面使用awk过滤出了get pods命令结果中的以deployment-demo开头的所有Pod资源，并显示了它们的标签。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods --show-labels | awk <span class="string">&#x27;/^deployment-demo-/&#123;print $1,$NF&#125;&#x27;</span></span></span><br><span class="line">deployment-demo-b479b6f9f-5phpr app=demoapp,pod-template-hash=b479b6f9f, release=stable</span><br><span class="line">deployment-demo-b479b6f9f-kqk2r app=demoapp,pod-template-hash=b479b6f9f, release=stable</span><br><span class="line">deployment-demo-b479b6f9f-lbsp4 app=demoapp,pod-template-hash=b479b6f9f, release=stable</span><br><span class="line">deployment-demo-b479b6f9f-sbnbj app=demoapp,pod-template-hash=b479b6f9f, release=stable</span><br></pre></td></tr></table></figure>

<p>事实上，Deployment及下级ReplicaSet真正使用的标签选择器也包含pod-template-hash标签，这正是确保Deployment通过多ReplicaSet资源进行滚动更新时，确保各ReplicaSet不会交叉引用同一组Pod对象的一种途径。</p>
<h3 id="Deployment更新策略"><a href="#Deployment更新策略" class="headerlink" title="Deployment更新策略"></a>Deployment更新策略</h3><p><font color="red">Deployment只需要由用户指定在Pod模板中要改动的内容，例如容器镜像文件的版本，余下的步骤可交由Deployment控制器自动完成。</font>未定义更新策略的Deployment资源，将以默认方式配置更新策略，资源详细描述能够输出更新策略的相关配置信息，下面以deployment-demo资源为例来了解默认的更新策略。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe deployments/deployment-demo</span></span><br><span class="line">Name:                   deployment-demo</span><br><span class="line">……</span><br><span class="line">Annotations:            deployment.kubernetes.io/revision: 1</span><br><span class="line">Selector:               app=demoapp,release=stable</span><br><span class="line">Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailableStrategyType:           RollingUpdate</span><br><span class="line">MinReadySeconds:        0RollingUpdateStrategy:  25% max unavailable, 25% max surge</span><br><span class="line">……</span><br><span class="line">OldReplicaSets:  &lt;none&gt;</span><br><span class="line">NewReplicaSet:   deployment-demo-b479b6f9f (4/4 replicas created)</span><br><span class="line">Events:</span><br><span class="line">  ……</span><br></pre></td></tr></table></figure>

<p>Deployment控制器支持滚动更新（rolling updates）和重新创建（recreate）两种更新策略，默认使用滚动更新策略。重建式更新类同前文中ReplicaSet的第一种更新方式，即先删除现存的Pod对象，而后由控制器基于新模板重新创建出新版本资源对象。通常，只有当应用的新旧版本不兼容（例如依赖的后端数据库的格式不同且无法兼容）时才会使用recreate策略。但重建策略会导致应用在更新期间不可用，因而建议用户使用蓝绿部署的方式进行，除非系统资源不足以支撑蓝绿部署的实现。<br>Deployment控制器的滚动更新操作并非在同一个ReplicaSet控制器对象下删除并创建Pod资源，而是将它们分置于两个不同的控制器之下，当前ReplicaSet对象的Pod副本数量不断减少的同时，新ReplicaSet对象的Pod对象数量不断增加，直到现有ReplicaSet对象的Pod副本数为0，而新控制器的副本数量变得完全符合期望值，如图8-9所示。新旧版本之间区别彼此Pod对象的关键标签为pod-template-hash。<br>多批次更新模式的默认间隔标准是前一批次的所有Pod对象均已就绪，方可启动后一批次的更新。而Deployment还提供了两个配置滚动更新批次的字段，以允许用户自定义更新过程的滚动速率，这两个字段分别用于定义滚动更新期间的Pod总数可向上或向下偏离期望值的幅度。</p>
<ul>
<li>spec.strategy.rollingUpdate.maxSurge：指定升级期间存在的总Pod对象数量最多可超出期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比；例如，如果期望值为10，maxSurge属性值为2，则表示Pod对象总数至多不能超过12个。</li>
<li>spec.strategy.rollingUpdate.maxUnavailable：升级期间正常可用的Pod副本数（包括新旧版本）最多不能低于期望值的个数，其值可以是0或正整数，也可以是相对于期望值的一个百分比；默认值为1，这意味着如果期望值是10，则升级期间至少要有9个Pod对象处于正常提供服务的状态。<br>如8.2.5节中的描述，我们通过组织maxSurge和maxUnavailable两个属性协同工作，可组合定义出3种不同的策略完成多批次的应用更新。</li>
<li>先增新，后减旧：将maxSurge设定为小于等于期望值的正整数或相对于期望值的一个百分比，而maxUnavailable的值为0。</li>
<li>先减旧，后增新：将maxUnavailable设定为小于等于期望值的正整数或相对于期望值的一个百分比，而maxSurge的值为0。</li>
<li>同时增减（少减多增）：将maxSurge和maxUnavailabe字段的值同时设定为小于等于期望值的正整数或相对于期望值的一个百分比，二者可以使用不同值。注意<br>maxSurge和maxUnavailable属性的值不可同时为0，否则Pod对象的副本数量在符合用户期望的数量后无法做出合理变动以进行滚动更新操作。<br>显然，deployment-demo的详细描述显示出，Deployment默认为滚动更新设置了同时增减的策略，增减的幅度为期望值的25%，它通过两个批次的创建和3个批次的删除即能完成整个应用的更新，具体过程如图8-12所示。不过，若Pod对象的整体副本数小于4的话，就只能按一次1个Pod对象的方式进行。</li>
</ul>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210133309242.png" alt="image-20220210133309242"></p>
<p>Deployment还支持使用spec.minReadySeconds字段来控制滚动更新的速度，其默认值为0，表示新建的Pod对象一旦“就绪”将立即被视作可用，随后即可开始下一轮更新过程。而为该字段指定一个正整数值能够定义新建的Pod对象至少要成功运行多久才会被视作可用，即就绪之后还要等待minReadySeconds指定的时长才能开始下一批次的更新。在一个批次内新建的所有Pod就绪后但转为可用状态前，更新操作会被阻塞，并且任何一个Pod就绪探测失败，都会导致滚动更新被终止。因此，为minReadySeconds赋予一个合理的正整数值，不仅能够减缓滚动更新的速度，还能够让Deployment提前发现一部分程序Bug导致的升级故障。<br>Deployment可保留一部分滚动更新历史（修订记录）中旧版本的ReplicaSet对象，如图8-13所示。Deployment资源可保存的历史版本数量由spec.revisionHistoryLimit属性进行定义。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210133334240.png" alt="image-20220210133334240"></p>
<p><font color="red">为了保存升级历史，需要在创建Deployment对象时为命令使用–record选项。</font><br>尽管滚动更新以节约系统资源著称，但它也存在着一些劣势。直接改动现有环境，会为系统引入不确定性风险，而且一旦在更新过程中遇到问题，回滚操作的过程会较为缓慢。有鉴于此，金丝雀部署可能是较为理想的实现方式。当然，如果不考虑系统资源的可用性，那么传统的蓝绿部署将是更好的选择。</p>
<h3 id="应用更新与回滚-1"><a href="#应用更新与回滚-1" class="headerlink" title="应用更新与回滚"></a>应用更新与回滚</h3><p>Pod模板内容的变动是触发Deployment执行更新操作的必要条件。对于声明式配置的Deployment来说，Pod模板的修改尤其适合使用apply和patch命令进行，不过，若仅是修改容器镜像，set image命令则更为易用。<br>接下来通过更新此前创建的deployment-demo资源来了解Deployment更新操作过程的执行细节。为了使得升级过程更易于观测，这里先使用kubectl patch命令为Deployment的spec.minReadySeconds字段定义一个等待时长，例如30秒：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch deployments/deployment-demo -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;minReadySeconds&quot;:30&#125;&#125;&#x27;</span></span></span><br><span class="line">deployment.apps/deployment-demo patched</span><br></pre></td></tr></table></figure>

<p>修改Deployment控制器的minReadySeconds、replicas和strategy等字段的值并不会触发Pod资源的更新操作，因为它们不属于template的内嵌字段，对现存的Pod对象不产生任何影响。<br>接下来，我们让Pod模板中的demoapp容器使用ikubernetes/demoapp:v1.1镜像文件，以触发deployment-demo启动滚动更新，下面先尝试使用kubectl apply命令完成更新操作：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ VERSION=&#x27;v1.1&#x27; envsubst &lt; deployment-demo.yaml | kubectl apply --record -f - </span><br><span class="line">deployment.apps/deployment-demo configured</span><br></pre></td></tr></table></figure>

<p>kubectl rollout status命令可用于打印滚动更新过程中的状态信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl rollout status deployments/deployment-demo</span></span><br></pre></td></tr></table></figure>

<p>另外，我们还可以使用kubectl get deployments -w命令监控其更新过程中Pod对象的变动过程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get deployments/deployment-demo -w</span></span><br></pre></td></tr></table></figure>

<p><font color="red">滚动更新时，deployment-demo会创建一个新的ReplicaSet控制器对象来管控新版本的Pod对象，升级完成后，旧版本的ReplicaSet会保留在历史记录中，但它的Pod副本数被降为0。</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets -l app=demoapp,release=stable</span></span><br><span class="line">NAME                      DESIRED   CURRENT    READY     AGE</span><br><span class="line">deployment-demo-59d9f4475b   4         4         4       1m32s</span><br><span class="line">deployment-demo-b479b6f9f    0         0         0       12m</span><br></pre></td></tr></table></figure>

<p>deployment-demo标签选择器作用域内的Pod资源对象也随之更新为以新版本ReplicaSet名称deployment-demo-59d9f4475b为前缀的Pod副本。<br>另一方面，因各种原因导致滚动更新无法正常进行，例如镜像文件获取失败等，或者更新后遇到的应用程序级故障，例如新版本Pod中的应用触发了未知Bug等，都应该将应用回滚至之前版本用户指定的历史记录中的版本。我们此前曾分别执行了deployment-demo资源的一次部署和一次更新操作，因此修订记录（revision history）分别记录有这两次操作，它们各有一个修订标识符，最大标识符为当前使用的版本。kubectl rollout history命令能够打印Deployment资源的修订历史：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl rollout <span class="built_in">history</span> deployments/deployment-demo</span></span><br><span class="line">deployment.apps/deployment-demo </span><br><span class="line">REVISION  CHANGE-CAUSE</span><br><span class="line">1         kubectl apply --record=true --filename=-</span><br><span class="line">2         kubectl apply --record=true --filename=-</span><br></pre></td></tr></table></figure>

<p>从某种意义上说，回滚亦是更新操作。因而，在deployment-demo之上执行回滚操作意味着将当前版本切换回前一个版本，但历史记录中，其REVISION记录也将随之变动，回滚操作会被当作一次滚动更新追加到历史记录中，而被回滚的条目则会被删除。因而，deployment-demo回滚后修订标识符将从1变为3。回滚操作可使用kubectl rollout undo命令完成：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl rollout undo deployments/deployment-demo</span> </span><br><span class="line">deployment.apps/deployment-demo rolled back</span><br></pre></td></tr></table></figure>

<p>回滚完成后，我们可根据客户端的访问结果来验证deployment-demo是否回滚完成，或者根据当前ReplicaSet对象是否恢复到指定的历史版本进行验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicasets | grep <span class="string">&quot;^deployment-demo&quot;</span></span></span><br><span class="line">deployment-demo-59d9f4475b   0         0         0       11m</span><br><span class="line">deployment-demo-b479b6f9f    4         4         4       13m</span><br></pre></td></tr></table></figure>

<p>另外，在kubectl rollout undo命令上使用–to-revision选项指定revision号码还可回滚到历史记录中的特定版本。需要注意的是，如果此前的滚动更新过程处于“暂停”状态，回滚操作就需要先将Pod模板的版本改回之前，然后“继续”更新，否则，其将一直处于暂停状态而无法回滚。</p>
<h3 id="金丝雀发布"><a href="#金丝雀发布" class="headerlink" title="金丝雀发布"></a>金丝雀发布</h3><p>Deployment资源允许用户控制更新过程中的滚动节奏，例如“暂停”或“继续”更新操作，尤其是借助于前文讲到的maxSurge和maxUnavailable属性还能实现更为精巧的过程控制。例如，在第一批新的Pod资源创建完成后立即暂停更新过程，此时，仅有一小部分新版本的应用存在，主体部分还是旧的版本。然后，通过应用层路由机制根据请求特征精心筛选出小部分用户的请求路由至新版本的Pod应用，并持续观察其是否能稳定地按期望方式运行。默认，Service只会随机或轮询地将用户请求分发给所有的Pod对象。确定没有问题后再继续进行完余下的所有Pod资源的滚动更新，否则便立即回滚至第一步更新操作。这便是所谓的金丝雀部署，如图8-14所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210133752402.png" alt="image-20220210133752402"><br>为了尽可能降低对现有系统及其容量的影响，基于Deployment的金丝雀发布过程通常建议采用“先增后减且可用Pod对象总数不低于期望值”的方式进行。首次添加的Pod对象数量取决于其接入的第一批请求的规则及单个Pod的承载能力，视具体需求而定，为了能更简单地说明问题，接下来采用首批添加1个Pod资源的方式。我们将Deployment控制器的maxSurge属性的值设置为1，并将maxUnavailable属性的值设置为0就能完成设定：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~]$ </span><span class="language-bash">kubectl patch deployments/deployment-demo  \</span></span><br><span class="line"><span class="language-bash">    -p <span class="string">&#x27;&#123;&quot;spec&quot;: &#123;&quot;strategy&quot;:&#123;&quot;rollingUpdate&quot;: &#123;&quot;maxSurge&quot;: 1, &quot;maxUnavailable&quot;:</span></span> </span><br><span class="line">    0&#125;&#125;&#125;&#125;&#x27;</span><br><span class="line">deployment.apps/deployment-demo patched</span><br></pre></td></tr></table></figure>

<p>随后，修改Pod模板触发deployment-demo资源的更新过程，进行第一批次更新后立即暂停该部署操作，则新生成的第一批Pod对象便是“金丝雀”，如图8-15所示。暂停Deployment资源的更新过程，需要将其spec.pause字段的值从false修改为true，这可通过修改资源规范后再次应用（apply）完成，也可通过kubectl rollout pause命令进行。例如，下面将deployment-demo资源的Pod模板中的容器镜像进行了修改以触发其更新，但同时使用shell操作符&amp;&amp;随后立即执行了暂停命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">VERSION=<span class="string">&#x27;v1.2&#x27;</span> envsubst &lt; deployment-demo.yaml | kubectl apply --record -f - &amp;&amp; \ </span></span><br><span class="line">     kubectl rollout pause deployments/deployment-demo</span><br><span class="line">deployment.apps/deployment-demo configured</span><br><span class="line">deployment.apps/deployment-demo paused</span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210133829935.png" alt="image-20220210133829935"></p>
<p>处于“暂停”状态中的Deployment资源的滚动状态也会暂停于某一批更新操作中，我们可以通过状态查看命令打印相关的信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl rollout status deployments/deployment-demo</span><br><span class="line">Waiting for deployment &quot;deployment-demo&quot; rollout to finish: 1 out of 4 new replicas have been updated...</span><br></pre></td></tr></table></figure>

<p>相关的Pod列表也能够显示出旧版本ReplicaSet的所有Pod副本仍在正常运行，而同时新版本ReplicaSet对象也有了一个Pod实例，相关Service对象能够在其就绪后将一定比例的客户端流量引入到该Pod之上。运行足够长的一段时间后，若确认新版本应用没有必须通过回滚才能解决的问题，随后即可使用kubectl rollout resume命令继续后续更新步骤，以完成滚动更新过程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl rollout resume deployments/deployment-demo</span><br><span class="line">deployment.apps/deployment-demo resumed</span><br></pre></td></tr></table></figure>

<p>kubectl rollout status命令监控到滚动更新过程完成后，即可通过deployment-demo资源及其作用域内的ReplicaSet和Pod对象的相关信息来了解其结果状态。然而，如果“金丝雀”遇险，回滚操作便成了接下来的紧要任务。</p>
<h2 id="StatefulSet控制器"><a href="#StatefulSet控制器" class="headerlink" title="StatefulSet控制器"></a>StatefulSet控制器</h2><p>无状态应用进程客户端的每次连接均可独立地处理，一次请求和响应即构成一个完整的事务，它们不受已完成的连接或现有其他连接的影响，且意外中断或关闭时仅需要重新建立连接即可，因而，无状态应用的Pod对象可随时由其他由同一模板创建的Pod平滑替代，这也正是Deployment控制器编排应用的方式。</p>
<h3 id="功能分析-1"><a href="#功能分析-1" class="headerlink" title="功能分析"></a>功能分析</h3><p>Kubernetes系统使用专用的StatefulSet控制器编排有状态应用。StatefulSet表示一组具有唯一持久身份和稳定主机名的Pod对象，任何指定该类型Pod的状态信息和其他弹性数据都存放在与该StatefulSet相关联的永久性磁盘存储空间中。<font color="red">StatefulSet旨在部署有状态应用和集群化应用，这些应用会将数据保存到永久性存储空间，它适合部署Kafka、MySQL、Redis、ZooKeeper以及其他需要唯一持久身份和稳定主机名的应用。</font><br><font color="red">一个典型的、完整可用的StatefulSet资源通常由两个组件构成：Headless Service和StatefulSet资源。</font><strong>Headless Service用于为各Pod资源固定、唯一的标识符生成可解析的DNS资源记录，StatefulSet用于编排Pod对象，并借助volumeClaimTemplate以静态或动态的PV供给方式为各Pod资源提供专有且固定的存储资源。</strong><br>对于拥有N个副本的StatefulSet资源来说，它会以{0…N–1}依次对各Pod对象进行编号及顺序创建，当前Pod对象就绪后才会创建下一个，删除则以相反的顺序进行，每个Pod删除完成后才会继续删除前一个。Pod资源的名称格式为$(statefulset name)-$(ordinal)，例如名称为web的StatefulSet资源生成的Pod对象的名称依次为web-0、web-1、web-2等，其域名后缀则由相关的Headless Service资源给出，格式为$(service name).$(namespace).svc.cluster.local。<font color="red">Kubernetes 1.7及其之后的版本也支持StatefulSet并行管理Pod对象。</font><br>配置了volumeClaimTemplate的StatefulSet资源会为每个Pod对象基于存储卷申请配置一个专用的PV，动静供给机制都支持，只是静态供给依赖于管理员的事前配置，如图8-16所示。而删除Pod对象甚至是StatefulSet控制器，并不会删除其相关的PV资源以确保数据可用性，因而Pod对象由节点故障或被驱逐等原因被重新调度至其他节点时，先前同名Pod实例专用的PV及其数据可安全复用。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210134157079.png" alt="image-20220210134157079">与Deployment略有不同的是，StatefulSet对应用规模的扩容意味着按索引顺序增加更多的Pod资源，而缩容则表示按逆序依次删除索引号最大的Pod资源，直到规模数量满足目标设定值为止。<font color="red">多数有状态应用都不支持规模性安全、快速的缩减操作，因此StatefulSet控制器不支持并行缩容机制，而是要严格遵守一次仅能终止一个Pod资源的法则，以免导致数据讹误。通常也意味着，存在错误且未恢复的Pod资源时，StatefulSet资源会拒绝启动缩容操作。此外，缩容操作导致的Pod资源终止同样不会删除其相关的PV，以确保数据可用。</font><br>StatefulSet也支持用户自定义的更新策略，它兼容支持之前版本中的OnDelete策略，以及新的RollingUpdate策略。RollingUpdate是默认的更新策略，更新过程中，更新顺序与终止Pod资源的顺序相同，由索引号最大的开始，终止一个Pod对象并完成其更新后继续进行前一个。此外，StatefulSet资源的滚动更新还支持分区（partition)机制，用户可基于某个用于分区的索引号对Pod资源进行分区，所有大于等于此索引号的Pod对象会被滚动更新，如图8-17所示，而小于此索引号的则不会被更新，而且，即便在此期间该范围内的某Pod对象被删除，它也一样会被基于旧版本的Pod模板重建。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210134256636.png" alt="image-20220210134256636"></p>
<p>若给定的分区号大于副本数量，意味着不存在大于此分区号的Pod资源索引号，因此，所有的Pod对象均不会被更新，这对于期望暂存发布、金丝雀发布或分段发布来说是有用的设定。</p>
<h3 id="StatefulSet基础应用"><a href="#StatefulSet基础应用" class="headerlink" title="StatefulSet基础应用"></a>StatefulSet基础应用</h3><p><font color="red">完整的StatefulSet资源需要由Headless Service和StatefulSet共同构成，StatefulSet资源规范中通过必选字段spec.serviceName指定关联的Headless类型的Service对象名称，但管理该Service是用户的责任，StatefulSet仅是强依赖于它，而不会自动管理它。</font>下面是StatefulSet资源的规范格式及简要说明。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span>               <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span>                 <span class="comment"># 资源类型的特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>                   <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 名称空间；StatefulSet隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">replicas</span> <span class="string">&lt;integer&gt;</span>              <span class="comment"># 期望的Pod副本数，默认为1</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;object&gt;</span>               <span class="comment"># 标签选择器，需匹配Pod模板中的标签，必选字段</span></span><br><span class="line">  <span class="string">template</span> <span class="string">&lt;object&gt;</span>               <span class="comment"># Pod模板对象，必选字段</span></span><br><span class="line">  <span class="string">revisionHistoryLimit</span> <span class="string">&lt;integer&gt;</span>  <span class="comment"># 滚动更新历史记录数量，默认为10</span></span><br><span class="line">  <span class="string">updateStrategy</span> <span class="string">&lt;Object&gt;</span>         <span class="comment"># 滚动更新策略</span></span><br><span class="line">    <span class="string">type</span> <span class="string">&lt;string&gt;</span>                 <span class="comment"># 滚动更新类型，可用值有OnDelete和Rollingupdate</span></span><br><span class="line">    <span class="string">rollingUpdate</span> <span class="string">&lt;Object&gt;</span>        <span class="comment"># 滚动更新参数，专用于RollingUpdate类型</span></span><br><span class="line">      <span class="string">partition</span> <span class="string">&lt;integer&gt;</span>         <span class="comment"># 分区指示索引值，默认为0</span></span><br><span class="line">  <span class="string">serviceName</span>  <span class="string">&lt;string&gt;</span>           <span class="comment"># 相关的Headless Service的名称，必选字段</span></span><br><span class="line">  <span class="string">volumeClaimTemplates</span> <span class="string">&lt;[]Object&gt;</span> <span class="comment"># 存储卷申请模板</span></span><br><span class="line">    <span class="string">apiVersion</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># PVC资源所属的API群组及版本，可省略</span></span><br><span class="line">    <span class="string">kind</span> <span class="string">&lt;string&gt;</span>                 <span class="comment"># PVC资源类型标识，可省略</span></span><br><span class="line">    <span class="string">metadata</span> <span class="string">&lt;Object&gt;</span>             <span class="comment"># 卷申请模板元数据</span></span><br><span class="line">    <span class="string">spec</span> <span class="string">&lt;Object&gt;</span>                 <span class="comment"># 期望的状态，可用字段同PVC</span></span><br><span class="line">  <span class="string">podManagementPolicy</span>  <span class="string">&lt;string&gt;</span>   <span class="comment"># Pod管理策略，默认的OrderedReady表示顺序创</span></span><br><span class="line">                                  <span class="comment">#建并逆序删除，另一可用值Parallel表示并行模式</span></span><br></pre></td></tr></table></figure>

<p>下面的配置清单示例中定义了一个名为demodb的Headless Service，以及一个同样名为demodb的StatefulSet资源，后者使用了存储卷申请模板，为Pod对象从fast-rbd存储类中请求动态供给并绑定PV。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demodb</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demodb</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">9907</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demodb</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StatefulSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demodb</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demodb</span></span><br><span class="line">  <span class="attr">serviceName:</span> <span class="string">&quot;demodb&quot;</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demodb</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demodb</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">ikubernetes/demodb:v0.1</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">9907</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">db</span></span><br><span class="line">        <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">DEMODB_DATADIR</span></span><br><span class="line">          <span class="attr">value:</span> <span class="string">&quot;/demodb/data&quot;</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/status</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">db</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">30</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/status?level=full</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">db</span> </span><br><span class="line">        <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">          <span class="attr">mountPath:</span> <span class="string">/demodb/data</span></span><br><span class="line">  <span class="attr">volumeClaimTemplates:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">data</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">accessModes:</span> [ <span class="string">&quot;ReadWriteOnce&quot;</span> ]</span><br><span class="line">      <span class="attr">storageClassName:</span> <span class="string">&quot;rbd&quot;</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">storage:</span> <span class="string">1Gi</span></span><br></pre></td></tr></table></figure>

<p>示例中用到的demodb是一个仅用于测试的分布式键值存储系统，支持持久化数据存储，它由一个Leader和一到多个Followers组成，Followers定期从Leader查询并请求同步数据。Leader支持读写请求，而各Followers节点仅支持只读操作，它们会把接收到的写请求通过307响应码重定向给Leader节点。用于读写请求的URI分别为/get/KEY和/set/KEY，/status则用于输出状态，/status?level=full能够以200响应码返回持有的键数量，否则响应以500状态码返回。demodb仅可由StatefulSet控制器编排运行，并且在程序中将Leader的名称固定为demodb-0，依赖的Headless Service的名称也固定为demodb，因此StatefulSet和Headless Service资源的名称必须要使用demodb。<br>默认情况下，StatefulSet资源使用OrderedReady这一Pod管理策略，它以串行的方式逐一创建各Pod实例及相关的PV，下面在创建后打印的statefulsets/demodb资源详细描述中的各事件的时间点也反映了这种事实。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f demodb.yaml</span></span><br><span class="line">service/demodb created</span><br><span class="line">statefulset.apps/demodb created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe statefulsets/demodb</span>    </span><br><span class="line">Name:               demodb</span><br><span class="line">Namespace:          default</span><br><span class="line">Selector:           app=demodb</span><br><span class="line">Labels:             &lt;none&gt;</span><br><span class="line">Annotations:        Replicas:  2 desired | 2 total</span><br><span class="line">Update Strategy:    RollingUpdate</span><br><span class="line">  Partition:        0</span><br><span class="line">Pods Status:        2 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br><span class="line">……</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age    From                    Message</span><br><span class="line">  ----    ------            ----   ----                    -------</span><br><span class="line">  Normal  SuccessfulCreate  2m22s  statefulset-controller  create Claim data-demodb-0 Pod demodb-0 in StatefulSet demodb success</span><br><span class="line">  Normal  SuccessfulCreate  2m22s  statefulset-controller  create Pod demodb-0 in StatefulSet demodb successful</span><br><span class="line">  Normal  SuccessfulCreate  97s    statefulset-controller  create Claim data-demodb-1 Pod demodb-1 in StatefulSet demodb success</span><br><span class="line">  Normal  SuccessfulCreate  97s    statefulset-controller  create Pod demodb-1 in StatefulSet demodb successful</span><br></pre></td></tr></table></figure>

<p>如前所述，由StatefulSet资源创建的Pod对象拥有固定且唯一的标识符，它们基于唯一的索引序号及相关的StatefulSet对象的名称生成，格式为&lt;statefulset name&gt;-&lt;ordinal index&gt;，例如上面事件信息中显示出由statefuls/demodb所创建的demodb-0和demodb-1两个Pod对象的名称即遵循该格式。事实上，这类Pod对象的主机名也与其资源名称相同，以demodb-0为例，下面的命令打出的主机名称正是Pod资源的名称标识。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> demodb-0 -- hostname</span></span><br><span class="line">demodb-0</span><br></pre></td></tr></table></figure>

<p>Headless Service的DNS名称解析会由ClusterDNS以该Service对象关联各Pod对象的IP地址加以响应。而StatefulSet创建的各Pod对象的名称则以相关Headless Service资源的DNS名称为后缀，具体格式为$(pod_name).$(svc_name).$(namespace).svc.cluster.local，例如demodb-0和demodb-1的资源名称分别为demodb-0.demodb.default.svc.cluster.local和demodb-1.demodb.default.svc.cluster.local。下面在一个新的专用终端创建一个临时的、基于Pod对象的交互客户端进行测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client --image=ikubernetes/admin-toolbox:v1.0 -it --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br></pre></td></tr></table></figure>

<p>首先，请求解析Pod的FQDN格式主机名称，它会返回相应Pod对象的IP地址；</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@client /]# nslookup -query=A demodb-0.demodb</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   demodb-0.demodb.default.svc.cluster.local</span><br><span class="line">Address: 10.244.1.208</span><br></pre></td></tr></table></figure>

<p>接着，创建一个测试文件，将之存储到demodb存储服务以发起数据存储测试。我们知道，CoreDNS默认以roundrobin的方式响应对同一个名称的解析请求，因而以名称方式发往demodb这一Headless Service的请求会轮询到demodb-0和demodb-1之上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@client /]# echo &quot;Advanced Kubernetes Practices&quot; &gt; /tmp/mydata</span><br><span class="line">[root@client /]# curl -L -XPUT -T /tmp/mydata http://demodb:9907/set/mydata</span><br><span class="line">WRITE completed</span><br></pre></td></tr></table></figure>

<p>调度至从节点（demodb-1）的写请求会自动重定向给主节点（demodb-0），且主节点数据存储完成后将自动同步至各个从节点；我们可从服务请求读取数据，或者直接从demodb-1读取数据，以进行测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@client /]# curl http://demodb:9907/get/mydata</span><br><span class="line">Advanced Kubernetes Practices</span><br><span class="line">[root@client /]# curl http://demodb-1.demodb:9907/get/mydata</span><br><span class="line">Advanced Kubernetes Practices</span><br></pre></td></tr></table></figure>

<p>demodb的所有节点会将数据存储在/demodb/data目录下，每个键被映射为一个子目录，数据存储在该子目录下的content文件中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> demodb-0 -- <span class="built_in">cat</span> /demodb/data/mydata/content</span></span><br><span class="line">Advanced Kubernetes Practices</span><br></pre></td></tr></table></figure>

<p>而各Pod对象的/demodb/data目录挂载到一个由statefulset/demodb存储卷申请模板创建的PVC之上，每个PVC又绑定在由存储类fast-rbd动态供给的PV之上。各PVC的名称由volumeClaimTemplate对象的名称与Pod对象的名称组合而成，格式为$(volume-ClaimTemplate_name).(Pod_name)，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc -l app=demodb</span></span><br><span class="line">NAME   STATUS  VOLUME   CAPACITY   ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">data-demodb-0  Bound   pvc-de6e81c1-…   2Gi      RWO     fast-rbd       4m50s</span><br><span class="line">data-demodb-1  Bound   pvc-e95d67ca-…   2Gi     RWO      fast-rbd       4m5s</span><br></pre></td></tr></table></figure>

<p>StatefulSet资源作用域内的Pod资源因被节点驱逐，或因节点故障、应用规模缩容被删除，甚至是手动误删除时，它挂载的由存储卷申请模板创建的PVC卷并不会被删除。因而，经StatefulSet资源重建或规模扩容回原来的规模后，每个Pod对象依然有固定的标识符并可关联到此前的PVC存储卷上。</p>
<h3 id="扩缩容与滚动更新"><a href="#扩缩容与滚动更新" class="headerlink" title="扩缩容与滚动更新"></a>扩缩容与滚动更新</h3><p>StatefulSet资源也支持类似于Deployment资源的应用规模的扩容、缩容以及更新机制。扩缩容通过简单地修改StatefulSet资源的副本数来改动期望的Pod资源数量就能完成，例如，下面的命令能将statefulsets/demodb中的Pod副本数量扩展至4个。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale statefulsets/demodb --replicas=4</span></span><br><span class="line">statefulset.apps/demodb scaled</span><br></pre></td></tr></table></figure>

<p>StatefulSet资源的扩容过程与创建过程管理Pod对象的策略相同，默认为顺次进行，而且其名称中的序号也将以现有Pod资源的最后一个序号为基准向后进行。若定义了存储卷申请模板，扩容操作所创建的每个Pod对象也会各自关联所需要的PVC存储卷。与扩容操作相对，将其副本数量调低即能完成缩容操作，例如，下面的命令能够将StatefulSet资源demodb的副本数量缩减至3个。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch statefulsets/demodb -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;replicas&quot;:3&#125;&#125;&#x27;</span></span>      </span><br><span class="line">statefulset.apps/demodb patched</span><br></pre></td></tr></table></figure>

<p>缩容过程中终止Pod资源的默认策略与删除机制相似，它会根据Pod对象的可用索引号逆序逐一进行，直到余下的数量满足期望的值为止。因缩容而终止的Pod资源的存储卷并不会被删除，因此，如果缩减规模后再将其扩展回来，此前的数据依然可用，且Pod资源名称不变。<br>如前所述，在应用更新方面，StatefulSet资源自Kubernetes 1.7版本开始支持自动更新机制，其更新策略则由spec.updateStrategy字段定义，默认为RollingUpdate，即滚动更新。kubectl set image命令也支持修改StatefulSet资源上Pod模板中的容器镜像，因而，触发statefulsets/demodb上的应用升级可使用类似如下一条命令完成。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">set</span> image statefulsets/demodb demodb-shard=<span class="string">&quot;ikubernetes/demodb:v0.2&quot;</span></span></span><br><span class="line">statefulset.apps/demodb image updated</span><br></pre></td></tr></table></figure>

<p>滚动更新StatefulSet资源的Pod对象以逆序的方式从其最大索引编号逐一进行，滚动条件为当前更新循环中的各个新Pod资源已然就绪。通常，对于主从复制类的集群应用来说，这种方式能保证担当主节点的Pod资源在最后进行更新，以确保其兼容性。例如，触发statefulsets/demodb更新后，可以看到类似如下命令中首先更新索引编号最大的Pod对象demodb-1的操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demodb</span></span><br><span class="line">NAME       READY   STATUS           RESTARTS      AGE</span><br><span class="line">demodb-0   1/1     Running             0          5m42s</span><br><span class="line">demodb-1   1/1     Running             0          4m42s</span><br><span class="line">demodb-2   0/1     ContainerCreating   0          5s</span><br></pre></td></tr></table></figure>

<p>StatefulSet资源滚动更新过程中的状态同样可以使用kubectl rollout history命令获取。更新完成后，我们可使用如下命令，确认相关Pod对象使用的容器镜像都已经变更为指定的新版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demodb -o \</span></span><br><span class="line"><span class="language-bash">    jsonpath=<span class="string">&#x27;&#123;range .items[*]&#125;&#123;.metadata.name&#125;: &#123;.spec.containers[0].image&#125;&#123;&quot;\n&quot;&#125;</span></span></span><br><span class="line">    &#123;end&#125;&#x27;</span><br><span class="line">demodb-0: ikubernetes/demodb:v0.2</span><br><span class="line">demodb-1: ikubernetes/demodb:v0.2</span><br><span class="line">demodb-2: ikubernetes/demodb:v0.2</span><br></pre></td></tr></table></figure>

<p>滚动更新过程不会影响相应的数据服务，此前的生成的数据键mydata及其数据在更新过程中同样可以正常访问，这在8.4.2节的交互式客户端测试结果中能够得到验证。但是，更新demodb-0期间写操作会有短暂的不可用区间。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@client /]# curl http://demodb:9907/get/mydata</span><br><span class="line">Advanced Kubernetes Practices</span><br></pre></td></tr></table></figure>

<p>进一步地，StatefulSet资源支持使用分区编号（.spec.updateStrategy.rollingUpdate.partition字段值）将其Pod对象分为两个部分，仅那些索引号大于等于分区编号的Pod对象会被更新，默认的分区编号为0，因而滚动更新时，所有的Pod对象都是待更新目标。于是，在更新操作之前，将partition字段的值置为Pod资源的副本数量N（或大于该值）会使得所有的Pod资源（索引号区间为0到N–1）都不再处于可直接更新的分区之内，那么这之后设定的更新操作不会真正执行而是被“暂存”起来，直到降低分区编号至现有Pod资源索引号范围内，才开始触发真正的滚动更新操作。来看下面的例子。<br>首先，将statefulsets/demodb的分区别编号设置为现有的Pod数量值3：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch statefulsets/demodb -p \</span></span><br><span class="line"><span class="language-bash">       <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:3&#125;&#125;&#125;&#125;&#x27;</span></span></span><br><span class="line">statefulset.apps/demodb patched</span><br></pre></td></tr></table></figure>

<p>而后，更新statefulsets/demodb的Pod模板中的容器镜像为ikubernetes/demodb:v0.3。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">set</span> image statefulsets/demodb demodb-shard=<span class="string">&#x27;ikubernetes/demodb:v0.3&#x27;</span></span></span><br><span class="line">statefulset.apps/demodb image updated</span><br></pre></td></tr></table></figure>

<p>接下来，我们验证出最大编号的Pod对象demodb-2的容器镜像并未因执行更新而发生变化，根据更新策略来说，这意味着其他更小索引号的Pod对象更不会发生任何变动：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/demodb-2 -o jsonpath=<span class="string">&#x27;&#123;.spec.containers[0].image&#125;&#x27;</span></span>                                                  </span><br><span class="line">ikubernetes/demodb:v0.2</span><br></pre></td></tr></table></figure>

<p>再接着，将分区编号降为statefulsets/demodb上的最大索引编号2之后可以验证，仅demodb-2执行了更新操作；如下第二条命令可于分区编号更改后，略等一段时间后再执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch statefulsets/demodb -p \</span></span><br><span class="line"><span class="language-bash">      <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;updateStrategy&quot;:&#123;&quot;rollingUpdate&quot;:&#123;&quot;partition&quot;:2&#125;&#125;&#125;&#125;&#x27;</span></span></span><br><span class="line">statefulset.apps/demodb patched</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demodb -o \</span></span><br><span class="line"><span class="language-bash">      jsonpath=<span class="string">&#x27;&#123;range .items[*]&#125;&#123;.metadata.name&#125;: &#123;.spec.containers[0].image&#125;</span></span></span><br><span class="line">      &#123;&quot;\n&quot;&#125;&#123;end&#125;&#x27;</span><br><span class="line">demodb-0: ikubernetes/demodb:v0.2</span><br><span class="line">demodb-1: ikubernetes/demodb:v0.2</span><br><span class="line">demodb-2: ikubernetes/demodb:v0.3</span><br></pre></td></tr></table></figure>

<p>demodb-2就像是一只“金丝雀”，安然渡过一定时长的测试期间后，我们便可继续其他Pod资源的更新操作。若后续待更新的Pod资源数量较少，我们可直接将partition字段的值设置为0，从而让StatefulSet逆序完成后续所有Pod资源的更新。而待更新的Pod资源较多时，也可以将Pod资源以线性或指数级增长的方式来分阶段完成更新操作，操作过程仅仅是分多次更改partition字段值，例如将statefulsets/demodb控制器的分区号以较慢的节奏依次设置为1和0来完成剩余Pod资源的线性分步更新，如图8-18所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210135151879.png" alt="image-20220210135151879"></p>
<p><font color="red">StatefulSet支持的另一更新策略是OnDelete，这类似于手动更新机制，它以用户的手动删除操作为触发时间点完成应用更新。</font></p>
<h3 id="StatefulSet的局限性"><a href="#StatefulSet的局限性" class="headerlink" title="StatefulSet的局限性"></a>StatefulSet的局限性</h3><p>应用于生产环境的分布式有状态应用的各实例间的关系并非像本节示例中的demodb那样简单，它们在拓扑上通常是基于复杂分布式协议的成员关系，例如ZooKeeper集群成员基于ZAB协议的Leader/Follower关系以及etcd集群成员基于Raft协议的对等（peer）关系等。这些分布式有状态应用的内生拓扑结构存在区别，对持久存储的依赖需求也有所不同，并且集群成员的增加、减少以及在故障后的恢复操作通常都会依赖一系列复杂且精细的步骤才能完成，于是StatefulSet控制器无法为其封装统一、标准的管理操作。于是，用户就不得不配置某个特定的有状态应用，在其YAML配置清单中通过“复杂的运维代码”手动编写相关的运维逻辑，例如下面的这段代码便是以StatefuSet资源来编排etcd应用时，在其Pod模板中编写的仅实现了简单功能的运维代码。这看上去既奇怪又低效——每个用户不得不学习相关应用的运维知识并重复“造轮子”，而StatefulSet对此却也爱莫能助。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">command:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;/bin/sh&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">&quot;-ecx&quot;</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">|</span></span><br><span class="line"><span class="string">   IP=$(hostname -i)</span></span><br><span class="line"><span class="string">   PEERS=&quot;&quot;</span></span><br><span class="line"><span class="string">   for i in $(seq 0 $(($&#123;CLUSTER_SIZE&#125; - 1))); do</span></span><br><span class="line"><span class="string"></span><span class="string">PEERS=&quot;$&#123;PEERS&#125;$&#123;PEERS:+,&#125;$&#123;SET_NAME&#125;-$&#123;i&#125;=http://$&#123;SET_NAME&#125;-$&#123;i&#125;.$&#123;SET_</span></span><br><span class="line"><span class="string">NAME&#125;:2380&quot;</span></span><br><span class="line">   <span class="string">done</span></span><br><span class="line">   <span class="comment"># start etcd. If cluster is already initialized the `--initial-*` options </span></span><br><span class="line">   <span class="string">will</span> <span class="string">be</span> <span class="string">ignored.</span></span><br><span class="line">   <span class="string">exec</span> <span class="string">etcd</span> <span class="string">--name</span> <span class="string">$&#123;HOSTNAME&#125;</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--listen-peer-urls</span> <span class="string">http://$&#123;IP&#125;:2380</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--listen-client-urls</span> <span class="string">http://$&#123;IP&#125;:2379,http://127.0.0.1:2379</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--advertise-client-urls</span> <span class="string">http://$&#123;HOSTNAME&#125;.$&#123;SET_NAME&#125;:2379</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--initial-advertise-peer-urls</span> <span class="string">http://$&#123;HOSTNAME&#125;.$&#123;SET_NAME&#125;:2380</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--initial-cluster-token</span> <span class="string">etcd-cluster-1</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--initial-cluster</span> <span class="string">$&#123;PEERS&#125;</span> <span class="string">\</span></span><br><span class="line">     <span class="string">--initial-cluster-state</span> <span class="string">new</span> <span class="string">\</span></span><br><span class="line">      <span class="string">--data-dir</span> <span class="string">/var/run/etcd/default.etcd</span></span><br></pre></td></tr></table></figure>

<p>面对这种境况，CoreOS为Kubernetes引入了一个称为Operator的新概念和新组件，它借助CRD（Customed Resource Definition）创建自定义资源类型来完整描述某个有状态应用集群，并相应创建自定义的控制器来编排这些自定义资源类型所创建的各个资源对象。简单来讲，Operator就是一个开发规范和SDK，它合理地利用Kubernetes API的CRD功能扩展出二级抽象，又巧妙地回归到Kubernetes的“控制器”逻辑，从而提供了一个有状态应用的实现接口，用户可利用它开发专用于管理某个特定有状态应用的运维控制器，并按需回馈给社区。<br>目前，Operator社区中涌现了大量的特定实现，例如coreos/etcd-operator、oracle/mysql-operator和jenkinsci/jenkins-operator等，有些分布式应用的可用Operator实现甚至不止一种。Operator官方维护着etcd、Rook、Prometheus和Vault几个Operator，并通过<a target="_blank" rel="noopener" href="https://github.com/operator-framework/awesome-operators%E7%BB%B4%E6%8A%A4%E7%9D%80%E4%B8%BB%E6%B5%81%E7%9A%84Operator%E9%A1%B9%E7%9B%AE%E5%88%97%E8%A1%A8%E3%80%82%E8%BF%99%E6%84%8F%E5%91%B3%E7%9D%80%EF%BC%8C%E5%9C%A8Kubernetes%E7%B3%BB%E7%BB%9F%E4%B8%8A%E9%83%A8%E7%BD%B2%E5%88%86%E5%B8%83%E5%BC%8F%E6%9C%89%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%9A%84%E5%B8%B8%E7%94%A8%E6%96%B9%E5%BC%8F%E6%98%AF%E4%BD%BF%E7%94%A8Operator%EF%BC%8C%E8%80%8C%E9%9D%9E%E8%87%AA%E5%AE%9A%E4%B9%89StatefulSet%E8%B5%84%E6%BA%90%EF%BC%8C%E6%88%91%E4%BB%AC%E5%B0%86%E5%9C%A8%E7%AC%AC12%E7%AB%A0%E4%B8%AD%E5%86%8D%E4%B8%BE%E4%BE%8B%E8%AF%B4%E6%98%8EOperator%E7%9A%84%E7%94%A8%E6%B3%95%E3%80%82">https://github.com/operator-framework/awesome-operators维护着主流的Operator项目列表。这意味着，在Kubernetes系统上部署分布式有状态应用的常用方式是使用Operator，而非自定义StatefulSet资源，我们将在第12章中再举例说明Operator的用法。</a></p>
<h2 id="DaemonSet控制器"><a href="#DaemonSet控制器" class="headerlink" title="DaemonSet控制器"></a>DaemonSet控制器</h2><p>Deployment仅用于保证在集群上精确运行多少个工作负载的实例，但有些系统级应用却需要在集群中的每个节点上精确运行单个实例，这就是DaemonSet控制器的核心功用所在。系统级工作负载的副本数量取决于集群中的节点数，而非由用户通过replicas进行定义，更重要的是，后续新加入集群的工作节点也会由DaemonSet对象自动创建并运行为一个相关Pod，而从集群移除节点时，该类Pod对象也将被自动回收且无须重建。此外，管理员也可以使用节点选择器或节点标签指定仅在部分具有特定特征的节点上运行指定的Pod对象。<br>简单来说，DaemonSet就是一种特殊的控制器，它有着特定的应用场景，通常用于运行那些执行系统级操作任务的应用，例如：<br>▪运行集群存储的守护进程，例如在每个节点上运行的glusterd可用于接入Gluster集群；<br>▪在每个节点上运行日志收集守护进程，例如fluentd、filebeat和logstash等；<br>▪在每个节点上运行监控系统的代理守护进程，例如Prometheus Node Exporter、collectd、Datadog agent、New Relic agent，或Ganglia gmond等。提示<br>以kubeadm部署的Kubernetes集群上，kube-proxy便是由DaemonSet控制器所编排；另外，Flannel网络插件运行在各节点之上的代理程序也使用了该类型的控制器。<br>既然是需要在集群内的每个节点或部分节点运行工作负载的单个实例，那么，也就可以把应用直接运行为工作节点上的系统级守护进程，只是这么一来也就失去了托管给Kubernetes所带来的便捷性。另外，当必须把Pod对象以单实例运行在固定的几个节点并且需要先于其他Pod启动时，才有必要使用DaemonSet控制器，否则就应该使用Deployment控制器。</p>
<h3 id="DaemonSet资源基础应用"><a href="#DaemonSet资源基础应用" class="headerlink" title="DaemonSet资源基础应用"></a>DaemonSet资源基础应用</h3><p>DaemonSet是标准的API资源类型，它的spec字段中嵌套使用的字段也需要使用selector、template和minReadySeconds，并且它们各自的功能和用法基本相同，但DaemonSet不支持使用replicas，毕竟DaemonSet不是基于期望的副本数，而是基于节点数量来控制Pod资源数量，但template是必选字段。另外，DaemonSet也支持策略式更新，它支持OnDelete和RollingUpdate两种策略，也能够为滚动更新保存修订记录。DaemonSet资源的简要配置规范如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span>              <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span>                  <span class="comment"># 资源类型特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>                  <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># 名称空间；DaemonSet资源隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">minReadySeconds</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># Pod就绪后多少秒内任一容器无崩溃方可视为“就绪”</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;object&gt;</span>              <span class="comment"># 标签选择器，必须匹配template字段中Pod模板的标签</span></span><br><span class="line">  <span class="string">template</span> <span class="string">&lt;object&gt;</span>              <span class="comment"># Pod模板对象</span></span><br><span class="line">  <span class="string">revisionHistoryLimit</span> <span class="string">&lt;integer&gt;</span> <span class="comment"># 滚动更新历史记录数量，默认为10</span></span><br><span class="line">  <span class="string">updateStrategy</span> <span class="string">&lt;Object&gt;</span>        <span class="comment"># 滚动更新策略</span></span><br><span class="line">    <span class="string">type</span> <span class="string">&lt;string&gt;</span>                <span class="comment"># 滚动更新类型，可用值有OnDelete和Rollingupdate</span></span><br><span class="line">    <span class="string">rollingUpdate</span> <span class="string">&lt;Object&gt;</span>       <span class="comment"># 滚动更新参数，专用于RollingUpdate类型</span></span><br><span class="line">      <span class="string">maxUnavailable</span> <span class="string">&lt;string&gt;</span>    <span class="comment"># 更新期间可比期望的Pod数量缺少的数量或比例</span></span><br></pre></td></tr></table></figure>

<p>下面的资源清单（daemonset-demo.yaml）示例中定义了一个DaemonSet资源，用于在每个节点运行一个Prometheus node_exporter进程以收集节点级别的监控数据，该进程共享节点的Network和PID名称空间。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">daemonset-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">prometheus</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">node-exporter</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">prometheus</span></span><br><span class="line">      <span class="attr">component:</span> <span class="string">node-exporter</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">prometheus-node-exporter</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">prometheus</span></span><br><span class="line">        <span class="attr">component:</span> <span class="string">node-exporter</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">prom/node-exporter:v0.18.0</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">prometheus-node-exporter</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">prom-node-exp</span></span><br><span class="line">          <span class="attr">containerPort:</span> <span class="number">9100</span></span><br><span class="line">          <span class="attr">hostPort:</span> <span class="number">9100</span></span><br><span class="line">        <span class="attr">livenessProbe:</span></span><br><span class="line">          <span class="attr">tcpSocket:</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">prom-node-exp</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">3</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/metrics&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="string">prom-node-exp</span></span><br><span class="line">            <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">hostPID:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>Prometheus node_exporter默认监听TCP协议的9100端口，基于HTTP协议及/metrics输出指标数据，我们可以将daemonset-demo创建到集群之上后，向任一节点IP发起访问，进行测试来验证。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">apply</span> <span class="string">-f</span> <span class="string">daemonset-demo.yaml</span> </span><br><span class="line"><span class="string">daemonset.apps/daemonset-demo</span> <span class="string">created</span></span><br><span class="line"><span class="string">~$</span> <span class="string">curl</span> <span class="string">-s</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.11</span><span class="string">:9100/metrics</span> </span><br><span class="line"><span class="comment"># HELP go_gc_duration_seconds A summary of the GC invocation durations.</span></span><br><span class="line"><span class="comment"># TYPE go_gc_duration_seconds summary</span></span><br><span class="line"><span class="string">go_gc_duration_seconds&#123;quantile=&quot;0&quot;&#125;</span> <span class="number">8.729e-06</span></span><br><span class="line"><span class="string">go_gc_duration_seconds&#123;quantile=&quot;0.25&quot;&#125;</span> <span class="number">3.1308e-05</span></span><br><span class="line"><span class="string">……</span></span><br></pre></td></tr></table></figure>

<p>DaemonSet资源在其详细描述信息输出了相关Pod对象的状态，包括应该在集群上运行的副本数和实际运行的副本数及相关的状态等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Desired Number of Nodes Scheduled: 3</span><br><span class="line">Current Number of Nodes Scheduled: 3</span><br><span class="line">Number of Nodes Scheduled with Up-to-date Pods: 3</span><br><span class="line">Number of Nodes Scheduled with Available Pods: 3</span><br><span class="line">Number of Nodes Misscheduled: 0</span><br><span class="line">Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed</span><br></pre></td></tr></table></figure>

<p>偶尔也存在需要将Pod对象以单一实例形式运行在集群中的部分工作节点，例如有些拥有特殊硬件节点需要运行特定的监控代理程序等。这仅需要在Pod模板的spec字段中嵌套使用nodeSelector字段，并确保其值定义的标签选择器与部分特定工作节点的标签匹配即可。<br>另外，考虑到大多数系统级应用的特殊性，DaemonSet资源的各Pod实例通常需要被单独访问而不能隐藏在某个Service对象之后，例如无论是监控代理程序或日志采集代理程序所在的节点都需要由其服务器端各自识别并单独进行通信。因此，各节点上的Pod应用推送数据至服务端，使用Headless Service或者直接让Pod应用共享节点的网络名称空间，并监听一个端口（例如node_exporter的9100端口）是满足这种需求的常见做法。</p>
<h3 id="DaemonSet更新策略"><a href="#DaemonSet更新策略" class="headerlink" title="DaemonSet更新策略"></a>DaemonSet更新策略</h3><p>DaemonSet自Kubernetes 1.6版本起也开始支持更新机制，相关配置定义在spec.update-Strategy嵌套字段中。目前，它支持RollingUpdate和OnDelete两种更新策略。<br>▪RollingUpdate为默认的策略，工作逻辑类似于Deployment控制器上的同名策略，不过节点难以临时弹性增设，因而DaemonSet仅能支持使用maxUnavailabe属性定义最大不可用Pod资源副本数（默认值为1）。<br>▪Ondelete是在相应节点的Pod资源被删除后重建为新版本，从而允许用户手动编排更新过程。<br>将此前创建的daemonset-demo中Pod模板的容器镜像修改为prom/node-exporter:v0.18.1便能测试其更新过程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">set</span> image daemonsets/daemonset-demo \</span></span><br><span class="line"><span class="language-bash">          prometheus-node-exporter=<span class="string">&quot;prom/node-exporter:v0.18.1&quot;</span></span></span><br><span class="line">daemonset.apps/daemonset-demo image updated</span><br></pre></td></tr></table></figure>

<p>按照默认的RollingUpdate策略，daemonset-demo资源将采用一次更新一个Pod对象，待新建Pod对象就绪后再更新下一个Pod对象的方式进行，资源相关的事件中会详细展示出其更新过程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe daemonsets daemonsets/daemonset-demo</span></span><br><span class="line">……</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age    From                  Message</span><br><span class="line">  ----    ------            ----   ----                  -------</span><br><span class="line">  Normal  SuccessfulDelete  7m44s  daemonset-controller  Deleted pod: daemonset-demo-btl8x</span><br><span class="line">  Normal  SuccessfulCreate  7m39s  daemonset-controller  Created pod: daemonset-demo-5z8q8</span><br><span class="line">  Normal  SuccessfulDelete  6m6s   daemonset-controller  Deleted pod: daemonset-demo-hf9lv</span><br><span class="line">  Normal  SuccessfulCreate  6m3s   daemonset-controller  Created pod: daemonset-demo-bw5qp</span><br><span class="line">  Normal  SuccessfulDelete  4m34s  daemonset-controller  Deleted pod: daemonset-demo-gzxd2</span><br><span class="line">  Normal  SuccessfulCreate  4m27s  daemonset-controller  Created pod: daemonset-demo-l9qgg</span><br></pre></td></tr></table></figure>

<p>规模较大的集群中，我们也可以增大RollingUpdate策略中maxUnavailable属性的值来加快其滚动过程，例如设置为20%、25%甚至是50%等。DaemonSet控制器的滚动更新机制同样支持借助minReadySeconds来自定义Pod对象必须处于“就绪”状态多少时长才能视作“可用”。另外，DaemonSet资源的更新操作也支持回滚，包括回滚至REVISION历史记录中的任何一个指定的版本等。<br>而对于需要精心组织每个实例更新过程才能确保其升级过程可靠进行的应用来说，我们就不得不使用OnDelete策略来替换默认的RollingUpdate策略。OnDelete策略的实施逻辑较为简单，这里就不再给出具体操作过程。</p>
<h2 id="Job控制器"><a href="#Job控制器" class="headerlink" title="Job控制器"></a>Job控制器</h2><p>与Deployment及DaemonSet控制器管理的守护进程类的服务应用所不同的是，Job控制器常用于管理那些运行一段时间就能够“完成”的任务，例如计算或备份操作。容器中的进程正常运行完成而结束后不需要再重启，而是由控制器把该Pod对象置于Completed（完成）状态，并能够在超过用户指定的生存周期后由系统自行删除。但是，若容器中的进程因“错误”（而非完成）而终止，则需要依据配置来确定其重启与否，通常，未运行完成的Pod对象因其所在的节点故障而意外终止后会被重新创建。Job控制器的Pod对象的状态转换如图8-19所示。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210135515103.png" alt="image-20220210135515103"></p>
<p>实践中，有的作业任务可能需要运行不止一次，用户可以配置它们以串行或并行方式运行。总结起来，这种类型的Job资源对象主要有两种。<br>▪单工作队列的串行式Job：将一个作业串行执行多次直到满足期望的次数，如图8-20所示；这种Job也可理解为并行度为1的作业执行方式，在某个时刻仅有一个Pod资源对象存在。</p>
<p><img src="/blog/2022/02/10/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/%E5%BA%94%E7%94%A8%E7%BC%96%E6%8E%92%E4%B8%8E%E7%AE%A1%E7%90%86-%E6%8E%A7%E5%88%B6%E5%99%A8/image-20220210135537338.png" alt="image-20220210135537338"></p>
<p>▪多工作队列的并行Job：这种方式中，可以设置工作队列数（即作业数），每个队列仅负责运行一个作业，如图8-21中的左图所示；也可以用有限的工作队列运行较多的作业，即工作队列数少于总作业数，它相当于运行着多个串行作业队列。如图8-21中的右图所示，工作队列数即同时可运行的Pod资源数。</p>
<p>具体运行中，我们需要根据作业的特性来选择合适的并行度及编排策略，对于有严格次序要求或者拥有“层进”特性的作业，单工作队列串行执行是其唯一可行的选择，反之，适度的并行能够提升作业运行速度。</p>
<h3 id="Job资源基础应用"><a href="#Job资源基础应用" class="headerlink" title="Job资源基础应用"></a>Job资源基础应用</h3><p>作为标准的API资源类型之一，Job规范同样由apiVersion、kind、metadata和spec等字段组成，由系统自行维护的status字段用于保存资源的当前状态，该资源的基本定义格式如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span>      <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span>                  <span class="comment"># 资源类型特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 名称空间；Job资源隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;object&gt;</span>        <span class="comment"># 标签选择器，必须匹配template字段中Pod模板的标签</span></span><br><span class="line">  <span class="string">template</span> <span class="string">&lt;object&gt;</span>        <span class="comment"># Pod模板对象</span></span><br><span class="line">  <span class="string">completions</span> <span class="string">&lt;integer&gt;</span>    <span class="comment"># 期望的成功完成的作业次数，成功运行结束的Pod数量</span></span><br><span class="line">  <span class="string">ttlSecondsAfterFinished</span>  <span class="string">&lt;integer&gt;</span>  <span class="comment"># 终止状态作业的生存时长，超期将被删除</span></span><br><span class="line">  <span class="string">parallelism</span>  <span class="string">&lt;integer&gt;</span>   <span class="comment"># 作业的最大并行度，默认为1</span></span><br><span class="line">  <span class="string">backoffLimit</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 将作业标记为Failed之前的重试次数，默认为6</span></span><br><span class="line">  <span class="string">activeDeadlineSeconds</span>  <span class="string">&lt;integer&gt;</span>    <span class="comment"># 作业启动后可处于活动状态的时长</span></span><br></pre></td></tr></table></figure>

<p>定义Job资源时，spec字段内嵌的必要字段仅有template一个，Job会为其Pod对象自动添加job-name=JOB_NAME和controller-uid=UID标签，并使用标签选择器完成对controller-uid标签的关联。例如，下面的资源清单（job-example.yaml）中定义了一个名为job-demo的Job资源：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">job-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myjob</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">alpine:3.11</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 60&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">ttlSecondsAfterFinished:</span> <span class="number">3600</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">activeDeadlineSeconds:</span> <span class="number">300</span></span><br></pre></td></tr></table></figure>

<p>注意<br>Pod模板中的spec.restartPolicy默认为Always，这对Job控制器来说并不适用，因此必须在Pod模板中显式设定restartPolicy属性的值为Never或OnFailure。<br>出于运行一段时长后可终止的目的，该示例中的Pod模板通过借助alpine镜像运行一个睡眠60秒（sleep 60）的应用来模拟该功能。将job-demo资源创建到集群之上便可查看相关的任务状态，如下第二条命令显示的简要状态信息中，COMPLETIONS字段（m/n）表示期望完成的作业数（n）和已经完成的作业数（m），DURATION为作业完成所运行的时长。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f job-demo.yaml</span> </span><br><span class="line">job.batch/job-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get <span class="built_in">jobs</span>/job-demo</span></span><br><span class="line">NAME     COMPLETIONS      DURATION     AGE</span><br><span class="line">job-demo    1/2             66s        66s</span><br></pre></td></tr></table></figure>

<p>相关的Pod资源能够以Job资源的名称为标签进行筛选，对于串行运行的作业来说，不同时刻能筛选出的Pod数量可能存在差异。下面的显示命令运行于第一次作业完成后，而第二次作业刚启动之时：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l job-name=job-demo</span></span><br><span class="line">NAME            READY    STATUS        RESTARTS   AGE</span><br><span class="line">job-demo-lb4vw   0/1     Completed     0          68s</span><br><span class="line">job-demo-xn7zq   1/1     Running       0          6s</span><br></pre></td></tr></table></figure>

<p>Job资源的详细描述中能够获得进一步的信息，包括为Pod自动添加的标签、使用的标签选择器、作业并行度、各Pod的相关状态及相应事件等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe <span class="built_in">jobs</span>/job-demo</span>        </span><br><span class="line">Name:                     job-demo</span><br><span class="line">Namespace:                default</span><br><span class="line">Selector:                 controller-uid=42101d29-8a2b-45bf-b003-13af317c1300</span><br><span class="line">Labels:                   controller-uid=42101d29-8a2b-45bf-b003-13af317c1300</span><br><span class="line">                          job-name=job-demo</span><br><span class="line">Annotations:              Parallelism:  1</span><br><span class="line">Completions:              2</span><br><span class="line">Start Time:               Sun, 20 Sep 2020 12:00:33 +0800</span><br><span class="line">Completed At:             Sun, 20 Sep 2020 12:02:37 +0800</span><br><span class="line">Duration:                 2m4s</span><br><span class="line">Active Deadline Seconds:  300s</span><br><span class="line">Pods Statuses:            0 Running / 2 Succeeded / 0 Failed</span><br><span class="line">Pod Template:</span><br><span class="line">  Labels:  controller-uid=42101d29-8a2b-45bf-b003-13af317c1300</span><br><span class="line">           job-name=job-demo</span><br><span class="line">  Containers:</span><br><span class="line">   ……</span><br><span class="line">Events:</span><br><span class="line">  Type    Reason            Age    From            Message</span><br><span class="line">  ----    ------            ----   ----            -------</span><br><span class="line">  Normal  SuccessfulCreate  3m51s  job-controller  Created pod: job-demo-lb4vw</span><br><span class="line">  Normal  SuccessfulCreate  2m49s  job-controller  Created pod: job-demo-xn7zq</span><br><span class="line">  Normal  Completed         107s   job-controller  Job completed</span><br></pre></td></tr></table></figure>

<p>由上面命令结果可知，Job的默认使用的并行度为1，这也是为什么上面示例中的两个作业要先后执行而非同时执行的原因，这意味着多次作业需要以串行方式运行，作业的总时长至少要相当于各任务各自的执行时长之和。<br>Job资源运行完成后便不再占用系统资源，用户可将其按需保留、手动删除或者设置相应属性执行自动删除操作。job-demo资源留给用户检查相关资源信息的时间窗口为3600秒（spec.ttlSecondsAfterFinished），超出该时长后，该作业将由控制器自行删除，而未定义该字段的作业将会一直保留。<br>现实中的作业未必能有精确的运行时长，若某Job资源的Pod程序因存在Bug或其他原因导致的作业无法“完成”并退出，而其restatPolicy又定义为了重启，则该Pod可能会一直处于重启和错误的循环当中。为此，Job控制器提供了两个属性用于抑制这种情况的发生：<br>▪.spec.activeDeadlineSeconds <integer>：用于为Job指定最大活动时间长度，超出此时长的作业将被终止并标记为失败；<br>▪.spec.backoffLimit <integer>：将作业标记为失败状态之前的重试次数，默认值为6。<br>由此可见，任务的总体可运行时长（activeDeadlineSeconds）也必须足够容纳作业的预测的总体运行时长。另外，不存在严格意义上先后次序的多次作业，适度的并行将能够显著提升其运行速度。</integer></integer></p>
<h3 id="并行式Job与扩容机制"><a href="#并行式Job与扩容机制" class="headerlink" title="并行式Job与扩容机制"></a>并行式Job与扩容机制</h3><p>将并行度属性.spec.parallelism设置为大于1的值，并设置总任务数.spec.completion属性大于并行度，便能够让Job资源以并行方式运行多任务。下面示例中定义了一个2路并行且总体运行10次任务的Job资源规范：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Job</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">job-para-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myjob</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">alpine:3.11</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;sleep 60&quot;</span>]</span><br><span class="line">      <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">  <span class="attr">completions:</span> <span class="number">10</span></span><br><span class="line">  <span class="attr">parallelism:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">ttlSecondsAfterFinished:</span> <span class="number">3600</span></span><br><span class="line">  <span class="attr">backoffLimit:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">activeDeadlineSeconds:</span> <span class="number">1200</span></span><br></pre></td></tr></table></figure>

<p>按照并行Job的运行法则，job-para-demo资源将允许最多同时运行两个Pod，这相当于存在两路虚拟作业管道，每个虚拟管道串行运行分配而来的Job。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f job-para-demo.yaml</span>                             </span><br><span class="line">job.batch/job-para-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l job-name=job-para-demo</span></span><br><span class="line">NAME                 READY    STATUS     RESTARTS    AGE</span><br><span class="line">job-para-demo-8fxj2   1/1     Running     0          10s</span><br><span class="line">job-para-demo-mblzj   1/1     Running     0          10s</span><br></pre></td></tr></table></figure>

<p>Job资源的作业并行度支持运行时修改，因而，我们还能够通过修改parallelism属性的值来动态提升作业并行度以实现Job资源扩容之目的。例如，下面的命令将尚未完成的job-para-demo并行度从2提升到了5：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch <span class="built_in">jobs</span>/job-para-demo -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;parallelism&quot;:5&#125;&#125;&#x27;</span></span></span><br><span class="line">job.batch/job-para-demo patched</span><br></pre></td></tr></table></figure>

<p>于是，pod-para-demo资源的并行度提升为5，Kubernetes系统为job-para-demo资源同时运行的Pod资源数量也随之提升到了5个，例如，对于刚启动不久的job-para-demo资源执行如下面的命令可生成类似如下运行于5个Pod作业的结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l job-name=job-para-demo -w</span></span><br><span class="line">NAME                  READY   STATUS      RESTARTS   AGE</span><br><span class="line">job-para-demo-8fxj2   0/1     Completed   0          82s</span><br><span class="line">job-para-demo-d6z98   1/1     Running     0          7s</span><br><span class="line">job-para-demo-hbh99   1/1     Running     0          20s</span><br><span class="line">job-para-demo-lfvj5   1/1     Running     0          7s</span><br><span class="line">job-para-demo-mblzj   0/1     Completed   0          82s</span><br><span class="line">job-para-demo-nq8h7   1/1     Running     0          7s</span><br><span class="line">job-para-demo-ss9t4   1/1     Running     0          20s</span><br></pre></td></tr></table></figure>

<p>另外，Job资源详细描述中，相关事件的发生时间点也是辅助了解Pod对象并行运行状态的有效辅助信息。</p>
<h2 id="CronJob控制器"><a href="#CronJob控制器" class="headerlink" title="CronJob控制器"></a>CronJob控制器</h2><p>CronJob资源用于管理Job资源的运行时间，它允许用户在特定的时间或以指定的间隔运行Job，它适合自动执行特定的任务，例如备份、报告、发送电子邮件或清理类的任务等。换句话说，CronJob能够以类似于Linux操作系统的周期性任务作业计划（crontab）的方式控制其运行的时间点及周期性运行的方式：<br>▪仅在未来某时间点将指定的作业运行一次；<br>▪在指定的周期性时间点重复运行指定的作业。<br>CronJob资源使用的时间格式类似于Linux系统上的crontab，稍具不同之处是，CronJob资源在指定时间点时，通配符“?”和“*”的意义相同，它们都表示任何可用的有效值。<br>CronJob资源使用Job对象来完成任务，它每次运行时都会创建一个Job对象，并使用类似于Job资源的创建、管理和扩容方式。Cronjob也是Kubernetes系统标准的API资源，其资源规范的基本格式如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span>                <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span>                            <span class="comment"># 资源类型特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>                          <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>                     <span class="comment"># 名称空间；CronJob资源隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">jobTemplate</span>  <span class="string">&lt;Object&gt;</span>                  <span class="comment"># Job作业模板，必选字段</span></span><br><span class="line">    <span class="string">metadata</span> <span class="string">&lt;object&gt;</span>                    <span class="comment"># 模板元数据</span></span><br><span class="line">    <span class="string">spec</span> <span class="string">&lt;object&gt;</span>                        <span class="comment"># 作业的期望状态</span></span><br><span class="line">  <span class="string">schedule</span> <span class="string">&lt;string&gt;</span>                      <span class="comment"># 调度时间设定，必选字段</span></span><br><span class="line">  <span class="string">concurrencyPolicy</span>  <span class="string">&lt;string&gt;</span>     <span class="comment"># 并发策略，可用值有Allow、Forbid和Replace</span></span><br><span class="line">  <span class="string">failedJobsHistoryLimit</span> <span class="string">&lt;integer&gt;</span>       <span class="comment"># 失败作业的历史记录数，默认为1</span></span><br><span class="line">  <span class="string">successfulJobsHistoryLimit</span>  <span class="string">&lt;integer&gt;</span>  <span class="comment"># 成功作业的历史记录数，默认为3</span></span><br><span class="line">  <span class="string">startingDeadlineSeconds</span>  <span class="string">&lt;integer&gt;</span>     <span class="comment"># 因错过时间点而未执行的作业的可超期时长</span></span><br><span class="line">  <span class="string">suspend</span>  <span class="string">&lt;boolean&gt;</span>              <span class="comment"># 是否挂起后续的作业，不影响当前作业，默认为false</span></span><br></pre></td></tr></table></figure>

<p>下面资源清单（cronjob-demo.yaml）定义了一个名为cronjob-demo的CronJob资源示例，它每隔2分钟运行一次由jobTemplate定义的示例任务，每次任务以单路并行的方式执行1次，每个任务的执行不超过60秒，且完成后600秒的Job将会被删除。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">cronjob-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">schedule:</span> <span class="string">&quot;*/2 * * * *&quot;</span></span><br><span class="line">  <span class="attr">jobTemplate:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">mycronjob-jobs</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">parallelism:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">completions:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">ttlSecondsAfterFinished:</span> <span class="number">3600</span></span><br><span class="line">      <span class="attr">backoffLimit:</span> <span class="number">3</span></span><br><span class="line">      <span class="attr">activeDeadlineSeconds:</span> <span class="number">60</span></span><br><span class="line">      <span class="attr">template:</span></span><br><span class="line">        <span class="attr">spec:</span></span><br><span class="line">          <span class="attr">containers:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">myjob</span></span><br><span class="line">            <span class="attr">image:</span> <span class="string">alpine</span></span><br><span class="line">            <span class="attr">command:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">            <span class="bullet">-</span> <span class="string">date;</span> <span class="string">echo</span> <span class="string">Hello</span> <span class="string">from</span> <span class="string">CronJob,</span> <span class="string">sleep</span> <span class="string">a</span> <span class="string">while…;</span> <span class="string">sleep</span> <span class="number">10</span><span class="string">;</span></span><br><span class="line">          <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br><span class="line">  <span class="attr">startingDeadlineSeconds:</span> <span class="number">300</span></span><br></pre></td></tr></table></figure>

<p>将cronjob-demo资源创建到集群上后便可通过资源对象的相关信息了解运行状态。下面第二条命令结果中的SCHEDULE是指其调度时间点，SUSPEND表示后续任务是否处于挂起状态，即暂停任务的调度及运行，ACTIVE表示活动状态的Job对象的数量，而LAST SCHEDULE则表示前一次调度运行至此刻的时长。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f cronjob-demo.yaml</span></span><br><span class="line">cronjob.batch/cronjob-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get cronjobs/cronjob-demo</span></span><br><span class="line">NAME           SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE</span><br><span class="line">cronjob-demo   */2 * * * *    False    1        6s              69s</span><br></pre></td></tr></table></figure>

<p>我们可借助示例中Job模板上定义的标签过滤出名称空间中相关的Job对象。一段时长后，cronjob-demo创建的Job对象可能会存在多个，但示例中Job模板的配置会使得Job控制器自动删除那些完成后超过3600秒的、由cronjob-demo生成的Job对象。另外，CronJob资源默认仅会在历史记录中保留最近运行成功的3个以及运行失败的1个Job，因此，最终保留多少个Job也取决于CronJob中的历史记录定义，而历史记录中保存的Job数也支持由用户自定义其配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get <span class="built_in">jobs</span> -l controller=cronjob-demo</span></span><br><span class="line">NAME                   COMPLETIONS      DURATION   AGE</span><br><span class="line">cronjob-demo-1589970720   1/1           24s        6m28s</span><br><span class="line">cronjob-demo-1589970840   1/1           27s        4m28s</span><br><span class="line">cronjob-demo-1589970960   1/1           20s        2m28s</span><br><span class="line">cronjob-demo-1589971080   0/1           27s        27s</span><br></pre></td></tr></table></figure>

<p>CloJob在Pod中运行，并会保留处于Completed状态的Pod日志。由CronJob资源通过模板创建的Job对象的名称以CronJob自身的名称为前缀，以Job创建时的时间戳为后缀，而各Job对象相关的Pod对象的名称则随机生成。已完成的CronJob资源相关Pod的状态为Completed，而失败的作业状态则存在RunContainerError、CrashLoopBackOff或其他表示失败的状态。<br>可选的spec.startingDeadlineSeconds字段指示当CronJob由于某种原因错过了计划时间的情况下而允许延迟启动的最长时间（以秒为单位），错过的CronJob将被视为处于Failed状态。而未定义该字段值，则意味着CronJob永远不会超时，这将会导致CronJob资源存在同时运行多个实例的可能性。<br>CronJob资源的Job对象可能不支持同时运行多个实例，用户可基于.spec.concurrencyPolicy属性来控制多个CronJob并存的机制，它的默认值为Allow，即允许不同时间点的多个CronJob实例同时运行。其他两个可用值中，Forbid用于禁止前后两个CronJob同时运行，如果前一个尚未结束，则后一个不能启动（跳过），Replace用于让后一个CronJob取代前一个，即终止前一个并启动后一个。</p>
<h2 id="Pod中断预算"><a href="#Pod中断预算" class="headerlink" title="Pod中断预算"></a>Pod中断预算</h2><p>尽管Deployment等一类的控制器能确保相应Pod对象的副本数量不断逼近期望的数量，但它却无法保证在某一时刻一定存在指定数量或一定百分比的Pod对象，然而这种需求在某些强调服务可用性的场景中是必备的。于是，Kubernetes自1.4版本起引入了PDB（PodDisruptionBudget，Pod中断预算）类型的资源，用于为那些自愿的中断做好预算方案，限制可自愿中断的最大Pod副本数或确保最少可用的Pod副本数，以确保服务的高可用性。<br>Pod对象创建后会一直存在，除非用户有意将其销毁，或者出现了不可避免的硬件或系统软件错误。非自愿中断是指那些由不可控的外界因素导致的Pod中止而退出的情形，例如硬件或系统内核故障、网络故障以及节点资源不足导致Pod对象被驱逐等；而那些由用户特地执行的管理操作导致的Pod中断则称为自愿中断，例如排空节点、人为删除Pod对象、由更新操作触发的Pod对象重建等。用户可以为那些部署在Kubernetes的任何应用程序创建一个对应PDB对象以限制自愿中断时最大可以中断的副本数或者最少应该保持可用的副本数，从而保证应用自身的可用性。<br>PDB资源的核心目标在于保护由控制器管理的应用，这必然意味着PDB将使用等同于相关控制器对象的标签选择器以精确关联至目标Pod对象。PDB支持的控制器类型包括Deployment、ReplicaSet和StatefulSet等。同时，PDB对象也可以用来保护那些纯粹是由定制的标签选择器自由选择的Pod对象。<br>并非所有的自愿中断都会受到PDB的约束，例如，删除Deployment或者Pod的操作就会绕过PDB。另外，尽管那些因删除或更新操作导致不可用的Pod也会计入预算，但是控制器（例如Deployment）滚动更新时并不会真的被相关联的PDB资源所限制。因此，用户应当明确遵守PDB的限制法则，而不能直接删除PDB相关的Pod或者控制器资源。但管理员在维护集群时对节点执行的排空操作会受到PDB的限制。<br>PDB也是标准的API资源类型，其资源规范如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">batch/v1beta1</span> <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">CronJob</span>             <span class="comment"># 资源类型特有标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 资源名称，在作用域中要唯一</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 名称空间；CronJob资源隶属名称空间级别</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;Object&gt;</span>       <span class="comment"># 标签选择器，通常要与目标控制器资源相同</span></span><br><span class="line">  <span class="string">minAvailable</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 至少可用的Pod对象百分比，100%意味着不支持自愿中断</span></span><br><span class="line">  <span class="string">maxUnavailable</span> <span class="string">&lt;string&gt;</span> <span class="comment"># 至多不可用的Pod对象百分比，0意味着不支持自愿中断；</span></span><br><span class="line">                          <span class="comment"># minAvailable和maxUnavailable互斥，不能同时定义</span></span><br></pre></td></tr></table></figure>

<p>下面的配置清单示例定义了名为pdb-demo的PDB资源，它对8.3.1节中由Deployment资源deployment-demo创建的Pod对象设置了PDB限制，要求其最少可用Pod对象数量为3个。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">policy/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PodDisruptionBudget</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pdb-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">maxUnAvailable:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">release:</span> <span class="string">stable</span></span><br></pre></td></tr></table></figure>

<p>pdb-demo资源对象创建完成后，我们能够从其YAML格式的资源规范状态信息中了解到该资源的当前状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pdb-demo.yaml</span> </span><br><span class="line">poddisruptionbudget.policy/pdb-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pdb/pdb-demo -o yaml</span></span><br><span class="line">……</span><br><span class="line">status:</span><br><span class="line">  currentHealthy: 4</span><br><span class="line">  desiredHealthy: 3</span><br><span class="line">  disruptionsAllowed: 1</span><br><span class="line">  expectedPods: 4</span><br><span class="line">  observedGeneration: 1</span><br></pre></td></tr></table></figure>

<p>接下来，我们可通过在1～2个节点上模拟驱逐deployment-demo资源作用域内的两个或以上数量的Pod对象模拟自愿中断过程，并监控各Pod对象被终止的过程来验证PDB资源对象的控制功效。<br>首先，我们先了解deployment-demo作用域内各Pod对象在集群节点上的分布状态，下面的命令结果显示出，它有两个Pod对象同时运行在节点k8s-node02.ilinux.io之上：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp,release=stable -o wide | awk <span class="string">&#x27;&#123;print $1,$7&#125;&#x27;</span></span></span><br><span class="line">NAME                              NODE</span><br><span class="line">deployment-demo-b479b6f9f-dn8cc   k8s-node02.ilinux.io</span><br><span class="line">deployment-demo-b479b6f9f-ndt8t   k8s-node03.ilinux.io</span><br><span class="line">deployment-demo-b479b6f9f-pm994   k8s-node03.ilinux.io</span><br><span class="line">deployment-demo-b479b6f9f-qcwj4   k8s-node02.ilinux.io</span><br></pre></td></tr></table></figure>

<p>接下来，我们使用命令排空该节点以使得该deployment-demo资源作用域内的Pod对象有两个同时被中止，从而查看其触发pdb-demo的状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl drain k8s-node02.ilinux.io --ignore-daemonsets</span></span><br><span class="line">node/k8s-node02.ilinux.io already cordoned</span><br><span class="line">evicting pod default/deployment-demo-b479b6f9f-dn8cc</span><br><span class="line">evicting pod default/deployment-demo-b479b6f9f-qcwj4</span><br><span class="line">error when evicting pod &quot;deployment-demo-b479b6f9f-dn8cc&quot; (will retry after 5s): Cannot evict pod as it would violate the pod&#x27;s disruption budget.</span><br><span class="line">……</span><br><span class="line">error when evicting pod &quot;deployment-demo-b479b6f9f-dn8cc&quot; (will retry after 5s): Cannot evict pod as it would violate the pod&#x27;s disruption budget.</span><br><span class="line">evicting pod default/deployment-demo-b479b6f9f-dn8cc</span><br><span class="line">pod/deployment-demo-b479b6f9f-qcwj4 evicted</span><br><span class="line">pod/deployment-demo-b479b6f9f-dn8cc evicted</span><br><span class="line">node/k8s-node02.ilinux.io evicted</span><br></pre></td></tr></table></figure>

<p>测试完成后，关闭节点k8s-node02.ilinux.io的SchedulingDisabled状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl uncordon k8s-node02.ilinux.io</span></span><br><span class="line">node/k8s-node02.ilinux.io uncordoned</span><br></pre></td></tr></table></figure>

<p>从测试中排空命令的返回结果可以看出，同时执行驱逐deployment-demo作用域内的两个Pod对象的操作时，一个Pod能立即完成驱逐，但另一个Pod的驱逐操作被pdb-demo所阻塞，直到deployment-demo请求补足该Pod副本的请求在其他节点创建完成并就绪后，第二个Pod的驱逐操作才能得以完成。<br>事实上，PDB资源对多实例的有状态应用来说尤为有用，如Consul、ZooKeeper或etcd等，用户可借助PDB资源来防止自愿中断场景中将实例的数量减少到低于法定数量（quorum），以避免错误的写操作。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/10/ConfigMap/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/10/ConfigMap/" class="post-title-link" itemprop="url">ConfigMap和Secret</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-10 09:24:05" itemprop="dateCreated datePublished" datetime="2022-02-10T09:24:05+08:00">2022-02-10</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-18 13:57:55" itemprop="dateModified" datetime="2022-02-18T13:57:55+08:00">2022-02-18</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="应用配置"><a href="#应用配置" class="headerlink" title="应用配置"></a>应用配置</h1><h2 id="容器化应用配置"><a href="#容器化应用配置" class="headerlink" title="容器化应用配置"></a>容器化应用配置</h2><h3 id="容器化应用配置的常见方式"><a href="#容器化应用配置的常见方式" class="headerlink" title="容器化应用配置的常见方式"></a>容器化应用配置的常见方式</h3><p>容器镜像一般由多个只读层叠加组成，构建完成后无法进行修改，另一方面，“黑盒化”运行的容器使用隔离的专用文件系统，那么，如何为容器化应用提供配置信息呢？传统实践中，通常有这么几种途径。</p>
<ul>
<li>启动容器时直接向应用程序传递参数。</li>
<li>将定义好的配置文件硬编码（嵌入）于镜像文件中。</li>
<li>通过环境变量传递配置数据。</li>
<li>基于存储卷传送配置文件。</li>
</ul>
<h4 id="命令行参数"><a href="#命令行参数" class="headerlink" title="命令行参数"></a>命令行参数</h4><p>Dockerfile中的ENTRYPOINT和CMD指令用于指定容器启动时要运行的程序及其相关的参数。其中，CMD指令以列表形式指定要运行的程序及其相关的参数，若同时存在ENTRYPOINT指令，则CMD指令中的列表所有元素均被视作由ENTRYPOINT指定程序的命令行参数。另外，在基于某镜像创建容器时，可以通过向ENTRYPOINT中的程序传递额外的自定义参数，甚至还可以修改要运行的应用程序本向。例如，使用docker run命令创建并启动容器的格式为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run [OPTIONS] IMAGE [COMMAND] [ARG...]</span><br></pre></td></tr></table></figure>

<p>其中的[COMMAND]即为自定义运行的程序，[ARG]则是传递给程序的参数。若定义相关的镜像文件时使用了ENTRYPOINT指令，则[COMMAND]和[ARG]都会被当作命令行参数传递给ENTRYPOINT指令中指定的程序，除非为docker run命令额外使用–entrypoint选项覆盖ENTRYPOINT指令而指定运行其他程序。<br>在Kubernetes系统上创建Pod资源时，也能够向容器化应用传递命令行参数，甚至指定运行其他应用程序，相关的字段分别为pods.spec.containers[].command和pods.spec.containers[].args，该话题在Pod资源的相关话题中已有过介绍。</p>
<h4 id="将配置文件嵌入镜像文件"><a href="#将配置文件嵌入镜像文件" class="headerlink" title="将配置文件嵌入镜像文件"></a>将配置文件嵌入镜像文件</h4><p>用户在Dockerfile中使用COPY指令把定义好的配置文件复制到镜像文件系统上的目标位置，或者使用RUN指令调用sed或echo一类的命令修改配置文件，从而达到为容器化应用提供自定义配置文件之目的。</p>
<h4 id="通过环境变量向容器注入配置信息"><a href="#通过环境变量向容器注入配置信息" class="headerlink" title="通过环境变量向容器注入配置信息"></a>通过环境变量向容器注入配置信息</h4><p>通过环境变量为镜像提供配置信息是最常见的容器应用配置方式之一，例如使用MySQL官方提供的镜像文件启动MySQL容器时使用的MYSQL_ROOT_PASSWORD环境变量，它用于为MySQL服务器的root用户设置登录密码。<br>在基于此类镜像启动容器时，通过docker run命令的-e选项向环境变量传值即能实现应用配置，命令的使用格式为docker run -e SETTING1=foo -e SETTING2=bar … &lt;image name&gt;。非云原生的应用程序容器化时通常会借助entrypoint启动脚本以在启动时获取到这些环境变量，并在启动容器应用之前，通过sed或echo等一类命令将变量值替换到配置文件中。<br>一般说来，容器的entrypoint启动脚本应该为这些环境变量提供默认值，以便在用户未为环境变量传值时也能基于此类必需环境变量的镜像启动容器。使用环境变量这种配置方式的优势在于配置信息的动态化供给，不过有些应用程序的配置也可能会复杂到难以通过键值格式的环境变量完成。<br>也可以让容器的entrypoint启动脚本通过网络中的键值存储系统获取配置参数，常用的该类存储系统有Consul或etcd等，它们能够支持多级嵌套的数据结构，因而能够提供较之环境变量更为复杂的配置信息。不过，这种方式为容器化应用引入了额外的依赖条件。<br>Kubernetes系统支持在为Pod资源配置容器时使用spec.containers.env为容器的环境变量传值从而完成应用的配置，我们在第4章中已经对该话题进行了说明并给出了使用示例。</p>
<h4 id="通过存储卷向容器注入配置信息"><a href="#通过存储卷向容器注入配置信息" class="headerlink" title="通过存储卷向容器注入配置信息"></a>通过存储卷向容器注入配置信息</h4><p>Docker存储卷能够将宿主机之上的任何文件或目录映射进容器文件系统上，因此，可以事先将配置文件放置于宿主机之上的某特定路径中，而后在启动容器时进行加载。这种方式灵活易用，但也依赖于用户事先将配置数据提供在宿主机上的特定路径。而且在多主机模型中，若容器存在被调度至任一主机运行的可能性时，用户还需要将配置共享在任一宿主机以确保容器能正确获取到它们。<br>Kubernetes系统把配置信息保存于标准的API资源ConfigMap和Secret中，Pod资源可通过抽象化的同名存储卷插件将相关的资源对象关联为存储卷，而后引用该存储卷上的数据赋值给环境变量，或者由容器直接挂载作为配置文件使用。ConfigMap和Secret资源是Kubernetes系统上的“一等公民”，也是配置Pod中容器应用最常用的方式。</p>
<h3 id="容器环境变量"><a href="#容器环境变量" class="headerlink" title="容器环境变量"></a>容器环境变量</h3><p>在运行时配置Docker容器中应用程序的第二种方式是在容器启动时向其传递环境变量。Docker原生的应用程序应该使用很小的配置文件，并且每一项参数都可由环境变量或命令行选项覆盖，从而能够在运行时完成任意的按需配置。然而，目前只有极少一部分应用程序是为容器环境原生设计，毕竟为容器原生重构应用程序工程浩大，且旷日持久。好在有利用容器启动脚本为应用程序预设运行环境的方法可用，通行的做法是在制作Docker镜像时，为ENTRYPOINT指令定义一个脚本，它能够在启动容器时将环境变量替换至应用程序的配置文件中，而后由此脚本启动相应的应用程序。基于这类镜像运行容器时，即可通过向环境变量传值的方式来配置应用程序。<br>在Kubernetes中使用此类镜像启动容器时，也可以在Pod资源或pod模板资源的中定义，通过为容器配置段使用env参数来定义使用的环境变量列表。事实上，即便容器中的应用本身不处理环境变量，也一样可以向容器传递环境变量，只不过它不被使用罢了。<br>通过环境变量配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构建的列表。每个环境变量通常由name和value（或valueFrom）字段构成。</p>
<ul>
<li>name &lt;string&gt;：环境变量的名称，必选字段。</li>
<li>value &lt;string&gt;：环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。</li>
<li>valueFrom &lt;Object&gt;：环境变量值的引用源，例如当前Pod资源的名称、名称空间、标签等，不能与非空值的value字段同时使用，即环境变量的值要么源于value字段，要么源于valueFrom字段，二者不可同时提供数据。<br>valueFrom字段可引用的值有多种来源，包括当前Pod资源的属性值，容器相关的系统资源配置、ConfigMap对象中的Key以及Secret对象中的Key，它们分别要使用不同的嵌套字段进行定义。</li>
<li>fieldRef &lt;Object&gt;：当前Pod资源的指定字段，目前支持使用的字段包括metadata.name、metadata.namespace、metadata.labels、metadata.annotations、spec.nodeName、spec.serviceAccountName、status.hostIP和status.podIP等。</li>
<li>configMapKeyRef &lt;Object&gt;：ConfigMap对象中的特定Key。</li>
<li>secretKeyRef &lt;Object&gt;：Secret对象中的特定Key。</li>
<li>resourceFieldRef &lt;Object&gt;：当前容器的特定系统资源的最小值（配额）或最大值（限额），目前支持的引用包括limits.cpu、limits.memory、limits.ephemeral-storage、requests.cpu、requests.memory和requests.ephemeral-storage。</li>
</ul>
<p>下面是定义在资源清单文件env-demo.yaml中的Pod资源，它配置容器通过环境变量引用当前Pod资源及其所在的节点的相关属性值。fieldRef字段的值是一个对象，它一般由apiVersion（创建当前Pod资源的API版本）或fieldPath嵌套字段所定义。事实上，这正是5.7节讲述的downwardAPI的一种应用示例。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">env-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">purpose:</span> <span class="string">demonstrate-environment-variables</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">env-demo-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;httpd&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&quot;-f&quot;</span>]</span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HELLO_WORLD</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">just</span> <span class="string">a</span> <span class="string">demo</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_NODE_NAME</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">spec.nodeName</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_NODE_IP</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">status.hostIP</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MY_POD_NAMESPACE</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">OnFailure</span></span><br></pre></td></tr></table></figure>

<p>创建上面资源清单中定义的Pod对象env-demo，而后打印它的环境变量列表，命令及其结果如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~]$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> env-demo <span class="built_in">printenv</span></span></span><br><span class="line">PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">HOSTNAME=env-demo</span><br><span class="line">MY_NODE_NAME=k8s-node02.ilinux.io</span><br><span class="line">MY_NODE_IP=172.16.0.67</span><br><span class="line">MY_POD_NAMESPACE=default</span><br><span class="line">HELLO_WORLD=just a demo</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>这两种配置方式有着一个共同的缺陷：无法在容器应用运行过程中更新环境变量从而达到更新应用的目的。这通常意味着用户不得不为production、development和stage等不同的环境分别配置Pod资源。好在，用户还有ConfigMap资源可用。</p>
<h2 id="应用程序配置管理与ConfigMap资源"><a href="#应用程序配置管理与ConfigMap资源" class="headerlink" title="应用程序配置管理与ConfigMap资源"></a>应用程序配置管理与ConfigMap资源</h2><p>ConfigMap资源用于在运行时将配置文件、命令行参数、环境变量、端口号以及其他配置工件绑定至Pod的容器和系统组件。ConfigMap使配置更易于更改和管理，并防止将配置数据硬编码到Pod配置清单中。但ConfigMap资源用于存储和共享非敏感、未加密的配置信息，若要在集群中使用敏感信息，则必须使用Secret资源。<br>简单来说，一个ConfigMap对象就是一系列配置数据的集合，这些数据可注入到Pod的容器当中为容器应用所使用，注入的途径有直接挂载存储卷和传递为环境变量两种。ConfigMap支持存储诸如单个属性一类的细粒度的信息，也可用于存储粗粒度的信息，例如将整个配置文件保存在ConfigMap对象之中。</p>
<h3 id="创建ConfigMap对象"><a href="#创建ConfigMap对象" class="headerlink" title="创建ConfigMap对象"></a>创建ConfigMap对象</h3><p>ConfigMap是Kubernetes标准的API资源类型，它隶属名称空间级别，支持命令式命令、命令式对象配置及声明式对象配置3种管理接口。命令式命令的创建操作可通过kubectl create configmap进行，它支持基于目录、文件或字面量（literal）值获取配置数据完成ConfigMap对象的创建。该命令的语法格式如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap &lt;map-name&gt; &lt;data-source&gt;</span><br></pre></td></tr></table></figure>

<p>命令中的&lt;data-source&gt;就是可以通过直接给定的键值、文件或目录（内部的一到多个文件）来获取的配置数据来源，但无论是哪一种数据供给方式，配置数据都要转换为键值类型，其中的键由用户在命令行给出或是文件类型数据源的文件名，且仅能由字母、数字、连接号和点号组成，而值则是字面量值或文件数据源的内容。</p>
<h4 id="字面量值数据源"><a href="#字面量值数据源" class="headerlink" title="字面量值数据源"></a>字面量值数据源</h4><p>为kubectl create configmap命令使用–from-literal选项可在命令行直接给出键值对来创建ConfigMap对象，重复使用此选项则可以一次传递多个键值对。命令格式如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap configmap_name --from-literal=key-1=value-1 …</span><br></pre></td></tr></table></figure>

<p>例如，下面的命令创建demoapp-config时传递了两个键值对，一个是demoapp.host= 0.0.0.0，一个是demoapp.port=8080。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl create configmap demoapp-config --from-literal=demoapp.host=<span class="string">&#x27;0.0.0.0&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    --from-literal=demoapp.port=<span class="string">&#x27;8080&#x27;</span> --namespace=<span class="string">&#x27;default&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>ConfigMap对象仅是Kubernetes API存储中的数据，并没有与之相关联的其他组件存在，因而无须status字段来区分期望的状态（desired state）和当前状态（current state）。我们从下面的get configmap命令中输出的demoapp-config对象YAML格式信息可以看出，ConfigMap资源没有spec和status字段，而是直接使用data字段嵌套键值数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get configmaps demoapp-config -o yamlapiVersion: v1data:</span></span><br><span class="line">  demoapp.host: 0.0.0.0</span><br><span class="line">  demoapp.port: &quot;8080&quot;kind: ConfigMapmetadata:</span><br><span class="line">  creationTimestamp: &quot;2020-08-13T06:18:30Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  ……</span><br><span class="line">  name: demoapp-config</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;2660869&quot;</span><br><span class="line">  selfLink: /api/v1/namespaces/default/configmaps/demoapp-config</span><br><span class="line">  uid: e86036cc-e677-4529-87ce-64f58e72ecc7</span><br></pre></td></tr></table></figure>

<p>显然，若要基于配置清单创建ConfigMap资源时，仅需要指定apiVersion、kind、metadata和data这4个字段，以类似上面的格式定义出相应的资源即可。</p>
<h4 id="文件数据源"><a href="#文件数据源" class="headerlink" title="文件数据源"></a>文件数据源</h4><p>ConfigMap资源也可用于为应用程序提供大段配置，这些大段配置通常保存于一到多个文本编码的文件中，可由kubectl create configmap命令通过–from-file选项一次加载一个配置文件的内容为指定键的值，多个文件的加载可重复使用–from-file选项完成。命令格式如下，省略键名时，将默认使用指定的目标文件的基名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap &lt;configmap_name&gt; \</span><br><span class="line">   --from-file[=&lt;key-name&gt;]=&lt;path-to-file&gt;</span><br></pre></td></tr></table></figure>

<p>例如，下面的命令可以把事先准备好的Nginx配置文件模板保存于ConfigMap对象nginx-confs中，其中一个直接使用myserver.conf文件名作为键名，而另一个myserver-status.cfg对应的键名则自定义为status.cfg。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create configmap nginx-confs --from-file=./nginx-conf.d/myserver.conf \</span></span><br><span class="line"><span class="language-bash">    --from-file=status.cfg=./nginx-conf.d/myserver-status.cfg --namespace=<span class="string">&#x27;default&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>我们可以从nginx-confs对象的配置清单来了解各键名及其相应的键值。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl get configmap nginx-confs -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  status.cfg: |                # “|”是键名及多行键值的分割符，多行键值要进行固定缩进</span><br><span class="line">    location /nginx-status &#123;   # 该缩进范围内的文本块即为多行键值</span><br><span class="line">        stub_status on;</span><br><span class="line">        access_log off;</span><br><span class="line">    &#125;</span><br><span class="line">  myserver.conf: |</span><br><span class="line">    server &#123;</span><br><span class="line">        listen 8080;</span><br><span class="line">        server_name www.ik8s.io;</span><br><span class="line"></span><br><span class="line">        include /etc/nginx/conf.d/myserver-*.cfg;</span><br><span class="line"></span><br><span class="line">        location / &#123;</span><br><span class="line">            root /usr/share/nginx/html;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">kind: ConfigMap</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>通过这种方式创建的ConfigMap资源可以直接以键值形式收纳应用程序的完整配置信息，各个文件的内容以键值的形式保存于专用的键名称之下。当需要配置清单保留ConfigMap资源的定义，而键数据又较为复杂时，也需要以类似上面命令输出结果中的格式，将配置文件内容直接定义在配置清单当中。</p>
<h4 id="目录数据源"><a href="#目录数据源" class="headerlink" title="目录数据源"></a>目录数据源</h4><p>对于配置文件较多且又无须自定义键名称的场景，可以直接在kubectl create configmap命令的–from-file选项上附加一个目录路径就能将该目录下的所有文件创建于同一ConfigMap资源中，各文件名为即为键名。命令格式如下。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl create configmap &lt;configmap_name&gt; --from-file=&lt;path-to-directory&gt;</span><br></pre></td></tr></table></figure>

<p>下面的命令把nginx-conf.d目录下的所有文件都保存于nginx-config-files对象中，从命令格式也不难揣测出，我们无法再为各文件内容自定义其键名称。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create configmap nginx-config-files --from-file=./nginx-conf.d/</span></span><br></pre></td></tr></table></figure>

<p>此目录中包含myserver.conf、status.cfg和gzip.cfg这3个配置文件，它们会被分别存储为3个键值数据，如下面的命令及其结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe configmap nginx-config-files</span></span><br><span class="line">Name:         nginx-config-files</span><br><span class="line">Namespace:    default</span><br><span class="line">Labels:       &lt;none&gt;</span><br><span class="line">Annotations:  &lt;none&gt;</span><br><span class="line"></span><br><span class="line">Data</span><br><span class="line">====</span><br><span class="line">myserver-gzip.cfg:      # 键值数据1，describe命令的输出中键和值使用“----”分割符</span><br><span class="line">----</span><br><span class="line">gzip on;</span><br><span class="line">……</span><br><span class="line"></span><br><span class="line">myserver.conf:          # 键值数据2</span><br><span class="line">----</span><br><span class="line">server &#123;</span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">myserver-status.cfg:    # 键值数据3</span><br><span class="line">----</span><br><span class="line">location /nginx-status &#123;</span><br><span class="line">    stub_status on;</span><br><span class="line">    access_log off;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Events:  &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>注意，describe命令和get -o yaml命令都可显示由文件创建而成的键与值，但二者使用的键和值之间的分隔符不同。另外需要说明的是，基于字面量值和基于文件创建的方式也可以混合使用。例如下面的命令创建demoapp-confs对象时，使用–from-file选项加载demoapp-conf.d目录下的所有文件（共有envoy.yaml和eds.conf两个），又同时使用了两次–from-literal选项分别以字面量值的方式定义了两个键值数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create configmap demoapp-confs --from-file=./demoapp-conf.d/ \</span></span><br><span class="line"><span class="language-bash">      --from-literal=demoapp.host=<span class="string">&#x27;0.0.0.0&#x27;</span> --from-literal=demoapp.port=<span class="string">&#x27;8080&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>该对象共有4个数据条目，它们分别是来自于demoapp-conf.d目录下的envoy.yaml和eds.conf，以及命令行直接给出的demoapp.host和demoapp.port，这可以从下面命令的结果中得以验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get configmaps/demoapp-confs</span></span><br><span class="line">NAME           DATA    AGE</span><br><span class="line">demoapp-confs   4      12s</span><br></pre></td></tr></table></figure>

<h4 id="ConfigMap资源配置清单"><a href="#ConfigMap资源配置清单" class="headerlink" title="ConfigMap资源配置清单"></a>ConfigMap资源配置清单</h4><p>基于配置文件创建ConfigMap资源时，它所使用的字段包括通常的apiVersion、kind和metadata字段，以及用于存储数据的关键字段data。例如下面的示例所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmap-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">  <span class="attr">port:</span> <span class="string">&quot;10080&quot;</span></span><br><span class="line">  <span class="attr">app.config:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    threads = 4</span></span><br><span class="line"><span class="string">    connections = 1024</span></span><br></pre></td></tr></table></figure>

<p>若键值来自文件内容，使用配置文件创建ConfigMap资源的便捷性远不如直接通过命令行进行创建，因此我们可先使用命令行加载文件或目录的方式进行创建，在创建完成后使用get -o yaml命令获取到相关信息后进行编辑留存。</p>
<h3 id="通过环境变量引用ConfigMap键值"><a href="#通过环境变量引用ConfigMap键值" class="headerlink" title="通过环境变量引用ConfigMap键值"></a>通过环境变量引用ConfigMap键值</h3><p>Pod资源配置清单中，除了使用value字段直接给定变量值之外，容器环境变量的赋值还支持通过在valueFrom字段中嵌套configMapKeyRef来引用ConfigMap对象的键值，它的具体使用格式如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">env:</span></span><br><span class="line"><span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 要赋值的环境变量名</span></span><br><span class="line">  <span class="attr">valueFrom:</span>             <span class="comment"># 定义变量值引用</span></span><br><span class="line">    <span class="attr">configMapKeyRef:</span>     <span class="comment"># 变量值来自ConfigMap对象的某个指定键的值</span></span><br><span class="line">      <span class="string">key</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 键名称</span></span><br><span class="line">      <span class="string">name</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># ConfigMap对象的名称</span></span><br><span class="line">      <span class="string">optional</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 指定的ConfigMap对象或者指定的键名称是否为可选</span></span><br></pre></td></tr></table></figure>

<p>这种方式赋值的环境变量的使用方式与直接赋值的环境变量并无区别，它们都可用于容器的启动脚本或直接传递给容器应用等。<br>下面是保存于配置文件configmaps-env-demo.yaml的资源定义示例，它包含了两个资源，彼此间使用“—”相分隔。第一个资源是名为demoapp-config的ConfigMap对象，它包含了两个键值数据；第二个资源是名为configmaps-env-demo的Pod对象，它在环境变量PORT和HOST中分别引用了demoapp-config对象中的demoapp.port和demoapp.host的键的值。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-config</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">demoapp.port:</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line">  <span class="attr">demoapp.host:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmaps-env-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">configMapKeyRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demoapp-config</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">demoapp.port</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">false</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">configMapKeyRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demoapp-config</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">demoapp.host</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>demoapp支持通过环境变量HOST和PORT为其指定监听的地址与端口。将上面配置文件中的资源创建完成后，我们便可以来验证Pod资源监听的端口等配置信息是否为demoapp-config对象中定义的内容，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f configmaps-env-demo.yaml</span></span><br><span class="line">configmap/demoapp-config created</span><br><span class="line">pod/configmaps-env-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> configmaps-env-demo -- netstat -tnl</span> </span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address      Foreign Address         State       </span><br><span class="line">tcp        0      0 0.0.0.0:8080        0.0.0.0:*             LISTEN</span><br></pre></td></tr></table></figure>

<p>需要注意的是，被引用的ConfigMap资源必须事先存在，否则将无法在Pod对象中启动引用了ConfigMap对象的容器，但未引用或不存在ConfigMap资源的容器将不受影响。另外，ConfigMap是名称空间级别的资源，它必须与引用它的Pod资源在同一空间内。提示<br>在容器清单中的command或args字段中引用环境变量要使用$(VAR_NAME)的格式。<br>若ConfigMap资源中存在较多的键值数据，而且其大部分甚至是全部键值数据都需要由容器进行引用时，为容器逐一配置相应的环境变量将是一件颇为劳心费神之事，而且极易出错。对此，Pod资源支持在容器中使用envFrom字段直接将ConfigMap资源中的所有键值一次性地导入。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">envFrom:</span> </span><br><span class="line"><span class="bullet">-</span> <span class="string">prefix</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 为引用的ConfigMap对象中的所有变量添加一个前缀名</span></span><br><span class="line">  <span class="attr">configMapRef:</span>          <span class="comment"># 定义引用的ConfigMap对象</span></span><br><span class="line">    <span class="string">name</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># ConfigMap对象的名称</span></span><br><span class="line">    <span class="string">optional</span> <span class="string">&lt;boolean&gt;</span>   <span class="comment"># 该ConfigMap对象是否为可选</span></span><br></pre></td></tr></table></figure>

<p>envFrom字段值是对象列表，用于同时从多个ConfigMap对象导入键值数据。为了避免从多个ConfigMap引用键值数据时产生键名冲突，可以为每个引用中将被导入的键使用prefix字段指定一个特定的前缀，例如HTCFG_一类的字符串，于是ConfigMap对象中的PORT键名将成为容器中名为HTCFG_PORT的变量。注意<br>如果键名中使用了连字符“-”，转换为变量名的过程会自动将其替换为下划线“_”。<br>例如，把上面示例中的配置清单转为如下形式的定义（configmap-envfrom-demo.yaml配置文件）后，引用ConfigMap进行配置的效果并无不同。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-config-for-envfrom</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">PORT:</span> <span class="string">&quot;8090&quot;</span></span><br><span class="line">  <span class="attr">HOST:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmaps-envfrom-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">    <span class="attr">envFrom:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">configMapRef:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demoapp-config-for-envfrom</span></span><br><span class="line">        <span class="attr">optional:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>由envFrom从ConfigMap对象一次性引入环境变量时无法自定义每个环境变量的名称，因此，ConfigMap对象中的键名称必须要与容器中的应用程序引用的变量名保持一致。待Pod资源创建完成后，可通过查看其环境变量验证其导入的结果</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f configmaps-envfrom-demo.yaml</span>      </span><br><span class="line">configmap/demoapp-config-for-envfrom created</span><br><span class="line">pod/configmaps-envfrom-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> configmaps-envfrom-demo -- <span class="built_in">printenv</span> | grep -E <span class="string">&#x27;^(PORT|HOST)\b&#x27;</span></span></span><br><span class="line">HOST=0.0.0.0</span><br><span class="line">PORT=8090</span><br></pre></td></tr></table></figure>

<p>值得提醒的是，从ConfigMap对象导入环境变量时若省略了可选的prefix字段，各变量名将直接引用ConfigMap资源中的键名。若不存在键名冲突的可能性，例如从单个ConfigMap对象导入变量或在ConfigMap对象中定义键名时已添加了特定前缀时，省略前缀的定义既不会导致键名冲突，又能保持变量的简洁。</p>
<h3 id="ConfigMap存储卷"><a href="#ConfigMap存储卷" class="headerlink" title="ConfigMap存储卷"></a>ConfigMap存储卷</h3><p>使用环境变量导入ConfigMap对象中来源于较长的内容文件的键值会导致占据过多的内存空间，而考虑此类数据通常用于为容器应用提供配置文件，将其内容直接以文件格式进行引用是为了更好地选择。Pod资源的configMap存储卷插件专用于以存储卷形式引用ConfigMap对象，其键值数据是容器中的ConfigMap存储卷挂载点路径或直接指向的配置文件。</p>
<h4 id="挂载整个存储卷"><a href="#挂载整个存储卷" class="headerlink" title="挂载整个存储卷"></a>挂载整个存储卷</h4><p>基于ConfigMap存储卷插件关联至Pod资源上的ConfigMap对象可由内部的容器挂载为一个目录，该ConfigMap对象的每个键名将转为容器挂载点路径下的一个文件名，键值则映射为相应文件的内容。显然，挂载点路径应该以容器加载配置文件的目录为其名称，每个键名也应该有意设计为对应容器应用加载的配置文件名称。<br>在Pod资源上以存储卷方式引用ConfigMap对象的方法非常简单，仅需要指明存储卷名称及要引用的ConfigMap对象名称即可。下面是在配置文件configmaps-volume-demo.yaml中定义的Pod资源，它引用了前面创建的ConfigMap对象nginx-config-files，并由nginx-server容器挂载至Nginx加载配置文件模块的目录/etc/nginx/conf.d之下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmaps-volume-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-server</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ngxconfs</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d/</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ngxconfs</span></span><br><span class="line">    <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-config-files</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>此Pod资源引用的nginx-config-files中包含3个配置文件，其中myserver.conf定义了一个虚拟主机<a target="_blank" rel="noopener" href="http://www.ik8s.io,并通过include指令包含/etc/nginx/conf.d/%E7%9B%AE%E5%BD%95%E4%B8%8B%E4%BB%A5myserver-%E4%B8%BA%E5%89%8D%E7%BC%80%E3%80%81%E4%BB%A5.cfg%E4%B8%BA%E5%90%8E%E7%BC%80%E7%9A%84%E6%89%80%E6%9C%89%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%8C%E4%BE%8B%E5%A6%82%E5%9C%A8nginx-config-files%E4%B8%AD%E5%8C%85%E5%90%AB%E7%9A%84myserver-status.cfg%E5%92%8Cmyserver-gzip.cfg%EF%BC%8C%E5%A6%82%E5%9B%BE6-1%E6%89%80%E7%A4%BA%E3%80%82">www.ik8s.io，并通过include指令包含/etc/nginx/conf.d/目录下以myserver-为前缀、以.cfg为后缀的所有配置文件，例如在nginx-config-files中包含的myserver-status.cfg和myserver-gzip.cfg，如图6-1所示。</a></p>
<p><img src="/blog/2022/02/10/ConfigMap/image-20220210094937757.png" alt="image-20220210094937757"></p>
<p>创建此Pod资源后，在Kubernetes集群中的某节点直接向Pod IP的8080端口发起访问请求，即可验证由nginx-config-files资源提供的配置信息是否生效，例如通过/nginx-status访问其内置的stub status。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">POD_IP=$(kubectl get pods configmaps-volume-demo -o go-template=&#123;&#123;.status.podIP&#125;&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl http://<span class="variable">$&#123;POD_IP&#125;</span>:8080/nginx-status</span></span><br><span class="line">Active connections: 1</span><br><span class="line">server accepts handled requests</span><br><span class="line"> 1 1 1</span><br><span class="line">Reading: 0 Writing: 1 Waiting: 0</span><br></pre></td></tr></table></figure>

<p>当然，我们也可以直接于Pod资源configmaps-volume-demo之上的相应容器中执行命令来确认文件是否存在于挂载点目录中：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> configmaps-volume-demo -- <span class="built_in">ls</span> /etc/nginx/conf.d/</span></span><br><span class="line">myserver-gzip.cfg</span><br><span class="line">myserver-status.cfg</span><br><span class="line">myserver.conf</span><br></pre></td></tr></table></figure>

<p>我们还可以在容器中运行其他命令来进一步测试由ConfigMap对象提供的配置信息是否已生效，以示例中的Nginx为例，我们可运行如下的配置测试与打印命令进行配置信息的生效确认。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span>  configmaps-volume-demo -- nginx -T</span></span><br><span class="line">……</span><br><span class="line"><span class="meta"># </span><span class="language-bash">configuration file /etc/nginx/conf.d/myserver.conf:</span></span><br><span class="line">server &#123;</span><br><span class="line">    listen 8080;</span><br><span class="line">    server_name www.ik8s.io;</span><br><span class="line"></span><br><span class="line">    ……</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">configuration file /etc/nginx/conf.d/myserver-gzip.cfg:</span></span><br><span class="line">……</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">configuration file /etc/nginx/conf.d/myserver-status.cfg:</span></span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>由上面两个命令的结果可见，nginx-config-files中的3个文件都被添加到了容器中，并且实现了由容器应用Nginx加载并生效。</p>
<h4 id="挂载存储卷中的部分键值"><a href="#挂载存储卷中的部分键值" class="headerlink" title="挂载存储卷中的部分键值"></a>挂载存储卷中的部分键值</h4><p>有些应用场景中，用户很可能期望仅向容器中的挂载点暴露Pod资源关联的ConfigMap对象上的部署键值，这在通过一个ConfigMap对象为单个Pod资源中的多个容器分别提供配置时尤其常见。例如前面曾创建了一个名为demoapp-confs的ConfigMap对象，它包含有4个键值，其中的envoy.yaml和eds.conf可为envoy代理提供配置文件，而demoapp.port能够为demoapp（通过环境变量）定义监听的端口。下面配置清单示例定义的Pod资源中定义了两个容器，envoy和demoapp，demoapp-confs为envoy容器提供两个配置文件，为demoapp容器提供了一个配置参数。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmaps-volume-demo2</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">envoyproxy/envoy-alpine:v1.14.1</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">appconfs</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/envoy</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">configMapKeyRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">demoapp-confs</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">demoapp.port</span></span><br><span class="line">          <span class="attr">optional:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">appconfs</span></span><br><span class="line">    <span class="attr">configMap:</span>    <span class="comment"># 存储卷插件类型</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">demoapp-confs</span></span><br><span class="line">      <span class="attr">items:</span>      <span class="comment"># 要暴露的键值数据</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">envoy.yaml</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">envoy.yaml</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="number">0644</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">lds.conf</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">lds.conf</span></span><br><span class="line">        <span class="attr">mode:</span> <span class="number">0644</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>configMap卷插件中的items字段的值是一个对象列表，可嵌套使用3个字段来组合指定要引用的特定键。</p>
<ul>
<li>key &lt;string&gt;：要引用的键名称，必选字段。</li>
<li>path &lt;string&gt;：对应的键在挂载点目录中映射的文件名称，它可不同于键名称，必选字段。</li>
<li>mode &lt;integer&gt;：文件的权限模型，可用范围为0～0777。</li>
</ul>
<p>上面的配置示例（configmap-volume-demo2.yaml）中，把envoy.yaml和eds.conf两个键名分别映射为/etc/envoy目录下的两个与键同名的文件，且均使用0644的权限。</p>
<h4 id="独立挂载存储卷中的单个键值"><a href="#独立挂载存储卷中的单个键值" class="headerlink" title="独立挂载存储卷中的单个键值"></a>独立挂载存储卷中的单个键值</h4><p>前面的两种方式中，无论是装载ConfigMap对象中的所有还是部分文件，挂载点目录下原有的文件都会被隐藏。对于期望将ConfigMap对象提供的配置文件补充在挂载点目录下的需求来说，这种方式显然难以如愿。以Nginx应用为例，基于nginx:alpine启动的容器的/etc/nginx/conf.d目录中原本就存在一些文件（例如default.conf等），有时候我们需要把nginx-config-files这个ConfigMap对象中的全部或部分文件装载进此目录中而不影响其原有的文件。<br>事实上，此种需求可以通过在容器上的volumeMounts字段中使用subPath字段来解决，该字段用于支持从存储卷挂载单个文件或单个目录而非整个存储卷。例如，下面的示例就单独挂载了两个文件在/etc/nginx/conf.d目录中，但保留了目录下原有的文件。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">configmaps-volume-demo3</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">nginx-server</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ngxconfs</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d/myserver.conf</span></span><br><span class="line">      <span class="attr">subPath:</span> <span class="string">myserver.conf</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ngxconfs</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d/myserver-gzip.cfg</span></span><br><span class="line">      <span class="attr">subPath:</span> <span class="string">myserver-gzip.cfg</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ngxconfs</span></span><br><span class="line">    <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-config-files</span></span><br></pre></td></tr></table></figure>

<p>基于上述配置创建了Pod资源后，即可通过命令验证/etc/nginx/conf.d目录中原有文件确实能够得以保留，如下面的命令及其结果所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">exec</span> <span class="string">configmaps-volume-demo3</span> <span class="string">--</span> <span class="string">ls</span> <span class="string">/etc/nginx/conf.d/</span></span><br><span class="line"><span class="string">default.conf</span></span><br><span class="line"><span class="string">myserver-gzip.cfg</span></span><br><span class="line"><span class="string">myserver.conf</span></span><br></pre></td></tr></table></figure>

<p>接下来也可将该Pod资源创建于集群上，验证myserver主机的配置，正常情况下，它应该启动了页面压缩功能，但因未装载myserver-status.cfg配置而不支持内置的status页面，感兴趣的读者可自行完成测试。</p>
<h3 id="容器应用重载新配置"><a href="#容器应用重载新配置" class="headerlink" title="容器应用重载新配置"></a>容器应用重载新配置</h3><p>相较于环境变量来说，使用ConfigMap资源为容器应用提供配置的优势之一在于支持容器应用动态更新其配置：用户直接更新ConfigMap对象，而后由相关Pod对象的容器应用重载其配置文件即可。<br>细心的读者或许已经发现，挂载有ConfigMap存储卷的容器上，挂载点目录中的文件都是符号链接，它们指向了挂载点目录中名为..data隐藏属性的子目录，而..data自身也是一个符号链接，它指向了名字形如..2020_05_15_03_34_10.435155001这样的以挂载操作时的时间戳命名的临时隐藏目录，该目录才是存储卷的真正挂载点。例如，查看Pod对象configmaps-volume-demo的容器中的挂载点目录下的文件列表，它将显示出类似如下结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl exec -it configmaps-volume-demo -- ls -lA /etc/nginx/conf.d</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x   2 root   root   79 Apr 14 03:34 ..2020_05_15_03_34_10.435155001</span><br><span class="line">lrwxrwxrwx   1 root   root   31 Apr 14 03:34 ..data -&gt; ..2020_05_15_03_34_10.435155001</span><br><span class="line">lrwxrwxrwx   1 root   root   24 Apr 14 03:34 myserver-gzip.cfg -&gt; ..data/myserver-gzip.cfg</span><br><span class="line">lrwxrwxrwx   1 root   root   26 Apr 14 03:34 myserver-status.cfg -&gt; ..data/myserver-status.cfg</span><br><span class="line">lrwxrwxrwx   1 root   root   20 Apr 14 03:34 myserver.conf -&gt; ..data/myserver.conf</span><br></pre></td></tr></table></figure>

<p>这种两级符号链接设定的好处在于，当引用的ConfigMap对象中的数据发生改变时，它将被重新挂载至一个以当前时间戳命名的新的临时目录下，而后将..data指向这个新的挂载点便达到了同时更新存储卷上所有文件数据的目的。例如，使用kubectl edit cm命令直接在ConfigMap对象nginx-config-files中的myserver-status.cfg配置段增加“allow 127.0.0.0/8;”和“deny all;”两行，稍等片刻之后再次查看configmap-volume-demo中容器挂载点目录中的文件列表，结果是其挂载点已经指向新的位置，例如下面的命令及其结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl edit configmaps/nginx-config-files -n default</span></span><br><span class="line">……</span><br><span class="line">data:</span><br><span class="line">  ……</span><br><span class="line">  myserver-status.cfg: |</span><br><span class="line">    location /nginx-status &#123;</span><br><span class="line">        stub_status on;</span><br><span class="line">        access_log off;</span><br><span class="line">        allow 127.0.0.0/8;</span><br><span class="line">        deny all;</span><br><span class="line">    &#125;</span><br><span class="line">……</span><br><span class="line">~ $ kubectl exec -it configmaps-volume-demo -- ls -lA /etc/nginx/conf.d</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x   2 root   root   79 Apr 14 03:45 ..2020_05_15_03_45_25.239510550</span><br><span class="line">lrwxrwxrwx   1 root   root   31 Apr 14 03:45 ..data -&gt; ..2020_05_15_03_45_25.239510550</span><br><span class="line">lrwxrwxrwx   1 root   root   24 Apr 14 03:34 myserver-gzip.cfg -&gt; ..data/myserver-gzip.cfg</span><br><span class="line">lrwxrwxrwx   1 root   root   26 Apr 14 03:34 myserver-status.cfg -&gt; ..data/myserver-status.cfg</span><br><span class="line">lrwxrwxrwx   1 root   root   20 Apr 14 03:34 myserver.conf -&gt; ..data/myserver.conf</span><br></pre></td></tr></table></figure>

<p>ConfigMap对象中的数据更新同步至应用容器后并不能直接触发生效新配置，还需要在容器上执行应用重载操作。例如Nginx可通过其nginx -s reload命令完成配置文件重载，如下面的命令所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> configmaps-volume-demo -- nginx -s reload</span></span><br><span class="line">2020/05/15 03:52:50 [notice] 32#32: signal process started</span><br></pre></td></tr></table></figure>

<p>新增的两行配置信息对/nginx-status这个URL施加了访问控制机制，它仅允许来自本地回环接口上的访问请求，因而此容器之外访问/nginx-status页面的请求将会被拒绝。<br>对于不支持配置文件重载操作的容器应用来说，仅那些在ConfigMap对象更新后创建的Pod资源中的容器会应用到新配置，因此手动重启旧有的容器之前会存在配置不一致的问题。即使对于支持重载操作的应用来说，由于新的配置信息并非同步推送进所有容器中，而且在各容器中进行的手动重载操作也未必能同时进行，因此在更新时，短时间内仍然会存在配置不一致的现象。还有，以单个文件形式独立挂载ConfigMap存储卷中的容器并未采用两级链接的方式进行文件映射，因此存储卷无法确保所有挂载的文件可以被同时更新至容器中。为了确保配置信息的一致性，目前这种类型的挂载不支持文件更新操作。<br>有些云原生应用支持配置更新时的自动重载功能，例如Envoy支持基于XDS协议订阅文件系统上的配置文件，并在该类配置文件更新时间戳发生变动时自动重载配置。然而，采用联合挂载多层叠加且进行写时复制的容器隔离文件系统来说，这种时间戳的更新未必能够触发内核中的通知机制，也就难以触发应用程序的自动重载功能。总结起来，在Pod资源中调用ConfigMap对象时要注意以下几个问题。</p>
<ul>
<li>以存储卷方式引用的ConfigMap对象必须先于Pod对象存在，除非在Pod对象中把它们统统标记为optional，否则将会导致Pod无法正常启动；同样，即使ConfigMap对象存在，但引用的键名不存在时，也会导致同样的错误。</li>
<li>以环境变量方式引用的ConfigMap对象的键不存在时会被忽略，Pod对象可以正常启动，但错误引用的信息会以InvalidVariableNames事件记录于日志中。</li>
<li>ConfigMap对象是名称空间级的资源，能够引用它的Pod对象必须位于同一名称空间。</li>
<li>Kubelet仅支持那些由API Server管理的Pod资源来引用ConfigMap对象，因而那些由kubelet在节点上通过–manifest-url或–config选项加载配置清单创建的静态Pod，以及由用户直接通过kubelet的RESTful API创建的Pod对象。<br>ConfigMap无法替代配置文件，它仅在Kubernetes系统上代表对应用程序配置文件的引用，我们可以将它类比为在Linux主机上表示/etc目录及内部文件的一种方法。</li>
</ul>
<h2 id="Secret资源：向容器注入配置信息"><a href="#Secret资源：向容器注入配置信息" class="headerlink" title="Secret资源：向容器注入配置信息"></a>Secret资源：向容器注入配置信息</h2><p>出于增强可移植性的需求，我们应该从容器镜像中解耦的不仅有配置数据，还有默认口令（例如Redis或MySQL服务的访问口令）、用于SSL通信时的数字证书和私钥、用于认证的令牌和ssh key等，但这些敏感数据不宜存储于ConfigMap资源中，而是要使用另一种称为Secret的资源类型。将敏感数据存储在Secret中比明文存储在ConfigMap或Pod配置清单中更加安全。借助Secret，我们可以控制敏感数据的使用方式，并降低将数据暴露给未经授权用户的风险。<br>Secret对象存储数据的机制及使用方式都类似于ConfigMap对象，它们以键值方式存储数据，在Pod资源中通过环境变量或存储卷进行数据访问。不同的地方在于，Secret对象仅会被分发至调用了该对象的Pod资源所在的工作节点，且仅支持由节点将其临时存储于内存中。另外，Secret对象的数据存储及打印格式为Base64编码的字符串而非明文字符，用户在创建Secret对象时需要事先手动完成数据的格式转换。但在容器中以环境变量或存储卷的方式访问时，它们会被自动解码为明文数据。注意<br>Base64编码并非加密机制，其编码的数据可使用base64 –decode一类的命令进行解码。<br>Secret对象以非加密格式存储于etcd中，管理员必须精心管控对etcd服务的访问以确保敏感数据的机密性，包括借助于TLS协议确保etcd集群节点间以及API Server间的加密通信和双向身份认证等。此外还要精心组织Kubernetes API Server服务的访问认证和授权，因为拥有创建Pod资源的用户都可以使用Secret资源并能够通过Pod对象中的容器访问其数据。<br>目前，Secret资源主要有两种用途：一是作为存储卷注入Pod对象上，供容器应用程序使用；二是用于kubelet为Pod里的容器拉取镜像时向私有仓库提供认证信息。不过，后面使用ServiceAccount资源自建的Secret对象是一种更安全的方式。</p>
<h3 id="创建Secret资源"><a href="#创建Secret资源" class="headerlink" title="创建Secret资源"></a>创建Secret资源</h3><p>类似于Config Map资源，创建Secret对象时也支持使用诸如字面量值、文件或目录等数据源，而根据其存储格式及用途的不同，Secret对象还会划分为如下3种类别。</p>
<ul>
<li>generic：基于本地文件、目录或字面量值创建的Secret，一般用来存储密码、密钥、信息、证书等数据。</li>
<li>docker-registry：用于认证到Docker Registry的Secret，以使用私有容器镜像。</li>
<li>tls：基于指定的公钥/私钥对创建TLS Secret，专用于TLS通信中；指定公钥和私钥必须事先存在，公钥证书必须采用PEM编码，且应该与指定的私钥相匹配。</li>
</ul>
<p>这些类别也体现在kubectl create secret generic|docker-registry|tls命令之中，每个类别代表一个子命令，并分别有着各自专用的命令行选项。</p>
<h4 id="通用Secret"><a href="#通用Secret" class="headerlink" title="通用Secret"></a>通用Secret</h4><p>通用类型的Secret资源用于保存除用于TLS通信之外的证书和私钥，以及专用于认证到Docker注册表服务之外的敏感信息，包括访问服务的用户名和口令、SSH密钥、OAuth令牌、CephX协议的认证密钥等。<br>使用Secret为容器中运行的服务提供用于认证的用户名和口令是一种较为常见的应用场景，以MySQL或PostgreSQL代表的开源关系型数据库系统的镜像就支持通过环境变量来设置管理员用户的默认密码。此类Secret对象可以直接使用kubectl create secret generic <SECRET_NAME> –from-literal=key=value命令，以给定的字面量值直接进行创建，通常用户名要使用username为键名，而密码则要使用password为键名。例如下面的命令，以root/iLinux分别为用户名和密码创建了一个名为mysql-root-authn的Secret对象：</SECRET_NAME></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic mysql-root-authn --from-literal=username=root \</span></span><br><span class="line"><span class="language-bash">    --from-literal=password=iLinux</span></span><br></pre></td></tr></table></figure>

<p>由下面获取Secret对象资源规范的命令及其输出结果可以看出：未指定类型时，以generic子命令创建的Secret对象是Opaque类型，其键值数据会以Base64编码格式保存和打印。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secrets/mysql-root-authn -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  password: aUxpbnV4</span><br><span class="line">  username: cm9vdA==</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-root-authn</span><br><span class="line">  namespace: default</span><br><span class="line">  ……</span><br><span class="line">type: Opaque</span><br></pre></td></tr></table></figure>

<p>但Kubernetes系统Secret对象的Base64编码数据并非加密格式，许多相关的工具程序可轻松完成解码，例如将上面命令结果中的password字段的值可交由下面所示的Base64命令进行解码。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">echo</span> aUxpbnV4 | base64 -d</span></span><br><span class="line">iLinux</span><br></pre></td></tr></table></figure>

<p>将用户名和密码用于Basic认证时，需要在创建命令中额外使用–type选项明确定义Secret对象的类型，该选项值固定为”kubernetes.io/basic-auth”，并要求用户名和密码各自的键名必须为username和password，如下面Secret对象的创建和显示命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic web-basic-authn --from-literal=username=ops \</span></span><br><span class="line"><span class="language-bash">    --from-literal=password=iK8S --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/basic-auth&quot;</span></span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secrets/mysql-ops-authn</span></span><br><span class="line">NAME                         TYPE            DATA   AGE</span><br><span class="line">web-basic-authn   kubernetes.io/basic-auth   2      1m</span><br></pre></td></tr></table></figure>

<p>有些应用场景仅需要在Secret中保存密钥信息即可，用户名能够以明文的形式由客户端直接提供而无须保存于Secret对象中。例如，在Pod或PV资源上使用的RBD存储卷插件以CephX协议认证到Ceph存储集群时，使用内嵌的user字段指定用户名，以secretRef字段引用保存有密钥的Secret对象，且创建该类型的Secret对象需要明确指定类型为kubernetes.io/rbd，如下面的命令所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl create secret generic ceph-kube-secret --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/rbd&quot;</span> \</span></span><br><span class="line"><span class="language-bash">  --from-literal=key=<span class="string">&#x27;AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA==&#x27;</span></span></span><br></pre></td></tr></table></figure>

<p>对于文件中的敏感数据，可以在命令上使用–from-file选项以直接将该文件作为数据源，例如创建用于SSH认证的Secret对象时就可以直接从认证的私钥文件加载认证信息，其键名需要使用ssh-privatekey，而类型标识为kubernetes.io/ssh-auth。下面的命令先创建出一对用于测试的认证密钥，而后将其私钥创建为Secret对象。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">ssh-keygen -t rsa -P <span class="string">&quot;&quot;</span> -f  <span class="variable">$&#123;HOME&#125;</span>/.ssh/id_rsa</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic ssh-key-secret</span> </span><br><span class="line">    --from-file=ssh-privatekey=$&#123;HOME&#125;/.ssh/id_rsa \</span><br><span class="line">    --type=&quot;kubernetes.io/ssh-auth&quot;</span><br></pre></td></tr></table></figure>

<p>Kubernetes系统上还有一种专用于保存ServiceAccount认证令牌的Secret对象，它存储有Kubernetes集群的私有CA的证书（ca.crt）以及当前Service账号的名称空间和认证令牌。该类资源以kubernetes.io/service-account-token为类型标识，并附加专用资源注解kubernetes.io/service-account.name和kubernetes.io/service-account.uid来指定所属的ServiceAccount账号名称及ID信息。kube-system名称空间中默认存在许多该类型的Secret对象，下面的第一个命令先获取到以node-controller开头的Secret资源（ServiceAccount/node-controller资源的专用Secret）的名称，而后第二个命令以YAML格式打印该资源的详细规范。下面命令用于打印kube-system名称空间下的Secret/node-controller资源对象的信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">secret_name=$(kubectl get secrets -n kube-system | awk <span class="string">&#x27;/^node-controller/&#123;print $1&#125;&#x27;</span>)</span></span><br><span class="line">~ $ kubectl get secrets $secret_name -o yaml -n kube-system</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  ca.crt: ……</span><br><span class="line">  namespace: a3ViZS1zeXN0ZW0=</span><br><span class="line">  token: ……</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  annotations:</span><br><span class="line">    kubernetes.io/service-account.name: node-controller</span><br><span class="line">    kubernetes.io/service-account.uid: 54dedc06-09db-4024-b756-e4e64ed1a1cf</span><br><span class="line">  name: node-controller-token-6wlvm</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  ……type: kubernetes.io/service-account-token</span><br></pre></td></tr></table></figure>

<p>还有一种专用于Kubernetes集群自动引导（bootstrap）过程的Secret类型，最早由kubeam引入，类型标识为bootstrap.kubernetes.io/token，它需要由auth-extra-groups、description、token-id和token-secret等专用键名来指定所需的数据。由kubeadm部署的集群上，会在kube-system名称空间中默认生成一个以bootstrap-token为前缀的该类Secret对象。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">bs_name=$(kubectl get secrets -n kube-system | awk <span class="string">&#x27;/^bootstrap-token/&#123;print $1&#125;&#x27;</span>)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secret <span class="variable">$bs_name</span> -o yaml -n kube-system</span>                                              </span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  auth-extra-groups: ……</span><br><span class="line">  description: ……</span><br><span class="line">  token-id: ZG5hY3Y3</span><br><span class="line">  token-secret: YjE1MjAzcm55ODV2ZW5kdw==</span><br><span class="line">  usage-bootstrap-authentication: dHJ1ZQ==</span><br><span class="line">  usage-bootstrap-signing: dHJ1ZQ==</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: bootstrap-token-dnacv7</span><br><span class="line">  namespace: kube-system</span><br><span class="line">  ……</span><br><span class="line">type: bootstrap.kubernetes.io/token</span><br></pre></td></tr></table></figure>

<p>以配置清单创建以上各种类型的通用Secret对象，除Opaque外，都需要使用type字段明确指定类型，并在data字段中嵌套使用符合要求的字段指定所需要数据。</p>
<h4 id="TLS-Secret"><a href="#TLS-Secret" class="headerlink" title="TLS Secret"></a>TLS Secret</h4><p>为TLS通信场景提供专用数字证书和私钥信息的Secret对象有其专用的TLS子命令，以及专用的选项–cert和–key。例如，为运行于Pod中的Nginx应用创建SSL虚拟主机之时，需要事先通过Secret对象向相应容器注入服务证书和配对的私钥信息，以供nginx进程加载使用。出于测试的目的，我们先使用类似如下命令生成私钥和自签证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl rand -writerand <span class="variable">$HOME</span>/.rnd</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">(<span class="built_in">umask</span> 077; openssl genrsa -out nginx.key 2048)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl req -new -x509 -key nginx.key -out nginx.crt \</span></span><br><span class="line"><span class="language-bash">    -subj /C=CN/ST=Beijing/L=Beijing/O=DevOps/CN=www.ilinux.io</span></span><br></pre></td></tr></table></figure>

<p>而后即可使用如下命令将这两个文件创建为secret对象。<font color="red">无论用户提供的证书和私钥文件使用什么名称，它们一律会分别转换为以tls.key（私钥）和tls.crt（证书）为其键名。</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret tls nginx-ssl-secret --key=./nginx.key --cert=./nginx.crt</span></span><br><span class="line">secret &quot;nginx-ssl-secret&quot; created</span><br></pre></td></tr></table></figure>

<p>该类型的Secret对象的类型标识符为kubernetes.io/tls，例如下面命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secret nginx-ssl -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  tls.crt: ……</span><br><span class="line">  tls.key: ……</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx-ssl-secret</span><br><span class="line">  namespace: default</span><br><span class="line">  ……</span><br><span class="line">type: kubernetes.io/tls</span><br></pre></td></tr></table></figure>

<h4 id="Docker-Registry-Secret"><a href="#Docker-Registry-Secret" class="headerlink" title="Docker Registry Secret"></a>Docker Registry Secret</h4><p>当Pod配置清单中定义容器时指定要使用的镜像来自私有仓库时，需要先认证到目标Registry以下载指定的镜像，pod.spec.imagePullSecrets字段指定认证Registry时使用的、保存有相关认证信息的Secret对象，以辅助kubelet从需要认证的私有镜像仓库获取镜像。该字段的值是一个列表对象，它支持指定多个不同的Secret对象以认证到不同的Resgistry，这在多容器Pod中尤为有用。<br>创建这种专用于认证到镜像Registry的Secret对象有其专用的docker-registry子命令。通常，认证到Registry的过程需要向kubelet提供Registry服务器地址、用户名和密码，以及用户的E-mail信息，因此docker-registry子命令需要同时使用以下4个选项。</p>
<ul>
<li>–docker-server：Docker Registry服务器的地址，默认为<a target="_blank" rel="noopener" href="https://index.docker.io/v1/%E3%80%82">https://index.docker.io/v1/。</a></li>
<li>–docker-user：请求Registry服务时使用的用户名。</li>
<li>–docker-password：请求访问Registry服务的用户密码。</li>
<li>–docker-email：请求访问Registry服务的用户E-mail。</li>
</ul>
<p>这4个选项指定的内容分别对应使用docker login命令进行交互式认证时所使用的认证信息，下面的命令创建了名为local-registry的docker-registry Secret对象。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret docker-registry local-registry --docker-username=Ops \</span></span><br><span class="line"><span class="language-bash">    --docker-password=Opspass --docker-email=ops@ilinux.io</span></span><br></pre></td></tr></table></figure>

<p>该类secret对象打印的类型信息为kubernetes.io/dockerconfigjson，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get secrets local-registry</span></span><br><span class="line">NAME             TYPE               DATA    AGE</span><br><span class="line">local-registry   kubernetes.io/dockerconfigjson   1         7s</span><br></pre></td></tr></table></figure>

<p>另外，创建docker-registry Secret对象时依赖的认证信息也可使用–from-file选项从dockercfg配置文件（例如<del>/.dockercfg）或JSON格式的Docker配置文件（例如</del>/.docker/config.json）中加载，但前者的类型标识为kubernetes.io/dockercfg，后者的类型则与前面使用字面量值的创建方式相同。<br>在Pod资源上使用docker-registry Secret对象的方法有两种。一种方法是使用spec.imagePullSecrets字段直接引用；另一种是将docker-registry Secret对象添加到某特定的ServiceAccount对象之上，而后配置Pod资源通过spec. serviceAccountName来引用该服务账号。第二种方法的实现我们放在ServiceAccount资源的相关话题中进行介绍，这里先以下面的示例说明第一种方法的用法。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secret-imagepull-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">imagePullSecrets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">local-registry</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">registry.ilinux.io/dev/myimage</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demoapp</span></span><br></pre></td></tr></table></figure>

<p>上面的配置清单仅是一个示例，付诸运行时，需要由读者将引用的Secret对象中的内容及清单资源的镜像等修改为实际可用的信息。<br>当运行的多数容器镜像均来自私有仓库时，为每个Pod资源在imagePullSecrets显式定义一或多个引用的Secret对象实在不是一个好主意，我们应该将docker-registry Secret对象的引用定义在一个特定的ServiceAccount之上，而后由各相关的Pod资源进行引用才是更好的选择。</p>
<h4 id="Secret资源清单"><a href="#Secret资源清单" class="headerlink" title="Secret资源清单"></a>Secret资源清单</h4><p>Secret资源是标准的Kubernetes API资源类型之一，但它仅是存储于API Server上的数据定义，无须区别期望状态与现实状态，无须使用spec和status字段。除了apiVersion、kind和metadata字段，它可用的其他字段如下。</p>
<ul>
<li>data &lt;map[string]string&gt;：key:value格式的数据，通常是敏感信息，数据格式需是以Base64格式编码的字符串，因此需要用户事先完成编码。另外，不同类型的Secret资源要求使用的嵌套字段（键名）也不尽相同，甚至ServiceAccount专用类型的Secret对象还要求使用专用的注解信息。</li>
<li>stringData &lt;map[string]string&gt;：以明文格式（非Base64编码）定义的键值数据。无须用户事先对数据进行Base64编码，而是在创建为Secret对象时自动进行编码并保存于data字段中。stringData字段中的明文不会被API Server输出，但使用kubectl apply命令进行创建的Secret对象，其注解信息可能会直接输出这些信息。</li>
<li>type &lt;string&gt;：仅为了便于编程处理Secret数据而提供的类型标识。<br>下面是保存于配置文件secrets-demo.yaml中的Secret资源定义示例，它使用stringData提供了明文格式的键–值数据，从而免去了事先手动编码的麻烦。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secrets-demo</span></span><br><span class="line"><span class="attr">stringData:</span></span><br><span class="line">  <span class="attr">username:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">password:</span> <span class="string">redisp@ss</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br></pre></td></tr></table></figure>

<p>为保存于配置清单文件中的敏感信息创建Secret对象时，用户需要先将敏感信息读出并转换为Base64编码格式，再将其创建为清单文件，过程烦琐，反而不如命令式创建来得便捷。不过，如果存在多次创建或者重构之需，将其保存为配置清单也是情势所需。</p>
<h3 id="使用Secret资源"><a href="#使用Secret资源" class="headerlink" title="使用Secret资源"></a>使用Secret资源</h3><p>类似于Pod资源使用ConfigMap对象的方式，Secret对象可以注入为容器环境变量，也能够通过Secret卷插件定义为存储卷并由容器挂载使用。但是，容器应用通常会在发生错误时将所有环境变量保存于日志信息中，甚至有些应用在启动时会将运行环境打印到日志中。另外，容器应用调用第三方程序为子进程时，这些子进程能够继承并使用父进程的所有环境变量。这都有可能导致敏感信息泄露，因而通常仅在必要的情况下才使用环境变量引用Secret对象中的数据。</p>
<h4 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h4><p>Pod资源以环境变量方式消费Secret对象也存在两种途径：① 一对一地将指定键的值传递给指定的环境变量；② 将Secret对象上的全部键名和键值一次性全部映射为容器的环境变量。前者在容器上使用env.valueFrom字段进行定义，而后者则直接使用envFrom字段，如下面给出的详细配置格式所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">  <span class="attr">image:</span> <span class="string">…</span></span><br><span class="line">  <span class="attr">env:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 变量名，其值来自某Secret对象上的指定键的值</span></span><br><span class="line">    <span class="attr">valueFrom:</span>               <span class="comment"># 键值引用</span></span><br><span class="line">      <span class="attr">secretKeyRef:</span>       </span><br><span class="line">        <span class="attr">name:</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 引用的Secret对象的名称，需要与该Pod位于同一名称空间</span></span><br><span class="line">        <span class="attr">key:</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 引用的Secret对象上的键，其值将传递给环境变量</span></span><br><span class="line">        <span class="attr">optional:</span> <span class="string">&lt;boolean&gt;</span>  <span class="comment"># 是否为可选引用</span></span><br><span class="line">  <span class="attr">envFrom:</span>                   <span class="comment"># 整体引用指定的Secret对象的全部键名和键值</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">prefix:</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 将所有键名引用为环境变量时统一添加的前缀</span></span><br><span class="line">    <span class="attr">secretRef:</span>        </span><br><span class="line">      <span class="attr">name:</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 引用的Secret对象名称</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="string">&lt;boolean&gt;</span>    <span class="comment"># 是否为可选引用</span></span><br></pre></td></tr></table></figure>

<p>下面Pod资源配置清单（secrets-env-demo.yaml）示例中，容器mariadb运行时初始化root用户的密码，引用自此前创建的Secret对象mysql-root-authn中的password键的值。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secrets-env-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mariadb</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mariadb</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">MYSQL_ROOT_PASSWORD</span></span><br><span class="line">      <span class="attr">valueFrom:</span></span><br><span class="line">        <span class="attr">secretKeyRef:</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">mysql-root-authn</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">password</span></span><br></pre></td></tr></table></figure>

<p>mariadb的镜像并不支持从某个文件中加载管理员root用户的初始密码，这里也就只能使用环境变量赋值的方式来引用Secret对象中的敏感数据。下面完成测试步骤，首先将清单中的Pod对象创建在集群上：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f secrets-env-demo.yaml</span> </span><br><span class="line">pod/secrets-env-demo created</span><br></pre></td></tr></table></figure>

<p>而后使用保存在mysql-root-authn对象中的password字段的值iLinux作为密码进行数据库访问，如下面命令所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it secrets-env-demo -- mysql -uroot -piLinux</span></span><br><span class="line">Welcome to the MariaDB monitor.  Commands end with ; or \g.</span><br><span class="line">Your MariaDB connection id is 8</span><br><span class="line">Server version: 10.4.12-MariaDB-1:10.4.12+maria~bionic mariadb.org binary distribution</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2018, Oracle, MariaDB Corporation Ab and others.</span><br><span class="line"></span><br><span class="line">Type &#x27;help;&#x27; or &#x27;\h&#x27; for help. Type &#x27;\c&#x27; to clear the current input statement.</span><br><span class="line"></span><br><span class="line">MariaDB [(none)]&gt;</span><br></pre></td></tr></table></figure>

<p>命令结果表明使用MySQL客户端工具以root用户和iLinux密码认证到容器mariadb的操作成功完成，经由环境变量向容器传递Secret对象中保存的敏感信息得以顺利实现。</p>
<h4 id="Secret存储卷"><a href="#Secret存储卷" class="headerlink" title="Secret存储卷"></a>Secret存储卷</h4><p>Pod资源上的Secret存储卷插件的使用方式同ConfigMap存储卷插件非常相似，除了其类型及引用标识要替换为secret及secretName之外，几乎完全类似于ConfigMap存储卷，包括支持使用挂载整个存储卷、只挂载存储卷中指定键值以及独立挂载存储卷中的键等使用方式。<br>下面是定义在配置清单文件secrets-volume-demo.yaml中的Secret资源使用示例，它将nginx-ssl-secret对象关联为Pod对象上名为nginxcert的存储卷，而后由容器ngxservrer挂载至/etc/nginx/certs目录下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">secrets-volume-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">ngxserver</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxcerts</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/nginx/certs/</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxconfs</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/nginx/conf.d/</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxcerts</span></span><br><span class="line">    <span class="attr">secret:</span></span><br><span class="line">      <span class="attr">secretName:</span> <span class="string">nginx-ssl-secret</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginxconfs</span></span><br><span class="line">    <span class="attr">configMap:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">nginx-sslvhosts-confs</span></span><br><span class="line">      <span class="attr">optional:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>ConfigMap对象nginx-sslvhosts-confs中存储有证书文件tls.cert和私钥文件tls.key，这些文件是可调用容器通过挂载nginx-ssl-secret在/etc/nginx/certs/目录下生成的，并根据证书与私钥文件定义了一个SSL类型的虚拟主机。并且，所有发往80端口的流量都会被重定向至SSL虚拟主机。其中的关键配置部分如下所示。</p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">443</span> ssl;</span><br><span class="line">    <span class="attribute">server_name</span> www.ik8s.io;</span><br><span class="line"></span><br><span class="line">    <span class="attribute">ssl_certificate</span> /etc/nginx/certs/tls.crt; </span><br><span class="line">    <span class="attribute">ssl_certificate_key</span> /etc/nginx/certs/tls.key;</span><br><span class="line">    <span class="attribute">ssl_session_timeout</span> <span class="number">5m</span>;</span><br><span class="line">    <span class="attribute">ssl_protocols</span> TLSv1 TLSv1.<span class="number">1</span> TLSv1.<span class="number">2</span>; </span><br><span class="line">    <span class="attribute">ssl_ciphers</span> ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE; </span><br><span class="line">    <span class="attribute">ssl_prefer_server_ciphers</span> <span class="literal">on</span>;</span><br><span class="line"></span><br><span class="line">    <span class="section">location</span> / &#123;</span><br><span class="line">        <span class="attribute">root</span> /usr/share/nginx/html;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="section">server</span> &#123;</span><br><span class="line">    <span class="attribute">listen</span> <span class="number">80</span>;</span><br><span class="line">    <span class="attribute">server_name</span> www.ilinux.io; </span><br><span class="line">    <span class="attribute">return</span> <span class="number">301</span> https://<span class="variable">$host</span><span class="variable">$request_uri</span>; </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们知道，由Pod资源引用的所有ConfigMap和Secret对象必须事先存在，除非它们被显式标记为optional: true。因此，在创建该Pod对象之前，我们需要事先生成其引用的ConfigMap对象nginx-sslvhosts-confs，相关的所有配置文件保存在nginx-ssl-conf.d/目录下，因而直接运行如下命令即可完成创建。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create configmap nginx-sslvhosts-confs --from-file=./nginx-ssl-conf.d/</span></span><br></pre></td></tr></table></figure>

<p>而后，将上面资源清单文件中定义的Pod资源创建于集群之上，待其正常启动后可查看容器挂载点目录中的文件，以确认其挂载是否成功完成，或直接向Pod中的Nginx服务发起访问请求进行验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f secrets-volume-demo.yaml</span> </span><br><span class="line">pod/secrets-volume-demo created</span><br></pre></td></tr></table></figure>

<p>而后，使用openssl s_cleint命令向该Pod对象的IP地址发起TLS访问请求，确认其证书是否为前面自签生成的测试证书。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods secrets-volume-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">openssl s_client -connect <span class="variable">$podIP</span>:443 -state</span></span><br></pre></td></tr></table></figure>

<p>不过，这里的测试请求使用了IP地址而非证书中的主体名称<a target="_blank" rel="noopener" href="http://www.ilinux.io,因而证书的验证会失败,但我们只需关注证书内容即可,尤其是证书链中显示的信息.若能成功证明响应中的证书来自nginx-ssl-secret对象中保存的自签证书,也就意味着通过存储卷方式向容器提供敏感信息的操作成功了./">www.ilinux.io，因而证书的验证会失败，但我们只需关注证书内容即可，尤其是证书链中显示的信息。若能成功证明响应中的证书来自nginx-ssl-secret对象中保存的自签证书，也就意味着通过存储卷方式向容器提供敏感信息的操作成功了。</a></p>
<h2 id="应用Downward-API存储卷配置信息"><a href="#应用Downward-API存储卷配置信息" class="headerlink" title="应用Downward API存储卷配置信息"></a>应用Downward API存储卷配置信息</h2><p>除了通过ConfigMap和Secret对象向容器注入配置信息之外，应用程序有时候还需要基于所运行的外在系统环境信息设定自身的运行特性。例如nginx进程可根据节点的CPU核心数量自动设定要启动的worker进程数，JVM虚拟机可根据节点内存资源自动设定其堆内存大小等。这种功能有点类似于编程中的反射机制，它旨在让对象加载与自身相关的重要环境信息并据此做出运行决策。<br>Kubernetes的Downward API支持通过环境变量与文件（downwardAPI卷插件）将Pod及节点环境相关的部分元数据和状态数据注入容器中，它们的使用方式同ConfigMaps和Secrets类似，用于完成将外部信息传递给Pod中容器的应用程序。然而，Downward API并不会将所有可用的元数据统统注入容器中，而是由用户在配置Pod对象自行选择需要注入容器中的元数据。可选择注入的信息包括Pod对象的IP、主机名、标签、注解、UID、请求的CPU与内存资源量及其限额，甚至是Pod所在的节点名称和节点IP等。Downward API的数据注入方式如图6-2所示。</p>
<p><img src="/blog/2022/02/10/ConfigMap/image-20220210102524660.png" alt="image-20220210102524660"></p>
<p>但是与ConfigMap和Secret这两个标准的API资源类型不同的是，Downward API自身便是一种附属于API Server之上API，在Pod资源的定义中可直接进行引用而无须事先进行任何资源定义。</p>
<h3 id="环境变量式元数据注入"><a href="#环境变量式元数据注入" class="headerlink" title="环境变量式元数据注入"></a>环境变量式元数据注入</h3><p>类似于ConfigMap或Secret资源，容器能够在环境变量valueFrom字段中嵌套fieldRef或resourceFieldRef字段来引用其所属Pod对象的元数据信息。不过，通常只有常量类型的属性才能够通过环境变量注入容器中，毕竟进程启动完成后无法再向其告知变量值的变动，于是环境变量也就不支持中途的更新操作。在容器规范中，可在环境变量中配置valueFrom字段内嵌fieldRef字段引用的信息包括如下这些。</p>
<ul>
<li>metadata.name：Pod对象的名称。</li>
<li>metadata.namespace：Pod对象隶属的名称空间。</li>
<li>metadata.uid：Pod对象的UID。</li>
<li>metadata.labels[‘<KEY>‘]：Pod对象标签中的指定键的值，例如metadata.labels[‘mylabel’]，仅Kubernetes 1.9及之后的版本才支持。</KEY></li>
<li>metadata.annotations[‘<KEY>‘]：Pod对象注解信息中的指定键的值，仅Kubernetes 1.9及之后的版本才支持。<br>容器上的计算资源需求和资源限制相关的信息，以及临时存储资源需求和资源限制相关的信息可通过容器规范中的resourceFieldRef字段引用，相关字段包括requests.cpu、limits.cpu、requests.memory和limits.memory等。另外，可通过环境变量引用的信息有如下几个。</KEY></li>
<li>status.podIP：Pod对象的IP地址。</li>
<li>spec.serviceAccountName：Pod对象使用的ServiceAccount资源名称。</li>
<li>spec.nodeName：节点名称。</li>
<li>status.hostIP：节点IP地址。</li>
</ul>
<p>另外，还可以通过resourceFieldRef字段引用当前容器的资源请求及资源限额的定义，因此它们包括requests.cpu、requests.memory、requests.ephemeral-storage、limits.cpu、limits.memory和limits.ephemeral-storage这6项。<br>下面的资源配置清单示例（downwardAPI-env.yaml）中定义的Pod对象通过环境变量向容器demoapp中注入了Pod对象的名称、隶属的名称空间、标签app的值以及容器自身的CPU资源限额和内存资源请求等信息。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">downwardapi-env-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">      <span class="attr">command:</span> [ <span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;env&quot;</span> ]</span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;32Mi&quot;</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">        <span class="attr">limits:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br><span class="line">      <span class="attr">env:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">THIS_POD_NAME</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.name</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">THIS_POD_NAMESPACE</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">THIS_APP_LABEL</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">fieldRef:</span></span><br><span class="line">              <span class="attr">fieldPath:</span> <span class="string">metadata.labels[&#x27;app&#x27;]</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">THIS_CPU_LIMIT</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">resourceFieldRef:</span></span><br><span class="line">              <span class="attr">resource:</span> <span class="string">limits.cpu</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">THIS_MEM_REQUEST</span></span><br><span class="line">          <span class="attr">valueFrom:</span></span><br><span class="line">            <span class="attr">resourceFieldRef:</span></span><br><span class="line">              <span class="attr">resource:</span> <span class="string">requests.memory</span></span><br><span class="line">              <span class="attr">divisor:</span> <span class="string">1Mi</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br></pre></td></tr></table></figure>

<p>该Pod对象创建并启动后向控制台打印所有的环境变量即终止运行，它仅用于测试通过环境变量注入信息到容器的使用效果。我们先根据下面的命令创建出配置清单中定义的Pod资源Pod/downwardapi-env-demo。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f downwardapi-env-demo.yaml</span></span><br><span class="line">pod/downwardapi-env-demo created</span><br></pre></td></tr></table></figure>

<p>等该Pod对象的状态转为Completed之后即可通过控制台日志获取注入的环境变量，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl logs downwardapi-env-demo | grep <span class="string">&quot;^THIS_&quot;</span></span></span><br><span class="line">THIS_CPU_LIMIT=1</span><br><span class="line">THIS_APP_LABEL=demoapp</span><br><span class="line">THIS_MEM_REQUEST=32</span><br><span class="line">THIS_POD_NAME=downwardapi-env-demo</span><br><span class="line">THIS_POD_NAMESPACE=default</span><br></pre></td></tr></table></figure>

<p>示例最后一个环境变量的定义中还额外指定了一个divisor字段，它用于为引用的值指定一个除数，以对引用的数据进行单位换算。CPU资源的divisor字段默认值为1，它表示为1个核心，相除的结果不足1个单位时则向上圆整（例如0.25向上圆整的结果为1），它的另一个可用单位为1m，即表示1个微核心。内存资源的divisor字段默认值也是1，不过它意指1个字节，此时32MiB的内存资源则要换算为33554432予以输出。其他可用的单位还有1KiB、1MiB、1GiB等，于是在将divisor字段的值设置为1MiB时，32MiB的内存资源换算的结果即为32。注意<br>未给容器定义资源请求及资源限额时，通过downwardAPI引用的值则默认为节点的可分配CPU及内存资源量。</p>
<h3 id="存储卷式元数据注入"><a href="#存储卷式元数据注入" class="headerlink" title="存储卷式元数据注入"></a>存储卷式元数据注入</h3><p>downwardAPI存储卷能够以文件方式向容器中注入元数据，将配置的字段数据映射为文件并可通过容器中的挂载点访问。事实上，6.4.1节中通过环境变量方式注入的元数据信息也都可以使用存储卷方式进行信息暴露，但除此之外，我们还能够在downwardAPI存储卷中使用fieldRef引用下面两个数据源。</p>
<ul>
<li>metadata.labels：Pod对象的所有标签信息，每行一个，格式为label-key=”escaped-label-value”。</li>
<li>metadata.annotations：Pod对象的所有注解信息，每行一个，格式为annotation-key=”escaped-annotation-value”。</li>
</ul>
<p>下面的资源配置清单示例（downwardapi-volumes-demo.yaml）中定义的Pod资源通过downwardAPI存储卷向容器demoapp中注入了Pod对象隶属的名称空间、标签、注解以及容器自身的CPU资源限额和内存资源请求等信息。存储卷在容器中的挂载点为/etc/podinfo目录，因而注入的每一项信息均会映射为此路径下的一个文件。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">downwardapi-volume-demo</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">zone:</span> <span class="string">zone1</span></span><br><span class="line">    <span class="attr">rack:</span> <span class="string">rack100</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">region:</span> <span class="string">ease-cn</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">      <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">      <span class="attr">resources:</span></span><br><span class="line">        <span class="attr">requests:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;32Mi&quot;</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;250m&quot;</span></span><br><span class="line">        <span class="attr">limits:</span></span><br><span class="line">          <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">          <span class="attr">cpu:</span> <span class="string">&quot;500m&quot;</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">podinfo</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/etc/podinfo</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">podinfo</span></span><br><span class="line">    <span class="attr">downwardAPI:</span></span><br><span class="line">      <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">      <span class="attr">items:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.namespace</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">pod_namespace</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.labels</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">pod_labels</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">fieldRef:</span></span><br><span class="line">          <span class="attr">fieldPath:</span> <span class="string">metadata.annotations</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">pod_annotations</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">resourceFieldRef:</span></span><br><span class="line">          <span class="attr">containerName:</span> <span class="string">demoapp</span></span><br><span class="line">          <span class="attr">resource:</span> <span class="string">limits.cpu</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&quot;cpu_limit&quot;</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">resourceFieldRef:</span></span><br><span class="line">          <span class="attr">containerName:</span> <span class="string">demoapp</span></span><br><span class="line">          <span class="attr">resource:</span> <span class="string">requests.memory</span></span><br><span class="line">          <span class="attr">divisor:</span> <span class="string">&quot;1Mi&quot;</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&quot;mem_request&quot;</span></span><br></pre></td></tr></table></figure>

<p>创建资源配置清单中定义的Pod对象后即可测试访问由downwardAPI存储卷映射的文件pod_namespace、pod_labels、pod_annotations、limits_cpu和mem_request等。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f downwardapi-volume-demo.yaml</span> </span><br><span class="line">pod/downwardapi-volume-demo created</span><br></pre></td></tr></table></figure>

<p>待Pod对象正常运行后即可测试访问上述的映射文件，例如访问/etc/podinfo/pod_labels文件以查看Pod对象的标签列表：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> downwardapi-volume-demo -- <span class="built_in">cat</span> /etc/podinfo/pod_labels</span>             </span><br><span class="line">app=&quot;demoapp&quot;</span><br><span class="line">rack=&quot;rack100&quot;</span><br><span class="line">zone=&quot;zone1&quot;</span><br></pre></td></tr></table></figure>

<p>如命令结果所示，Pod对象的标签信息每行一个地映射于自定义的路径/etc/podinfo/pod_labels文件中，类似地，注解信息也以这种方式进行处理。如前面的章节所述，标签和注解支持运行时修改，其改动的结果也会实时映射进downwardAPI生成的文件中。例如，为downwardapi-volume-demo对象添加新的标签：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label pods/downwardapi-volume-demo release=<span class="string">&quot;Canary&quot;</span></span></span><br><span class="line">pod/downwardapi-volume-demo labeled</span><br></pre></td></tr></table></figure>

<p>而后再次查看容器内的pod_labels文件的内容，由如下的命令结果可知新的标签已经能够通过相关的文件获取到。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> downwardapi-volume-demo -- <span class="built_in">cat</span> /etc/podinfo/pod_labels</span></span><br><span class="line">app=&quot;demoapp&quot;</span><br><span class="line">rack=&quot;rack100&quot;</span><br><span class="line">release=&quot;Canary&quot;</span><br><span class="line">zone=&quot;zone1&quot;</span><br></pre></td></tr></table></figure>

<p>downwardAPI存储卷为Kubernetes上运行容器化应用提供了获取外部环境信息的有效途径，这对那些非云原生应用在不进行代码重构的前提下获取环境信息，以进行自身配置等操作时尤为有用。事实上，5.6节中Longhorn存储系统在其Longhorn Manager相关的资源清单中就使用了downwardAPI。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/" class="post-title-link" itemprop="url">Kubernetes存储卷与数据持久化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-09 20:57:32" itemprop="dateCreated datePublished" datetime="2022-02-09T20:57:32+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-11 09:08:54" itemprop="dateModified" datetime="2022-02-11T09:08:54+08:00">2022-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="存储卷与数据持久化"><a href="#存储卷与数据持久化" class="headerlink" title="存储卷与数据持久化"></a>存储卷与数据持久化</h1><h2 id="存储卷基础"><a href="#存储卷基础" class="headerlink" title="存储卷基础"></a>存储卷基础</h2><p>Pod本身有生命周期，其应用容器及生成的数据自身均无法独立于该生命周期之外持久存在，并且同一Pod中的容器可共享PID、Network、IPC和UTS名称空间，但Mount和USER名称空间却各自独立，因而跨容器的进程彼此间默认无法基于共享的存储空间交换文件或数据。因而，借助特定的存储机制甚至是独立于Pod生命周期的存储设备完成数据持久化也是必然之需。</p>
<h3 id="存储卷概述"><a href="#存储卷概述" class="headerlink" title="存储卷概述"></a>存储卷概述</h3><p>存储卷是定义在Pod资源之上可被其内部的所有容器挂载的共享目录，该目录关联至宿主机或某外部的存储设备之上的存储空间，可由Pod内的多个容器同时挂载使用。Pod存储卷独立于容器自身的文件系统，因而也独立于容器的生命周期，它存储的数据可于容器重启或重建后继续使用。图5-1展示了Pod容器与存储卷之间的关系。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123092524003.png" alt="image-20220123092524003"></p>
<p>每个工作节点基于本地内存或目录向Pod提供存储空间，也能够使用借助驱动程序挂载的网络文件系统或附加的块设备，例如使用挂载至本地某路径上的NFS文件系统等。Kubernetes系统具体支持的存储卷类型要取决于存储卷插件的内置定义，如图5-2所示，不过Kubernetes也支持管理员基于扩展接口配置使用第三方存储。另外，Kubernetes甚至还支持一些有着特殊功用的存储卷，例如将外部信息投射至Pod之中的ConfigMap、Secret和Downward API等。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123092541384.png" alt="image-20220123092541384"></p>
<p>存储卷并非Kubernetes上一种独立的API资源类型，它隶属于Pod资源，且与所属的特定Pod对象有着相同的生命周期，因而通过API Server管理声明了存储卷资源的Pod对象时也会相应触发存储卷的管理操作。在具体的执行过程中，首选由调度器将该Pod对象绑到一个工作节点之上，若该Pod定义存储卷尚未被挂载，Controller Manager中的AD控制器（Attach/Detach Controller）会先借助相应的存储卷插件把远程的存储设备附加到该目标节点，而由内置在kubelet中的Pod管理器（Pod Manager）触发本地的存储卷操作实现，它借助存储卷管理器（Volume Manager）调用存储卷插件进行关联并驱动相应存储服务，并完成设备的挂载、格式化和卸载等操作。存储卷独立于Pod对象中容器的生命周期，从而使得容器重启或更新之后数据依然可用，但删除Pod对象时也必将删除其存储卷。<br>Kubernetes系统内置了多种类型的存储卷插件，因而能够直接支持多种类型存储系统（即存储服务方），例如CephFS、NFS、RBD、iscsi和vSphereVolume等。定义Pod资源时，用户可在其spec.volumes字段中嵌套配置选定的存储卷插件，并结合相应的存储服务来使用特定类型的存储卷，甚至使用CS或flexVolume存储卷插件来扩展支持更多的存储服务系统。<br>对Pod对象来说，卷类型主要是为关联适配的存储系统时提供相关的配置参数。例如，关联节点本地的存储目录与关联GlusterFS存储系统所需要的配置参数差异巨大，因此指定了存储卷类型也就限定了其关联到的后端存储设备。目前，Kubernetes支持的存储卷可简单归为以下类别，它们也各自有着不少的实现插件。</p>
<ul>
<li>1）临时存储卷：emptyDir。</li>
<li>2）本地存储卷：hostPath和local。</li>
<li>3）网络存储卷：<ul>
<li>云存储——awsElasticBlockStore、gcePersistentDisk、azureDisk和azureFile。</li>
<li>网络文件系统——NFS、GlusterFS、CephFS和Cinder。</li>
<li>网络块设备——iscsi、FC、RBD和vSphereVolume。</li>
<li>网络存储平台——Quobyte、PortworxVolume、StorageOS和ScaleIO。</li>
</ul>
</li>
<li>4）特殊存储卷：Secret、ConfigMap、DownwardAPI和Projected。</li>
<li>5）扩展支持第三方存储的存储接口（Out-of-Tree卷插件）：CSI和FlexVolume。</li>
</ul>
<p><font color="red">Kubernetes内置提供的存储卷插件可归类为In-Tree类型，它们同Kubernetes源代码一同发布和迭代，而由存储服务商借助于CSI或FlexVolume接口扩展的独立于Kubernetes代码的存储卷插件则统称为Out-Of-Tree类型</font>，集群管理员也可根据需要创建自定义的扩展插件，目前CSI是较为推荐的扩展接口，如图5-3所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123092624623.png" alt="image-20220123092624623"></p>
<p>尽管网络存储基本都具有持久存储能力，但它们都要求Pod资源清单的编写人员了解可用的真实网络存储的基础结构，并且能够准确配置用到的每一种存储服务。例如，要创建基于Ceph RBD的存储卷，用户必须要了解Ceph集群服务器（尤其是Monitor服务器）的地址，并且能够理解接入Ceph集群的必要配置及其意义。</p>
<h3 id="配置Pod存储卷"><a href="#配置Pod存储卷" class="headerlink" title="配置Pod存储卷"></a>配置Pod存储卷</h3><p>在Pod中定义使用存储卷的配置由两部分组成：一部分通过.spec.volumes字段定义在Pod之上的存储卷列表，它经由特定的存储卷插件并结合特定的存储系统的访问接口进行定义；另一部分是嵌套定义在容器的volumeMounts字段上的存储卷挂载列表，它只能挂载当前Pod对象中定义的存储卷。不过，定义了存储卷的Pod内的容器也可以选择不挂载任何存储卷。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 存储卷名称标识，仅可使用DNS标签格式的字符，在当前Pod中必须唯一</span></span><br><span class="line">     <span class="string">VOL_TYPE</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 存储卷插件及具体的目标存储系统的相关配置</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>             <span class="comment"># 要挂载的存储卷的名称，必须匹配存储卷列表中某项的定义</span></span><br><span class="line">      <span class="string">mountPath</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 容器文件系统上的挂载点路径</span></span><br><span class="line">      <span class="string">readOnly</span> <span class="string">&lt;boolean&gt;</span>        <span class="comment"># 是否挂载为只读模式，默认为“否”</span></span><br><span class="line">      <span class="string">subPath</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 挂载存储卷上的一个子目录至指定的挂载点</span></span><br><span class="line">      <span class="string">subPathExpr</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 挂载由指定模式匹配到的存储卷的文件或目录至挂载点</span></span><br><span class="line">      <span class="string">mountPropagation</span> <span class="string">&lt;string&gt;</span> <span class="comment"># 挂载卷的传播模式</span></span><br></pre></td></tr></table></figure>

<p>Pod配置清单中的.spec.volumes字段的值是一个对象列表，每个列表项定义一个存储卷，它由存储卷名称（.spec.volumes.name &lt;String&gt;）和存储卷对象（.spec.volumes.VOL_TYPE &lt;Object&gt;）组成，其中VOL_TYPE是使用的存储卷类型名称，它的内嵌字段随类型的不同而不同，具体参数需要参阅Pod上各存储卷插件的相关文档说明。<br>定义好的存储卷可由当前Pod资源内的各容器进行挂载。Pod中仅有一个容器时，使用存储卷的目的通常在于数据持久化，以免重启时导致数据丢失，而只有多个容器挂载同一个存储卷时，“共享”才有了具体的意义。挂载卷的传播模式（mountPropagation）就是用于配置容器将其挂载卷上的数据变动传播给同一Pod中的其他容器，甚至是传播给同一个节点上的其他Pod的一个特性，该字段的可用值包括如下几项。</p>
<ul>
<li>None：该挂载卷不支持传播机制，当前容器不向其他容器或Pod传播自己的挂载操作，也不会感知主机后续在该挂载卷或其任何子目录上执行的挂载变动；此为默认值。</li>
<li>HostToContainer：主机向容器的单向传播，即当前容器能感知主机后续对该挂载卷或其任何子目录上执行的挂载变动。</li>
<li>Bidirectional：主机和容器间的双向传播，当前容器创建的存储卷挂载操作会传播给主机及使用了同一存储卷的所有Pod的所有容器，也能感知主机上后续对该挂载卷或其任何子目录上执行的挂载变动；该行为存在破坏主机操作系统的危险，因而仅可用于特权模式下的容器中。</li>
</ul>
<h2 id="临时存储卷"><a href="#临时存储卷" class="headerlink" title="临时存储卷"></a>临时存储卷</h2><p>Kubernetes支持的存储卷类型中，emptyDir存储卷的生命周期与其所属的Pod对象相同，它无法脱离Pod对象的生命周期提供数据存储功能，因此通常仅用于数据缓存或临时存储。不过，基于emptyDir构建的gitRepo存储卷可以在Pod对象的生命周期起始时，从相应的Git仓库中克隆相应的数据文件到底层的emptyDir中，也就使得它具有了一定意义上的持久性。</p>
<h3 id="emptyDir存储卷"><a href="#emptyDir存储卷" class="headerlink" title="emptyDir存储卷"></a>emptyDir存储卷</h3><p>emptyDir存储卷可以理解为Pod对象上的一个临时目录，类似于Docker上的“Docker挂载卷”，在Pod启动时被创建，而在Pod对象被移除时一并被删除。因此，emptyDir存储卷只能用于某些特殊场景中，例如同一Pod内的多个容器间的文件共享，或作为容器数据的临时存储目录用于数据缓存系统等。<br>emptyDir存储卷嵌套定义在.spec.volumes.emptyDir字段中，可用字段主要有两个。</p>
<ul>
<li>medium：此目录所在的存储介质的类型，可用值为default或Memory，默认为default，表示使用节点的默认存储介质；Memory表示使用基于RAM的临时文件系统tmpfs，总体可用空间受限于内存，但性能非常好，通常用于为容器中的应用提供缓存存储。</li>
<li>sizeLimit：当前存储卷的空间限额，默认值为nil，表示不限制；不过，在medium字段值为Memory时，建议务必定义此限额。<br>下面是一个使用了emptyDir存储卷的简单示例，它保存在volumes-emptydir-demo.yaml配置文件中。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-emptydir-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-downloader</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/admin-box</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /data/envoy.yaml https://raw.</span></span><br><span class="line"><span class="string">    githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/</span></span><br><span class="line"><span class="string">    master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">envoy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/etc/envoy</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">config-file-store</span></span><br><span class="line">    <span class="attr">emptyDir:</span></span><br><span class="line">      <span class="attr">medium:</span> <span class="string">Memory</span></span><br><span class="line">      <span class="attr">sizeLimit:</span> <span class="string">16Mi</span></span><br></pre></td></tr></table></figure>

<p>在该示例清单中，为Pod对象定义了一个名为config-file-store的、基于emptyDir存储插件的存储卷。初始化容器将该存储卷挂载至/data目录后，下载envoy.yaml配置文件并保存于该挂载点目录下。主容器将该存储卷挂载至/etc/envoy目录，再通过自定义命令让容器应用在启动时加载的配置文件/etc/envoy/envoy.yaml上，如图5-4所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123092733861.png" alt="image-20220123092733861"></p>
<p>Pod资源的详细信息中会显示存储卷的相关状态，包括其是否创建成功（Events字段中输出）、相关的类型及参数（Volumes字段中输出），以及容器中的挂载状态等信息（Containers字段中输出）。如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods volumes-emptydir-demo</span></span><br><span class="line">……</span><br><span class="line">Init Containers:</span><br><span class="line">  config-file-downloader:</span><br><span class="line">  ……</span><br><span class="line">    Mounts:</span><br><span class="line">      /data from config-file-store (rw)</span><br><span class="line">  ……</span><br><span class="line">Containers:</span><br><span class="line">  envoy:</span><br><span class="line">    Mounts:</span><br><span class="line">      /etc/envoy from config-file-store (ro)</span><br><span class="line">  ……</span><br><span class="line">Volumes:</span><br><span class="line">  config-file-store:</span><br><span class="line">    Type:       EmptyDir (a temporary directory that shares a pod&#x27;s lifetime)</span><br><span class="line">    Medium:     Memory</span><br><span class="line">    SizeLimit:  16Mi</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>为Envoy下载的配置文件中定义了一个监听所有可用IP地上TCP 80端口的Ingress侦听器，以及一个监听所有可用IP地址上TCP的9901端口的Admin接口，这与Envoy镜像中默认配置文件中的定义均有不同。下面命令的结果显示它吻合自定义配置文件的内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> volumes-emptydir-demo -- netstat -tnl</span></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       </span><br><span class="line">tcp        0      0 0.0.0.0:80              0.0.0.0:*               LISTEN      </span><br><span class="line">tcp        0      0 0.0.0.0:9901            0.0.0.0:*               LISTEN  </span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/volumes-emptydir-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl <span class="variable">$podIP</span>:9901/listeners</span></span><br><span class="line">listener_0::0.0.0.0:80</span><br></pre></td></tr></table></figure>

<p>emptyDir卷简单易用，但仅能用于临时存储。另外存在一些类型的存储卷构建在emptyDir之上，并额外提供了它所没有功能，例如将于下一节介绍的gitRepo存储卷。</p>
<h3 id="gitRepo存储卷"><a href="#gitRepo存储卷" class="headerlink" title="gitRepo存储卷"></a>gitRepo存储卷</h3><p>gitRepo存储卷可以看作是emptyDir存储卷的一种实际应用，使用该存储卷的Pod资源可以通过挂载目录访问指定的代码仓库中的数据。使用gitRepo存储卷的Pod资源在创建时，会首先创建一个空目录（emptyDir）并克隆（clone）一份指定的Git仓库中的数据至该目录，而后再创建容器并挂载该存储卷。<br>定义gitRepo类型的存储卷时，其可嵌套使用字段有如下3个。</p>
<ul>
<li>repository &lt;string&gt;：Git仓库的URL，必选字段。</li>
<li>directory &lt;string&gt;：目标目录名称，但名称中不能包含“..”字符；“.”表示将仓库中的数据直接克隆至存储卷映射的目录中，其他字符则表示将数据克隆至存储卷上以用户指定的字符串为名称的子目录中。</li>
<li>revision &lt;string&gt;：特定revision的提交哈希码。</li>
</ul>
<p><font color="red">注意:使用gitRepo存储卷的Pod资源运行的工作节点上必须安装有Git程序，否则克隆仓库的操作将无法完成。</font></p>
<p>下面的配置清单示例（volumes-gitrepo-demo.yaml）中的Pod资源在创建时，会先创建一个空目录，将指定的Git仓库<a target="_blank" rel="noopener" href="https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%85%8B%E9%9A%86%E4%B8%80%E4%BB%BD%E7%9B%B4%E6%8E%A5%E4%BF%9D%E5%AD%98%E5%9C%A8%E6%AD%A4%E7%9B%AE%E5%BD%95%E4%B8%AD%EF%BC%8C%E8%80%8C%E5%90%8E%E5%B0%86%E6%AD%A4%E7%9B%AE%E5%BD%95%E5%88%9B%E5%BB%BA%E4%B8%BA%E5%AD%98%E5%82%A8%E5%8D%B7html%EF%BC%8C%E5%86%8D%E7%94%B1%E5%AE%B9%E5%99%A8nginx%E5%B0%86%E6%AD%A4%E5%AD%98%E5%82%A8%E5%8D%B7%E6%8C%82%E8%BD%BD%E5%88%B0/usr/share/nginx/html%E7%9B%AE%E5%BD%95%E4%B8%8A%E3%80%82">https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git中的数据克隆一份直接保存在此目录中，而后将此目录创建为存储卷html，再由容器nginx将此存储卷挂载到/usr/share/nginx/html目录上。</a></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-gitrepo-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx:alpine</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">html</span></span><br><span class="line">    <span class="attr">gitRepo:</span></span><br><span class="line">      <span class="attr">repository:</span> </span><br><span class="line">      <span class="string">https://github.com/iKubernetes/Kubernetes_Advanced_Practical_2rd.git</span></span><br><span class="line">      <span class="attr">directory:</span> <span class="string">.</span></span><br><span class="line">      <span class="attr">revision:</span> <span class="string">&quot;master&quot;</span></span><br></pre></td></tr></table></figure>

<p>访问此Pod资源中的nginx服务，即可看到它来自Git仓库中的页面资源。不过，gitRepo存储卷在其创建完成后不会再与指定的仓库执行同步操作，这意味着在Pod资源运行期间，如果仓库中的数据发生了变化，gitRepo存储卷不会同步到这些内容。当然，此时可以为Pod资源创建一个Sidecar容器来执行此类的同步操作，尤其是数据来源于私有仓库时，通过Sidecar容器完成认证等必要步骤后再进行克隆操作就更为必要。<br>gitrRepo存储卷构建于emptyDir之上，其生命周期与Pod资源一样，故使用中不应在此类存储卷中保存由容器生成的重要数据。另外，gitRepo存储插件即将废弃，建议在初始化容器或Sidecar容器中运行git命令来完成相应的功能。</p>
<h2 id="hostPath存储卷"><a href="#hostPath存储卷" class="headerlink" title="hostPath存储卷"></a>hostPath存储卷</h2><p>hostPath存储卷插件是将工作节点上某文件系统的目录或文件关联到Pod上的一种存储卷类型，其数据具有同工作节点生命周期一样的持久性。hostPath存储卷使用的是工作节点本地的存储空间，所以仅适用于特定情况下的存储卷使用需求，例如将工作节点上的文件系统关联为Pod的存储卷，从而让容器访问节点文件系统上的数据，或者排布分布式存储系统的存储设备等。hostPath存储卷在运行有管理任务的系统级Pod资源，以及Pod资源需要访问节点上的文件时尤为有用。<br>配置hostPath存储卷的嵌套字段有两个：一个用于指定工作节点上的目录路径的必选字段path；另一个用于指定节点之上存储类型的type。hostPath支持使用的节点存储类型有如下几种。</p>
<ul>
<li>DirectoryOrCreate：指定的路径不存在时，自动将其创建为0755权限的空目录，属主和属组均为kubelet。</li>
<li>Directory：事先必须存在的目录路径。</li>
<li>FileOrCreate：指定的路径不存在时，自动将其创建为0644权限的空文件，属主和属组均为kubelet。</li>
<li>File：事先必须存在的文件路径。</li>
<li>Socket：事先必须存在的Socket文件路径。</li>
<li>CharDevice：事先必须存在的字符设备文件路径。</li>
<li>BlockDevice：事先必须存在的块设备文件路径。</li>
<li>“”：空字符串，默认配置，在关联hostPath存储卷之前不进行任何检查。</li>
</ul>
<p>这类Pod对象通常受控于DaemonSet类型的Pod控制器，它运行在集群中的每个工作节点上，负责收集工作节点上系统级的相关数据，因此使用hostPath存储卷也理所应当。然而，基于同一个模板创建Pod对象仍可能会因节点上文件的不同而存在着不同的行为，而且在节点上创建的文件或目录默认仅root用户可写，若期望容器内的进程拥有写权限，则需要将该容器运行于特权模式，不过这存在潜在的安全风险。<br>下面是定义在配置清单volumes-hostpath-demo.yaml中的Pod对象，容器中的filebeat进程负责收集工作节点及容器相关的日志信息并发往Redis服务器，它使用了3个hostPath类型的存储卷，第一个指向了宿主机的日志文件目录/var/logs，后面两个则与宿主机上的Docker运行时环境有关。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">vol-hostpath-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">filebeat</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/filebeat:5.6.7-alpine</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">REDIS_HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">redis.ilinux.io:6379</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">LOG_LEVEL</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">info</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/log</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/run/docker.sock</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">      <span class="attr">mountPath:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">terminationGracePeriodSeconds:</span> <span class="number">30</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlog</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/log</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">varlibdockercontainers</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/lib/docker/containers</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">socket</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/var/run/docker.sock</span></span><br></pre></td></tr></table></figure>

<p>上面配置清单中Pod对象的正确运行要依赖于REDIS_HOST和LOG_LEVEL环境变量，它们分别用于定义日志缓冲队列服务和日志级别。如果有可用的Redis服务器，我们就可通过环境变量REDIS_HOST将其对应的主机名或IP地址传递给Pod对象，待Pod对象准备好之后即可通过Redis服务器查看到由该Pod发送的日志信息。测试时，我们仅需要给REDIS_HOST环境变量传递一个任意值（例如清单中的redis.ilinux.io）便可直接创建Pod对象，只不过该Pod中容器的日志会报出无法解析指定主机名的错误，但这并不影响存储卷的配置和使用。<br>对于由Deployment或StatefulSet等一类控制器管控的、使用了hostPath存储卷的Pod对象来说，需要注意在基于资源可用状态的调度器调度Pod对象时，并不支持参考目标节点之上hostPath类型的存储卷，在Pod对象被重新调度至其他节点时，容器进程此前创建的文件或目录则大多不会存在。一个常用的解决办法是通过在Pod对象上使用nodeSelector或者nodeAffinity赋予该Pod对象指定要绑定到的具体节点来影响调度器的决策，但即便如此，管理员仍然不得不手动管理涉及的多个节点之上的目录，低效且易错。因此，hostPath存储卷虽然能持久保存数据，但对于由调度器按需调度的应用来说并不适用。</p>
<h2 id="网络存储卷"><a href="#网络存储卷" class="headerlink" title="网络存储卷"></a>网络存储卷</h2><p>5.4.1 NFS存储卷<br>Kubernetes的NFS存储卷用于关联某事先存在的NFS服务器上导出的存储空间到Pod对象中以供容器使用，该类型的存储卷在Pod对象终止后仅是被卸载而非被删除。而且，NFS是文件系统级共享服务，它支持同时存在的多路挂载请求，可由多个Pod对象同时关联使用。定义NFS存储卷时支持嵌套使用以下几个字段。</p>
<ul>
<li>server &lt;string&gt;：NFS服务器的IP地址或主机名，必选字段。</li>
<li>path &lt;string&gt;：NFS服务器导出（共享）的文件系统路径，必选字段。</li>
<li>readOnly &lt;boolean&gt;：是否以只读方式挂载，默认为false。</li>
</ul>
<p>Redis基于内存存储运行，数据持久化存储的需求通过周期性地将数据同步到主机磁盘之上完成，因此将Redis抽象为Pod对象部署运行于Kubernetes系统之上时，需要考虑节点级或网络级的持久化存储卷的支持，本示例就是以NFS存储卷为例，为Redis进程提供跨Pod对象生命周期的数据持久化功能。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-nfs-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">999</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">      <span class="attr">nfs:</span>   <span class="comment"># NFS存储卷插件</span></span><br><span class="line">        <span class="attr">server:</span> <span class="string">nfs.ilinux.io</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/data/redis</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>上面的示例定义在名为volumes-nfs-demo.yaml资源清单文件中，容器镜像文件redis:alpine默认会以redis用户（UID是999）运行redis-server进程，并将数据持久保存在容器文件系统上的/data目录中，因而需要确保UID为999的用户有权限读写该目录。与此对应，NFS服务器上用于该Pod对象的存储卷的导出目录（本示例中为/data/redis目录）也需要确保让UID为999的用户拥有读写权限，因而需要在nfs.ilinux.io服务器上创建该用户，将该用户设置为/data/redis目录的属主，或通过facl设置该用户拥有读写权限。<br>以Ubuntu Server18.04为例，在一个专用的主机（nfs.ilinux.io）上以root用户设定所需的NFS服务器的步骤如下。</p>
<ul>
<li>1）安装NFS Server程序包，Ubuntu 18.04上的程序包名为nfs-kernel-server。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">apt -y install nfs-kernel-server</span></span><br></pre></td></tr></table></figure>

<ul>
<li>2）设定基础环境，包括用户、数据目录及相应授权。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">mkdir</span> /data/redis</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash">useradd -u 999 redis</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">chown</span> redis /data/redis</span></span><br></pre></td></tr></table></figure>

<ul>
<li>3）编辑/etc/exports配置文件，填入类似如下内容：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/data/redis     172.29.0.0/16(rw,no_root_squash) 10.244.0.0/16(rw,no_root_squash)</span><br></pre></td></tr></table></figure>

<ul>
<li>4）启动NFS服务器：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">systemctl start nfs-server</span></span><br></pre></td></tr></table></figure>

<ul>
<li>5）在各工作节点安装NFS服务客户端程序包，Ubuntu 18.04上的程序包名为nfs-common。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">apt install -y nfs-common</span></span><br></pre></td></tr></table></figure>

<p>待上述步骤执行完成后，切换回Kubernetes集群可运行kubectl命令的主机之上，运行命令创建配置清单中的Pod对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-nfs-demo.yaml</span> </span><br><span class="line">pod/volumes-nfs-demo created</span><br></pre></td></tr></table></figure>

<p>资源创建完成后，可通过其命令客户端redis-cli创建测试数据，并手动触发其与存储系统同步，下面加粗部分的字体为要执行的Redis命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it volumes-nfs-demo -- redis-cli</span></span><br><span class="line">127.0.0.1:6379&gt; set mykey &quot;hello ilinux.io&quot;</span><br><span class="line">OK</span><br><span class="line">127.0.0.1:6379&gt; get mykey</span><br><span class="line">&quot;hello ilinux.io&quot;</span><br><span class="line">127.0.0.1:6379&gt; BGSAVE</span><br><span class="line">Background saving started</span><br><span class="line">127.0.0.1:6379&gt; exit</span><br></pre></td></tr></table></figure>

<p>为了测试其数据持久化效果，下面先删除此前创建的Pod对象vol-nfs-pod，而后待重建该Pod对象后检测数据是否依然能够访问。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods/volumes-nfs-demo</span></span><br><span class="line">pod &quot;volumes-nfs-demo&quot; deleted</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-nfs-demo.yaml</span> </span><br><span class="line">pod/volumes-nfs-demo created</span><br></pre></td></tr></table></figure>

<p>待其重建完成后，通过再次创建的Pod资源的详细描述信息可以观察到它挂载使用NFS存储卷的相关状态，也可通过下面的命令来检查redis-server中是否还保存有此前存储的数据。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it volumes-nfs-demo -- redis-cli</span></span><br><span class="line">127.0.0.1:6379&gt; get mykey</span><br><span class="line">&quot;hello ilinux.io&quot;</span><br><span class="line">127.0.0.1:6379&gt;</span><br></pre></td></tr></table></figure>

<p>上面的命令结果显示出此前创建的键mykey及其数据在Pod对象删除并重建后依然存在，这表明删除Pod对象后，其关联的外部存储设备及数据并不会被一同删除，因而才具有了跨Pod生命周期的数据持久性。若需要在删除Pod后清除具有持久存储功能的存储设备上的数据，则需要用户或管理员通过存储系统的管理接口手动进行。</p>
<h3 id="RBD存储卷"><a href="#RBD存储卷" class="headerlink" title="RBD存储卷"></a>RBD存储卷</h3><p>Ceph是一个专注于分布式的、弹性可扩展的、高可靠的、性能优异的存储系统平台，同时支持提供块设备、文件系统和对象存储3种存储接口。它是个高度可配置的系统，并提供了一个命令行界面用于监视和控制其存储集群。Kubernetes支持通过RBD卷插件和CephFS卷插件，基于Ceph存储系统为Pod提供存储卷。要配置Pod对象使用RBD存储卷，需要事先满足以下前提条件。<br>▪存在某可用的Ceph RBD存储集群，否则需要创建一个。<br>▪在Ceph RBD集群中创建一个能满足Pod资源数据存储需要的存储映像。<br>▪在Kubernetes集群内的各节点上安装Ceph客户端程序包（ceph-common）。<br>定义RBD类型的存储卷时需要指定要连接的目标服务器和认证信息等配置，它们依赖如下几个可用的嵌套字段。<br>▪monitors &lt;[]string&gt;：Ceph存储监视器，逗号分隔的字符串列表；必选字段。<br>▪image <string>：rados image（映像）的名称，必选字段。<br>▪pool <string>：Ceph存储池名称，默认为rbd。<br>▪user <string>：Ceph用户名，默认为admin。<br>▪keyring <string>：用户认证到Ceph集群时使用的keyring文件路径，默认为/etc/ceph/keyring。<br>▪secretRef <Object>：用户认证到Ceph集群时使用的保存有相应认证信息的Secret资源对象，该字段会覆盖由keyring字段提供的密钥信息。<br>▪readOnly <boolean>：是否以只读方式访问。<br>▪fsType：要挂载的存储卷的文件系统类型，至少应该是节点操作系统支持的文件系统，例如Ext4、xfs、NTFS等，默认为Ext4。<br>下面提供的RBD存储卷插件使用示例定义在volumes-rbd-demo.yaml配置清单文件中，它使用kube用户认证到Ceph集群中，并关联RDB存储池kube中的存储映像redis-img1为Pod对象volumes-rbd-demo的存储卷，由容器进程挂载至/data目录进行数据存取。</boolean></Object></string></string></string></string></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-rbd-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">      <span class="attr">rbd:</span></span><br><span class="line">        <span class="attr">monitors:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.1:6789&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.2:6789&#x27;</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">&#x27;172.29.200.3:6789&#x27;</span></span><br><span class="line">        <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">redis-img1</span></span><br><span class="line">        <span class="attr">fsType:</span> <span class="string">xfs</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">user:</span> <span class="string">kube</span></span><br><span class="line">        <span class="attr">keyring:</span> <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br></pre></td></tr></table></figure>

<p>RBD存储卷插件依赖Ceph存储集群作为存储系统，这里假设其监视器（MON）的地址为172.29.200.1、172.29.200.2和172.29.200.3，集群上的存储池kube中需要有事先创建好的存储映像redis-img1。客户端访问集群时要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了用户的keyring文件。该示例实现的逻辑架构如图5-5所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123095502153.png" alt="image-20220123095502153"></p>
<p>为了完成示例中定义的资源的测试，需要事先完成如下几个步骤。<br>1）在Ceph集群上的kube存储池中创建用作Pod存储卷的RBD映像文件，并设置映像特性。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">rbd create --pool kube --size 1G redis-img1</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash">rbd feature <span class="built_in">disable</span> -p kube redis-img1 object-map fast-diff deep-flatten</span></span><br></pre></td></tr></table></figure>

<p>2）在Ceph集群上创建存储卷客户端账号并进行合理授权。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">ceph auth get-or-create client.kube mon <span class="string">&#x27;allow r&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    osd <span class="string">&#x27;allow class-read object_prefix rbd_children, allow rwx pool=kube&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">    -o /etc/ceph/ceph.client.kube.keyring</span></span><br></pre></td></tr></table></figure>

<p>3）在Kubernetes集群的各工作节点上执行如下命令安装Ceph客户端库。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">echo</span> deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \</span></span><br><span class="line"><span class="language-bash">    | <span class="built_in">tee</span> /etc/apt/sources.list.d/ceph.list</span> </span><br><span class="line"><span class="meta">~# </span><span class="language-bash">apt update &amp;&amp; apt install ceph-common</span></span><br></pre></td></tr></table></figure>

<p>4）在Ceph集群某节点上执行如下命令，以复制Ceph集群的配置文件及客户端认证使用的keyring文件到Kubernetes集群的各工作节点之上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="keyword">for</span> kubehost <span class="keyword">in</span> k8s-node01 k8s-node02 k8s-node03; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="language-bash">    scp -p /etc/ceph/&#123;ceph.conf,ceph.client.kube.keyring&#125; <span class="variable">$&#123;kubehost&#125;</span>:/etc/ceph/; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure>

<p>待完成如上必要的准备步骤后，便可执行如下命令将前面定义在volumes-rbd-demo.yaml中的Pod资源创建在Kubernetes集群上进行测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-rbd-demo.yaml</span> </span><br><span class="line">pod/volumes-rbd-demo created</span><br></pre></td></tr></table></figure>

<p>随后从集群上的Pod对象volumes-rbd-demo的详细描述中获取存储的相关状态信息，确保其创建操作得以成功执行。下面是相关的存储卷信息示例。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Volumes:</span></span><br><span class="line">  <span class="attr">redis-rbd-vol:</span></span><br><span class="line">    <span class="attr">Type:</span>  <span class="string">RBD</span> <span class="string">(a</span> <span class="string">Rados</span> <span class="string">Block</span> <span class="string">Device</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">CephMonitors:</span>  [<span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span>]</span><br><span class="line">    <span class="attr">RBDImage:</span>      <span class="string">redis-img1</span></span><br><span class="line">    <span class="attr">FSType:</span>        <span class="string">xfs</span></span><br><span class="line">    <span class="attr">RBDPool:</span>       <span class="string">kube</span></span><br><span class="line">    <span class="attr">RadosUser:</span>     <span class="string">kube</span></span><br><span class="line">    <span class="attr">Keyring:</span>       <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br><span class="line">    <span class="attr">SecretRef:</span>     <span class="string">nil</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>      <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>删除Pod对象仅会解除它对RBD映像的引用而非级联删除它，因而RBD映像及数据将依然存在，除非管理员手动进行删除。我们可使用类似前一节测试Redis数据持久性的方式来测试本示例中的容器数据的持久能力，这里不再给出具体步骤。另外，实践中，应该把认证到Ceph集群上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用keyring字段引用相应用户的keyring文件。</p>
<h3 id="CephFS存储卷"><a href="#CephFS存储卷" class="headerlink" title="CephFS存储卷"></a>CephFS存储卷</h3><p>CephFS（Ceph文件系统）是在分布式对象存储RADOS之上构建的POSIX兼容的文件系统，它致力于为各种应用程序提供多用途、高可用和高性能的文件存储。CephFS将文件元数据和文件数据分别存储在各自专用的RADOS存储池中，其中MDS通过元数据子树分区等支持高吞吐量的工作负载，而数据则由客户端直接相关的存储池直接进行读写操作，其扩展能跟随底层RADOS存储的大小进行线性扩展。Kubernetes的CephFS存储卷插件以CephFS为存储方案为Pod提供存储卷，因而可受益于CephFS的存储扩展和性能优势。<br>CephFS存储卷插件嵌套定义于Pod资源的spec.volumes.cephfs字段中，它支持通过如下字段的定义接入到存储预配服务中。</p>
<ul>
<li>monitors &lt;[]string&gt;：Ceph存储监视器，为逗号分隔的字符串列表；必选字段。</li>
<li>user &lt;string&gt;：Ceph集群用户名，默认为admin。</li>
<li>secretFile &lt;string&gt;：用户认证到Ceph集群时使用的Base64格式的密钥文件（非keyring文件），默认为/etc/ceph/user.secret。</li>
<li>secretRef &lt;Object&gt;：用户认证到Ceph集群过程中加载其密钥时使用的Kubernetes Secret资源对象。</li>
<li>path &lt;string&gt;：挂载的文件系统路径，默认为CephFS文件系统的根（/），可以使用CephFS文件系统上的子路径，例如/kube/namespaces/default/redis1等。</li>
<li>readOnly &lt;boolean&gt;：是否挂载为只读模式，默认为false。</li>
</ul>
<p>下面提供的CephFS存储卷插件使用示例定义在volumes-cephfs-demo.yaml配置清单文件中，它使用fsclient用户认证到Ceph集群中，并关联CephFS上的子路径/kube/namespaces/default/redis1，作为Pod对象volumes-cephfs-demo的存储卷，并由容器进程挂载至/data目录进行数据存取。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-cephfs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span> </span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/data&quot;</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-cephfs-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-cephfs-vol</span></span><br><span class="line">    <span class="attr">cephfs:</span></span><br><span class="line">      <span class="attr">monitors:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span></span><br><span class="line">      <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span></span><br><span class="line">      <span class="attr">path:</span> <span class="string">/kube/namespaces/default/redis1</span></span><br><span class="line">      <span class="attr">user:</span> <span class="string">fsclient</span></span><br><span class="line">      <span class="attr">secretFile:</span> <span class="string">&quot;/etc/ceph/fsclient.key&quot;</span></span><br><span class="line">      <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>Kubernetes集群上需要启用了CephFS，并提供了满足条件的用户账号及授权才能使用CephFS存储卷插件。客户端访问集群时需要事先认证到Ceph集群并获得相应授权才能进行后续的访问操作，该示例使用了保存在/etc/ceph/fsclient.key文件中的CephFS专用用户认证信息。要完成示例清单中定义的资源的测试，需要事先完成如下几个步骤。</p>
<ul>
<li>1）将授权访问CephFS的用户fsclient的Secret文件fsclient.key复制到Kubernetes集群的各工作节点，以便kubelet可加载并使用它。在生成fsclient.key的Ceph节点上执行如下命令以复制必要的文件。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="keyword">for</span> kubehost <span class="keyword">in</span> k8s-node01 k8s-node02 k8s-node03; <span class="keyword">do</span> \</span></span><br><span class="line"><span class="language-bash">    scp -p /etc/ceph/fsclient.key /etc/ceph/ceph.conf <span class="variable">$&#123;kubehost&#125;</span>:/etc/ceph/; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure>

<ul>
<li>2）在Kubernetes集群的各工作节点上执行如下命令，以安装Ceph客户端库。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key add -</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">echo</span> deb https://mirrors.aliyun.com/ceph/debian-nautilus/ $(lsb_release -sc) main \</span></span><br><span class="line"><span class="language-bash">    | <span class="built_in">tee</span> /etc/apt/sources.list.d/ceph.list</span> </span><br><span class="line"><span class="meta">~# </span><span class="language-bash">apt update &amp;&amp; apt install ceph-common</span></span><br></pre></td></tr></table></figure>

<ul>
<li>3）在Kubernetes的某工作节点上手动挂载CephFS，以创建由Pod对象使用的数据目录。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">mount -t ceph ceph01:6789:/ /mnt -o name=fsclient,secretfile=/etc/ceph/fsclient.key</span></span><br><span class="line"><span class="meta">~# </span><span class="language-bash"><span class="built_in">mkdir</span> -p /mnt/kube/namespaces/default/redis1</span></span><br></pre></td></tr></table></figure>

<p>上述准备步骤执行完成后即可运行如下命令创建清单volumes-cephfs-demo.yaml中定义的Pod资源，并进行测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-cephfs-demo.yaml</span>        </span><br><span class="line">pod/volumes-cephfs-demo created</span><br></pre></td></tr></table></figure>

<p>随后通过Pod对象volumes-cephfs-demo的详细描述了解其创建及运行状态，若一切无误，则相应的存储卷会显示出类似如下的描述信息：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Volumes:</span></span><br><span class="line">  <span class="attr">redis-cephfs-vol:</span></span><br><span class="line">    <span class="attr">Type:</span>        <span class="string">CephFS</span> <span class="string">(a</span> <span class="string">CephFS</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">Monitors:</span>    [<span class="number">172.29</span><span class="number">.200</span><span class="number">.1</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.2</span><span class="string">:6789</span> <span class="number">172.29</span><span class="number">.200</span><span class="number">.3</span><span class="string">:6789</span>]</span><br><span class="line">    <span class="attr">Path:</span>        <span class="string">/kube/namespaces/default/redis1</span></span><br><span class="line">    <span class="attr">User:</span>        <span class="string">fsclient</span></span><br><span class="line">    <span class="attr">SecretFile:</span>  <span class="string">/etc/ceph/fsclient.key</span></span><br><span class="line">    <span class="attr">SecretRef:</span>   <span class="string">nil</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>    <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>删除Pod对象仅会卸载其挂载的CephFS文件系统（或子目录），因而文件系统（或目录）及相关数据将依然存在，除非管理员手动进行删除。我们可使用类似5.4.1节中测试Redis数据持久性的方式来测试本示例中的容器数据的持久性，这里不再给出具体步骤。另外在实践中，应该把认证到CephFS文件系统上的用户的认证信息存储为Kubernetes集群上的Secret资源，并通过secretRef字段进行指定，而非像该示例中那样，直接使用secretFile字段引用相应用户密钥信息文件。</p>
<h2 id="GlusterFS存储卷"><a href="#GlusterFS存储卷" class="headerlink" title="GlusterFS存储卷"></a>GlusterFS存储卷</h2><p>GlusterFS（Gluster File System）是一个开源的分布式文件系统，是水平扩展存储解决方案Gluster的核心，它具有强大的横向扩展能力，通过扩展能够支持PB级的存储容量和数千个客户端。GlusterFS借助TCP/IP或InfiniBand RDMA网络将物理分布的存储资源聚集在一起，使用单一全局命名空间来管理数据，它基于可堆叠的用户空间设计，可为各种不同的数据负载提供优异的性能，是另一种流行的分布式存储解决方案。Kubernetes的GlusterFS存储卷插件依赖于GlusterFS存储集群作为存储方案。要配置Pod资源使用GlusterFS存储卷，需要事先满足以下前提条件：</p>
<blockquote>
<p>1）存在某可用的GlusterFS存储集群，否则要创建一个。<br>2）在GlusterFS集群中创建一个能满足Pod资源数据存储需要的卷。<br>3）在Kubernetes集群内的各节点上安装GlusterFS客户端程序包（glusterfs和glusterfs-fuse）。<br>GlusterFS存储卷嵌套定义在Pod资源的spec.volumes.glusterfs字段中，它常用的配置字段有如下几个。</p>
<ul>
<li><p>endpoints &lt;string&gt;：Endpoints资源的名称，此资源需要事先存在，用于提供Gluster集群的部分节点信息作为其访问入口；必选字段。</p>
</li>
<li><p>path &lt;string&gt;：用到的GlusterFS集群的卷路径，例如kube-redis；必选字段。</p>
</li>
<li><p>readOnly &lt;boolean&gt;：是否为只读卷。</p>
</li>
</ul>
</blockquote>
<p>下面提供的GlusterFS存储卷插件使用示例定义在volumes-glusterfs-demo.yaml配置清单文件中，它通过glusterfs-endpoints资源中定义的GlusterFS集群节点信息接入集群，并以kube-redis卷作为Pod资源的存储卷。glusterfs-endpoints资源需要在Kubernetes集群中事先创建，而kube-redis则需要先于Gluster集群创建。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-glusterfs-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">redis</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redisdata</span></span><br><span class="line">      <span class="attr">glusterfs:</span></span><br><span class="line">        <span class="attr">endpoints:</span> <span class="string">glusterfs-endpoints</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">kube-redis</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br></pre></td></tr></table></figure>

<p>用于访问Gluster集群的相关节点信息要事先保存在某特定的Endpoint资源中，例如上面示例中调用的glusterfs-endpoints。此类的Endpoint资源依赖用户根据实际需求手动创建，例如，下面保存在glusterfs-endpoints.yaml文件中的资源示例定义了3个接入相关的Gluster存储集群的节点：gfs01.ilinux.io、gfs02.ilinux.io和gfs03.ilinux.io，其中的端口信息仅为满足Endpoint资源的必选字段要求，因此其值可以随意填写。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">glusterfs-endpoints</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs01.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs02.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">gfs03.ilinux.io</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">24007</span> </span><br><span class="line">      <span class="attr">name:</span> <span class="string">glusterd</span></span><br></pre></td></tr></table></figure>

<p>准备好必要的存储供给条件后，先创建Endpoint资源glusterfs-endpoints，之后创建Pod资源vol-glusterfs-pod，即可测试其数据持久存储的效果</p>
<h2 id="持久存储卷"><a href="#持久存储卷" class="headerlink" title="持久存储卷"></a>持久存储卷</h2><p>通过5.4节网络存储卷及使用示例可知，用户必须要清晰了解用到的网络存储系统的访问细节才能完成存储卷相关的配置任务，例如RBD存储卷插件配置中的监视器（monitor）、存储池（pool）、存储映像（image）和密钥环（keyring）等来自于Ceph存储系统中的概念，这就要求用户对该类存储系统有着一定的了解才能够顺利使用。这与Kubernetes向用户和开发隐藏底层架构的目标有所背离，最好对存储资源的使用也能像计算资源一样，用户和开发人员既无须了解Pod资源究竟运行在哪个节点，也不用了解存储系统是什么设备、位于何处以及如何访问。<br>PV（PersistentVolume）与PVC（PersistentVolumeClaim）就是在用户与存储服务之间添加的一个中间层，管理员事先根据PV支持的存储卷插件及适配的存储方案（目标存储系统）细节定义好可以支撑存储卷的底层存储空间，而后由用户通过PVC声明要使用的存储特性来绑定符合条件的最佳PV定义存储卷，从而实现存储系统的使用与管理职能的解耦，大大简化了用户使用存储的方式。<br>PV和PVC的生命周期由Controller Manager中专用的PV控制器（PV Controller）独立管理，这种机制的存储卷不再依附并受限于Pod对象的生命周期，从而实现了用户和集群管理员的职责相分离，也充分体现出Kubernetes把简单留给用户，把复杂留给自己的管理理念。</p>
<h3 id="PV与PVC基础"><a href="#PV与PVC基础" class="headerlink" title="PV与PVC基础"></a>PV与PVC基础</h3><p>PV是由集群管理员于全局级别配置的预挂载存储空间，它通过支持的存储卷插件及给定的配置参数关联至某个存储系统上可用数据存储的一段空间，这段存储空间可能是Ceph存储系统上的一个存储映像、一个文件系统（CephFS）或其子目录，也可能是NFS存储系统上的一个导出目录等。PV将存储系统之上的存储空间抽象为Kubernetes系统全局级别的API资源，由集群管理员负责管理和维护。<br>将PV提供的存储空间用于Pod对象的存储卷时，用户需要事先使用PVC在名称空间级别声明所需要的存储空间大小及访问模式并提交给Kubernetes API Server，接下来由PV控制器负责查找与之匹配的PV资源并完成绑定。随后，用户在Pod资源中使用persistentVolumeClaim类型的存储卷插件指明要使用的PVC对象的名称即可使用其绑定到的PV所指向的存储空间，如图5-6所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123100113403.png" alt="image-20220123100113403"></p>
<p>由此可见，尽管PVC及PV将存储资源管理与使用的职责分离至用户和集群管理员两类不同的人群之上，简化了用户对存储资源的使用机制，但也对二者之间的协同能力提出了要求。管理员需要精心预测和规划集群用户的存储使用需求，提前创建出多种规格的PV，以便于在用户声明PVC后能够由PV控制器在集群中找寻到合适的甚至是最佳匹配的PV进行绑定。<br>不难揣测，这种通过管理员手动创建PV来满足PVC需求的静态预配（static provisioning）存在着不少的问题。<br>第一，集群管理员难以预测出用户的真实需求，很容易导致某些类型的PVC无法匹配到PV而被挂起，直到管理员参与到问题的解决过程中。<br>第二，那些能够匹配到PV的PVC也很有可能存在资源利用率不佳的状况，例如一个声明使用5G存储空间的PVC绑定到一个20GB的PV之上。<br>更好的解决方案是一种称为动态预配、按需创建PV的机制。集群管理员要做的仅是事先借助存储类（StorageClass）的API资源创建出一到多个“PV模板”，并在模板中定义好基于某个存储系统创建PV所依赖的存储组件（例如Ceph RBD存储映像或CephfFS文件系统等）时需要用到的配置参数。创建PVC时，用户需要为其指定要使用PV模板（StorageClass资源），而后PV控制器会自动连接相应存储类上定义的目标存储系统的管理接口，请求创建匹配该PVC需求的存储组件，并将该存储组件创建为Kubernetes集群上可由该PVC绑定的PV资源。<br>需要说明的是，静态预配的PV可能属于某存储类，也可能没有存储类，这取决于管理员的设定。但动态PV预配依赖存储类的辅助，PVC必须向一个事先存在的存储类发起动态分配PV的请求，没有指定存储类的PVC不支持使用动态预配PV的方式。<br><font color="red">PV和PVC是一对一的关系：一个PVC仅能绑定一个PV，而一个PV在某一时刻也仅可被一个PVC所绑定。</font>为了能够让用户更精细地表达存储需求，PV资源对象的定义支持存储容量、存储类、卷模型和访问模式等属性维度的约束。相应地，PVC资源能够从访问模式、数据源、存储资源容量需求和限制、标签选择器、存储类名称、卷模型和卷名称等多个不同的维度向PV资源发起匹配请求并完成筛选。</p>
<h3 id="PV的生命周期"><a href="#PV的生命周期" class="headerlink" title="PV的生命周期"></a>PV的生命周期</h3><p>从较为高级的实现上来讲，Kubernetes系统与存储相关的组件主要有存储卷插件、存储卷管理器、PV/PVC控制器和AD控制器（Attach/Detach Controller）这4种，如图5-7所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123100142256.png" alt="image-20220123100142256"></p>
<p>存储卷插件：Kubernetes存储卷功能的基础设施，是存储任务相关操作的执行方；它是存储相关的扩展接口，用于对接各类存储设备。</p>
<ul>
<li><p>存储卷管理器：kubelet内置管理器组件之一，用于在当前节点上执行存储设备的挂载（mount）、卸载（unmount）和格式化（format）等操作；另外，存储卷管理器也可执行节点级别设备的附加（attach）及拆除（detach）操作。</p>
</li>
<li><p>PV控制器：负责PV及PVC的绑定和生命周期管理，并根据需求进行存储卷的预配和删除操作；</p>
</li>
<li><p>AD控制器：专用于存储设备的附加和拆除操作的组件，能够将存储设备关联（attach）至目标节点或从目标节点之上剥离（detach）。<br>这4个组件中，存储卷插件是其他3个组件的基础库，换句话说，PV控制器、AD控制器和存储卷管理器均构建于存储卷插件之上，以提供不同维度管理功能的接口，具体的实现逻辑均由存储卷插件完成。<br>除了创建、删除PV对象，以及完成PV和PVC的状态迁移等生命周期管理之外，PV控制器还要负责绑定PVC与PV对象，而且PVC只能在绑定到PV之后方可由Pod作为存储卷使用。创建后未能正确关联到存储设备的PV将处于Pending状态，直到成功关联后转为Available状态。而后一旦该PV被某个PVC请求并成功绑定，其状态也就顺应转为Bound，直到相应的PVC删除后而自动解除绑定，PV才会再次发生状态转换，此时的状态为（Released），随后PV的去向将由其“回收策略”（reclaim policy）所决定，具体如下。</p>
</li>
</ul>
<blockquote>
<p>1）Retain（保留）：删除PVC后将保留其绑定的PV及存储的数据，但会把该PV置为Released状态，它不可再被其他PVC所绑定，且需要由管理员手动进行后续的回收操作：首先删除PV，接着手动清理其关联的外部存储组件上的数据，最后手动删除该存储组件或者基于该组件重新创建PV。<br>2）Delete（删除）：对于支持该回收策略的卷插件，删除一个PVC将同时删除其绑定的PV资源以及该PV关联的外部存储组件；动态的PV回收策略继承自StorageClass资源，默认为Delete。多数情况下，管理员都需要根据用户的期望修改此默认策略，以免导致数据非计划内的删除。<br>3）Recycle（回收）：对于支持该回收策略的卷插件，删除PVC时，其绑定的PV所关联的外部存储组件上的数据会被清空，随后，该PV将转为Available状态，可再次接受其他PVC的绑定请求。不过，该策略已被废弃。</p>
</blockquote>
<p>相应地，创建后的PVC也将处于Pending状态，仅在遇到条件匹配、状态为Available的PV，且PVC请求绑定成功才会转为Bound状态。PV和PVC的状态迁移如图5-8所示。总结起来，PV和PVC的生命周期存在以几个关键阶段。</p>
<blockquote>
<p>1）存储预配（provision）：存储预配是指为PVC准备PV的途径，Kubernetes支持静态和动态两种PV预配方式，前者是指由管理员以手动方式创建PV的操作，而后者则是由PVC基于StorageClass定义的模板，按需请求创建PV的机制。<br>2）存储绑定：用户基于一系列存储需求和访问模式定义好PVC后，PV控制器即会为其查找匹配的PV，完成关联后它们二者同时转为已绑定状态，而且动态预配的PV与PVC之间存在强关联关系。无法找到可满足条件的PV的PVC将一直处于Pending状态，直到有符合条件的PV出现并完成绑定为止。<br>3）存储使用：Pod资源基于persistenVolumeClaim存储卷插件的定义，可将选定的PVC关联为存储卷并用于内部容器应用的数据存取。<br>4）存储回收：存储卷的使用目标完成之后，删除PVC对象可使得此前绑定的PV资源进入Released状态，并由PV控制器根据PV回收策略对PV作出相应的处置。目前，可用的回收策略有Retaine、Delete和Recycle这3种。<br>如前所述，处于绑定状态的PVC删除后，相应的PV将转为Released状态，之后的处理机制依赖于其回收策略。而处于绑定状态的PV将会导致相应的PVC转为Lost状态，而无法再由Pod正常使用，除非PVC再绑定至其他Available状态的PV之上，但应用是否能正常运行，则取决于对此前数据的依赖度。另一方面，为了避免使用中的存储卷被移除而导致数据丢失，Kubernetes自1.9版引入了“PVC保护机制”，其目的在于，用户删除了仍被某Pod对象使用中的PVC时，Kubernetes不会立即移除该PVC，而是会推迟到它不再被任何Pod对象使用后方才真正执行删除操作。处于保护阶段的PVC资源的status字段值为Termination，并且其Finalizers字段值中包含有kubernetes.io/pvc-protection。</p>
</blockquote>
<h3 id="静态PV资源"><a href="#静态PV资源" class="headerlink" title="静态PV资源"></a>静态PV资源</h3><p>PersistentVolume是隶属于Kubernetes核心API群组中的标准资源类型，它的目标在于通过存储卷插件机制，将支持的外部存储系统上的存储组件定义为可被PVC声明所绑定的资源对象。但PV资源隶属于Kubernetes集群级别，因而它只能由集群管理员进行创建。这种由管理员手动定义和创建的PV被人们习惯地称为静态PV资源。<br>PV支持的存储卷插件类型是Pod对象支持的存储卷插件类型的一个子集，它仅涵盖Pod支持的网络存储卷类别中的所有存储插件以及local卷插件。除了存储卷插件之外，PersistentVolume资源规范Spec字段主要支持嵌套以下几个通用字段，它们用于定义PV的容量、访问模式和回收策略等属性。</p>
<ul>
<li>capacity &lt;map[string]string&gt;：指定PV的容量；目前，Capacity仅支持存储容量设定，将来应该还可以指定IOPS和吞吐量（throughput）。</li>
<li>accessModes &lt;[]string&gt;：指定当前PV支持的访问模式；存储系统支持的存取能力大体可分为ReadWriteOnce（单路读写）、ReadOnlyMany（多路只读）和ReadWrite-Many（多路读写）3种类型，某个特定的存储系统可能会支持其中的部分或全部的能力。</li>
<li>persistentVolumeReclaimPolicy &lt;string&gt;：PV空间被释放时的处理机制；可用类型仅为Retain（默认）、Recycle或Delete。目前，仅NFS和hostPath支持Recycle策略，也仅有部分存储系统支持Delete策略。</li>
<li>volumeMode &lt;string&gt;：该PV的卷模型，用于指定此存储卷被格式化为文件系统使用还是直接使用裸格式的块设备；默认值为Filesystem，仅块设备接口的存储系统支持该功能。</li>
<li>storageClassName &lt;string&gt;：当前PV所属的StorageClass资源的名称，指定的存储类需要事先存在；默认为空值，即不属于任何存储类。</li>
<li>mountOptions &lt;string&gt;：挂载选项组成的列表，例如ro、soft和hard等。</li>
<li>nodeAffinity &lt;Object&gt;：节点亲和性，用于限制能够访问该PV的节点，进而会影响与该PV关联的PVC的Pod的调度结果。</li>
</ul>
<p>PV的访问模式用于反映它关联的存储系统所支持的某个或全部存取能力，例如NFS存储系统支持以上3种存取能力，于是NFS PV可以仅支持ReadWriteOnce访问模式。需要注意的是，PV在某个特定时刻仅可基于一种模式进行存取，哪怕它同时支持多种模式。</p>
<h4 id="NFS-PV示例"><a href="#NFS-PV示例" class="headerlink" title="NFS PV示例"></a>NFS PV示例</h4><p>下面的配置示例来自于pv-nfs-demo.yaml资源清单，它定义了一个使用NFS存储系统的PV资源，它将空间大小限制为5GB，并支持多路的读写操作。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-nfs-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteMany</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br><span class="line">  <span class="attr">mountOptions:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">hard</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">nfsvers=4.1</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span>  <span class="string">&quot;/data/redis002&quot;</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">nfs.ilinux.io</span></span><br></pre></td></tr></table></figure>

<p>在NFS服务器nfs.ilinux.io上导出/data/redis002目录后，便可使用如下命令创建该PV资源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pv-nfs-demo.yaml</span></span><br><span class="line">persistentvolume/pv-nfs-demo created</span><br></pre></td></tr></table></figure>

<p>若能够正确关联到指定的后端存储，该PV对象的状态将显示为Available，否则其状态为Pending，直至能够正确完成存储资源关联或者被删除。我们同样可使用describe命令来获取PV资源的详细描述信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pv/pv-nfs-demo</span></span><br><span class="line">Name:            pv-nfs-demo</span><br><span class="line">……  </span><br><span class="line">Status:          Available</span><br><span class="line">……        </span><br><span class="line">Source:</span><br><span class="line">    Type:      NFS (an NFS mount that lasts the lifetime of a pod)</span><br><span class="line">    Server:    nfs.ilinux.io</span><br><span class="line">    Path:      /data/redis002</span><br><span class="line">    ReadOnly:  false</span><br><span class="line">Events:        &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>描述信息中的Available表明该PV已经可以接受PVC的绑定请求，并在绑定完成后转变其状态至Bound。2. RBD PV示例<br>下面是另一个PV资源的配置清单（pv-rbd-demo.yaml），它使用了RBD存储后端，空间大小等同于指定的RBD存储映像的大小（这里为2GB），并限定支持的访问模式为RWO，回收策略为Retain。除此之外，该PV资源还拥有一个名为usedof的资源标签，该标签可被PVC的标签选择器作为筛选PV资源的标准之一。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pv-rbd-demo</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">usedof:</span> <span class="string">redisdata</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">rbd:</span></span><br><span class="line">    <span class="attr">monitors:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph01.ilinux.io</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph02.ilinux.io</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ceph03.ilinux.io</span></span><br><span class="line">    <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">pv-test</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">kube</span></span><br><span class="line">    <span class="attr">keyring:</span> <span class="string">/etc/ceph/ceph.client.kube.keyring</span></span><br><span class="line">    <span class="attr">fsType:</span> <span class="string">xfs</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">persistentVolumeReclaimPolicy:</span> <span class="string">Retain</span></span><br></pre></td></tr></table></figure>

<p>将RBD卷插件内嵌字段相关属性值设定为Ceph存储系统的实际的环境，包括监视器地址、存储池、存储映像、用户名和认证信息（keyring或secretRef）等。测试时，请事先部署好Ceph集群，参考5.4.2节中设定专用用户账号和Kubernetes集群工作节点的方式，准备好基础环境，并在Ceph集群的管理节点运行如下命令创建用到的存储映像：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd create pv-test --size 2G --pool kube</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd feature <span class="built_in">disable</span> -p kube pv-test object-map fast-diff deep-flatten</span></span><br></pre></td></tr></table></figure>

<p>待所有准备工作就绪后，即可运行如下命令创建示例清单中定义的PV资源pv-rbd-demo：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl apply -f pv-rbd-demo.yaml</span> </span><br><span class="line">persistentvolume/pv-rbd-demo created</span><br></pre></td></tr></table></figure>

<p>我们同样可以使用describe命令了解pv-rbd-demo的详细描述，若处于Pending状态则需要详细检查存储卷插件的定义是否能吻合存储系统的真实环境。</p>
<h3 id="PVC资源"><a href="#PVC资源" class="headerlink" title="PVC资源"></a>PVC资源</h3><p>PersistentVolumeClaim也是Kubernetes系统上标准的API资源类型之一，它位于核心API群组，属于名称空间级别。用户提交新建的PVC资源最初处于Pending状态，由PV控制器找寻最佳匹配的PV并完成二者绑定后，两者都将转入Bound状态，随后Pod对象便可基于persistentVolumeClaim存储卷插件配置使用该PVC对应的持久存储卷。<br>定义PVC时，用户可通过访问模式（accessModes）、数据源（dataSource）、存储资源空间需求和限制（resources）、存储类、标签选择器、卷模型和卷名称等匹配标准来筛选集群上的PV资源，其中，resources和accessModes是最重要的筛选标准。PVC的Spec字段的可嵌套字段有如下几个。</p>
<ul>
<li>accessModes &lt;[]string&gt;：PVC的访问模式；它同样支持RWO、RWX和ROX这3种模式。</li>
<li>dataSrouces &lt;Object&gt;：用于从指定的数据源恢复该PVC卷，它目前支持的数据源包括一个现存的卷快照对象（snapshot.storage.k8s.io/VolumeSnapshot）、一个既有的PVC对象（PersistentVolumeClaim）或一个既有的用于数据转存的自定义资源对象（resource/object）。</li>
<li>resources &lt;Object&gt;：声明使用的存储空间的最小值和最大值；目前，PVC的资源限定仅支持空间大小一个维度。</li>
<li>selector &lt;Object&gt;：筛选PV时额外使用的标签选择器（matchLabels）或匹配条件表达式（matchExpressions）。</li>
<li>storageClassName &lt;string&gt;：该PVC资源隶属的存储类资源名称；指定了存储类资源的PVC仅能在同一个存储类下筛选PV资源，否则就只能从所有不具有存储类的PV中进行筛选。</li>
<li>volumeMode &lt;string&gt;：卷模型，用于指定此卷可被用作文件系统还是裸格式的块设备；默认值为Filesystem。</li>
<li>volumeName &lt;string&gt;：直接指定要绑定的PV资源的名称。<br>下面通过匹配前一节中创建的PV资源的两个具体示例来说明PVC资源的配置方法，两个PV资源目前的状态如下所示，它仅截取了命令结果中的一部分。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$</span><span class="language-bash">kubectl get pv</span></span><br><span class="line">NAME        CAPACITY  ACCESS MODES  RECLAIM POLICY       STATUS        CLAIM   </span><br><span class="line">pv-nfs-demo   5Gi        RWX            Retain           Available                                  </span><br><span class="line">pv-rbd-demo   2Gi        RWO            Retain           Available</span><br></pre></td></tr></table></figure>

<h4 id="NFS-PVC示例"><a href="#NFS-PVC示例" class="headerlink" title="NFS PVC示例"></a>NFS PVC示例</h4><p>下面的配置清单（pvc-demo-0001.yaml）定义了一个名为pvc-nfs-demo的PVC资源示例，它仅定义了期望的存储空间范围、访问模式和卷模式以筛选集群上的PV资源。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-demo-0001</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteMany&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br></pre></td></tr></table></figure>

<p>显然，此前创建的两个PV资源中，pv-nfs-demo能够完全满足该PVC的筛选条件，因而创建示例清单中的资源后，它能够迅速绑定至PV之上，如下面的创建和资源查看命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-demo-0001.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-demo-0001 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc pvc-nfs-0001</span></span><br><span class="line">NAME   STATUS  VOLUME  CAPACITY  ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">pvc-demo-0001  Bound  pv-nfs-demo  5Gi      RWX                        3s</span><br></pre></td></tr></table></figure>

<p>被PVC资源pvc-demo-0001绑定的PV资源pv-nfs-demo的状态也将从Available转为Bound，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pv/pv-nfs-demo -o jsonpath=&#123;.status.phase&#125;</span></span><br><span class="line">Bound</span><br></pre></td></tr></table></figure>

<p>集群上的PV资源数量很多时，用户可通过指定多维度的过滤条件来缩小PV资源的筛选范围，以获取到最佳匹配。2. RBD PVC示例<br>下面这个定义在pvc-demo-0002.yaml中的配置清单定义了一个PVC资源，除了期望的访问模式、卷模型和存储空间容量边界之外，它使用了标签选择器来匹配PV资源的标签。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-demo-0002</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">usedof:</span> <span class="string">&quot;redisdata&quot;</span></span><br></pre></td></tr></table></figure>

<p>配置清单中的资源PVC/pvc-demo-0002特地为绑定此前创建的资源PV/pv-rbd-demo而创建，其筛选条件可由该PV完全满足，因而创建配置清单中的PVC/pvc-demo-0002资源后会即刻绑定于PV/pv-rbd-demo之上，如下面命令的结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-demo-0002.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-demo-0002 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-demo-0002</span></span><br><span class="line">NAME   STATUS  VOLUME  CAPACITY   ACCESS MODES  STORAGECLASS  AGE</span><br><span class="line">pvc-demo-0002   Bound   pv-rbd-demo  2Gi     RWO                       10s</span><br></pre></td></tr></table></figure>

<p>删除一个PVC将导致其绑定的PV资源转入Released状态，并由相应的回收策略完成资源回收。反过来，直接删除一个仍由某PVC绑定的PV资源，会由PVC保护机制延迟该删除操作至相关的PVC资源被删除。</p>
<h3 id="在Pod中使用PVC"><a href="#在Pod中使用PVC" class="headerlink" title="在Pod中使用PVC"></a>在Pod中使用PVC</h3><p>需要特别说明的是，PVC资源隶属名称空间级别，它仅可被同一名称空间中的Pod对象通过persistentVolumeClaim插件所引用并作为存储卷使用，该存储卷插件可嵌套使用如下两个字段。</p>
<ul>
<li>claimName：要调用的PVC存储卷的名称，PVC卷要与Pod在同一名称空间中。</li>
<li>readOnly：是否强制将存储卷挂载为只读模式，默认为false。<br>下面的配置清单（volumes-pvc-demo.yaml）定义了一个Pod资源，该配置清单将5.5.2节中直接使用RBD存储的Pod资源改为了调用指定的PVC存储卷。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">volumes-pvc-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">redis:alpine</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">6379</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redisport</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">  <span class="attr">volumes:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">redis-rbd-vol</span></span><br><span class="line">    <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">      <span class="attr">claimName:</span> <span class="string">pvc-demo-0002</span></span><br></pre></td></tr></table></figure>

<h3 id="存储类"><a href="#存储类" class="headerlink" title="存储类"></a>存储类</h3><p>存储类也是Kubernetes系统上的API资源类型之一，它位于storage.k8s.io群组中。存储类通常由集群管理员为管理PV资源之便而按需创建的存储资源类别（逻辑组），例如可将存储系统按照其性能高低或者综合服务质量级别分类（见图5-9）、依照备份策略分类，甚至直接按管理员自定义的标准分类等。存储类也是PVC筛选PV时的过滤条件之一，这意味着PVC仅能在其隶属的存储类之下找寻匹配的PV资源。不过，Kubernetes系统自身无法理解“类别”到底意味着什么，它仅仅把存储类中的信息当作PV资源的特性描述使用。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123100927132.png" alt="image-20220123100927132"></p>
<p>存储类的最重要功能之一便是对PV资源动态预配机制的支持，它可被视作动态PV资源的创建模板，能够让集群管理员从维护PVC和PV资源之间的耦合关系的束缚中解脱出来。需要用到具有持久化功能的存储卷资源时，用户只需要向满足其存储特性要求的存储类声明一个PVC资源，存储类将会根据该声明创建恰好匹配其需求的PV对象。</p>
<h4 id="StorageClass资源"><a href="#StorageClass资源" class="headerlink" title="StorageClass资源"></a>StorageClass资源</h4><p>StorageClass资源的期望状态直接与apiVersion、kind和metadata定义在同一级别而无须嵌套在spec字段中，它支持使用的字段包括如下几个。</p>
<ul>
<li>allowVolumeExpansion &lt;boolean&gt;：是否支持存储卷空间扩展功能。</li>
<li>allowedTopologies &lt;[]Object&gt;：定义可以动态配置存储卷的节点拓扑，仅启用了卷调度功能的服务器才会用到该字段；每个卷插件都有自己支持的拓扑规范，空的拓扑选择器表示无拓扑限制。</li>
<li>provisioner &lt;string&gt;：必选字段，用于指定存储服务方（provisioner，或称为预配器），存储类要基于该字段值来判定要使用的存储插件，以便适配到目标存储系统；Kubernetes内置支持许多的provisioner，它们的名字都以kubernetes.io/为前缀，例如kubernetes.io/glusterfs等。</li>
<li>parameters &lt;map[string]string&gt;：定义连接至指定的provisioner类别下的某特定存储时需要使用的各相关参数；不同provisioner的可用参数各不相同。</li>
<li>reclaimPolicy &lt;string&gt;：由当前存储类动态创建的PV资源的默认回收策略，可用值为Delete（默认）和Retain两个；但那些静态PV的回收策略则取决于它们自身的定义。</li>
<li>volumeBindingMode &lt;string&gt;：定义如何为PVC完成预配和绑定，默认值为Volume-BindingImmediate；该字段仅在启用了存储卷调度功能时才能生效。</li>
<li>mountOptions &lt;[]string&gt;：由当前类动态创建的PV资源的默认挂载选项列表。</li>
</ul>
<p>下面是一个定义在storageclass-rbd-demo.yaml配置文件中的StorageClass资源清单，它定义了一个以Ceph存储系统的RBD接口为后端存储的StorageClass资源fast-rbd，因此，其存储预配标识为kubernetes.io/rbd。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">fast-rbd</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">monitors:</span> <span class="string">ceph01.ilinux.io:6789,ceph02.ilinux.io:6789</span></span><br><span class="line">  <span class="attr">adminId:</span> <span class="string">admin</span></span><br><span class="line">  <span class="attr">adminSecretName:</span> <span class="string">ceph-admin-secret</span></span><br><span class="line">  <span class="attr">adminSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">pool:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userId:</span> <span class="string">kube</span></span><br><span class="line">  <span class="attr">userSecretName:</span> <span class="string">ceph-kube-secret</span></span><br><span class="line">  <span class="attr">userSecretNamespace:</span> <span class="string">kube-system</span></span><br><span class="line">  <span class="attr">fsType:</span> <span class="string">ext4</span></span><br><span class="line">  <span class="attr">imageFormat:</span> <span class="string">&quot;2&quot;</span></span><br><span class="line">  <span class="attr">imageFeatures:</span> <span class="string">&quot;layering&quot;</span></span><br><span class="line"><span class="attr">reclaimPolicy:</span> <span class="string">Retain</span></span><br></pre></td></tr></table></figure>

<p>不同的provisioner的parameters字段中可嵌套使用的字段各有不同，上面示例中Ceph RBD存储服务可使用的各字段及意义如下。</p>
<ul>
<li>monitors &lt;string&gt;：Ceph存储系统的监视器访问接口，多个套接字间以逗号分隔。</li>
<li>adminId &lt;string&gt;：有权限在指定的存储池中创建image的管理员用户名，默认为admin。</li>
<li>adminSecretName &lt;string&gt;：存储有管理员账号认证密钥的Secret资源名称。</li>
<li>adminSecretNamespace &lt;string&gt;：管理员账号相关的Secret资源所在的名称空间。</li>
<li>pool &lt;string&gt;：Ceph存储系统的RBD存储池名称，默认为rbd。</li>
<li>userId &lt;string&gt;：用于映射RBD镜像的Ceph用户账号，默认同adminId字段。</li>
<li>userSecretName &lt;string&gt;：存储有用户账号认证密钥的Secret资源名称。</li>
<li>userSecretNamespace &lt;string&gt;：用户账号相关的Secret资源所在的名称空间。</li>
<li>fsType &lt;string&gt;：存储映像格式化的文件系统类型，默认为ext4。</li>
<li>imageFormat &lt;string&gt;：存储映像的格式，其可用值仅有“1”和“2”，默认值为“2”。</li>
<li>imageFeatures &lt;string&gt;：“2”格式的存储映像支持的特性，目前仅支持layering，默认为空值，并且不支持任何功能。</li>
</ul>
<p>存储类接入其他存储系统时使用的参数请参考<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/storage/storage-classes/%E3%80%82">https://kubernetes.io/docs/concepts/storage/storage-classes/。</a><br><font color="red">与Pod或PV资源上的RBD卷插件配置格式不同的是，StorageClass上的RBD供给者参数不支持使用keyring直接认证到Ceph，它仅能引用Secret资源中存储的认证密钥完成认证操作。</font>因而，我们需要先将Ceph用户admin和kube的认证密钥分别创建为Secret资源对象。</p>
<ul>
<li>1）在Ceph管理节点上分别获取admin和kube的认证密钥，不同Ceph集群上的输出结果应该有所不同：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">ceph auth get-key client.admin</span> </span><br><span class="line">AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">ceph auth get-key client.kube</span></span><br><span class="line">AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA=</span><br></pre></td></tr></table></figure>

<ul>
<li>2）在Kubernetes集群管理客户端上使用kubectl命令分别将二者创建为Secret资源，在具体测试操作中，需要将其中的密钥分别替换为前一步中的命令输出结果：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic ceph-admin-secret --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/rbd&quot;</span> \</span></span><br><span class="line"><span class="language-bash">  --from-literal=key=<span class="string">&#x27;AQAl+Ite/2/cBBAA8yRfa6p1VwLKcywcEMS7YA==&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">  --namespace=kube-system</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create secret generic ceph-kube-secret --<span class="built_in">type</span>=<span class="string">&quot;kubernetes.io/rbd&quot;</span> \</span></span><br><span class="line"><span class="language-bash">  --from-literal=key=<span class="string">&#x27;AQB9+4teoywxFxAAr2d63xPmV3Yl/E2ohfgOxA==&#x27;</span> \</span></span><br><span class="line"><span class="language-bash">  --namespace=kube-system</span></span><br></pre></td></tr></table></figure>

<p>示例中使用的账号及存储池的管理方式请参考5.4节和5.5节给出的步骤。待相关Secret资源准备完成后，将示例清单中的StorageClass资源创建在集群上，即可由PVC或PV资源将其作为存储类</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f storageclass-rbd-demo.yaml</span> </span><br><span class="line">storageclass.storage.k8s.io/fast-rbd created</span><br></pre></td></tr></table></figure>

<p>我们还可以使用kubectl get sc/NAME命令打印存储类的相关信息，或者使用kubectl describe sc NAME命令获取详细描述来进一步了解其状态。2. PV动态预配<br>动态PV预配功能的使用有两个前提条件：支持动态PV创建功能的卷插件，以及一个使用了对应于该存储卷插件的后端存储系统的StorageClass资源。不过，Kubernetes并非内置支持所有的存储卷插件的PV动态预配功能，具体信息如图5-10所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123101116752.png" alt="image-20220123101116752"></p>
<p>RBD存储卷插件，结合5.5.4节中定义关联至Ceph RBD存储系统接口的存储类资源fast-rbd就能实现PV的动态预配功能，用户于该存储类中创建PVC资源后，运行于kube-controller-manager守护进程中的PV控制器会根据fast-rbd存储类的定义接入Ceph存储系统创建出相应的存储映像，并在自动创建一个关联至该存储映像的PV资源后，将其绑定至PVC资源。<br>动态PV预配的过程中，PVC控制器会调用相关存储系统的管理接口API或专用的客户端工具来完成后端存储系统上的存储组件管理。以Ceph RBD为例，PV控制器会以存储类参数adminId中指定的用户身份调用rbd命令创建存储映像。然而，以kubeadm部署且运行为静态Pod资源的kube-controller-manager容器并未自行附带此类工具，如ceph-common程序包。常见的解决方案有3种：在Kubernetes系统上部署kubernetes-incubator/external-storage中的rbd-provisioner，从而以外置的方式提供相关工具程序，或基于CSI卷插件使用ceph-csi项目来支持更加丰富的卷功能，或定制kube-controller-manager的容器镜像，为其安装ceph-common程序包。本节将给出第三种方式的实现过程。提示<br>若以二进制程序包部署Kubernetes集群，则直接在Master节点安装ceph-common就能解决问题。<br>首先，我们使用如下的Dockerfile文件，并基于现有kube-controller-manager镜像文件为其额外安装ceph-common程序包，随后重新打包为容器镜像。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">ARG KUBE_VERSION=&quot;v1.19.0&quot;</span><br><span class="line"></span><br><span class="line">FROM registry.aliyuncs.com/google_containers/kube-controller-manager:$&#123;KUBE_VERSION&#125;</span><br><span class="line"></span><br><span class="line">RUN apt update &amp;&amp; apt install -y wget  gnupg lsb-release</span><br><span class="line"></span><br><span class="line">ARG CEPH_VERSION=&quot;octopus&quot;</span><br><span class="line">RUN wget -q -O - https://mirrors.aliyun.com/ceph/keys/release.asc | apt-key </span><br><span class="line">add - &amp;&amp; \</span><br><span class="line">     echo deb https://mirrors.aliyun.com/ceph/debian-$&#123;CEPH_VERSION&#125;/ $(lsb_</span><br><span class="line">     release -sc) main &gt; /etc/apt/sources.list.d/ceph.list &amp;&amp; \</span><br><span class="line">     apt update &amp;&amp; \</span><br><span class="line">     apt install -y ceph-common ceph-fuse</span><br><span class="line"></span><br><span class="line">RUN rm -rf /var/lib/apt/lists/* /var/cache/apt/archives/*</span><br></pre></td></tr></table></figure>

<p>将上面的内容保存于某专用目录下（例如kube-controller-manager）的名为Dockerfile的文件中，而后使用如下命令将其打包为镜像即可。其中，构建时参数KUBE_VERSION和CEPH_VERSION可分别修改为适用的版本。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="built_in">cd</span> kube-controller-manager</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">docker image build . --build-args KUBE_VERSION= <span class="string">&quot;v1.19.0&quot;</span> \</span></span><br><span class="line"><span class="language-bash">    --build-args CEPH_VERSION=“octopus”\</span></span><br><span class="line"><span class="language-bash">    -t ikubernetes/kube-controller-manager:v1.19.0</span></span><br></pre></td></tr></table></figure>

<p>而后，将该镜像分发至各Master节点，并分别修改它们的/etc/kubernetes/manifests/kube-controller-manager.yaml配置清单中的容器镜像为定制的镜像ikubernetes/kube-controller-manager:v1.19.0，待Controller Manager相关的Pod自动重启后即可进行动态PV的创建测试。下面是定义于pvc-dyn-rbd-demo.yaml配置清单中的PVC资源，它向存储类fast-rbd声明了需要的存储空间及访问模式。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-dyn-rbd-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">3Gi</span></span><br><span class="line">    <span class="attr">limits:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">10Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">fast-rbd</span></span><br></pre></td></tr></table></figure>

<p>将示例清单中的PVC资源创建至Kubernetes集群之上，便会触发PV控制器在指定的存储类中自动创建匹配的PV资源。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-dyn-rbd-demo.yaml</span></span><br><span class="line">persistentvolumeclaim/pvc-dyn-rbd-demo created</span><br></pre></td></tr></table></figure>

<p>下面的命令显示出该PVC资源已经绑定到了一个名为pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce的PV之上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-dyn-rbd-demo -o jsonpath=&#123;.spec.volumeName&#125;</span></span><br><span class="line">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span><br></pre></td></tr></table></figure>

<p>如下命令输出的该PV的详细描述之中，Annotations中的kubernetes.io/createdby: rbd-dynamic-provisioner表示它是由rbd-dynamic-provisioner动态创建，而Source段中的信息更能印证这种结论。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">describe</span> <span class="string">pv</span> <span class="string">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span></span><br><span class="line"><span class="attr">Name:</span>            <span class="string">pvc-6c4b09cd-a74b-4b53-b106-7b16a98cf8ce</span></span><br><span class="line"><span class="attr">Labels:</span>          <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Annotations:     kubernetes.io/createdby:</span> <span class="string">rbd-dynamic-provisioner</span></span><br><span class="line">                 <span class="attr">pv.kubernetes.io/bound-by-controller:</span> <span class="literal">yes</span></span><br><span class="line">                 <span class="attr">pv.kubernetes.io/provisioned-by:</span> <span class="string">kubernetes.io/rbd</span></span><br><span class="line"><span class="attr">Finalizers:</span>      [<span class="string">kubernetes.io/pv-protection</span>]</span><br><span class="line"><span class="attr">StorageClass:</span>    <span class="string">fast-rbd</span></span><br><span class="line"><span class="attr">Status:</span>          <span class="string">Bound</span></span><br><span class="line"><span class="attr">Claim:</span>           <span class="string">default/pvc-sc-rbd-demo</span></span><br><span class="line"><span class="attr">Reclaim Policy:</span>  <span class="string">Delete</span>      <span class="comment"># 回收策略</span></span><br><span class="line"><span class="attr">Access Modes:</span>    <span class="string">RWO</span>         <span class="comment"># 访问模式</span></span><br><span class="line"><span class="attr">VolumeMode:</span>      <span class="string">Filesystem</span>  <span class="comment"># 卷模式</span></span><br><span class="line"><span class="attr">Capacity:</span>        <span class="string">3Gi</span>         <span class="comment"># 卷空间容量</span></span><br><span class="line"><span class="attr">Node Affinity:</span>   <span class="string">&lt;none&gt;</span></span><br><span class="line"><span class="attr">Message:</span>         </span><br><span class="line"><span class="attr">Source:</span>  <span class="comment"># 数据源标识</span></span><br><span class="line">    <span class="attr">Type:</span>          <span class="string">RBD</span> <span class="string">(a</span> <span class="string">Rados</span> <span class="string">Block</span> <span class="string">Device</span> <span class="string">mount</span> <span class="string">on</span> <span class="string">the</span> <span class="string">host</span> <span class="string">that</span> <span class="string">shares</span> <span class="string">a</span> </span><br><span class="line">    <span class="string">pod&#x27;s</span> <span class="string">lifetime)</span></span><br><span class="line">    <span class="attr">CephMonitors:</span>  [<span class="string">ceph01.ilinux.io:6789</span> <span class="string">ceph02.ilinux.io:6789</span> <span class="string">ceph03.ilinux.</span></span><br><span class="line">    <span class="string">io:6789</span>]</span><br><span class="line">    <span class="attr">RBDImage:</span>      <span class="string">kubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e</span></span><br><span class="line">    <span class="attr">FSType:</span>        <span class="string">ext4</span></span><br><span class="line">    <span class="attr">RBDPool:</span>       <span class="string">kube</span></span><br><span class="line">    <span class="attr">RadosUser:</span>     <span class="string">kube</span></span><br><span class="line">    <span class="attr">Keyring:</span>       <span class="string">/etc/ceph/keyring</span></span><br><span class="line">    <span class="attr">SecretRef:</span>     <span class="string">&amp;SecretReference&#123;Name:ceph-kube-secret,Namespace:kube-system,&#125;</span></span><br><span class="line">    <span class="attr">ReadOnly:</span>      <span class="literal">false</span></span><br><span class="line"><span class="attr">Events:</span>            <span class="string">&lt;none&gt;</span></span><br></pre></td></tr></table></figure>

<p>上面命令结果中显示出，该PV的容量、访问模式和卷模式均符合PVC所声明的要求，并且能够通过下面的命令验证相关的存储映像已经存在于Ceph存储集群之上：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">rbd <span class="built_in">ls</span> -p kube</span></span><br><span class="line">kubernetes-dynamic-pvc-9a016d53-df1b-4118-9cf4-545ac058441e</span><br></pre></td></tr></table></figure>

<p>另外，该PV继承自存储类fast-rbd中的回收策略为Delete，这也是存储类默认使用的回收策略，因此，删除其绑定的PVC对象也将删除该PV对象。对于多数持久存储场景而言，这可能是存在着一定风险的策略，建议定义存储类时手动修改该策略。感兴趣的读者可自行测试这种级联删除的效果。</p>
<h2 id="容器存储接口CSI"><a href="#容器存储接口CSI" class="headerlink" title="容器存储接口CSI"></a>容器存储接口CSI</h2><p>存储卷管理器通过调用存储卷插件实现当前节点上存储卷相关的附加、分离、挂载/卸载等操作，对于未被Kubernetes内置（In-Tree）的卷插件所支持的存储系统或服务来说，扩展定义新的卷插件是解决问题的唯一途径。但将存储供应商提供的第三方存储代码打包到Kubernetes的核心代码可能会导致可靠性及安全性方面的问题，因而这就需要一种简单、便捷的、外置于Kubernetes代码树（Out-Of-Tree）的扩展方式，FlexVolume和CSI（容器存储接口）就是这样的存储卷插件，换句话说，它们自身是内置的存储卷插件，但实现的却是第三方存储卷的扩展接口。</p>
<h3 id="CSI基础"><a href="#CSI基础" class="headerlink" title="CSI基础"></a>CSI基础</h3><p>FlexVolume是Kubernetes自v1.8版本进入GA（高可用）阶段的一种存储插件扩展方式，它要求将外部插件的二进制文件部署在预先配置的路径中（例如/usr/libexec/kubernetes/kubelet-plugins/volume/exec/），并设定系统环境满足其正常运行所需要的全部依赖关系。事实上，一个FlexVolume类型的插件就是一款可被kubelet驱动的可执行文件，它实现了特定存储的挂载、卸载等存储插件接口，而对该类插件的调用相当于请求运行该程序文件，并要求返回JSON格式的响应内容。<br>第三方需要提供的CSI组件主要是两个CSI存储卷驱动程序，一个是节点插件（Identity+Node），用于同kubelet交互实现存储卷的挂载和卸载等功能，另一个是自定义控制器（Identity+Controller），负责处理来自API Server的存储卷管理请求，例如创建和删除等，它的功能类似于控制器管理器中的PV控制器，如图5-11中实线的圆角方框所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220207105317056.png" alt="image-20220207105317056"></p>
<p>kubelet对存储卷的挂载和卸载操作将通过UNIX Socket调用在同一主机上运行的外部CSI卷驱动程序完成。初始化外部CSI卷驱动程序时，kubelet必须调用CSI方法NodeGetInfo才能将Kubernetes的节点名称映射为CSI的节点标识（NodeID）。于是，为了降低部署外部容器化的CSI卷驱动程序时的复杂度，Kubernetes团队提供了一个以Sidecar容器运行的应用——Kubernetes CSI Helper，以辅助自动完成UNIX Sock套接字注册及NodeID的初始化，如图5-11中的node-driver-registrar容器所示。<br>不受Kubernetes信任的第三方卷驱动程序运行为独立的容器，它无法直接同控制器管理器通信，而是要借助于Kubernetes API Server进行；换句话说，CSI存储卷驱动需要注册监视（watch）API Server上的特定资源并针对存储卷管理器面向其存储卷的请求执行预配、删除、附加和分离等操作。同样为了降低外部容器化CSI卷驱动及控制器程序部署的复杂度，Kubernetes团队提供了一到多个以Sidecar容器运行的代理应用Kubernetes to CSI来负责监视Kubernetes API，并触发针对CSI卷驱动程序容器的相应操作，如图5-11中的external-attacher和external-privisioner等，它们各自的简要功能如下所示。</p>
<ul>
<li>external-privisioner：CSI存储卷的创建和删除。</li>
<li>external-attacher：CSI存储卷的附加和分离。</li>
<li>external-resizer：CSI存储卷的容量调整（扩缩容）。</li>
<li>external-snapshotter：CSI存储卷的快照管理（创建和删除等）。</li>
</ul>
<p>尽管Kubernetes并未指定CSI卷驱动程序的打包标准，但它提供了以下建议，以简化容器化CSI卷驱动程序的部署。</p>
<blockquote>
<p>1）创建一个独立CSI卷驱动容器镜像，由其实现存储卷插件的标准行为，并在运行时通过UNIX Socket公开其API。<br>2）将控制器级别的各辅助容器（external-privisioner和external-attacher等）以Sidecar的形式同带有自定义控制器功能的CSI卷驱动程序容器运行在同一个Pod中，而后借助StatefulSet或Deployment控制器资源确保各辅助容器可正常运行相应数目的实例副本，将负责各容器间通信的UNIX Socket存储到共享的emptyDir存储卷上。<br>3）将节点上需要的辅助容器node-driver-registrar以Sidecar的形式与运行CSI卷驱动程序的容器运行在同一Pod中，而后借助DaemonSet控制器资源确保辅助容器可在每个节点上运行一个实例。<br>下一节将以Longhorn存储系统为例简单说明CSI卷插件解决方案的部署及简单使用方式。</p>
</blockquote>
<h3 id="Longhorn存储系统"><a href="#Longhorn存储系统" class="headerlink" title="Longhorn存储系统"></a>Longhorn存储系统</h3><p> Longhorn是由Rancher实验室创建的一款云原生的、轻量级、可靠且易用的开源分布式块存储系统，后来由CNCF孵化。它借助CSI存储卷插件以外置的存储解决方案形式运行。Longhorn遵循微服务的原则，利用容器将小型独立组件构建为分布式块存储，并使用编排工具来协调这些组件，从而形成弹性分布式系统。部署到Kubernetes集群上之后，Longhorn会自动将集群中所有节点上可用的本地存储（默认为/var/lib/longhorn/目录所在的设备）聚集为存储集群，而后利用这些存储管理分布式、带有复制功能的块存储，且支持快照及数据备份操作。<br> 面向现代云环境设计的存储系统的控制器随着待编排存储卷数量的急速增加也变得高度复杂。为了摆脱这种困境，Longhorn充分利用了近年来关于如何编排大量容器的关键技术，采用微服务的设计模式，将大型复杂的存储控制器切分为每个存储卷一个专用的、小型存储控制器，而后借助现代编排工具来管理这些控制器，从而将每个CSI卷构建为一个独立的微服务。如图5-12所示的存储架构中，3个Pod分别使用了一个Longhorn存储卷，每个卷有一个专用的控制器（Engine）资源和两个副本（Replica）资源，它们都是为了便于描述其应用而由Longhorn引入的自定义资源类型。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123101611997.png" alt="image-20220123101611997"></p>
<p>Engine容器仅负责单个存储卷的管理，其生命周期与存储卷相同，因而它并非真正的CSI插件级别的卷控制器或节点插件。Longhorn上负责处理来自Kubernetes CSI卷插件的API调用，以及完成存储卷管理的组件是Longhorn Manager（node-driver-registrar），它是一个容器化应用且受DaemonSet控制器资源编排，在Kubernetes集群的每个节点上运行一个副本。Longhorn Manager持续监视Kubernetes API上与Longhorn存储卷相关的资源变动，一旦发现新的资源创建，它负责在该卷附加的节点（即Pod被Kubernetes调度器绑定的目标节点）上创建一个Engine资源对象，并在副本相关的每个目标节点上相应创建一个Replica资源对象。<br>Kubernetes集群内部通过CSI插件接口调用Longhorn插件以管理相关类型的存储卷，而Longhorn存储插件则基于Longhorn API与Longhorn Manager进行通信，卷管理之外的其他功能则要依赖Longhorn UI完成，例如快照、备份、节点和磁盘的管理等。另外，Longhorn的块设备存储卷的实现建立在iSCSI协议之上，因而需要调用Longhorn存储卷的Pod所在节点必须部署了相关的程序包，例如open-iscsi或iscsiadm等。<br>目前版本（v1.0.1）的Longhorn要求运行于v.1.13或更高版本的Docker环境下，以及v.1.4或更高版本的Kubernetes之上，并且要求各节点部署了open-iscsi、curl、findmnt、grep、awk、blkid和lsblk等程序包。基础环境准备完成后，我们使用类似如下的命令即能完成Longhorn应用的部署。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f \</span></span><br><span class="line"><span class="language-bash">    https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml</span></span><br></pre></td></tr></table></figure>

<p>该部署清单会在默认的longhorn-system名称空间下部署csi-attacher、csi-provisioner、csi-resizer、engine-image-ei、longhorn-csi-plugin和longhorn-manager等应用相关的Pod对象，待这些Pod对象成功转为Running状态之后即可测试使用Longhorn CSI插件。<br>该部署清单还会默认创建如下面资源清单中定义的名为longhorn的StorageClass资源，它以部署好的Longhorn为后端存储系统，支持存储卷动态预配机制。我们也能够以类似的方式定义基于该存储系统的、使用了不同配置的其他StorageClass资源，例如仅有一个副本以用于测试场景或对数据可靠性要求并非特别高的应用等。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">StorageClass</span>               <span class="comment"># 资源类型</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">storage.k8s.io/v1</span>    <span class="comment"># API群组及版本</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">longhorn</span></span><br><span class="line"><span class="attr">provisioner:</span> <span class="string">driver.longhorn.io</span>  <span class="comment"># 存储供给驱动</span></span><br><span class="line"><span class="attr">allowVolumeExpansion:</span> <span class="literal">true</span>       <span class="comment"># 是否支持存储卷弹性扩缩容</span></span><br><span class="line"><span class="attr">parameters:</span></span><br><span class="line">  <span class="attr">numberOfReplicas:</span> <span class="string">&quot;3&quot;</span>          <span class="comment"># 副本数量</span></span><br><span class="line">  <span class="attr">staleReplicaTimeout:</span> <span class="string">&quot;2880&quot;</span>    <span class="comment"># 过期副本超时时长</span></span><br><span class="line">  <span class="attr">fromBackup:</span> <span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>随后，我们随时可以按需创建基于该存储类的PVC资源来使用Longhorn存储系统上的持久存储卷提供的存储空间。下面的示例资源清单（pvc-dyn-longhorn-demo.yaml）便定义了一个基于Longhorn存储类的PVC，它请求使用2GB的空间。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolumeClaim</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pvc-dyn-longhorn-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">volumeMode:</span> <span class="string">Filesystem</span></span><br><span class="line">  <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">      <span class="attr">storage:</span> <span class="string">2Gi</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">longhorn</span></span><br></pre></td></tr></table></figure>

<p>如前所述，Longhorn存储设备支持动态预配，于是以默认创建的存储类Longhorn为模板的PVC在无满足其请求条件的PV时，可由控制器自动创建出适配的PV卷来。下面两条命令及结果也反映了这种预配机制。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pvc-dyn-longhorn-demo.yaml</span> </span><br><span class="line">persistentvolumeclaim/pvc-dyn-longhorn-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pvc/pvc-dyn-longhorn-demo</span></span><br><span class="line">NAME                    STATUS  VOLUME                                 CAPACITY…</span><br><span class="line">pvc-dyn-longhorn-demo   Bound   pvc-c67415ae-560b-49c7-8515-3467f4160794   2Gi…</span><br></pre></td></tr></table></figure>

<p>对于每个存储卷，Longhorn存储系统都会使用自定义的Volumes类型资源对象维持及跟踪其运行状态，每个Volumes资源都会有一个Engines资源对象作为其存储控制器，如下面的两个命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get volumes -n longhorn-system</span></span><br><span class="line">NAME                                  AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794   90s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get engines -n longhorn-system</span></span><br><span class="line">NAME                                            AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204   2m10s</span><br></pre></td></tr></table></figure>

<p>Engines资源对象的详细描述或资源规范中的spec和status字段记录有当前资源的详细信息，包括关联的副本、purge状态、恢复状态和快照信息等，为了节约篇幅，下面的命令仅给出了部分运行结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe engines pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \</span></span><br><span class="line"><span class="language-bash">          -n longhorn-system</span></span><br><span class="line">……</span><br><span class="line">Spec:</span><br><span class="line">  Backup Volume:     </span><br><span class="line">  Desire State:      stopped</span><br><span class="line">  Disable Frontend:  false</span><br><span class="line">  Engine Image:      longhornio/longhorn-engine:v1.0.1</span><br><span class="line">  Frontend:          blockdev</span><br><span class="line">  Log Requested:     false</span><br><span class="line">  Node ID:               # 绑定的节点，它必须与调用了该存储卷的Pod运行于同一节点</span><br><span class="line">  Replica Address Map:   # 关联的存储卷副本</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3:  10.244.3.58:10000</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050:  10.244.2.53:10000</span><br><span class="line">    pvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db:  10.244.1.61:10000</span><br><span class="line">  Volume Name:  pvc-c67415ae-560b-49c7-8515-3467f4160794</span><br><span class="line">  Volume Size:  2147483648</span><br></pre></td></tr></table></figure>

<p>Replicas也是Longhorn提供的一个独立资源类型，每个资源对象对应着一个存储卷副本，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get replicas -n longhorn-system</span></span><br><span class="line">NAME                                         AGE</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-4e2755e3   2m36s</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-ba483050   2m36s</span><br><span class="line">pvc-c67415ae-560b-49c7-8515-3467f4160794-r-daccc0db   2m36s</span><br></pre></td></tr></table></figure>

<p>基于Longhorn存储卷的PVC被Pod引用后，Pod所在的节点便是该存储卷Engine对象运行所在的节点，Engine的状态也才会由Stopped转为Running。示例清单volumes-pvc-longhorn-demo.yaml定义了一个调用pvc/pvc-dyn-longhorn-demo资源的Pod资源，因而该Pod所在的节点便是该PVC后端PV相关的Engine绑定的节点，如下面3个命令及其结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f volumes-pvc-longhorn-demo.yaml</span> </span><br><span class="line">pod/volumes-pvc-longhorn-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/volumes-pvc-longhorn-demo -o jsonpath=<span class="string">&#x27;&#123;.spec.nodeName&#125;&#x27;</span></span></span><br><span class="line">k8s-node03.ilinux.io</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get engines/pvc-c67415ae-560b-49c7-8515-3467f4160794-e-eb822204 \</span></span><br><span class="line"><span class="language-bash">    -n longhorn-system -o jsonpath=<span class="string">&#x27;&#123;.spec.nodeID&#125;&#x27;</span></span></span><br><span class="line">k8s-node03.ilinux.io</span><br></pre></td></tr></table></figure>

<p>由以上Longhorn存储系统的部署及测试结果可知，该存储系统不依赖于任何外部存储设备，仅基于Kubernetes集群工作节点本地的存储即能正常提供存储卷服务，且支持动态预配等功能。但应用于生产环境时，还是有许多步骤需要优化，例如将数据存储与操作系统等分离到不同的磁盘设备，是否可以考虑关闭底层的RAID设备等，具体请参考Longhorn文档中的最佳实践。<br>为了便于通过Kubernetes集群外部的浏览器访问该用户接口，我们需要把相关的Service对象的类型修改为NodePort。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch svc/longhorn-frontend -p <span class="string">&#x27;&#123;&quot;spec&quot;:&#123;&quot;type&quot;:&quot;NodePort&quot;&#125;&#125;&#x27;</span> -n longhorn-system</span></span><br><span class="line">service/longhorn-frontend patched</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get svc/longhorn-frontend -n longhorn-system -o jsonpath=<span class="string">&#x27;&#123;.spec.ports[0].nodePort&#125;&#x27;</span></span></span><br><span class="line">30180</span><br></pre></td></tr></table></figure>

<p>随后，我们经由任意一个节点的IP地址节点端口（例如上面命令中自动分配而来的30180）即可访问该UI，如图5-13所示。节点、存储卷、备份和系统设置导航标签各自给出了相关功能的配置入口，感兴趣的读者可自行探索其使用细节。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%AD%98%E5%82%A8%E5%8D%B7%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%8C%81%E4%B9%85%E5%8C%96/image-20220123102448325.png" alt="image-20220123102448325"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/" class="post-title-link" itemprop="url">Service和服务发现</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-02-09 19:48:21 / 修改时间：19:49:31" itemprop="dateCreated datePublished" datetime="2022-02-09T19:48:21+08:00">2022-02-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="标签与标签选择器"><a href="#标签与标签选择器" class="headerlink" title="标签与标签选择器"></a>标签与标签选择器</h1><p>它是附加在Kubernetes任何资源对象之上的键值型数据，常用于标签选择器的匹配度检查，从而完成资源筛选。Kubernetes系统的部分基础功能的实现也要依赖标签和标签选择器，例如Service筛选并关联后端Pod对象，由ReplicaSet、StatefulSet和DaemonSet等控制器过滤并关联后端Pod对象等，从而提升用户的资源管理效率。</p>
<h2 id="资源标签"><a href="#资源标签" class="headerlink" title="资源标签"></a>资源标签</h2><p>标签可在资源创建时直接指定，也可随时按需添加在活动对象上。一个对象可拥有不止一个标签，而同一个标签也可添加至多个对象之上。下面是较为常用的标签。</p>
<ul>
<li>版本标签：”release” : “stable”，”release” : “canary”，”release” : “beta”。</li>
<li>环境标签：”environment” : “dev”，”environment” : “qa”，”environment” : “prod”。</li>
<li>应用标签：”app” : “ui”，”app” : “as”，”app” : “pc”，”app” : “sc”。</li>
<li>架构层级标签：”tier” : “frontend”，”tier” : “backend”，”tier” : “cache”。</li>
<li>分区标签：”partition” : “customerA”，”partition” : “customerB”。</li>
<li>品控级别标签：”track” : “daily”，”track” : “weekly”。</li>
</ul>
<p>标签中的<font color="red">键名称通常由“键前缀”和“键名”组成，其格式形如KEY_PREFIX/KEY_NAME，键前缀为可选部分。键名至多能使用63个字符，支持字母、数字、连接号（-）、下划线（）、点号（.）等字符，且只能以字母或数字开头。而键前缀必须为DNS子域名格式，且不能超过253个字符。省略键前缀时，键将被视为用户的私有数据。由Kubernetes系统组件或第三方组件自动为用户资源添加的键必须使用键前缀，kubernetes.io/和k8s.io/前缀预留给了Kubernetes的核心组件使用，例如Node对象上常用的kubernetes.io/os、kubernetes.io/arch和kubernetes.io/hostname等。<br><font color="red">标签的键值必须不能多于63个字符，键值要么为空，要么以字母或数字开头及结尾，且中间只能使用字母、数字、连接号（-）、下划线（）或点号（.）等字符。</font></font></p>
<h3 id="创建资源时定义标签"><a href="#创建资源时定义标签" class="headerlink" title="创建资源时定义标签"></a>创建资源时定义标签</h3><p>创建资源时，可直接在其metadata中嵌套使用labels字段定义要附加的标签项。例如在下面的Namespace资源配置清单文件中，示例ns-with-labels.yaml中使用了两个标签，env=dev和app=eshop。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Namespace</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eshop</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">eshop</span></span><br><span class="line">    <span class="attr">env:</span> <span class="string">dev</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">finalizers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">kubernetes</span></span><br></pre></td></tr></table></figure>

<ul>
<li>可在kubectl get namespaces命令中使用–show-labels选项，以额外显示对象的标签信息。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f ns-with-labels.yaml</span> </span><br><span class="line">namespace/eshop created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop --show-labels</span></span><br><span class="line">NAME  STATUS   AGE   LABELS</span><br><span class="line">demoapp   Active      11s   app=eshop,env=dev</span><br></pre></td></tr></table></figure>

<ul>
<li>kubectl get命令上使用-L key1,key2,…选项可指定有特定键的标签信息。例如，仅显示eshop名称空间上的env和app标签：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop -L <span class="built_in">env</span>,app</span></span><br><span class="line">NAME    STATUS   AGE   ENV   APP</span><br><span class="line">eshop       Active     89s    dev   eshop</span><br></pre></td></tr></table></figure>

<ul>
<li>kubectl label命令可直接管理活动对象的标签，以按需进行添加或修改等操作。例如为eshop名称空间添加release=beta标签：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop release=beta</span></span><br><span class="line">namespace/eshop labeled</span><br></pre></td></tr></table></figure>

<ul>
<li>已经附带了指定键名的标签，使用kubectl label为其设定新的键值时需同时使用–overwrite命令，强制覆盖原有键值。例如，将eshop名称空间的release标签值修改为canary：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop release=canary --overwrite</span></span><br><span class="line">namespace/eshop labeled</span><br></pre></td></tr></table></figure>

<ul>
<li>删除活动对象上的标签时同样要使用kubectl label命令，但仅需要指定标签名称并紧跟一个减号“–”，例如，下面的命令首先删除eshop名称空间中的env标签，而后显示其现有的所有标签：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl label namespaces/eshop <span class="built_in">env</span>-</span></span><br><span class="line">namespace/eshop labeled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces eshop --show-labels</span> </span><br><span class="line">NAME    STATUS   AGE     LABELS</span><br><span class="line">eshop   Active   6m46s   app=eshop,release=beta</span><br></pre></td></tr></table></figure>

<h2 id="标签选择器"><a href="#标签选择器" class="headerlink" title="标签选择器"></a>标签选择器</h2><p>标签选择器用于表达标签的查询条件或选择标准，目前Kubernetes API支持两个选择器：基于等值关系（equality-based）的标签选项器与基于集合关系（set-based）的标签选择器。在指定多个选择器时需要以逗号分隔，<font color="red">各选择器之间遵循逻辑“与”，即必须要满足所有条件，而且空值的选择器将不选择任何对象。</font></p>
<ul>
<li>基于等值关系的标签选择器</li>
</ul>
<p>可用操作符有=、==和!=，其中前两个意义相同，都表示“等值”关系，最后一个表示“不等”。例如env=dev和env!=prod都是基于等值关系的选择器。</p>
<ul>
<li>基于集合的标签选择器</li>
</ul>
<p>根据标签名的一组值进行筛选，它支持in、notin和exists这3种操作符，例如tier in (frontend,backend)表示所有包含tier标签且其值为frontend或backend的资源对象。</p>
<ul>
<li>kubectl get命令的“-l”选项能够指定使用标签选择器筛选目标资源，例如，如下命令显示标签release的值不等于beta，且标签app的值等于eshop的所有名称空间：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get namespaces -l <span class="string">&#x27;release!=beta,app=eshop&#x27;</span>  -L app,release</span></span><br><span class="line">NAME      STATUS      AGE   APP     RELEASE</span><br><span class="line">eshop     Active      60m   eshop   canary</span><br></pre></td></tr></table></figure>

<ul>
<li>基于集合关系的标签选择器用于基于一组值进行过滤，它支持in、notin和exists 3种操作符，各操作符的使用格式及意义如下。<ul>
<li>KEY in (VALUE1,VALUE2,…)：指定键名的值存在于给定的列表中即满足条件。</li>
<li>KEY notin (VALUE1,VALUE2,…)：指定键名的值不存在于给定列表中即满足条件。</li>
<li>KEY：所有存在此键名标签的资源。</li>
<li>!KEY：所有不存在此键名标签的资源。</li>
<li>例如，下面的命令可以过滤出标签键名release的值为beta或canary的所有Namespace对象：</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">~$ kubectl get namespaces -l &#x27;release in (beta,canary)&#x27; -L release</span><br><span class="line">NAME       STATUS     AGE   RELEASE</span><br><span class="line">eshop      Active     63m   canary</span><br></pre></td></tr></table></figure>

<ul>
<li>再如，下面的命令可以列出集群中拥有node-role.kubernetes.io标签的各Node对象：</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get nodes -l <span class="string">&#x27;node-role.kubernetes.io/master&#x27;</span> -L kubernetes.io/hostname</span></span><br><span class="line">NAME                     STATUS   ROLES    AGE   VERSION   HOSTNAME</span><br><span class="line">k8s-master01.ilinux.io   Ready    master   25d   v1.17.3   k8s-master01.ilinux.io</span><br><span class="line">k8s-master02.ilinux.io   Ready    master   25d   v1.17.3   k8s-master02.ilinux.io</span><br><span class="line">k8s-master03.ilinux.io   Ready    master   25d   v1.17.3   k8s-master03.ilinux.io</span><br></pre></td></tr></table></figure>


<p>此外，Kubernetes的诸多资源对象必须以标签选择器的方式关联到Pod资源对象，例如Service资源在spec字段中嵌套使用selector字段定义标签选择器，而Deployment与StatefulSet等资源在selector字段中通过matchLabels和matchExpressions构造复杂的标签选择机制。</p>
<ul>
<li>matchLabels：直接给定键值对指定标签选择器。</li>
<li>matchExpressions：基于表达式指定的标签选择器列表，每个选择器形如{key: KEY_NAME, operator: OPERATOR, values: [VALUE1,VALUE2,…]}，选择器列表间为“逻辑与”关系；使用In或NotIn操作符时，其values必须为非空的字符串列表，而使用Exists或DostNotExist时，其values必须为空。</li>
<li>下面的资源清单片段是一个示例，它同时定义了两类标签选择器。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">selector:</span></span><br><span class="line">  <span class="attr">matchLabels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">redis</span></span><br><span class="line">  <span class="attr">matchExpressions:</span></span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">tier</span>, <span class="attr">operator:</span> <span class="string">In</span>, <span class="attr">values:</span> [<span class="string">cache</span>]&#125;</span><br><span class="line">    <span class="bullet">-</span> &#123;<span class="attr">key:</span> <span class="string">environment</span>, <span class="attr">operator:</span> <span class="string">Exists</span>, <span class="string">values:</span>&#125;</span><br></pre></td></tr></table></figure>

<h1 id="Service与服务发现"><a href="#Service与服务发现" class="headerlink" title="Service与服务发现"></a>Service与服务发现</h1><p>Service对象的IP地址都仅在Kubernetes集群内可达，它们无法接入集群外部的访问流量。在解决此类问题时，除了可以在单一节点上做端口（hostPort）暴露及让Pod资源共享使用工作节点的网络名称空间（hostNetwork）之外，更推荐用户使用NodePort或LoadBalancer类型的Service资源，或者是有七层负载均衡能力的Ingress资源。</p>
<h2 id="Service资源及其实现模型"><a href="#Service资源及其实现模型" class="headerlink" title="Service资源及其实现模型"></a>Service资源及其实现模型</h2><p>Service是Kubernetes的核心资源类型之一。它事实上是一种抽象：通过规则定义出由多个Pod对象组合而成的逻辑集合，以及访问这组Pod的策略。Service关联Pod资源的规则要借助标签选择器完成。</p>
<h3 id="Service资源概述"><a href="#Service资源概述" class="headerlink" title="Service资源概述"></a>Service资源概述</h3><p>Service资源基于标签选择器把筛选出的一组Pod对象定义成一个逻辑组合，并通过自己的IP地址和端口将请求分发给该组内的Pod对象。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131215422.png" alt="image-20220123131215422"></p>
<p>Service对象的IP地址（可称为ClusterIP或ServiceIP）是虚拟IP地址，由Kubernetes系统在Service对象创建时在专用网络（Service Network）地址中自动分配或由用户手动指定，并且在Service对象的生命周期中保持不变。Service基于端口过滤到达其IP地址的客户端请求，并根据定义将请求转发至其后端的Pod对象的相应端口之上，因此这种代理机制也称为“端口代理”或四层代理，工作于TCP/IP协议栈的传输层。<br>Service对象会通过API Server持续监视（watch）标签选择器匹配到的后端Pod对象，并实时跟踪这些Pod对象的变动情况，例如IP地址变动以及Pod对象的增加或删除等。不过，Service并不直接连接至Pod对象，它们之间还有一个中间层——Endpoints资源对象，该资源对象是一个由IP地址和端口组成的列表，这些IP地址和端口则来自由Service的标签选择器匹配到的Pod对象。这也是很多场景中会使用“Service的后端端点”这一术语的原因。默认情况下，创建Service资源对象时，其关联的Endpoints对象会被自动创建。</p>
<h3 id="kube-proxy代理模型"><a href="#kube-proxy代理模型" class="headerlink" title="kube-proxy代理模型"></a>kube-proxy代理模型</h3><p>每个工作节点的kube-proxy组件通过API Server持续监控着各Service及其关联的Pod对象，并将Service对象的创建或变动实时反映至当前工作节点上相应的iptables或ipvs规则上。客户端、Service及Pod对象的关系如图7-3所示。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131237759.png" alt="image-20220123131237759"></p>
<p>Service对象的ClusterIP事实上是用于生成iptables或ipvs规则时使用的IP地址，它仅用于实现Kubernetes集群网络内部通信，且仅能够以规则中定义的转发服务的请求作为目标地址予以响应，这也是它之所以被称作虚拟IP的原因之一。kube-proxy把请求代理至相应端点的方式有3种：userspace、iptables和ipvs。</p>
<h4 id="userspace代理模型"><a href="#userspace代理模型" class="headerlink" title="userspace代理模型"></a>userspace代理模型</h4><p>此处的userspace是指Linux操作系统的用户空间。在这种模型中，kube-proxy负责跟踪API Server上Service和Endpoints对象的变动（创建或移除），并据此调整Service资源的定义。对于每个Service对象，它会随机打开一个本地端口（运行于用户空间的kube-proxy进程负责监听），任何到达此代理端口的连接请求都将被代理至当前Service资源后端的各Pod对象，至于哪个Pod对象会被选中则取决于当前Service资源的调度方式，默认调度算法是轮询（round-robin）。userspace代理模型工作逻辑如图7-4所示。另外，此类Service对象还会创建iptables规则以捕获任何到达ClusterIP和端口的流量。在Kubernetes 1.1版本之前，userspace是默认的代理模型。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131341607.png" alt="image-20220123131341607"></p>
<p>在这种代理模型中，请求流量到达内核空间后经由套接字送往用户空间中的kube-proxy进程，而后由该进程送回内核空间，发往调度分配的目标后端Pod对象。因请求报文在内核空间和用户空间来回转发，所以必然导致模型效率不高。</p>
<h4 id="iptables代理模型"><a href="#iptables代理模型" class="headerlink" title="iptables代理模型"></a>iptables代理模型</h4><p>创建Service对象的操作会触发集群中的每个kube-proxy并将其转换为定义在所属节点上的iptables规则，用于转发工作接口接收到的、与此Service资源ClusterIP和端口相关的流量。客户端发来请求将直接由相关的iptables规则进行目标地址转换（DNAT）后根据算法调度并转发至集群内的Pod对象之上，而无须再经由kube-proxy进程进行处理，因而称为iptables代理模型，如图7-5所示。对于每个Endpoints对象，Service资源会为其创建iptables规则并指向其iptables地址和端口，而流量转发到多个Endpoint对象之上的默认调度机制是随机算法。iptables代理模型由Kubernetes v1.1版本引入，并于v1.2版本成为默认的类型。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131409842.png" alt="image-20220123131409842"></p>
<p>在iptables代理模型中，Service的服务发现和负载均衡功能都使用iptables规则实现，而无须将流量在用户空间和内核空间来回切换，因此更为高效和可靠，但是性能一般，而且受规模影响较大，仅适用于少量Service规模的集群。</p>
<h4 id="ipvs代理模型"><a href="#ipvs代理模型" class="headerlink" title="ipvs代理模型"></a>ipvs代理模型</h4><p>Kubernetes自v1.9版本起引入ipvs代理模型，且自v1.11版本起成为默认设置。在此种模型中，kube-proxy跟踪API Server上Service和Endpoints对象的变动，并据此来调用netlink接口创建或变更ipvs（NAT）规则，如图7-6所示。它与iptables规则的不同之处仅在于客户端请求流量的调度功能由ipvs实现，余下的其他功能仍由iptables完成。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131440521.png" alt="image-20220123131440521"></p>
<p>ipvs代理模型中Service的服务发现和负载均衡功能均基于内核中的ipvs规则实现。类似于iptables，ipvs也构建于内核中的netfilter之上，但它使用hash表作为底层数据结构且工作于内核空间，因此具有流量转发速度快、规则同步性能好的特性，适用于存在大量Service资源且对性能要求较高的场景。ipvs代理模型支持rr、lc、dh、sh、sed和nq等多种调度算法。</p>
<h3 id="Service资源类型"><a href="#Service资源类型" class="headerlink" title="Service资源类型"></a>Service资源类型</h3><p>无论哪一种代理模型，Service资源都可统一根据其工作逻辑分为ClusterIP、NodePort、LoadBalancer和ExternalName这4种类型。</p>
<h4 id="（1）ClusterIP"><a href="#（1）ClusterIP" class="headerlink" title="（1）ClusterIP"></a>（1）ClusterIP</h4><p>通过集群内部IP地址暴露服务，ClusterIP地址仅在集群内部可达，因而无法被集群外部的客户端访问。此为默认的Service类型。</p>
<h4 id="（2）NodePort"><a href="#（2）NodePort" class="headerlink" title="（2）NodePort"></a>（2）NodePort</h4><p>NodePort类型是对ClusterIP类型Service资源的扩展，它支持通过特定的节点端口接入集群外部的请求流量，并分发给后端的Server Pod处理和响应。因此，这种类型的Service既可以被集群内部客户端通过ClusterIP直接访问，也可以通过套接字&lt;NodeIP&gt;: &lt;NodePort&gt;与集群外部客户端进行通信，如图7-7所示。显然，若集群外部的请求报文首先到的节点并非Service调度的目标Server Pod所在的节点，<font color="red">该请求必然因需要额外的转发过程（跃点）和更多的处理步骤而产生更多延迟</font></p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131506181.png" alt="image-20220123131506181"></p>
<h4 id="（3）LoadBalancer"><a href="#（3）LoadBalancer" class="headerlink" title="（3）LoadBalancer"></a>（3）LoadBalancer</h4><p>这种类型的Service依赖于部署在IaaS云计算服务之上并且能够调用其API接口创建软件负载均衡器的Kubernetes集群环境。LoadBalancer Service构建在NodePort类型的基础上，通过云服务商提供的软负载均衡器将服务暴露到集群外部，因此它也会具有NodePort和ClusterIP。简言之，创建LoadBalancer类型的Service对象时会在集群上创建一个NodePort类型的Service，并额外触发Kubernetes调用底层的IaaS服务的API创建一个软件负载均衡器，而集群外部的请求流量会先路由至该负载均衡器，并由该负载均衡器调度至各节点上该Service对象的NodePort，如图7-8所示。该Service类型的优势在于，<font color="red">它能够把来自集群外部客户端的请求调度至所有节点（或部分节点）的NodePort之上，而不是让客户端自行决定连接哪个节点，也避免了因客户端指定的节点故障而导致的服务不可用。</font></p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131528411.png" alt="image-20220123131528411"></p>
<h4 id="（4）ExternalName"><a href="#（4）ExternalName" class="headerlink" title="（4）ExternalName"></a>（4）ExternalName</h4><p>通过将Service映射至由externalName字段的内容指定的主机名来暴露服务，此主机名需要被DNS服务解析至CNAME类型的记录中。换言之，此种类型不是定义由Kubernetes集群提供的服务，而是把集群外部的某服务以DNS CNAME记录的方式映射到集群内，从而让集群内的Pod资源能够访问外部服务的一种实现方式，如图7-9所示。因此，这种类型的Service没有ClusterIP和NodePort，没有标签选择器用于选择Pod资源，也不会有Endpoints存在。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123131606248.png" alt="image-20220123131606248"></p>
<p>总体来说，若需要将Service资源发布至集群外部，应该将其配置为NodePort或Load-Balancer类型，而若要把外部的服务发布于集群内部供Pod对象使用，则需要定义一个ExternalName类型的Service资源，只是这种类型的实现要依赖于v1.7及更高版本的Kubernetes。</p>
<h2 id="应用Service资源"><a href="#应用Service资源" class="headerlink" title="应用Service资源"></a>应用Service资源</h2><p>Service是Kubernetes核心API群组（core）中的标准资源类型之一，其管理操作的基本逻辑类似于Namespace和ConfigMap等资源，支持基于命令行和配置清单的管理方式。Service资源配置规范中常用的字段及意义如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">…</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">type</span> <span class="string">&lt;string&gt;</span>                 <span class="comment"># Service类型，默认为ClusterIP</span></span><br><span class="line">  <span class="string">selector</span> <span class="string">&lt;map[string]string&gt;</span>  <span class="comment"># 等值类型的标签选择器，内含“与”逻辑</span></span><br><span class="line">  <span class="string">ports：</span>                       <span class="comment"># Service的端口对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># 端口名称</span></span><br><span class="line">    <span class="string">protocol</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 协议，目前仅支持TCP、UDP和SCTP，默认为TCP</span></span><br><span class="line">    <span class="string">port</span> <span class="string">&lt;integer&gt;</span>              <span class="comment"># Service的端口号，被映射进Pod上的应用程序监听的端口； 而且如果后端Pod有多个端口，并且每个端口都想通过SErvice暴露的话，每个都要单独定义。 最终接收请求的是PodIP和containerPort；</span></span><br><span class="line">    <span class="string">targetPort</span>  <span class="string">&lt;string&gt;</span>        <span class="comment">#  后端目标进程的端口号或名称，名称需由Pod规范定义</span></span><br><span class="line">    <span class="string">nodePort</span> <span class="string">&lt;integer&gt;</span>          <span class="comment"># 节点端口号，仅适用于NodePort和LoadBalancer类型</span></span><br><span class="line">  <span class="string">clusterIP</span>  <span class="string">&lt;string&gt;</span>           <span class="comment"># Service的集群IP，建议由系统自动分配,也支持由用户手动分配</span></span><br><span class="line">  <span class="string">externalTrafficPolicy</span>  <span class="string">&lt;string&gt;</span> <span class="comment"># 外部流量策略处理方式，Local表示由当前节点处理，</span></span><br><span class="line">　　　                            <span class="comment"># Cluster表示向集群范围内调度</span></span><br><span class="line">  <span class="string">loadBalancerIP</span>  <span class="string">&lt;string&gt;</span>        <span class="comment"># 外部负载均衡器使用的IP地址，仅适用于LoadBlancer</span></span><br><span class="line">  <span class="string">externalName</span> <span class="string">&lt;string&gt;</span>           <span class="comment"># 外部服务名称，该名称将作为Service的DNS CNAME值</span></span><br></pre></td></tr></table></figure>

<h3 id="应用ClusterIP-Service资源"><a href="#应用ClusterIP-Service资源" class="headerlink" title="应用ClusterIP Service资源"></a>应用ClusterIP Service资源</h3><p>创建Service对象的常用方法有两种：一是利用此前曾使用过的kubectl create service命令创建，另一个则是利用资源配置清单创建。Service资源对象的期望状态定义在spec字段中，较为常用的内嵌字段为selector和ports，用于定义标签选择器和服务端口。下面的配置清单是定义在services-clusterip-demo.yaml中的一个Service资源示例：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span>       <span class="comment"># 端口名称标识</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span>    <span class="comment"># 协议，支持TCP、UDP和SCTP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span>         <span class="comment"># Service自身的端口号</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span>   <span class="comment"># 目标端口号，即Endpoint上定义的端口号</span></span><br></pre></td></tr></table></figure>

<p>Service资源的spec.selector仅支持以映射（字典）格式定义的等值类型的标签选择器，例如上面示例中的app: demoapp。定义服务端口的字段spec.ports的值则是一个对象列表，它主要定义Service对象自身的端口与目标后端端口的映射关系。我们可以将示例中的Service对象创建于集群中，通过其详细描述了解其特性，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-clusterip-demo.yaml</span> </span><br><span class="line">service/demoapp-svc created</span><br><span class="line">~ $ kubectl describe services/demoapp-svc</span><br><span class="line">Name:              demoapp-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       Selector:  app=demoapp</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                10.97.72.1</span><br><span class="line">Port:              http  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         &lt;none&gt;</span><br><span class="line">Session Affinity:  None</span><br><span class="line">Events:            &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>上面命令中的结果显示，demoapp-svc默认设定为ClusterIP类型，并得到一个自动分配的IP地址10.97.72.1。创建Service对象的同时会创建一个与之同名且拥有相同标签选择器的Endpoint对象，若该标签选择器无法匹配到任何Pod对象的标签，则Endpoint对象无任何可用端点数据，于是Service对象的Endpoints字段值便成了<none>。<br>我们知道，Service对象自身只是iptables或ipvs规则，它并不能处理客户端的服务请求，而是需要把请求报文通过目标地址转换（DNAT）后转发至后端某个Server Pod，这意味着没有可用的后端端点的Service对象是无法响应客户端任何服务请求的，如下面从集群节点上发起的请求命令结果所示。</none></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~$ curl 10.97.72.1</span><br><span class="line">curl: (7) Failed to connect to 10.97.72.1 port 80: Connection refused</span><br></pre></td></tr></table></figure>

<p>下面使用命令式命令手动创建一个与该Service对象具有相同标签选择器的Deployment对象demoapp，它默认会自动创建一个拥有标签app: demoapp的Pod对象。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create deploy demoapp --image=ikubernetes/demoapp:v1.0</span></span><br><span class="line">deployment.apps/demoapp created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=demoapp</span> </span><br><span class="line">NAME                READY   STATUS   RESTARTS   AGE</span><br><span class="line">demoapp-6c5d545684-g85gl   1/1     Running   0          8s</span><br></pre></td></tr></table></figure>

<p>Service对象demoapp-svc通过API Server获知这种匹配变动后，会立即创建一个以该Pod对象的IP和端口为列表项的名为demoapp-svc的Endpoints对象，而该Service对象详细描述信息中的Endpoint字段便以此列表项为值，如下面的命令结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME       ENDPOINTS     AGE</span><br><span class="line">demoapp-svc   10.244.2.7:80    42s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.2.7:80</span><br></pre></td></tr></table></figure>

<p>扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale deployments/demoapp --replicas=3</span></span><br><span class="line">deployment.apps/demoapp scaled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME          ENDPOINTS                       AGE</span><br><span class="line">demoapp-svc   10.244.1.11:80,10.244.2.7:80,10.244.3.9:80   96s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br></pre></td></tr></table></figure>

<p>扩展Deployment对象demoapp的应用规模引起的变动也将立即反映到相关的Endpoint和Service对象之上，例如将deployments/demoapp对象的副本扩展至3个，再来验证services/demoapp-svc的端点信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl scale deployments/demoapp --replicas=3</span></span><br><span class="line">deployment.apps/demoapp scaled</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/demoapp-svc</span></span><br><span class="line">NAME          ENDPOINTS                       AGE</span><br><span class="line">demoapp-svc   10.244.1.11:80,10.244.2.7:80,10.244.3.9:80   96s</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services/demoapp-svc | grep <span class="string">&quot;^Endpoints&quot;</span></span></span><br><span class="line">Endpoints:         10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br></pre></td></tr></table></figure>

<p>接下来可于集群中的某节点上再次向服务对象demoapp-svc发起访问请求以进行测试，多次的访问请求还可评估负载均衡算法的调度效果，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mageedu@k8s-master01:~$ while true; do curl -s 10.97.72.1/hostname; sleep .2; done</span><br><span class="line">ServerName: demoapp-6c5d545684-89w4f</span><br><span class="line">ServerName: demoapp-6c5d545684-zlm2w</span><br><span class="line">ServerName: demoapp-6c5d545684-g85gl</span><br><span class="line">ServerName: demoapp-6c5d545684-g85gl</span><br></pre></td></tr></table></figure>

<p><font color="red">kubeadm部署的Kubernetes集群的Service代理模型默认为iptables，它使用随机调度算法，因此Service会把客户端请求随机调度至其关联的某个后端Pod对象。请求取样次数越多，其调度效果也越接近算法的目标效果。</font></p>
<h3 id="应用NodePort-Service资源"><a href="#应用NodePort-Service资源" class="headerlink" title="应用NodePort Service资源"></a>应用NodePort Service资源</h3><p>部署Kubernetes集群系统时会预留一个端口范围，专用于分配给需要用到NodePort的Service对象，该端口范围默认为30000～32767。<font color="red">与Cluster类型的Service资源的一个显著不同之处在于，NodePort类型的Service资源需要显式定义.spec.type字段值为NodePort</font>，必要时还可以手动指定具体的节点端口号。例如下面的配置清单（services-nodeport-demo.yaml）中定义的Service资源对象demoapp-nodeport-svc，它使用了NodePort类型，且人为指定了32223这个节点端口。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-nodeport-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">NodePort</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">32223</span></span><br></pre></td></tr></table></figure>

<p>实践中，并不鼓励用户自定义节点端口，除非能事先确定它不会与某个现存的Service资源产生冲突。无论如何，只要没有特别需要，留给系统自动配置总是较好的选择。将配置清单中定义的Service对象demoapp-nodeport-svc创建于集群之上，以便通过详细描述了解其状态细节。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-nodeport-demo.yaml</span> </span><br><span class="line">service/demoapp-nodeport-svc created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe services demoapp-nodeport-svc</span>   </span><br><span class="line">Name:                     demoapp-nodeport-svc</span><br><span class="line">Namespace:                default</span><br><span class="line">Labels:                   &lt;none&gt;</span><br><span class="line">Annotations:              Selector:  app=demoapp</span><br><span class="line">Type:                     NodePort</span><br><span class="line">IP:                       10.97.227.67</span><br><span class="line">Port:                     http  80/TCP</span><br><span class="line">TargetPort:               80/TCP</span><br><span class="line">NodePort:                 http  32223/TCP</span><br><span class="line">Endpoints:                10.244.1.11:80,10.244.2.7:80,10.244.3.9:80</span><br><span class="line">Session Affinity:         None</span><br><span class="line">External Traffic Policy:  Cluster</span><br><span class="line">Events:                   &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>命令结果显示，该Service对象用于调度集群外部流量时使用默认的Cluster策略，该策略优先考虑负载均衡效果，哪怕目标Pod对象位于另外的节点之上而带来额外的网络跃点，因而针对该NodePort的请求将会被分散调度至该Serivce对象关联的所有端点之上。可以在集群外的某节点上对任一工作节点的NodePort端口发起HTTP请求以进行测试。以节点k8s-node03.ilinux.io为例，我们以如下命令向它的IP地址172.29.9.13的32223端口发起多次请求。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> curl -s 172.29.9.13:32223; <span class="built_in">sleep</span> 1; <span class="keyword">done</span></span></span><br><span class="line">…… ClientIP: 10.244.3.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-zlm2w, ServerIP: 10.244.1.11!</span><br><span class="line">…… ClientIP: 10.244.3.0, ServerName: demoapp-6c5d545684-g85gl, ServerIP: 10.244.2.7!</span><br></pre></td></tr></table></figure>

<p>上面命令的结果显示出外部客户端的请求被调度至该Service对象的每一个后端Pod之上，而这些Pod对象可能会分散于集群中的不同节点。命令结果还显示，请求报文的客户端IP地址是最先接收到请求报文的节点上用于集群内部通信的IP地址，而非外部客户端地址，这也能够在Pod对象的应用访问日志中得到进一步验证，如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl logs demoapp-6c5d545684-g85gl | <span class="built_in">tail</span> -n 1</span></span><br><span class="line">10.244.3.0 - - [31/Aug/2020 02:30:00] &quot;GET / HTTP/1.1&quot; 200 -</span><br></pre></td></tr></table></figure>

<p>NodePort类型的Service对象会对请求报文同时进行源地址转换（SNAT）和目标地址转换（DNAT）操作。<br>另一个外部流量策略Local则仅会将流量调度至请求的目标节点本地运行的Pod对象之上，以减少网络跃点，降低网络延迟，但当请求报文指向的节点本地不存在目标Service相关的Pod对象时将直接丢弃该报文。下面先把demoapp-nodeport-svc的外部流量策略修改为Local，而后再进行访问测试。简单起见，这里使用kubectl patch命令来修改Service对象的流量策略。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl patch services/demoapp-nodeport-svc -p <span class="string">&#x27;&#123;&quot;spec&quot;: &#123;&quot;externalTrafficPolicy&quot;: &quot;Local&quot;&#125;&#125;&#x27;</span></span>  </span><br><span class="line">service/demoapp-nodeport-svc patched</span><br></pre></td></tr></table></figure>

<p>-p选项中指定的补丁是一个JSON格式的配置清单片段，它引用了spec.externalTrafficPolicy字段，并为其赋一个新的值。配置完成后，我们再次发起测试请求时会看到，请求都被调度给了目标节点本地运行的Pod对象。另外，Local策略下无须在集群中转发流量至其他节点，也就不用再对请求报文进行源地址转换，Server Pod所看到的客户端IP就是外部客户端的真实地址。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash"><span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> curl -s 172.29.9.13:32223; <span class="built_in">sleep</span> 1; <span class="keyword">done</span></span>         </span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br><span class="line">…… ClientIP: 172.29.0.1, ServerName: demoapp-6c5d545684-89w4f, ServerIP: 10.244.3.9!</span><br></pre></td></tr></table></figure>

<p>NodePort类型的Service资源同样会被配置ClusterIP，以确保集群内的客户端对该服务的访问请求可在集群范围的通信中完成。</p>
<h3 id="应用LoadBalancer-Service资源"><a href="#应用LoadBalancer-Service资源" class="headerlink" title="应用LoadBalancer Service资源"></a>应用LoadBalancer Service资源</h3><p>NodePort类型的Service资源虽然能够在集群外部访问，但外部客户端必须事先得知NodePort和集群中至少一个节点IP地址，一旦被选定的节点发生故障，客户端还得自行选择请求访问其他的节点，因而一个有着固定IP地址的固定接入端点将是更好的选择。此外，集群节点很可能是某IaaS云环境中仅具有私有IP地址的虚拟主机，这类地址对互联网客户端不可达，为此类节点接入流量也要依赖于集群外部具有公网IP地址的负载均衡器，由其负责接入并调度外部客户端的服务请求至集群节点相应的NodePort之上。<br>IaaS云计算环境通常提供了LBaaS（Load Balancer as a Service）服务，它允许租户动态地在自己的网络创建一个负载均衡设备。部署在此类环境之上的Kubernetes集群可借助于CCM（Cloud Controller Manager）在创建LoadBalancer类型的Service资源时调用IaaS的相应API，按需创建出一个软件负载均衡器。但CCM不会为那些非LoadBalancer类型的Service对象创建负载均衡器，而且当用户将LoadBalancer类型的Service调整为其他类型时也将删除此前创建的负载均衡器。<font color="red">kubeadm在部署Kubernetes集群时并不会默认部署CCM，有需要的用户需要自行部署。</font><br>对于没有此类API可用的Kubernetes集群，管理员也可以为NodePort类型的Service手动部署一个外部的负载均衡器（推荐使用HA配置模型），并配置将请求流量调度至各节点的NodePort之上，这种方式的缺点是管理员需要手动维护从外部负载均衡器到内部服务的映射关系。<br>从实现方式上来说，LoadBalancer类型的Service就是在NodePort类型的基础上请求外部管理系统的API，并在Kubernetes集群外部额外创建一个负载均衡器，将流量调度至该NodePort Service之上。Kubernetes以异步方式请求创建负载均衡器，并将有关配置保存在Service对象的.status.loadBalancer字段中。下面是定义在services-loadbalancer-demo.yam配置清单中的LoadBalancer类型Service资源，在最简单的配置模型中，用户仅需要修改NodePort Service服务定义中type字段的值为LoadBalancer即可。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-loadbalancer-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">LoadBalancer</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>Service对象的loadBalancerIP负责承接外部发来的流量，该IP地址通常由云服务商系统动态配置，或者借助.spec.loadBalancerIP字段显式指定，但有些云服务商不支持用户设定该IP地址，这种情况下，即便提供了也会被忽略。外部负载均衡器的流量会直接调度至Service后端的Pod对象之上，而如何调度流量则取决于云服务商，有些环境可能还需要为Service资源的配置定义添加注解，必要时请自行参考云服务商文档说明。另外，LoadBalancer Service还支持使用.spec. loadBalancerSourceRanges字段指定负载均衡器允许的客户端来源的地址范围。</p>
<h3 id="外部IP"><a href="#外部IP" class="headerlink" title="外部IP"></a>外部IP</h3><p>若集群中部分或全部节点除了有用于集群通信的节点IP地址之外，还有可用于外部通信的IP地址，如图7-10中的EIP-1和EIP-2，那么我们还可以在Service资源上启用spec.externalIPs字段来基于这些外部IP地址向外发布服务。所有路由到指定的外部IP（externalIP）地址某端口的请求流量都可由该Service代理到后端Pod对象之上，如图7-10所示。从这个角度来说，请求流量到达外部IP与节点IP并没有本质区别，但外部IP却可能仅存在于一部分的集群节点之上，而且它不受Kubernetes集群管理，需要管理员手动介入其配置和回收等操作任务中。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123132640583.png" alt="image-20220123132640583"></p>
<p>外部IP地址可结合ClusterIP、NodePort或LoadBalancer任一类型的Service资源使用，而到达外部IP的请求流量会直接由相关联的Service调度转发至相应的后端Pod对象进行处理。假设示例Kubernetes集群中的k8s-node01节点上拥有一个可被路由到的IP地址172.29.9.26，我们期望能够将demoapp的服务通过该外部IP地址发布到集群外部，则可以使用下列配置清单（services-externalip-demo.yaml）中的Service资源实现。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-externalip-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">externalIPs:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.26</span></span><br></pre></td></tr></table></figure>

<p>节点k8s-node01故障也必然导致该外部IP上公开的服务不再可达，除非该IP地址可以浮动到其他节点上。如今，大多数云服务商都支持浮动IP的功能，该IP地址可绑定在某个主机，并在其故障时通过某种触发机制自动迁移至其他主机。在不具有浮动IP功能的环境中进行测试之前，需要先在k8s-node01上（或根据规划的其他的节点上）手动配置172.29.9.26这个外部IP地址。而且，在模拟节点故障并手动将外部IP地址配置在其他节点进行浮动IP测试时，还需要清理之前的ARP地址缓存。</p>
<h2 id="Service与Endpoint资源"><a href="#Service与Endpoint资源" class="headerlink" title="Service与Endpoint资源"></a>Service与Endpoint资源</h2><p>端点是指通过LAN或WAN连接的能够用于网络通信的硬件设备，它在广义上可以指代任何与网络连接的设备。在Kubernetes语境中，端点通常代表Pod或节点上能够建立网络通信的套接字，并由专用的资源类型Endpoint进行定义和跟踪。</p>
<h3 id="Endpoint与容器探针"><a href="#Endpoint与容器探针" class="headerlink" title="Endpoint与容器探针"></a>Endpoint与容器探针</h3><p>Service对象借助于Endpoint资源来跟踪其关联的后端端点，但Endpoint是“二等公民”，<font color="red">Service对象可根据标签选择器直接创建同名的Endpoint对象</font>，不过用户几乎很少有直接使用该类型资源的需求。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">services-readiness-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span>      <span class="comment"># 定义Deployment对象，它使用Pod模板创建Pod对象</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span>         <span class="comment"># 该Deployment对象要求满足的Pod对象数量</span></span><br><span class="line">  <span class="attr">selector:</span>           <span class="comment"># Deployment对象的标签选择器，用于筛选Pod对象并完成计数</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">  <span class="attr">template:</span>           <span class="comment"># 由Deployment对象使用的Pod模板，用于创建足额的Pod对象</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">demoapp-with-readiness</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">readinessProbe:</span></span><br><span class="line">          <span class="attr">httpGet:</span>    <span class="comment"># 定义探针类型和探测方式</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">            <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">          <span class="attr">initialDelaySeconds:</span> <span class="number">15</span>   <span class="comment"># 初次检测延迟时长</span></span><br><span class="line">          <span class="attr">periodSeconds:</span> <span class="number">10</span>         <span class="comment"># 检测周期</span></span><br></pre></td></tr></table></figure>

<p>Endpoint对象会根据就绪状态把同名Service对象标签选择器筛选出的后端端点的IP地址分别保存在subsets.addresses字段和subsets.notReadyAddresses字段中，通过API Server持续、动态跟踪每个端点的状态变动，并即时反映到端点IP所属的字段。仅那些位于subsets.addresses字段的端点地址可由相关的Service用作后端端点。此外，相关Service对象标签选择器筛选出的Pod对象数量的变动也将会导致Endpoint对象上的端点数量变动。<br>上面配置清单中定义Endpoint对象services-readiness-demo会筛选出Deployment对象demoapp2创建的两个Pod对象，将它们的IP地址和服务端口创建为端点对象。但延迟15秒启动的容器探针会导致这两个Pod对象至少要在15秒以后才能转为“就绪”状态，这意味着在上面配置清单中的Service资源创建后至少15秒之内无可用后端端点，例如下面的资源创建和Endpoint资源监视命令结果中，在20秒之后，Endpoint资源services-readiness-demo才得到第一个可用的后端端点IP。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f services-readiness-demo.yaml</span> </span><br><span class="line">service/services-readiness-demo created</span><br><span class="line">deployment.apps/demoapp2 created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/services-readiness-demo -w</span> </span><br><span class="line">NAME                      ENDPOINTS                      AGE</span><br><span class="line">services-readiness-demo                                  6s</span><br><span class="line">services-readiness-demo   10.244.1.15:80                 20s</span><br><span class="line">services-readiness-demo   10.244.1.15:80,10.244.2.9:80   31s</span><br></pre></td></tr></table></figure>

<p>因任何原因导致的后端端点就绪状态检测失败，都会触发Endpoint对象将该端点的IP地址从subsets.addresses字段移至subsets.notReadyAddresses字段。例如，我们使用如下命令人为地将地址10.244.2.9的Pod对象中的容器就绪状态检测设置为失败，以进行验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s -X POST -d <span class="string">&#x27;readyz=FAIL&#x27;</span> 10.244.2.9/readyz</span></span><br></pre></td></tr></table></figure>

<p>等待至少3个检测周期共30秒之后，获取Endpoint对象services-readiness-demo的资源清单的命令将返回类似如下信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpoints/services-readiness-demo -o yaml</span></span><br><span class="line">……</span><br><span class="line">subsets:</span><br><span class="line">- addresses:</span><br><span class="line">  - ip: 10.244.1.15</span><br><span class="line">    nodeName: k8s-node01.ilinux.io</span><br><span class="line">    targetRef:</span><br><span class="line">      kind: Pod</span><br><span class="line">      name: demoapp2-85595465d-dhbzs</span><br><span class="line">      namespace: default</span><br><span class="line">      resourceVersion: &quot;321388&quot;</span><br><span class="line">      uid: 8d2a3bb6-c628-4558-917a-f8f6df9b8573</span><br><span class="line">  notReadyAddresses:</span><br><span class="line">  - ip: 10.244.2.9</span><br><span class="line">    nodeName: k8s-node02.ilinux.io</span><br><span class="line">    targetRef:</span><br><span class="line">      kind: Pod</span><br><span class="line">      name: demoapp2-85595465d-z7w5h</span><br><span class="line">      namespace: default</span><br><span class="line">      resourceVersion: &quot;323328&quot;</span><br><span class="line">      uid: 380050ae-4e32-4724-af22-e079ab2ec02e</span><br><span class="line">  ports:</span><br><span class="line">  - name: http</span><br><span class="line">    port: 80</span><br><span class="line">    protocol: TCP</span><br></pre></td></tr></table></figure>

<p>该故障端点重新转回就绪状态后，Endpoints对象会将其移回subsets.addresses字段中。这种处理机制确保了Service对象不会将客户端请求流量调度给那些处于运行状态但服务未就绪（notReady）的端点。</p>
<h3 id="自定义Endpoint资源"><a href="#自定义Endpoint资源" class="headerlink" title="自定义Endpoint资源"></a>自定义Endpoint资源</h3><p>除了借助Service对象的标签选择器自动关联后端端点外，Kubernetes也支持自定义Endpoint对象，用户可通过配置清单创建具有固定数量端点的Endpoint对象，而调用这类Endpoint对象的同名Service对象无须再使用标签选择器。Endpoint资源的API规范如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoint</span></span><br><span class="line"><span class="attr">metadata:</span>                  <span class="comment"># 对象元数据</span></span><br><span class="line">  <span class="attr">name:</span></span><br><span class="line">  <span class="attr">namespace:</span></span><br><span class="line"><span class="attr">subsets:</span>                   <span class="comment"># 端点对象的列表</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">addresses:</span>               <span class="comment"># 处于“就绪”状态的端点地址对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">hostname</span>  <span class="string">&lt;string&gt;</span>     <span class="comment"># 端点主机名</span></span><br><span class="line">    <span class="string">ip</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 端点的IP地址，必选字段</span></span><br><span class="line">    <span class="string">nodeName</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 节点主机名</span></span><br><span class="line">    <span class="string">targetRef：</span>            <span class="comment"># 提供了该端点的对象引用</span></span><br><span class="line">      <span class="string">apiVersion</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 被引用对象所属的API群组及版本</span></span><br><span class="line">      <span class="string">kind</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 被引用对象的资源类型，多为Pod</span></span><br><span class="line">      <span class="string">name</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 对象名称</span></span><br><span class="line">      <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 对象所属的名称空间</span></span><br><span class="line">      <span class="string">fieldPath</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 被引用的对象的字段，在未引用整个对象时使用，通常仅引用</span></span><br><span class="line">                           <span class="comment"># 指定Pod对象中的单容器，例如spec.containers[1]</span></span><br><span class="line">      <span class="string">uid</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 对象的标识符</span></span><br><span class="line">  <span class="attr">notReadyAddresses:</span>       <span class="comment"># 处于“未就绪”状态的端点地址对象列表，格式与address相同</span></span><br><span class="line">  <span class="attr">ports:</span>                   <span class="comment"># 端口对象列表</span></span><br><span class="line">  <span class="bullet">-</span> <span class="string">name</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 端口名称</span></span><br><span class="line">    <span class="string">port</span> <span class="string">&lt;integer&gt;</span>         <span class="comment"># 端口号，必选字段</span></span><br><span class="line">    <span class="string">protocol</span> <span class="string">&lt;string&gt;</span>      <span class="comment"># 协议类型，仅支持UDP、TCP和SCTP，默认为TCP</span></span><br><span class="line">    <span class="string">appProtocol</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 应用层协议</span></span><br></pre></td></tr></table></figure>

<p>自定义Endpoint常将那些不是由编排程序编排的应用定义为Kubernetes系统的Service对象，从而让客户端像访问集群上的Pod应用一样请求外部服务。例如，假设要把Kubernetes集群外部一个可经由172.29.9.51:3306或172.29.9.52:3306任一端点访问的MySQL数据库服务引入集群中，便可使用如下清单中的配置完成。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Endpoints</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-external</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">subsets:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">addresses:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.51</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.52</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">mysql-external</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">3306</span></span><br><span class="line">    <span class="attr">protocol:</span> <span class="string">TCP</span></span><br></pre></td></tr></table></figure>

<p>显然，非经Kubernetes管理的端点，其就绪状态难以由Endpoint通过注册监视特定的API资源对象进行跟踪，因而用户需要手动维护这种调用关系的正确性。<br>Endpoint资源提供了在Kubernetes集群上跟踪端点的简单途径，但对于有着大量端点的Service来说，将所有的网络端点信息都存储在单个Endpoint资源中，会对Kubernetes控制平面组件产生较大的负面影响，且每次端点资源变动也会导致大量的网络流量。EndpointSlice（端点切片）通过将一个服务相关的所有端点按固定大小（默认为100个）切割为多个分片，提供了一种更具伸缩性和可扩展性的端点替代方案。<br>EndpointSlice由引用的端点资源组成，类似于Endpoint，它可由用户手动创建，也可由EndpointSlice控制器根据用户在创建Service资源时指定的标签选择器筛选集群上的Pod对象自动创建。单个EndpointSlice资源默认不能超过100个端点，小于该数量时，EndpointSlice与Endpoint存在1:1的映射关系且性能相同。EndpointSlice控制器会尽可能地填满每一个EndpointSlice资源，但不会主动进行重新平衡，新增的端点会尝试添加到现有的EndpointSlice资源上，若超出现有任何EndpointSlice对象的可用的空余空间，则将创建新的EndpointSlice，而非分散填充。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get endpointslice -n kube-system</span></span><br><span class="line">NAME           ADDRESSTYPE        PORTS            ENDPOINTS        AGE</span><br><span class="line">kube-dns-mbdj5   IPv4          53,9153,53   10.244.0.6,10.244.0.7   13d</span><br></pre></td></tr></table></figure>

<p>EndpointSlice资源根据其关联的Service与端口划分成组，每个组隶属于同一个Service。更具体的使用方式请参考Kubernetes的相关文档。</p>
<h2 id="深入理解Service资源"><a href="#深入理解Service资源" class="headerlink" title="深入理解Service资源"></a>深入理解Service资源</h2><p>本质上，Service对象代表着由kube-proxy借助于自身的程序逻辑（userspace）、iptables或ipvs，甚至是某种形式的组合所构建出的流量代理和调度转发机制，每个Service对象的创建、更新与删除都会经由kube-proxy反映为程序配置、iptables规则或ipvs规则的相应操作。</p>
<h3 id="iptables代理模型-1"><a href="#iptables代理模型-1" class="headerlink" title="iptables代理模型"></a>iptables代理模型</h3><p>由集群中每个节点上的kube-proxy进程将Service定义、转换且配置于节点内核上的iptables规则。每个Service的定义主要由Service流量匹配规则、流量调度规则和以每个后端Endpoint为单位的DNAT规则组成，这些规则负责完成Service资源的核心功能。此外，iptables代理模型还会额外在filter表和mangle表上使用一些辅助类的规则。</p>
<h4 id="ClusterIP-Service"><a href="#ClusterIP-Service" class="headerlink" title="ClusterIP Service"></a>ClusterIP Service</h4><p>ClusterIP类型Service资源的请求流量是指以某个特定Service对象的ClusterIP（或称为Service_IP）为目标地址，同时以Service_Port为目标端口的报文，它们可能源自Kubernetes集群中某个特定节点上的Pod、独立容器（非托管至Kubernetes集群）或进程，也可能源自节点之外。通常，源自独立容器或节点外部的请求报文的源IP地址为Pod网络（例如Flannel默认的10.244.0.0/16）之外的IP地址。<br>Cluster类型Service对象的相关规则主要位于KUBE-SERVICES、KUBE-MARQ-MASK和KUBE-POSTROUTING这3个自定义链，以及那些以KUBE-SVC或KUBE-SEP为前缀的各个自定义链上，用于实现Service流量筛选、分发和目标地址转换（端点地址），以及为非源自Pod网络的请求报文进行源地址转换。各相关的规则链及调用关系如图7-11所示。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123133725398.png" alt="image-20220123133725398"></p>
<p>▪KUBE-SERVICES：包含所有ClusterIP类型Service的流量匹配规则，由PREROUTING和OUTPUT两个内置链直接调用。每个Service对象包含两条规则定义，对于所有发往该Service（目标IP为Service_IP且目标端口为Service_Port）的请求报文：前一条规则用于为非源自Pod网络（! -s 10.244.0.0/16）中的请求报文打上特有的防火墙标记，而打标签的操作则要借助KUBE-MARQ-MASK自定义链中的规则，后一条规则负责将所有报文转至专用的以KUBE-SVC为名称前缀的自定义链，后缀是Service信息的HASH值。<br>▪KUBE-MARQ-MASK：专用目的自定义链，所有转至该自定义链的报文都将被打上特有的防火墙标记（0x4000），以便于将特定类型的报文定义为单独的分类，进而在将该类报文转发到目标端点之前由POSTROUTING规则链进行源地址转换。<br>▪KUBE-SVC-<HASH>：定义一个服务的流量调度规则，它通过随机调度算法将请求分发给该Service的所有后端端点，每个后端端点定义在以KUBE-SEP为前缀名称的自定义链上，后缀是端点信息的hash值。<br>▪KUBE-SEP-<HASH>：定义一个端点相关的流量处理规则。它通常包含两条规则：前一条用于为那些源自该端点自身（-s ep_ip）的流量请求调用自定义链KUBE-MARQ-MASK，打上特有的防火墙标记；后一条负责对发往该端点的所有流量进行目标IP地址和端口转换，新目标为该端点的IP和端口（-j DNAT –to-destination ep_ip:ep_port）。<br>▪KUBE-POSTROUTING：专用的自定义链，由内置链POSTROUTING无条件调用，负责对带特有防火墙标记0x4000的请求报文进行源地址转换或地址伪装（MASQUERADE），新的源地址为报文离开协议栈时流经接口的主IP地址。<br>我们可通过实际存在的Service对象来验证这些设定，以7.2.1节创建的demoapp-svc为例，在集群中的任何一个工作节点上使用iptables -t nat -vnL或iptables -t nat -S命令打印与它相关的iptables规则。下面的命令打印了该Service对象用于流量匹配的相关规则，它定义在KUBE-SERVICES自定义链上。</HASH></HASH></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;default/demoapp-svc&quot;</span><br><span class="line">-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 10.97.72.1/32 -p tcp -m comment --comment &quot;default/demoapp-svc:http cluster IP&quot; -m tcp --dport 80 -j KUBE-SVC-ZAGXFVDPX7HH4UMW</span><br></pre></td></tr></table></figure>

<p>第一条规则用于将那些发往demoapp-svc的、来自10.244.0.0/16网络之外的请求报文交由自定义链KUBE-MARK-MASQ上的规则添加专用标记0x4000，该条规则如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-MARK-MASQ | grep &quot;^-A&quot;</span><br><span class="line">-A KUBE-MARK-MASQ -j MARK --set-xmark 0x4000/0x4000</span><br></pre></td></tr></table></figure>

<p>添加标记的处理并不会短路iptables规则链KUBE-SERVICES对流量的处理，因此所有发往demoapp-svc的流量还会继续由后一条规则指向的、以KUBE-SVC为名称前缀的自定义链KUBE-SVC-ZAGXFVDPX7HH4UMW中的规则处理。该自定义链专用于为demoapp-svc中的所有可用端点定义流量调度规则，它包含如下3条规则：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SVC-ZAGXFVDPX7HH4UMW | grep &quot;^-A&quot; </span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.33333333349 -j KUBE-SEP-HDIVJIPCJU2JBJVX</span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-ZAFCYSF77K72PY72</span><br><span class="line">-A KUBE-SVC-ZAGXFVDPX7HH4UMW -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-SEP-FUO5ALUGHUE426HZ</span><br></pre></td></tr></table></figure>

<p>注意<br>所有以KUBE-SEP和KUBE-SVC为前缀的自定义链的名称在重新创建Service或重启Kubernetes集群后都有可能发生改变，但它们的引用关系不变。</p>
<p>这3条规则的处理目标分别为3个以KUBE-SEP为名称前缀的自定义链，每个链上定义了一个端点的流量处理规则，因而意味着该Service对象共有3个Endpoint对象，所有流量将在这3个Endpoint之间随机（–mode random）分配。到达KUBE-SVC-ZAGXFVDPX7HH4UMW的流量将由这3条规则以“短路”方式进行匹配检查和处理，任何一条规则处理后都不会再匹配后续的其他规则。第一条规则将处理大约1/3（–probability 0.33333333349）的流量，余下的所有流量（即由第一条规则处理后余下的2/3）将由第二条规则处理一半（–probability 0.50000000000），再余下的所有流量都将由第三条规则处理，因此3个Endpoint将各自得到大约1/3的流量。<br>每个Endpoint专用的自定义链以KUBE-SEP为名称前缀，它包含某单点端点相关的流量处理规则。以专用IP地址为10.244.1.11的Endpoint对象为例，它对应于自定义链KUBE-SEP-HZPGLN57HG6GZW4O，该链下包含两个iptables规则，如下面的命令结果所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~#  iptables -t nat -S KUBE-SEP-HDIVJIPCJU2JBJVX | grep &quot;^-A&quot;                         </span><br><span class="line">-A KUBE-SEP-HDIVJIPCJU2JBJVX -s 10.244.1.11/32 -m comment --comment &quot;default/demoapp-svc:http&quot; -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SEP-HDIVJIPCJU2JBJVX -p tcp -m comment --comment &quot;default/demoapp-svc:http&quot; -m tcp -j DNAT --to-destination 10.244.1.11:80</span><br></pre></td></tr></table></figure>

<p>Pod对象也可能会向自己所属的Service对象发起访问请求，而且该请求经由OUTPUT链到达KUBE-SERVICES链后存在被调度回当前Pod对象的可能性。第一条规则就是为该类报文添加专有的流量标记。第二条规则将接收到的所有流量进行目标地址转换（DNAT），新的目标为10.244.1.11:80，它对应Kubernetes集群上由Service对象demoapp-svc匹配到的一个特定Pod对象。<br>不难猜测，特定节点（例如前面示例中的k8s-node01）接收到的请求报文的源地址为Pod网络中的IP地地址的，必然源自该节点或节点上的Pod对象。它们的IP地址位于该节点的PodCIDR之中，这些流量离开节点之前无须进行源地址转换，因而目标端点直接响应给客户端IP就能够正确到达请求方。而请求报文的源地址并非为Pod网络中的IP地址的，例如请求方为该节点上的某独立容器，则Service必须在其离开本节点之前，将请求报文的源地址转换为该节点上报文离开时要经由接口的IP地址（例如cni0上的10.244.1.0），以确保响应报文可正确回送至该节点，并由该节点响应给相应的客户端，由内置链POSTROUTING所调用的自定义链KUBE-POSTROUTING上的规则便用于实现此类功能。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-POSTROUTING | grep &quot;^-A&quot;</span><br><span class="line">-A KUBE-POSTROUTING -m comment --comment &quot;kubernetes service traffic requiring SNAT&quot; -m mark --mark 0x4000/0x4000 -j MASQUERADE</span><br></pre></td></tr></table></figure>

<p>由此可见，对于集群内部的后端端点来说，它们收到的请求报文的源地址，要么是Pod的IP地址，要么是节点IP地址，因而直接发送响应报文给请求方即可。但那些本身并非源自Pod或节点的请求的响应报文，还需要由节点自动执行一次目标地址转换，以便把报文送达真正的请求方。注意<br>kube-proxy也支持在iptables代理模型上使用masquerade all，从而对通过ClusterIP地址访问的所有请求进行源地址转换，但在大多数场景中，这都不是必要的选择。2. NodePort Service<br>相较于ClusterIP类型来说，所有发往NodePort类型的Service对象的请求流量的目标IP和端口分别是节点IP和NodePort，这类报文无法由KUBE-SERVICES自定义链上那些基于Service IP和Service Port定义的流量匹配规则所匹配，但会由该自定义链上的最后一条规则转给KUBE-NODEPORTS自定义链。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | tail -n 1</span><br><span class="line">-A KUBE-SERVICES -m comment --comment &quot;kubernetes service nodeports; NOTE: this must be the last rule in this chain&quot; -m addrtype --dst-type LOCAL -j KUBE-NODEPORTS</span><br></pre></td></tr></table></figure>

<p>KUBE-NODEPORTS链以类似ClusterIP Service拦截规则的方式定义了NodePort Service对象的拦截规则，其中每个Service对象包含两条规则定义。对于所有发往该Service（目标IP为该NodeIP，目标端口为NodePort）的请求报文：前一条规则为发往该Service对象的所有请求报文，基于KUBE-MARQ-MASK自定义链中的规则打上特有的防火墙标记；后一条规则负责将这些报文转至专用的、以KUBE-SVC为前缀的自定义链。以前面创建的demoapp-nodeport-svc为例，它拥有以下两条iptables规则。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~#  iptables -t nat -S KUBE-NODEPORTS | grep &quot;default/demoapp-nodeport-svc&quot;</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-NODEPORTS -p tcp -m comment --comment &quot;default/demoapp-nodeport-svc:http&quot; -m tcp --dport 31398 -j KUBE-SVC-HCTPASJ7WVWOBYLM</span><br></pre></td></tr></table></figure>

<p>我们已经知道，Service对象的专用自定义链定义了一组调度规则，以调度发往该Service对象匹配的所有后端端点的相关流量，而其中的每一个后端端点又有自己专用的自定义链，用于对请求报文进行目标地址转换。另外，NodePort类型的Service为所有从NodePort进入的请求报文都打了特有防火墙标记，因此这些请求报文会按照POSTROUTING和KUBE-POSTROUTING链上的规则将源地址转换为该报文离开节点时所经由的接口的IP地址。这些处理步骤与ClusterIP类型的Service对象几乎完全相同。完整的处理流程如图7-12所示。</p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123134247463.png" alt="image-20220123134247463"></p>
<p>对于集群内部的后端端点来说，它们收到的请求报文的源地址都是节点IP地址。以Flannel插件环境中10.244.1.0/24这个Pod CIDR为例，该IP地址可能是flannel.1接口上的10.244.1.0/24，也可能是cni0上的10.244.1.1/24。于是，后端端点会把报文响应给请求报文进入时的节点，再由该节点将目标地址转换为客户端IP后发送。<br>但是，对于将外部流量策略定义为Local的NodePort Service对象来说，由于流量报文不会在集群内跨节点转发，也就没有必要对请求报文进行SNAT操作，所以后端端点可以看到真实的客户端IP。它的具体处理流程如图7-13所示。<br><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123134329215.png" alt="image-20220123134329215"></p>
<p>1）KUBE-SERVICES链把目标地址指向当前节点的报文，并转给KUBE-NODEPORTS处理。<br>2）对于一个Local策略的NodePort Service来说，KUBE-NODEPORTS会定义两条规则：前一条负则将源地址位于127.0.0.0/8网络的请求报文借助KUBE-MARK-MASQ打上0x4000防火墙标记；后一条则将报文转给该Service专用的KUBE-XLB-<HASH>自定义链。<br>3）KUBE-XLB-<HASH>自定义链将源自Pod网络（10.244.0.0/16）的请求报文以类似ClusterIP Service使用的方式进行处理，只转换请求报文目标地址；将源自当前节点所处的本地网络中的请求报文，按照常规的NodePort Service使用的方式进行处理，并同时转换源地址和目标地址；而将其他类型的请求报文直接转交给指定的本地后端端点处理，这也体现了本地流量策略的真正意义。<br>显然，若某节点自身未运行NodePort Service后端Pod，则本地策略类型的请求将得到失败的响应结果。提示<br>未配置外部IP地址的LoadBalancer类型的Service对象的工作方式与NodePort类型几乎完全相同，这里不再专门描述。3. External IP<br>在iptables中，外部IP表现为一种专有的Service访问入口。在KUBE-SERVICES自定义链上，每个外部IP都有3条相关的iptables规则：第1条用于为发往该外部IP的服务端口的请求流量，借助KUBE-MARK-MASQ自定义链打上特有的防火墙标记0x4000；第2条将这些请求流量中从非物理接口进入且源地址类型不是本地地址的流量，交由相应Service的专用自定义链进行流量分发；第3条用于将这些流量中目标地址类型是本地地址的请求报文，也交由相应Service的专用自定义链进行流量分发。具体的处理过程如图7-14所示。</HASH></HASH></p>
<p><img src="/blog/2022/02/09/Service%E5%92%8C%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0/image-20220123134355773.png" alt="image-20220123134355773"></p>
<p>以前面定义的default/demoapp-externalip-svc中使用的外部IP 172.29.9.26为例，下面的命令可以在KUBE-SERVICES获取到相应的专用规则。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# iptables -t nat -S KUBE-SERVICES | grep &quot;172.29.9.26&quot;           </span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -j KUBE-MARK-MASQ</span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m physdev ! --physdev-is-in -m addrtype ! --src-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56</span><br><span class="line">-A KUBE-SERVICES -d 172.29.9.26/32 -p tcp -m comment --comment &quot;default/demoapp-externalip-svc:http external IP&quot; -m tcp --dport 80 -m addrtype --dst-type LOCAL -j KUBE-SVC-PX62EIGZ4HAB6Y56</span><br></pre></td></tr></table></figure>

<p>由此可见，尽管外部IP需要结合ClusterIP、NodePort或LoadBalancer中任一类型的Service对象使用，但到达外部IP的服务请求流量却有着专用的拦截规则，请求报文也是交由相应Service的专用自定义链直接进行向后分发。</p>
<h3 id="ipvs代理模型-1"><a href="#ipvs代理模型-1" class="headerlink" title="ipvs代理模型"></a>ipvs代理模型</h3><p>由前一节的介绍可知，单个Service对象的iptables数量与后端端点的数量正相关，对于拥有较多Service对象和大规模Pod对象的Kubernetes集群，每个节点的内核上将充斥着大量的iptables规则。Service对象的变动会导致所有节点刷新netfilter上的iptables规则，而且每次的Service请求也都将经历多次的规则匹配检测和处理过程，这会占用节点上相当比例的系统资源。因此，iptables代理模型不适用于Service和Pod数量较多的集群。ipvs代理模型通过将流量匹配和分发功能配置为少量ipvs规则，有效降低了对系统资源的占用，从而能够承载更大规模的Kubernetes集群。</p>
<ol>
<li>调整kube-proxy代理模型<br>kube-proxy使用的代理模型定义在配置文件中，kubeadm部署的Kubernetes集群以DaemonSet控制器编排kube-proxy在每个节点上运行一个实例，配置文件则以kube-system名称空间中名为kube-proxy的ConfigMap对象的形式提供，默认使用iptables代理模型。在测试集群环境中，可直接使用kubectl edit configmaps/kube-proxy -n kube-system命令编辑该ConfigMap对象，将代理模型修改为ipvs，配置要点如下所示。</li>
</ol>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">config.conf:</span> <span class="string">|-</span></span><br><span class="line"><span class="string">    apiVersion: kubeproxy.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="string">    bindAddress: 0.0.0.0</span></span><br><span class="line"><span class="string">    ……</span></span><br><span class="line"><span class="string">    iptables:                 # iptables配置细节</span></span><br><span class="line"><span class="string">      masqueradeAll: false    # 是否将通过ClusterIP访问的流量全部进行SNAT</span></span><br><span class="line"><span class="string">      masqueradeBit: null</span></span><br><span class="line"><span class="string">      minSyncPeriod: 0s</span></span><br><span class="line"><span class="string">      syncPeriod: 0s</span></span><br><span class="line"><span class="string">    ipvs:                     # ipvs配置细节</span></span><br><span class="line"><span class="string">      excludeCIDRs: null</span></span><br><span class="line"><span class="string">      minSyncPeriod: 0s</span></span><br><span class="line"><span class="string">      scheduler: &quot;&quot;           # 调度算法，默认为rr</span></span><br><span class="line"><span class="string">      strictARP: false</span></span><br><span class="line"><span class="string">      syncPeriod: 0s</span></span><br><span class="line"><span class="string">      tcpFinTimeout: 0s</span></span><br><span class="line"><span class="string">      tcpTimeout: 0s</span></span><br><span class="line"><span class="string">      udpTimeout: 0s</span></span><br><span class="line"><span class="string">    kind: KubeProxyConfiguration</span></span><br><span class="line"><span class="string">    metricsBindAddress: &quot;&quot;</span></span><br><span class="line"><span class="string">    mode: &quot;ipvs&quot;              # 代理模型，空值代表是iptables</span></span><br><span class="line"><span class="string">    nodePortAddresses: null</span></span><br><span class="line"><span class="string">    ……</span></span><br></pre></td></tr></table></figure>

<p>配置完成后，以灰度模式手动逐个或分批次删除kube-system名称空间中kube-proxy旧版本的Pod实例，全部更新完成后便切换到了ipvs代理模型。或者，在测试环境中，可以直接使用如下命令一次性完成所有实例的强制更新。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl delete pods -l k8s-app=kube-proxy -n kube-system</span></span><br></pre></td></tr></table></figure>

<p>提示<br>用于生产环境时，建议在部署Kubernetes集群时直接选定要使用的代理模型，或在集群部署完成后立即调整代理模型，而后再部署其他应用。</p>
<ol start="2">
<li>ipvs代理模型下的Service资源<br>相较于iptables代理模型的复杂表示逻辑，ipvs的代理逻辑也较为简单，它仅有两个关键配置要素。首先，kube-proxy会在每个节点上创建一个名为kube-ipvs0的虚拟网络接口，并将集群上所有Service对象的ClusterIP和ExternalIP配置到该接口，使相应IP地址的流量都可被当前节点捕获。其次，kube-proxy会为每个Service生成相关的ipvs虚拟服务器（Virtual Server）定义，该虚拟服务器的真实服务器（Real Server）是由相应Service对象的后端端点组成，到达虚拟服务器VIP（虚拟IP地址）上的服务端口的请求流量由默认或指定的调度算法分发至相关的各真实服务器。<br>但kube-proxy对ClusterIP和NodePort类型Service对象的虚拟服务定义方式略有不同。对于每个ClusterIP类型的Service，kube-proxy仅针对Service_IP生成单个虚拟服务，协议和端口遵循Service的定义。以前面创建的demoapp-svc为例，它的ClusterIP是10.97.72.1，它的虚拟服务定义如下，这些可以通过ipvsadm -Ln命令在集群中的任意一个节点上获取。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep -A 3 &quot;10.97.72.1&quot;</span><br><span class="line">TCP  10.97.72.1:80 rr</span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.1.11:80              Masq    1      0          0</span>         </span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.2.7:80               Masq    1      0          0</span>         </span><br><span class="line"><span class="meta">  -&gt; </span><span class="language-bash">10.244.3.9:80               Masq    1      0          0</span></span><br></pre></td></tr></table></figure>

<p>而对于NodePort类型Service，kube-proxy会针对kube-ipvs0上的Service_IP:Service_Port，以及当前节点上的所有活动接口的主IP地址的NodePort各定义一个虚拟服务，下面的命令用于获取前面创建的NodePort类型Service对象的demoapp-nodeport-svc的相关虚拟服务的定义。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep -E &quot;31398|10.97.56.1&quot;</span><br><span class="line">TCP  172.29.9.11:31398 rr     # 节点IP</span><br><span class="line">TCP  10.97.56.1:80 rr         # ClusterIP</span><br><span class="line">TCP  10.244.1.0:31398 rr      # flannel.1接口IP</span><br><span class="line">TCP  10.244.1.1:31398 rr      # cni0接口IP</span><br><span class="line">TCP  127.0.0.1:31398 rr       # lo接口IP</span><br><span class="line">TCP  172.17.0.1:31398 rr      # docker0接口IP</span><br></pre></td></tr></table></figure>

<p>LoadBalancer类型Service的配置方式与NodePort类型相似，这里不再单独说明。另外，对于每个ExternalI，kube-proxy也会根据每个ExternalIP:Service_Port的组合生成一个虚拟服务，下面的命令及结果显示出前面创建的外部IP地址172.29.9.26相关的虚拟服务。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@k8s-node01:~# ipvsadm -Ln | grep  &quot;172.29.9.26&quot;</span><br><span class="line">TCP  172.29.9.26:80 rr</span><br></pre></td></tr></table></figure>

<p>上述每种Service类型对应的所有虚拟服务内部同样都使用NAT模式进行请求代理，除了更加多样的调度算法选择外，它的转发性能并没有显著提升，不过因为避免了使用大量的iptables规则，所以系统资源开销显著降低。ipvs仅实现了代理和调度机制，Service资源中的报文过滤和源地址转换等功能，依旧要由iptables完成，但相应的规则数量较少且较为固定。</p>
<h2 id="Kubernetes服务发现"><a href="#Kubernetes服务发现" class="headerlink" title="Kubernetes服务发现"></a>Kubernetes服务发现</h2><p>Kubernetes系统上的Service为Pod中的服务类应用提供了一个固定的访问入口，但Pod客户端中的应用还需要借助服务发现机制获取特定服务的IP和端口。</p>
<h3 id="服务发现概述"><a href="#服务发现概述" class="headerlink" title="服务发现概述"></a>服务发现概述</h3><p>服务发现机制的基本实现，一般是事先部署好一个网络位置较为稳定的服务注册中心（也称为服务总线），由服务提供者（服务端）向注册中心注册自己的位置信息，并在变动后及时予以更新，相应地，服务消费方周期性地从注册中心获取服务提供者的最新位置信息，从而“发现”要访问的目标服务资源。复杂的服务发现机制还会让服务提供者提供其描述信息、状态信息及资源使用信息等，以供消费者实现更为复杂的服务选择逻辑。<br>根据其发现过程的实现方式，服务发现还可分为两种类型：客户端发现和服务端发现。</p>
<ul>
<li>客户端发现：由客户端到服务注册中心发现其依赖的服务的相关信息，因此，它需要内置特定的服务发现程序和发现逻辑。</li>
<li>服务端发现：这种方式额外要用到一个称为中央路由器或服务均衡器的组件；服务消费者将请求发往中央路由器或者负载均衡器，由它们负责查询服务注册中心获取服务提供者的位置信息，并将服务消费者的请求转发给服务提供者。<br>服务注册中心是服务发现得以落地的核心组件。</li>
</ul>
<p>在传统实践中，常见的服务注册中心是ZooKeeper和etcd等分布式键值存储系统，它们可提供基本的数据存储功能，但距离实现完整的服务发现机制还有大量的二次开发任务需要完成。而且，它们更注重数据一致性而不得不弱化可用性（分布式系统的CAP理论），这背离了微服务发现场景中更注重服务可用性的需求。<br>Netflix的Eureka是专用于服务发现的分布式系统，遵从“存在少量的错误数据，总比完全不可用要好”的设计原则，服务发现和可用性是其核心目标，能够在多种故障期间保持服务发现和服务注册的功能。另一个同级别的实现是Consul，它于服务发现的基础功能之外还提供了多数据中心的部署等一众出色的特性。<br>尽管传统的DNS系统不适于微服务环境中的服务发现，但SkyDNS项目结合古老的DNS技术和时髦的Go语言、Raft算法，并构建于etcd存储系统之上，为Kubernetes系统实现了一种独特且实用的服务发现机制。Kubernetes在v1.3版本引入的KubeDNS由kubedns、dnsmasq和sidecar这3个部分组合而成。第一个部分包含kubedns和skydns两个组件，前者负责将Service和Endpoint转换为SkyDNS可以理解的格式；第二部分用于增强解析功能；第三部分为前两者添加健康状态检查机制，因而我们可以把KubeDNS视为SkyDNS的增强版。<br>而另一个基于DNS较新的服务发现项目是由CNCF孵化的CoreDNS，它基于Go语言开发，通过串接一组实现DNS功能的插件的插件链实现所有功能，也允许用户自行开发和添加必要的插件，但所有功能运行在单个容器之中。另外，CoreDNS使用Caddy作为底层的Web Server，可以支持以UDP、TLS、gRPC和HTTPS等方式对外提供DNS服务。自Kubernetes 1.11版本起，CoreDNS取代kubeDNS成为默认的DNS附件。</p>
<h3 id="基于环境变量的服务发现"><a href="#基于环境变量的服务发现" class="headerlink" title="基于环境变量的服务发现"></a>基于环境变量的服务发现</h3><p>创建Pod资源时，kubelet会将其所属名称空间内的每个活动的Service对象以一系列环境变量的形式注入其中。它支持使用Kubernetes Service环境变量以及与Docker的Link兼容的变量。</p>
<h4 id="（1）Kubernetes-Service环境变量"><a href="#（1）Kubernetes-Service环境变量" class="headerlink" title="（1）Kubernetes Service环境变量"></a>（1）Kubernetes Service环境变量</h4><p>Kubernetes为每个Service资源生成包括以下形式的环境变量在内的一系列环境变量，在同一名称空间中创建的Pod对象都会自动拥有这些变量：</p>
<ul>
<li>{SVCNAME}SERVICE_HOST</li>
<li>{SVCNAME}_SERVICE_PORT注意<br>如果SVCNAME中使用了连接线，Kubernetes会在定义环境变量时将其转换为下划线。</li>
</ul>
<h4 id="（2）Docker-Link形式的环境变量"><a href="#（2）Docker-Link形式的环境变量" class="headerlink" title="（2）Docker Link形式的环境变量"></a>（2）Docker Link形式的环境变量</h4><p>Docker使用–link选项实现容器连接时所设置的环境变量形式，具体使用方式请参考Docker的相关文档。在创建Pod对象时，Kubernetes也会把与此形式兼容的一系列环境变量注入Pod对象中。<br>例如，在Service资源demoapp-svc创建后创建的Pod对象中查看可用的环境变量，其中以DEMOAPP_SVC_SERVICE开头的为Kubernetes Service环境变量，名称中不包含SERVICE字符串的环境变量为Docker Link形式的环境变量。下面的命令创建了一个临时Pod对象，并在其命令行列出与demoapp-svc的相关环境变量。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client-pod --image=ikubernetes/admin-toolbox:v1.0 -it --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@client-pod /]# printenv | grep DEMOAPP_SVC</span><br><span class="line">DEMOAPP_SVC_SERVICE_PORT_HTTP=80</span><br><span class="line">DEMOAPP_SVC_SERVICE_HOST=10.97.72.1</span><br><span class="line">DEMOAPP_SVC_SERVICE_PORT=80</span><br><span class="line">DEMOAPP_SVC_PORT=tcp://10.97.72.1:80</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_ADDR=10.97.72.1</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_PORT=80</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP_PROTO=tcp</span><br><span class="line">DEMOAPP_SVC_PORT_80_TCP=tcp://10.97.72.1:80</span><br></pre></td></tr></table></figure>

<p>基于环境变量的服务发现功能简单、易用，但存在一定局限，例如只有那些与新建Pod对象在同一名称空间中且事先存在的Service对象的信息才会以环境变量形式注入，而那些不在同一名称空间，或者在Pod资源创建之后才创建的Service对象的相关环境变量则不会被添加。</p>
<h3 id="基于DNS的服务发现"><a href="#基于DNS的服务发现" class="headerlink" title="基于DNS的服务发现"></a>基于DNS的服务发现</h3><p>名称解析和服务发现是Kubernetes系统许多功能得以实现的基础服务，ClusterDNS通常是集群安装完成后应该立即部署的附加组件。Kubernetes集群上的每个Service资源对象在创建时都会被自动指派一个遵循&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的名称，并由ClusterDNS为该名称自动生成资源记录，service、ns和zone分别代表服务的名称、名称空间的名称和集群的域名。例如demoapp-svc的DNS名称为demoapp-svc.default.svc.cluster.local.，其中cluster.local.是未明确指定域名后缀的集群默认使用的域名。<br>无论使用kubeDNS还是CoreDNS，它们提供的基于DNS的服务发现解决方案都会负责为该DNS名称解析相应的资源记录类型以实现服务发现。以拥有ClusterIP的多种Service资源类型（ClusterIP、NodePort和LoadBalancer）为例，每个Service对象都会具有以下3个类型的DNS资源记录。</p>
<ul>
<li>1）根据ClusterIP的地址类型，为IPv4生成A记录，为IPv6生成AAAA记录。<ul>
<li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;cluster-ip&gt;</li>
<li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;cluster-ip&gt;</li>
</ul>
</li>
<li>2）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。<ul>
<li>_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.&lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li>
</ul>
</li>
<li>3）对于每个给定的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4）都要生成PTR记录，它们各自的格式如下所示：<ul>
<li>&lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li>
<li>h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.<br>例如，</li>
</ul>
</li>
</ul>
<p>前面在default名称空间中创建的Service对象demoapp-svc的地址为10.97.72.1，且为TCP协议的80端口取名http，对于默认的cluster.local域名来说，它会拥有如下3个DNS资源记录。</p>
<ul>
<li>A记录：demoapp-svc.default.svc.cluster.local. 30 IN A 10.97.72.1</li>
<li>SRV记录：_http._tcp.demoapp-svc.default.svc.cluster.local. 30 IN SRV 0 100 80 demoapp- svc.default.svc.cluster.local.</li>
<li>PTR记录：1.72.97.10.in-addr.arpa. 30 IN PTR demoapp-svc.default.svc.cluster.local。</li>
</ul>
<p>kubelet会为创建的每一个容器在/etc/resolv.conf配置文件中生成DNS查询客户端依赖的必要配置，相关的配置信息源自kubelet的配置参数。各容器的DNS服务器由clusterDNS参数的值设定，它的取值为kube-system名称空间中的Service对象kube-dns的ClusterIP，默认为10.96.0.10，而DNS搜索域的值由clusterDomain参数的值设定，若部署Kubernetes集群时未特别指定，其值将为cluster.local、svc.cluster.local和NAMESPACENAME.svc.cluster.local。下面的示例取自集群上一个随机选择的Pod中的容器。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">search default.svc.cluster.local svc.cluster.local cluster.local</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<p>上述search参数中指定的DNS各搜索域，是以次序指定的几个域名后缀，它们各自的域名如下所示。</p>
<ul>
<li>&lt;ns&gt;.svc.&lt;zone&gt;：附带有特定名称空间的域名，例如default.svc.cluster.local。</li>
<li>svc. &lt;zone&gt;：附带了Kubernetes标识Service专用子域svc的域名，例如svc.cluster.local。</li>
<li>&lt;zone&gt;：集群本地域名，例如cluster.local。</li>
</ul>
<p>各容器能够直接向集群上的ClusterDNS发起服务名称和端口名称解析请求完成服务发现，各名称也支持短格式，由搜索域自动补全相关的后缀。我们可以在Kubernetes集群上通过任意一个有nslookup等DNS测试工具的容器进行测试。下面基于此前创建专用于测试的客户端Pod对象client-pod的交互式接口完成后续测试操作。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client-pod -- /bin/sh</span></span><br><span class="line">[root@client-pod /]#</span><br></pre></td></tr></table></figure>

<p>接下来便可以进行名称解析测试。例如，下面的命令用于请求同一名称空间（default）中的服务名称demoapp-svc的解析结果，并获得了正确的返回值。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=A demoapp-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   demoapp-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.97.72.1</span><br></pre></td></tr></table></figure>

<p>ClusterDNS解析demoapp-svc服务名称的搜索次序依次是default.svc.cluster.local、svc.cluster.local和cluster.local，因此基于DNS的服务发现不受Service资源所在名称空间和创建时间的限制。上面的解析结果也正是默认的default名称空间中创建的demoapp-svc服务的IP地址。<br>SRV记录中的端口名称的格式_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，同样可使用短格式名称。下面的命令用于请求解析demoapp-svc上的http端口，它返回的结果为80。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod ~]# nslookup -query=SRV _http._tcp.demoapp-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">_http._tcp.demoapp-svc.default.svc.cluster.local        service = 0 100 80 demoapp-svc.default.svc.cluster.local.</span><br></pre></td></tr></table></figure>

<p>请求解析其他名称空间中的Service对象名称时需要明确指定服务名称和名称空间，下面以kube-dns.kube-system为例进行解析请求。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=A kube-dns.kube-system</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   kube-dns.kube-system.svc.cluster.local</span><br><span class="line">Address: 10.96.0.10</span><br></pre></td></tr></table></figure>

<p>端口名称解析时同样需要指定Service名称及其所在的名称空间，下面的命令用于请求解析kube-dns.kube-system上的metrics端口，它返回了9153的端口号。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=SRV _metrics._tcp.kube-dns.kube-system</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">_metrics._tcp.kube-dns.kube-system.svc.cluster.local    service = 0 100 9153 kube-dns.kube-system.svc.cluster.local.</span><br></pre></td></tr></table></figure>

<p>为了减少搜索次数，无论是否处于同一名称空间，客户端都可以直接使用FQDN格式的名称解析Service名称和端口名称，这也是在某应用的配置文件中引用其他服务时建议遵循的方式。</p>
<h3 id="Pod的DNS解析策略与配置"><a href="#Pod的DNS解析策略与配置" class="headerlink" title="Pod的DNS解析策略与配置"></a>Pod的DNS解析策略与配置</h3><p>Kubernetes还支持在单个Pod资源规范上自定义DNS解析策略和配置，它们分别使用spec.dnsPolicy和spec.dnsConfig进行定义，并组合生效。目前，Kubernetes支持如下DNS解析策略，它们定义在spec.dnsPolicy字段上。</p>
<ul>
<li>Default：从运行所在的节点继承DNS名称解析相关的配置。</li>
<li>ClusterFirst：在集群DNS服务器上解析集群域内的名称，其他域名的解析则交由从节点继承而来的上游名称服务器。</li>
<li>ClusterFirstWithHostNet：专用于在设置了hostNetwork的Pod对象上使用的ClusterFirst策略，任何配置了hostNetwork的Pod对象都应该显式使用该策略。</li>
<li>None：用于忽略Kubernetes集群的默认设定，而仅使用由dnsConfig自定义的配置。<br>Pod资源的自定义DNS配置要通过嵌套在spec.dnsConfig字段中的如下几个字段进行，它们的最终生效结果要结合dnsPolicy的定义生成。</li>
<li>nameservers &lt;[]string&gt;：DNS名称服务器列表，它附加在由dnsPolicy生成的DNS名称服务器之后。</li>
<li>searches &lt;[]string&gt;：DNS名称解析时的搜索域，它附加在dnsPolicy生成的搜索域之后。</li>
<li>options &lt;[]Object&gt;：DNS解析选项列表，它将会同dnsPolicy生成的解析选项合并成最终生效的定义。<br>下面配置清单示例（pod-with-dnspolicy.yaml）中定义的Pod资源完全使用自定义的配置，它通过将dnsPolicy设置为None而拒绝从节点继承DNS配置信息，并在dnsConfig中自定义了要使用的DNS服务、搜索域和DNS选项。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-with-dnspolicy</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">dnsPolicy:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">dnsConfig:</span></span><br><span class="line">    <span class="attr">nameservers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.10</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">223.5</span><span class="number">.5</span><span class="number">.5</span></span><br><span class="line">    <span class="bullet">-</span> <span class="number">223.6</span><span class="number">.6</span><span class="number">.6</span></span><br><span class="line">    <span class="attr">searches:</span> </span><br><span class="line">    <span class="bullet">-</span> <span class="string">svc.cluster.local</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">cluster.local</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ilinux.io</span></span><br><span class="line">    <span class="attr">options:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">ndots</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;5&quot;</span></span><br></pre></td></tr></table></figure>

<p>将上述配置清单中定义的Pod资源创建到集群之上，它最终会生成类似如下内容的/etc/resolv.conf配置文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">nameserver 10.96.0.10</span><br><span class="line">nameserver 223.5.5.5</span><br><span class="line">nameserver 223.6.6.6</span><br><span class="line">search svc.cluster.local cluster.local ilinux.io</span><br><span class="line">options ndots:5</span><br></pre></td></tr></table></figure>

<p>上面配置中的搜索域要求，即便是客户端与目标服务位于同一名称空间，也要求在短格式的服务名称上显式指定其所处的名称空间。感兴趣的读者可自行测试其效果。</p>
<h3 id="配置CoreDNS"><a href="#配置CoreDNS" class="headerlink" title="配置CoreDNS"></a>配置CoreDNS</h3><p>CoreDNS是高度模块化的DNS服务器，几乎全部功能均由可插拔的插件实现。CoreDNS调用的插件及相关的配置定义在称为Corefile的配置文件中。CoreDNS主要用于定义各服务器监听地址和端口、授权解析的区域以及加载的插件等，配置格式如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ZONE:[PORT] &#123;</span><br><span class="line">    [PLUGIN]...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>参数说明如下。</p>
<ul>
<li>ZONE：定义该服务器授权解析的区域，它监听由PORT指定的端口。</li>
<li>PLUGIN：定义要加载的插件，每个插件可能存在一系列属性，而每个属性还可能存在可配置的参数。<br>由kubeadm在部署Kubernetes集群时自动部署的CoreDNS的Corefile存储为kube-system名称空间中名为coredns的ConfigMap对象，定义了一个监听53号端口授权解析根区域的服务器，详细的配置信息及各插件的简单说明如下所示。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    .:53 &#123;</span></span><br><span class="line"><span class="string">        errors  # 将错误日志发往标准输出stdout</span></span><br><span class="line"><span class="string">        health &#123;  </span></span><br><span class="line"><span class="string">           lameduck 5s</span></span><br><span class="line"><span class="string">        &#125;       # 通过http://localhost:8080/health报告健康状态</span></span><br><span class="line"><span class="string">        ready   # 待所有插件就绪后通过8181端口响应“200 OK”以报告就绪状态</span></span><br><span class="line"><span class="string">        kubernetes cluster.local in-addr.arpa ip6.arpa &#123;</span></span><br><span class="line"><span class="string">           pods insecure</span></span><br><span class="line"><span class="string">           fallthrough in-addr.arpa ip6.arpa</span></span><br><span class="line"><span class="string">           ttl 30</span></span><br><span class="line"><span class="string">        &#125;       # Kubernetes系统的本地区域及专用的名称解析配置</span></span><br><span class="line"><span class="string">        prometheus :9153  # 通过http://localhost:9153/metrics输出指标数据</span></span><br><span class="line"><span class="string">        forward . /etc/resolv.conf  # 非Kubernetes本地域名的解析转发逻辑</span></span><br><span class="line"><span class="string">        cache 30     # 缓存时长</span></span><br><span class="line"><span class="string">        loop         # 探测转发循环并终止其过程</span></span><br><span class="line"><span class="string">        reload       # Corefile内容改变时自动重载配置信息</span></span><br><span class="line"><span class="string">        loadbalance  # A、AAAA或MX记录的负载均衡器，使用round-robin算法</span></span><br><span class="line"><span class="string">    &#125;</span></span><br></pre></td></tr></table></figure>

<p>在该配置文件中，专用于Kubernetes系统上的名称解析服务由名为kubernetes的插件进行定义，该插件负责处理指定的权威区域中的所有查询，例如上面示例中的正向解析区域cluster.local，以及反向解析区域in-addr.arpa和ip6.arpa。该插件支持多个配置参数，例如endpoint、tls、kubeconfig、namespaces、labels、pods、ttl和fallthrough等，上面示例中用到的3个参数的功能如下。</p>
<ul>
<li>1）pods POD-MODE：设置用于处理基于Pod IP地址的A记录的工作模式，以便在直接同Pod建立SSL通信时验证证书信息；默认值为disabled，表示不处理Pod请求，总是响应NXDOMAIN；在其他可用值中，insecure表示直接响应A记录而无须向Kubernetes进行校验，目标在于兼容kube-dns；而verified表示仅在指定的名称空间中存在一个与A记录中的IP地址相匹配的Pod对象时才会将结果响应给客户端。</li>
<li>2）fallthrough [ZONES…]：常规情况下，该插件的权威区域解析结果为NXDOMAIN时即为最终结果，而该参数允许将该响应的请求继续转给后续的其他插件处理；省略指定目标区域时表示生效于所有区域，否则，将仅生效于指定的区域。</li>
<li>3）ttl：自定义响应结果的可缓存时长，默认为5秒，可用值范围为[0,3600]。<br>那些非由kubernetes插件所负责解析的本地匹配的名称，将由forward插件定义的方式转发给其他DNS服务器进行解析，示例中的配置表示将根区域的解析请求转发给主机配置文件/etc/resolv.conf中指定的DNS服务器进行。若要将请求直接转发给指定的DNS服务器，则将该文件路径替换为目标DNS服务器的IP地址即可，多个IP地址之间以空白字符分隔。例如，下面的配置示例表示将除了ilinux.io区域之外的其他请求转给223.5.5.5或223.6.6.6进行解析。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">. &#123;</span><br><span class="line">  forward . 223.5.5.5 223.6.6.6 &#123;</span><br><span class="line">    except ilinux.io</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>CoreDNS的各插件与相关的配置属性、参数及详细使用方式请参考官方文档中的介绍：<a target="_blank" rel="noopener" href="https://coredns.io/plugins/%E3%80%82">https://coredns.io/plugins/。</a></p>
<h2 id="Headless-Service资源解析"><a href="#Headless-Service资源解析" class="headerlink" title="Headless Service资源解析"></a>Headless Service资源解析</h2><p>常规的ClusterIP、NodePort和LoadBalancer类型的Service对象可通过不同的入口来接收和分发客户端请求，且它们都拥有集群IP地址（ClusterIP）。然而，个别场景也可能不必或无须使用Service对象的负载均衡功能以及集群IP地址，而是借助ClusterDNS服务来代替实现这部分功能。Kubernetes把这类不具有ClusterIP的Service资源形象地称为Headless Service，该Service的请求流量无须kube-proxy处理，也不会有负载均衡和路由相关的iptables或ipvs规则。至于ClusterDNS如何自动配置Headless Service，则取决于Service标签选择器的定义。</p>
<ul>
<li>有标签选择器：由端点控制器自动创建与Service同名的Endpoint资源，而ClusterDNS则将Service名称的A记录直接解析为后端各端点的IP而非ClusterIP。</li>
<li>无标签选择器：ClusterDNS的配置分为两种情形，为ExternalName类型的服务（配置了spec.externalName字段）创建CNAME记录，而为与该Service同名的Endpoint对象上的每个端点创建一个A记录。<br>显然，ClusterDNS对待无标签选择器的第二种情形的Headless Service与对待有标签选择器的Headless </li>
</ul>
<p>Service的方式相同，区别仅在于相应的Endpoint资源是否由端点控制器基于标签选择器自动创建。通常，我们把无标签选择器的第一种情形（使用CNAME记录）的Headless Service当作一种独立的Service类型使用，即ExternalName Service，而将那些把Service名称使用A记录解析为端点IP地址的类型统一称为Headless Service。</p>
<h3 id="ExternalName-Service"><a href="#ExternalName-Service" class="headerlink" title="ExternalName Service"></a>ExternalName Service</h3><p>ExternalName Service是一种特殊类型的Service资源，它不需要使用标签选择器关联任何Pod对象，也无须定义任何端口或Endpoints，但必须要使用spec.externalName属性定义一个CNAME记录，用于返回真正提供服务的服务名称的别名。ClusterDNS会为这种类型的Service资源自动生成&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN CNAME &lt;extname&gt;.格式的DNS资源记录。<br>下面配置清单示例（externalname-redis-svc.yaml）中定义了一个名为externalname-redis-svc的Service资源，它使用DNS CNAME记录指向集群外部的redis.ik8s.io这一FQDN。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">externalname-redis-svc</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ExternalName</span></span><br><span class="line">  <span class="attr">externalName:</span> <span class="string">redis.ik8s.io</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">6379</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">6379</span></span><br><span class="line">    <span class="attr">nodePort:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">selector:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>

<p>待Service资源externalname-redis-svc创建完成后，各Pod对象即可通过短格式或FQDN格式的Service名称访问相应的服务。ClusterDNS会把该名称以CNAME格式解析为.spec.externalName字段中的名称，而后通过DNS服务将其解析为相应主机的IP地址。我们可通过此前Pod对象client-pod对该名称进行解析测试。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client -- /bin/sh</span>                                            </span><br><span class="line">[root@client-pod /]#</span><br></pre></td></tr></table></figure>

<p>未指定解析类型的，nslookup命令会对解析得到的CNAME结果自动进行更进一步的解析。例如下面命令中，请求解析externalname-redis-svc.default.svc.cluster.local名称得到CNAME格式的结果redis.ik8s.io将被进一步解析为A记录格式的结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup externalname-redis-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">externalname-redis-svc.default.svc.cluster.local    canonical name = redis.ik8s.io.</span><br><span class="line">Name:   redis.ik8s.io</span><br><span class="line">Address: 1.2.3.4</span><br></pre></td></tr></table></figure>

<p>ExternalName用于通过DNS别名将外部服务发布到Kubernetes集群上，这类的DNS别名同本地服务的DNS名称具有相同的形式。因而Pod对象可像发现和访问集群内部服务一样来访问这些发布到集群之上的外部服务，这样隐藏了服务的位置信息，使得各工作负载能够以相同的方式调用本地和外部服务。等到了能够或者需要把该外部服务引入到Kubernetes集群上之时，管理员只需要修改相应ExternalName Service对象的类型为集群本地服务即可。</p>
<h3 id="Headless-Service"><a href="#Headless-Service" class="headerlink" title="Headless Service"></a>Headless Service</h3><p>除了为每个Service资源对象在创建时自动指派一个遵循&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的DNS名称，ClusterDNS还会为Headless Service中的每个端点指派一个遵循&lt;hostname&gt;. &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;格式的DNS名称，因此，每个Headless Service资源对象的名称都会由ClusterDNS自动生成以下几种类型的资源记录。</p>
<ul>
<li>1）根据端点IP地址的类型，在Service名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。<ul>
<li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt;</li>
<li>&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt;</li>
</ul>
</li>
<li>2）根据端点IP地址的类型，在端点自身的hostname名称上为每个IPv4地址的端点生成A记录，为IPv6地址的端点生成AAAA记录。<ul>
<li>&lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN A &lt;endpoint-ip&gt;</li>
<li>&lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN AAAA &lt;endpoint-ip&gt;</li>
</ul>
</li>
<li>3）为每个定义了名称的端口生成一个SRV记录，未命名的端口号则不具有该记录。</li>
<li>_&lt;port&gt;._&lt;proto&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;. &lt;ttl&gt; IN SRV &lt;weight&gt; &lt;priority&gt; &lt;port-number&gt; &lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li>
<li>4）对于每个给定的每个端点的主机名称的A记录（例如a.b.c.d）或AAAA记录（例如a1a2a3a4:b1b2b3b4:c1c2c3c4:d1d2d3d4:e1e2e3e4:f1f2f3f4:g1g2g3g4:h1h2h3h4），都要生成PTR记录，它们各自的格式如下所示。<ul>
<li>&lt;d&gt;.&lt;c&gt;.&lt;b&gt;.&lt;a&gt;.in-addr.arpa. &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.</li>
<li>h4.h3.h2.h1.g4.g3.g2.g1.f4.f3.f2.f1.e4.e3.e2.e1.d4.d3.d2.d1.c4.c3.c2.c1.b4.b3.b2.b1.a4.a3.a2.a1.ip6.arpa &lt;ttl&gt; IN PTR &lt;hostname&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;.<br>定义Service资源时，只需要将其ClusterIP字段的值显式设置为None即可将其定义为Headless类型。下面是一个Headless Service资源配置示例，它拥有标签选择器，因而能够自动创建同名的Endpoint资源。</li>
</ul>
</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp-headless-svc</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">ports:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">targetPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">http</span></span><br></pre></td></tr></table></figure>

<p>将上面定义的Headless Service资源创建到集群上，我们从其资源详细描述中可以看出，demoapp-headless-svc没有ClusterIP，但因标签选择器能够匹配到Pod资源，因此它拥有端点记录。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f demoapp-headless-svc.yaml</span> </span><br><span class="line">service/demoapp-headless-svc created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe svc demoapp-headless-svc</span></span><br><span class="line">Name:              demoapp-headless-svc</span><br><span class="line">Namespace:         default</span><br><span class="line">Labels:            &lt;none&gt;</span><br><span class="line">Annotations:       Selector:  app=demoapp</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                None</span><br><span class="line">Port:              http  80/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         10.244.1.16:80,10.244.2.10:80,10.244.3.11:80</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>根据Headless Service的工作特性可知，它记录在ClusterDNS的A记录的相关解析结果是后端端点的IP地址，这就意味着客户端通过此Service资源的名称发现的是各Pod资源。下面依然通过Pod对象client-pod的交互式接口进行测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it client-pod -- /bin/sh</span></span><br><span class="line">[root@client-pod /]# nslookup -query=A demoapp-headless-svc</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line"></span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.3.11</span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.1.16</span><br><span class="line">Name:   demoapp-headless-svc.default.svc.cluster.local</span><br><span class="line">Address: 10.244.2.10</span><br></pre></td></tr></table></figure>

<p>其解析结果正是Headless Service通过标签选择器关联到的所有Pod资源的IP地址。于是，客户端向此Service对象发起的请求将直接接入Pod资源中的应用之上，而不再由Service资源进行代理转发，它每次接入的Pod资源是由DNS服务器接收到查询请求时以轮询方式返回的IP地址。<br>另一方面，每个IP地址的反向解析记录（PTR）对应的FQDN名称是相应端点所在主机的主机名称。对于Kubernetes上的容器来说，其所在主机的主机名是指Pod对象上的主机名称，它由Pod资源的spec.hostname字段和spec.subdomain组合定义，格式为&lt;hostname&gt;.subdomain&gt;.&lt;service&gt;.&lt;ns&gt;.svc.&lt;zone&gt;，其中的&lt;subdomain&gt;可省略。若此两者都未定义，则&lt;hostname&gt;值取自IP地址，IP地址a.b.c.d对应的主机名为a-b-c-d，如下面命令的解析结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@client-pod /]# nslookup -query=PTR 10.244.3.11</span><br><span class="line">Server:         10.96.0.10</span><br><span class="line">Address:        10.96.0.10#53</span><br><span class="line">11.3.244.10.in-addr.arpa        name = 10-244-3-11.demoapp-headless-svc.default.svc.cluster.local.</span><br></pre></td></tr></table></figure>

<p>StatefulSet控制器对象是Headless Service资源的一个典型应用场景，相关话题将会在第8章中详细描</p>
<h2 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h2><p>本章重点讲解了Kubernetes的Service资源基础概念、类型、实现机制及其发布方式等话题，并介绍了服务发现及Headless Service。</p>
<ul>
<li>Service资源通过标签选择器为一组任务负载创建一个统一的访问入口，它把客户端请求代理调度至后端各端点。</li>
<li>Service支持userspace、iptables和ipvs代理模型，iptables模式更为成熟稳定，而ipvs则在有大规模Service的场景中有着更好的性能表现。</li>
<li>ClusterIP是最基础的Service类型，它仅适用于集群内通信，NodePort和LoadBalancer能够将服务发布到集群外部；外部IP能够与这3种类型的Service组合使用，从而开放特定的IP接入外部流量。</li>
<li>Endpoint和EndpointSlice用于跟踪端点资源，并将端点信息提供给Service等。</li>
<li>Headless Service是没有ClusterIP的Service资源类型，它要么结合externalName以CNAME资源记录的形式映射至其他服务，要么以A记录或AAAA记录的形式解析至端点IP地址。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/" class="post-title-link" itemprop="url">Kubernetes基础</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-02-09 17:56:37 / 修改时间：18:16:52" itemprop="dateCreated datePublished" datetime="2022-02-09T17:56:37+08:00">2022-02-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Kubernetes集群架构"><a href="#Kubernetes集群架构" class="headerlink" title="Kubernetes集群架构"></a>Kubernetes集群架构</h1><p>Kubernetes属于典型的Server-Client形式的二层架构，在程序级别，Master主要由API Server（kube-apiserver）、Controller-Manager（kube-controller-manager）和Scheduler（kube-scheduler）这3个组件，以及一个用于集群状态存储的etcd存储服务组成，它们构成整个集群的控制平面；而每个Node节点则主要包含kubelet、kube-proxy及容器运行时（Docker是最为常用的实现）3个组件，它们承载运行各类应用容器。</p>
<h2 id="Kubernetes系统组件"><a href="#Kubernetes系统组件" class="headerlink" title="Kubernetes系统组件"></a>Kubernetes系统组件</h2><h3 id="Master组件"><a href="#Master组件" class="headerlink" title="Master组件"></a>Master组件</h3><p>Master 它维护有Kubernetes的所有对象记录，负责持续管理对象状态并响应集群中各种资源对象的管理操作，以及确保各资源对象的实际状态与所需状态相匹配。控制平面的各组件支持以单副本形式运行于单一主机，也能够将每个组件以多副本方式同时运行于多个主机上，提高服务可用级别。控制平面各组件及其主要功能如下。</p>
<h4 id="API-Server"><a href="#API-Server" class="headerlink" title="API Server"></a>API Server</h4><p>API Server是Kubernetes控制平面的前端，支持不同类型应用的生命周期编排，包括部署、缩放和滚动更新等。它还是整个集群的网关接口，由kube-apiserver守护程序运行为服务，通过HTTP/HTTPS协议将RESTful API公开给用户，是发往集群的所有REST操作命令的接入点，用于接收、校验以及响应所有的REST请求，并将结果状态持久存储于集群状态存储系统（etcd）中。</p>
<h4 id="集群状态存储-ETCD"><a href="#集群状态存储-ETCD" class="headerlink" title="集群状态存储(ETCD)"></a>集群状态存储(ETCD)</h4><p>Kubernetes集群的所有状态信息都需要持久存储于存储系统etcd中。</p>
<h4 id="控制器管理器-kube-controller-manager"><a href="#控制器管理器-kube-controller-manager" class="headerlink" title="控制器管理器(kube-controller-manager)"></a>控制器管理器(kube-controller-manager)</h4><p>控制器负责实现用户通过API Server提交的终态声明，驱动API对象的当前状态逼近或等同于期望状态。Kubernetes提供了驱动Node、Pod、Server、Endpoint、ServiceAccount和Token等API对象的控制器。</p>
<h4 id="调度器-kube-scheduler"><a href="#调度器-kube-scheduler" class="headerlink" title="调度器(kube-scheduler)"></a>调度器(kube-scheduler)</h4><p>Kubernetes系统上的调度是指为API Server接收到的每一个Pod创建请求，并在集群中为其匹配出一个最佳工作节点。kube-scheduler是默认调度器程序。</p>
<h3 id="Node组件"><a href="#Node组件" class="headerlink" title="Node组件"></a>Node组件</h3><p>Node组件是集群的“体力”输出者，每个Node会定期向Master报告自身的状态变动，并接受Master的管理。</p>
<h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>kubelet是运行于每个Node之上的“节点代理”服务，负责接收并执行Master发来的指令，以及管理当前Node上Pod对象的容器等任务。<br>kubelet会持续监视当前节点上各Pod的健康状态，包括基于用户自定义的探针进行存活状态探测，并在任何Pod出现问题时将其重建为新实例。它还内置了一个HTTP服务器，监听TCP协议的10248和10250端口：10248端口通过/healthz响应对kubelet程序自身的健康状态进行检测；10250端口用于暴露kubelet API，以验证、接收并响应API Server的通信请求。</p>
<h4 id="容器运行时环境"><a href="#容器运行时环境" class="headerlink" title="容器运行时环境"></a>容器运行时环境</h4><p>Pod是一组容器组成的集合并包含这些容器的管理机制。kubelet通过CRI（容器运行时接口）可支持多种类型的OCI容器运行时，例如docker、containerd、CRI-O、runC、fraki和Kata Containers等。</p>
<h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>kube-proxy，它把API Server上的Service资源对象转换为当前节点上的iptables或（与）ipvs规则，这些规则能够将那些发往该Service对象ClusterIP的流量分发至它后端的Pod端点之上。kube-proxy是Kubernetes的核心网络组件，它本质上更像是Pod的代理及负载均衡器，负责确保集群中Node、Service和Pod对象之间的有效通信。</p>
<h3 id="核心附件"><a href="#核心附件" class="headerlink" title="核心附件"></a>核心附件</h3><p>附件（add-ons）用于扩展Kubernetes的基本功能，它们通常运行于Kubernetes集群自身之上，可根据重要程度将其划分为必要和可选两个类别。网络插件是必要附件，管理员需要从众多解决方案中根据需要及项目特性选择，常用的有Flannel、Calico、Canal、Cilium和Weave Net等。KubeDNS通常也是必要附件之一，而Web UI（Dashboard）、容器资源监控系统、集群日志系统和Ingress Controller等是常用附件。</p>
<h4 id="CoreDNS"><a href="#CoreDNS" class="headerlink" title="CoreDNS"></a>CoreDNS</h4><p>Kubernetes使用定制的DNS应用程序实现名称解析和服务发现功能，它自1.11版本起默认使用CoreDNS——一种灵活、可扩展的DNS服务器；之前的版本中用到的是kube-dns项目，SkyDNS则是更早一代的解决方案。</p>
<h4 id="Dashboard"><a href="#Dashboard" class="headerlink" title="Dashboard"></a>Dashboard</h4><p>基于Web的用户接口，用于可视化Kubernetes集群。</p>
<h4 id="容器资源监控系统"><a href="#容器资源监控系统" class="headerlink" title="容器资源监控系统"></a>容器资源监控系统</h4><p>Kubernetes常用的指标监控附件有Metrics-Server、kube-state-metrics和Prometheus等。</p>
<h4 id="集群日志系统"><a href="#集群日志系统" class="headerlink" title="集群日志系统"></a>集群日志系统</h4><p>Kubernetes常用的集中式日志系统是由ElasticSearch、Fluentd和Kibana（称之为EFK）组合提供的整体解决方案。</p>
<h4 id="Ingress-Controller"><a href="#Ingress-Controller" class="headerlink" title="Ingress Controller"></a>Ingress Controller</h4><ul>
<li>Pod与Service</li>
</ul>
<p>Pod本质上是共享Network、IPC和UTS名称空间以及存储资源的容器集合。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220116110055992.png" alt="image-20220116110055992"></p>
<p>同一Pod内部的容器，它们共享网络协议栈、网络设备、路由、IP地址和端口等网络资源，可以基于本地回环接口lo互相通信。每个Pod上还可附加一组“存储卷”（volume）资源，它们同样可由内部所有容器使用而实现数据共享。持久类型的存储卷还能够确保在容器终止后被重启，甚至容器被删除后数据也不会丢失。<br>同时，这些以Pod形式运行于Kubernetes之上的应用通常以服务类程序居多，其客户端可能来自集群之外，例如现实中的用户，也可能是当前集群中其他Pod中的应用，如图1-10所示。Kubernetes集群的网络模型要求其各Pod对象的IP地址位于同一网络平面内（同一IP网段），各Pod间可使用真实IP地址直接进行通信而无须NAT功能介入，无论它们运行于集群内的哪个工作节点之上，这些Pod对象就像是运行于同一局域网中的多个主机上。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220116110307410.png" alt="image-20220116110307410"></p>
<p>Service是由基于匹配规则在集群中挑选出的一组Pod对象的集合、访问这组Pod集合的固定IP地址，以及对请求进行调度的方法等功能所构成的一种API资源类型，是Pod资源的代理和负载均衡器。Service匹配Pod对象的规则可用“标签选择器”进行体现，并根据标签来过滤符合条件的资源对象，如图1-11所示。标签是附加在Kubernetes API资源对象之上的具有辨识性的分类标识符，使用键值型数据表达，通常仅对用户具有特定意义。一个对象可以拥有多个标签，一个标签也可以附加于多个对象（通常是同一类对象）之上。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220116110413899.png" alt="image-20220116110413899"></p>
<p>每个节点上运行的kube-proxy组件负责管理各Pod与Service之间的网络连接，它并非Kubernetes内置的代理服务器，而是一个基于出站流量的负载均衡器组件。针对每个Service，kube-proxy都会在当前节点上转换并添加相应iptables DNAT规则或ipvs规则，从而将目标地址为某Service对象的ClusterIP的流量调度至该Service根据标签选择器匹配出的Pod对象之上。<br>CoreDNS附件会为集群中的每个Service对象（包括DNS服务自身)生成唯一的DNS名称标识，以及相应的DNS资源记录，服务的DNS名称遵循标准的svc.namespace.svc.cluster-domain格式。例如CoreDNS自身的服务名称为kube-dns.kube-system.svc.cluster.local.，则它的ClusterIP通常是10.96.0.10。<br>除非出于管理目的有意调整，Service资源的名称和ClusterIP在其整个生命周期内都不会发生变动。kubelet会在创建Pod容器时，自动在/etc/resolv.conf文件中配置Pod容器使用集群上CoreDNS服务的ClusterIP作为DNS服务器，因而各Pod可针对任何Service的名称直接请求相应的服务。换句话说，Pod可通过kube-dns.kube-system.svc.cluster.local.来访问集群DNS服务。Ingress资源是Kubernetes将集群外部HTTP/HTTPS流量引入到集群内部专用的资源类型，它仅用于控制流量的规则和配置的集合，其自身并不能进行“流量穿透”，要通过Ingress控制器发挥作用；目前，此类的常用项目有Nginx、Traefik、Envoy、Gloo、kong及HAProxy等。</p>
<h1 id="应用部署、运行与管理"><a href="#应用部署、运行与管理" class="headerlink" title="应用部署、运行与管理"></a>应用部署、运行与管理</h1><h2 id="应用容器与Pod资源"><a href="#应用容器与Pod资源" class="headerlink" title="应用容器与Pod资源"></a>应用容器与Pod资源</h2><p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120182840600.png" alt="image-20220120182840600"></p>
<p>同一Pod中，这些容器共享PID、IPC、Network和UTS名称空间的容器彼此间可通过IPC通信，共享使用主机名和网络接口、IP地址、端口和路由等各种网络资源，因而各容器进程能够通过lo网络接口通信且不能使用相同的网络套接字地址。一个Pod内通常仅应该运行具有强耦合关系的容器，否则除了pause以外，只应该存在单个容器，或者只存在单个主容器和一个以上的辅助类容器（例如服务网格中的Sidecar容器等）。</p>
<h3 id="容器设计模式"><a href="#容器设计模式" class="headerlink" title="容器设计模式"></a>容器设计模式</h3><h4 id="单容器模式"><a href="#单容器模式" class="headerlink" title="单容器模式"></a>单容器模式</h4><p>单容器模式是指将应用程序封装为应用容器运行。该模式需要遵循简单和单一原则，每个容器仅承载一种工作负载。</p>
<h4 id="单节点多容器模式"><a href="#单节点多容器模式" class="headerlink" title="单节点多容器模式"></a>单节点多容器模式</h4><p>单节点多容器模式的常见实现有Sidecar（边车）、适配器（Adapter）、大使（Ambassador）、初始化（Initializer）容器模式等。</p>
<h5 id="1-Sidecar模式"><a href="#1-Sidecar模式" class="headerlink" title="(1) Sidecar模式"></a>(1) Sidecar模式</h5><p>Sidecar模式是多容器系统设计的最常用模式，它由一个主应用程序（通常是Web应用程序）以及一个辅助容器（Sidecar容器）组成，该辅助容器用于为主容器提供辅助服务以增强主容器的功能，是主应用程序是必不可少的一部分，但却不一定非得存在于应用程序本身内部。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120195851648.png" alt="image-20220120195851648"></p>
<ul>
<li><p>sidecar的优势</p>
<ul>
<li>辅助应用的运行时环境和编程语言与主应用程序无关，因而无须为每种编程语言分别开发一个辅助工具；</li>
</ul>
</li>
</ul>
<ul>
<li><p>二者可基于IPC、lo接口或共享存储进行数据交换，不存在明显的通信延迟；</p>
</li>
<li><p>容器镜像是发布的基本单位，将主应用与辅助应用划分为两个容器使得其可由不同团队开发和维护，从而变得方便及高效，单独测试及集成测试也变得可能；</p>
</li>
<li><p>容器限制了故障边界，使得系统整体可以优雅降级，例如Sidecar容器异常时，主容器仍可继续提供服务；</p>
</li>
<li><p>容器是部署的基本单元，每个功能模块均可独立部署及回滚。<br><font color="red">事实上，这些优势对于其他模型来说同样存在。</font></p>
</li>
</ul>
<h5 id="2-大使模式"><a href="#2-大使模式" class="headerlink" title="(2) 大使模式"></a>(2) 大使模式</h5><p>大使模式本质上是一类代理程序，它代表主容器发送网络请求至外部环境中，因此可以将其视作与客户端（主容器应用）位于同一位置的“外交官”。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120195935929.png" alt="image-20220120195935929"></p>
<p><font color="red">大使模式的最佳用例之一是提供对数据库的访问。</font>实践中，开发环境、测试环境和生产环境中的主应用程序可能需要分别连接到不同的数据库服务。更好的方案是让应用程序始终通过localhost连接至大使容器，而如何正确连接到目标数据的责任则由大使容器完成。</p>
<h5 id="3-适配器模式"><a href="#3-适配器模式" class="headerlink" title="(3) 适配器模式"></a>(3) 适配器模式</h5><p>适配器模式（见图4-4）用于为主应用程序提供一致的接口，实现了模块重用，支持标准化和规范化主容器应用程序的输出以便于外部服务进行聚合。大使模式为内部容器提供了简化统一的外部服务视图，适配器模式则刚好反过来，它通过标准化容器的输出和接口，为外界展示了一个简化的应用视图。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120200043248.png" alt="image-20220120200043248"></p>
<h5 id="4-初始化容器模式"><a href="#4-初始化容器模式" class="headerlink" title="(4) 初始化容器模式"></a>(4) 初始化容器模式</h5><p>初始化容器模式（见图4-5）负责以不同于主容器的生命周期来完成那些必要的初始化任务，</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120200118062.png" alt="image-20220120200118062"></p>
<p>初始化容器将Pod内部的容器分成了两组：初始化容器和应用程序容器（主容器和Sidecar容器等），初始化容器可以不止一个，但它们需要以特定的顺序串行运行，并需要在启动应用程序容器之前成功终止。不过，多个应用程序容器一般需要并行启动和运行。<br>就Kubernetes来说，除了初始化容器之外，还有一些其他可用的初始化技术，例如admission controllers、admission webhooks和PodPresets等。</p>
<h4 id="多节点模式"><a href="#多节点模式" class="headerlink" title="多节点模式"></a>多节点模式</h4><h5 id="1-领导者选举模式"><a href="#1-领导者选举模式" class="headerlink" title="(1) 领导者选举模式"></a>(1) 领导者选举模式</h5><p>领导者选举模式示意图。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120200251976.png" alt="image-20220120200251976"></p>
<h5 id="2-工作队列模式"><a href="#2-工作队列模式" class="headerlink" title="(2) 工作队列模式"></a>(2) 工作队列模式</h5><p>分布式应用程序的各组件间存在大量的事件传递需求，当某应用组件需要将信息广播至大量订阅者时，可能需要与多个独立开发的，可能使用了不同平台、编程语言和通信协议的应用程序或服务通信，并且无须订阅者实时响应地通信，它具有解耦子系统、提高伸缩能力和可靠性、支持延迟事件处理、简化异构组件间的集成等优势。图4-7为工作队列模式示意图。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120200324527.png" alt="image-20220120200324527"></p>
<h5 id="3-分散-聚集"><a href="#3-分散-聚集" class="headerlink" title="(3) 分散/聚集"></a>(3) 分散/聚集</h5><p>分散/聚集模式与工作队列模式非常相似，它同样将大型任务拆分为较小的任务，区别是容器会立即将响应返回给用户，一个很好的例子是MapReduce算法。该模式需要两类组件：一个称为“根”节点或“父”节点的组件，将来自客户端的请求切分成多个小任务并分散到多个节点并行计算；另一类称为“计算”节点或“叶子”节点，每个节点负责运行一部分任务分片并返回结果数据，“根”节点收集这些结果数据并聚合为有意义的数据返回给客户端。开发这类分布式系统需要请求扇出、结果聚合以及与客户端交互等大量的模板代码，但大部分都比较通用。因而要实现该模式，我们只需要分别将两类组件各自构建为容器即可。</p>
<h3 id="Pod的生命周期"><a href="#Pod的生命周期" class="headerlink" title="Pod的生命周期"></a>Pod的生命周期</h3><p>Kubernetes为Pod资源严格定义了5种相位，并将特定Pod对象的当前相位存储在其内部的子对象PodStatus的phase字段上，因而它总是应该处于其生命进程中以下几个相位之一。</p>
<ul>
<li><p>Pending：API Server创建了Pod资源对象并已存入etcd中，但它尚未被调度完成，或仍处于从仓库中下载容器镜像的过程中。</p>
</li>
<li><p>Running：Pod已经被调度至某节点，所有容器都已经被kubelet创建完成，且至少有一个容器处于启动、重启或运行过程中。</p>
</li>
<li><p>Succeeded：Pod中的所有容器都已经成功终止且不会再重启。</p>
</li>
<li><p>Failed：所有容器都已经终止，但至少有一个容器终止失败，即容器以非0状态码退出或已经被系统终止。</p>
</li>
<li><p>Unknown：API Server无法正常获取到Pod对象的状态信息，通常是由于其无法与所在工作节点的kubelet通信所致。</p>
</li>
</ul>
<p><font color="red">阶段仅是对Pod对象生命周期运行阶段的概括性描述，而非Pod或内部容器状态的综合汇总，因此Pod对象的status字段中的状态值未必一定是可用的相位，它也有可能是Pod的某个错误状态，例如CrashLoopBackOff或Error等。</font><br>Pod资源的核心职责是运行和维护称为主容器的应用程序容器，在其整个生命周期之中的多种可选行为也是围绕更好地实现该功能而进行，如图4-8所示。其中，初始化容器（init container）是常用的Pod环境初始化方式，健康状态检测（startupProbe、livenessProbe和readinessProbe）为编排工具提供了监测容器运行状态的编程接口，而事件钩子（preStop和postStart）则赋予了应用容器读取来自编排工具上自定义事件的机制。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220120200439355.png" alt="image-20220120200439355"></p>
<p>若用户给出了上述全部定义，则一个Pod对象生命周期的运行步骤如下。</p>
<blockquote>
<p>1）在启动包括初始化容器在内的任何容器之前先创建pause基础容器，它初始化Pod环境并为后续加入的容器提供共享的名称空间。<br>2）按顺序以串行方式运行用户定义的各个初始化容器进行Pod环境初始化；任何一个初始化容器运行失败都将导致Pod创建失败，并按其restartPolicy的策略进行处理，默认为重启。<br>3）待所有初始化容器成功完成后，启动应用程序容器，多容器Pod环境中，此步骤会并行启动所有应用容器，例如主容器和Sidecar容器，它们各自按其定义展开其生命周期；本步骤及后面的几个步骤都将以主容器为例进行说明；容器启动的那一刻会同时运行主容器上定义的PostStart钩子事件，该步骤失败将导致相关容器被重启。<br>4）运行容器启动健康状态监测（startupProbe），判定容器是否启动成功；该步骤失败，同样参照restartPolicy定义的策略进行处理；未定义时，默认状态为Success。<br>5）容器启动成功后，定期进行存活状态监测（liveness）和就绪状态监测（readiness）；存活状态监测失败将导致容器重启，而就绪状态监测失败会使得该容器从其所属的Service对象的可用端点列表中移除。<br>6）终止Pod对象时，会先运行preStop钩子事件，并在宽限期（terminationGrace-PeriodSeconds）结束后终止主容器，宽限期默认为30秒。</p>
</blockquote>
<h2 id="在Pod中运行应用"><a href="#在Pod中运行应用" class="headerlink" title="在Pod中运行应用"></a>在Pod中运行应用</h2><p>Pod资源中可同时存在初始化容器、应用容器和临时容器3种类型的容器，不过创建并运行一个具体的Pod对象时，仅有应用容器是必选项，并且可以仅为其定义单个容器。</p>
<h2 id="使用单容器Pod资源"><a href="#使用单容器Pod资源" class="headerlink" title="使用单容器Pod资源"></a>使用单容器Pod资源</h2><p>一个Pod对象的核心职责在于以主容器形式运行单个应用，因而定义API资源的关键配置就在于定义该容器，它以对象形式定义在Pod对象的spec.containers字段中，基本格式如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">…</span>               <span class="comment"># Pod的标识名，在名称空间中必须唯一</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">…</span>          <span class="comment"># 该Pod所属的名称空间，省略时使用默认名称空间，例如default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span>           <span class="comment"># 定义容器，它是一个列表对象，可包括多个容器的定义，至少得有一个</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span>             <span class="comment"># 容器名称，必选字段，在当前Pod中必须唯一</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">…</span>            <span class="comment"># 创建容器时使用的镜像</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">…</span>  <span class="comment"># 容器镜像下载策略，可选字段</span></span><br></pre></td></tr></table></figure>

<p>image虽为可选字段，这只是为方便更高级别的管理类资源（例如Deployment等）能覆盖它以实现某种高级管理功能而设置，对于非控制器管理的自主式Pod来说并不能省略该字段。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br></pre></td></tr></table></figure>

<p>把上面的内容保存于配置文件pod-demo.yaml中，随后即可使用kubectl apply或kubectl create命令进行资源对象创建</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-demo.yaml</span></span><br><span class="line">pod/pod-demo created</span><br></pre></td></tr></table></figure>

<p>该Pod对象由调度器绑定至特定工作节点后，由相应的kubelet负责创建和维护，实时状态也将同步给API Server并由其存储至etcd中。Pod创建并尝试启动的过程中，可能会经历Pending、ContainerCreating、Running等多种不同的状态，若Pod可正常启动，则kubectl get pods/POD命令输出字段中的状态（STATUS）则显示为Running</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/pod-demo -n default</span></span><br><span class="line">NAME  READY   STATUS  RESTARTS  AGE</span><br><span class="line">pod-demo   1/1     Running   0          5m</span><br></pre></td></tr></table></figure>

<p>随后即可对Pod中运行着的主容器的服务发起访问请求。镜像demoapp默认运行了一个Web服务程序，该服务监听TCP协议的80端口，镜像可通过“/”、/hostname、/user-agent、/livez、/readyz和/configs等路径服务于客户端的请求。例如，下面的命令先获取到Pod的IP地址，而后对其支持的Web资源路径/和/user-agent分别发出了一个访问请求：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">demoIP=$(kubectl get pods/pod-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line">~ $ curl -s http://$demoIP</span><br><span class="line">iKubernetes demoapp v1.0 ! ClientIP: 10.244.0.0, ServerName: pod-demo, ServerIP: 10.244.2.3!</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -s http://<span class="variable">$demoIP</span>/user-agent</span></span><br><span class="line">User-Agent: curl/7.58.0</span><br></pre></td></tr></table></figure>

<p>容器的imagePullPolicy字段用于为其指定镜像获取策略，可用值包括如下几个。</p>
<ul>
<li><p>Always：每次启动Pod时都要从指定的仓库下载镜像。</p>
</li>
<li><p>IfNotPresent：仅本地镜像缺失时方才从目标仓库wp下载镜像。</p>
</li>
<li><p>Never：禁止从仓库下载镜像，仅使用本地镜像。</p>
</li>
</ul>
<p>对于标签为<font color="red">latest的镜像文件，其默认的镜像获取策略为Always，其他标签的镜像，默认策略则为IfNotPresent。</font>需要注意的是，从私有仓库中下载镜像时通常需要事先到Registry服务器认证后才能进行。认证过程要么需要在相关节点上交互式执行docker login命令，要么将认证信息定义为专有的Secret资源，并配置Pod通过imagePullSecretes字段调用此认证信息完成。<br>删除Pod对象则使用kubectl delete命令。</p>
<ul>
<li>命令式命令：kubectl delete pods/NAME。</li>
<li>命令式对象配置：kubectl delete -f FILENAME。</li>
</ul>
<p>若删除后Pod一直处于Terminating状态，则可再一次执行删除命令，并同时使用–force和–grace-period=0选项进行强制删除。</p>
<h2 id="获取Pod与容器状态详情"><a href="#获取Pod与容器状态详情" class="headerlink" title="获取Pod与容器状态详情"></a>获取Pod与容器状态详情</h2><p>kubectl有多个子命令，用于从不同角度显示对象的状态信息，这些信息有助于用户了解对象的运行状态、属性详情等。</p>
<ul>
<li>kubectl describe：显示资源的详情，包括运行状态、事件等信息，但不同的资源类型输出内容不尽相同。</li>
<li>kubectl logs：查看Pod对象中容器输出到控制台的日志信息；当Pod中运行有多个容器时，需要使用选项-c指定容器名称。</li>
<li>kubectl exec：在Pod对象某容器内运行指定的程序，其功能类似于docker exec命令，可用于了解容器各方面的相关信息或执行必需的设定操作等，具体功能取决于容器内可用的程序。</li>
</ul>
<h4 id="打印Pod对象的状态"><a href="#打印Pod对象的状态" class="headerlink" title="打印Pod对象的状态"></a>打印Pod对象的状态</h4><p>kubectl describe pods/NAME -n NAMESPACE命令可打印Pod对象的详细描述信息，包括events和controllers等关系的子对象等，Priority、Status、Containers和Events等字段通常是重点关注的目标字段。<br>另外，也可以通过kubectl get pods/POD -o yaml|json命令的status字段来了解Pod的状态详情，它保存有Pod对象的当前状态。如下命令显示了pod-demo的状态信息，结果输出做了尽可能的省略。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">pods/pod-demo</span> <span class="string">-o</span> <span class="string">yaml</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">conditions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">lastTransitionTime:</span> <span class="string">&quot;2020-08-16T03:36:48Z&quot;</span></span><br><span class="line">    <span class="attr">message:</span> <span class="string">&#x27;containers with unready status: [demo]&#x27;</span></span><br><span class="line">    <span class="attr">reason:</span> <span class="string">ContainersNotReady</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;False&quot;</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">ContainersReady</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">  <span class="attr">containerStatuses:</span>   <span class="comment"># 容器级别的状态信息</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">containerID:</span> <span class="string">docker://……</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imageID:</span> <span class="string">docker-pullable://ikubernetes/demoapp@sha256:……</span></span><br><span class="line">    <span class="attr">lastState:</span> &#123;&#125;      <span class="comment"># 前一次的状态</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">ready:</span> <span class="literal">true</span>        <span class="comment"># 是否已经就绪</span></span><br><span class="line">    <span class="attr">restartCount:</span> <span class="number">0</span>    <span class="comment"># 重启次数</span></span><br><span class="line">    <span class="attr">started:</span> <span class="literal">true</span>  </span><br><span class="line">    <span class="attr">state:</span>             <span class="comment"># 当前状态</span></span><br><span class="line">      <span class="attr">running:</span></span><br><span class="line">        <span class="attr">startedAt:</span> <span class="string">&quot;2020-08-16T03:36:48Z&quot;</span>   <span class="comment"># 启动时间</span></span><br><span class="line">  <span class="attr">hostIP:</span> <span class="number">172.29</span><span class="number">.9</span><span class="number">.12</span>  <span class="comment"># 节点IP</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Running</span>       <span class="comment"># Pod当前的相位</span></span><br><span class="line">  <span class="attr">podIP:</span> <span class="number">10.244</span><span class="number">.2</span><span class="number">.3</span>    <span class="comment"># Pod的主IP地址</span></span><br><span class="line">  <span class="attr">podIPs:</span>     <span class="comment"># Pod上的所有IP地址</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">ip:</span> <span class="number">10.244</span><span class="number">.2</span><span class="number">.3</span></span><br><span class="line">  <span class="attr">qosClass:</span> <span class="string">BestEffort</span> <span class="comment"># QoS类别</span></span><br></pre></td></tr></table></figure>

<p>上面的命令结果中，conditions字段是一个称为PodConditions的数组，它记录了Pod所处的“境况”或者“条件”，其中的每个数组元素都可能由如下6个字段组成。</p>
<ul>
<li>lastProbeTime：上次进行Pod探测时的时间戳。</li>
<li>lastTransitionTime：Pod上次发生状态转换的时间戳。</li>
<li>message：上次状态转换相关的易读格式信息。</li>
<li>reason：上次状态转换原因，用驼峰格式的单个单词表示。</li>
<li>status：是否为状态信息，可取值有True、False和Unknown。</li>
<li>type：境况的类型或名称，有4个固定值；PodScheduled表示已经与节点绑定；Ready表示已经就绪，可服务客户端请求；Initialized表示所有的初始化容器都已经成功启动；ContainersReady则表示所有容器均已就绪。</li>
</ul>
<p>另外，containerStatuses字段描述了Pod中各容器的相关状态信息，包括容器ID、镜像和镜像ID、上一次的状态、名称、启动与否、就绪与否、重启次数和状态等。</p>
<h4 id="查看容器日志"><a href="#查看容器日志" class="headerlink" title="查看容器日志"></a>查看容器日志</h4><p>kubectl logs POD [-c CONTAINER]命令可直接获取并打印控制台日志，不过，若Pod对象中仅运行有一个容器，则可以省略-c选项及容器名称。例如，下面的命令打印了pod-demo中唯一的主容器的控制台日志：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">logs</span> <span class="string">pod-demo</span></span><br><span class="line"><span class="string">*</span> <span class="string">Running</span> <span class="string">on</span> <span class="string">http://0.0.0.0:80/</span> <span class="string">(Press</span> <span class="string">CTRL+C</span> <span class="string">to</span> <span class="string">quit)</span></span><br><span class="line"><span class="number">172.29</span><span class="number">.9</span><span class="number">.1</span> <span class="bullet">-</span> <span class="bullet">-</span> [<span class="number">16</span><span class="string">/Aug/2020</span> <span class="number">03</span><span class="string">:54:42</span>] <span class="string">&quot;GET / HTTP/1.1&quot;</span> <span class="number">200</span> <span class="bullet">-</span></span><br><span class="line"><span class="number">172.29</span><span class="number">.9</span><span class="number">.11</span> <span class="bullet">-</span> <span class="bullet">-</span> [<span class="number">16</span><span class="string">/Aug/2020</span> <span class="number">03</span><span class="string">:54:50</span>] <span class="string">&quot;GET / HTTP/1.1&quot;</span> <span class="number">200</span> <span class="bullet">-</span></span><br></pre></td></tr></table></figure>

<h5 id="在容器中额外运行其他程序"><a href="#在容器中额外运行其他程序" class="headerlink" title="在容器中额外运行其他程序"></a>在容器中额外运行其他程序</h5><p>kubectl exec可以让用户在Pod的某容器中运行用户所需要的任何存在于容器中的程序。在kubectl logs获取的信息不够全面时，此命令可以通过在Pod中运行其他指定的命令（前提是容器中存在此程序）来辅助用户获取更多信息。一个更便捷的使用接口是直接交互式运行容器中的某shell程序。例如，直接查看Pod中的容器运行的进程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-demo -- ps aux</span></span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:01 python3 /usr/local/bin/demo.py</span><br><span class="line">    8 root      0:00 ps aux</span><br></pre></td></tr></table></figure>

<p><font color="red">注意:如果Pod中运行多个容器，需要使用-c <container_name>选项指定运行程序的容器名称。</container_name></font></p>
<p>有时候需要打开容器的交互式shell接口以方便多次执行命令，为kubectl exec命令额外使用-it选项，并指定运行镜像中可用的shell程序就能进入交互式接口</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl -it <span class="built_in">exec</span> pod-demo /bin/sh</span></span><br><span class="line">[root@pod-demo /]# hostname</span><br><span class="line">pod-demo</span><br><span class="line">[root@pod-demo /]# netstat -tnl</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address     Foreign Address     State   </span><br><span class="line">tcp        0      0 0.0.0.0:80          0.0.0.0:*         LISTEN</span><br></pre></td></tr></table></figure>

<h3 id="自定义容器应用与参数"><a href="#自定义容器应用与参数" class="headerlink" title="自定义容器应用与参数"></a>自定义容器应用与参数</h3><p>容器镜像启动容器时运行的默认应用程序由其Dockerfile文件中的ENTRYPOINT指令进行定义，传递给程序的参数则通过CMD指令设定，ETRYPOINT指令不存在时，CMD可同时指定程序及其参数。例如，要了解镜像ikubernetes/demoapp:v1.0中定义的ENTRYPOINT和CMD，可以在任何存在此镜像的节点上执行类似如下命令来获取：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~# </span><span class="language-bash">docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Entrypoint&#125;&#125;</span></span><br><span class="line">[/bin/sh -c python3 /usr/local/bin/demo.py]</span><br><span class="line"><span class="meta">~# </span><span class="language-bash">docker inspect ikubernetes/demoapp:v1.0 -f &#123;&#123;.Config.Cmd&#125;&#125;</span></span><br><span class="line">[]</span><br></pre></td></tr></table></figure>

<p>Pod配置中，spec.containers[].command字段可在容器上指定非镜像默认运行的应用程序，且可同时使用spec.containers[].args字段进行参数传递，它们将覆盖镜像中默认定义的参数。若定义了args字段，该字段值将作为参数传递给镜像中默认指定运行的应用程序；而仅定义了command字段时，其值将覆盖镜像中定义的程序及参数。下面的资源配置清单保存在pod-demo-with-cmd-and-args.yaml文件中，它把镜像ikubernetes/demoapp:v1.0的默认应用程序修改为/bin/sh -c，参数定义为python3 /usr/local/bin/demo.py -p 8080，其中的-p选项可修改服务监听的端口为指定的自定义端口</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-demo-with-cmd-and-args</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;python3 /usr/local/bin/demo.py -p 8080&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>下面将上述清单中定义的Pod对象创建到集群上，验证其监听的端口是否从默认的80变为了指定的8080：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create -f pod-demo-with-cmd-and-args.yaml</span> </span><br><span class="line">pod/pod-demo-with-cmd-and-args created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-demo-with-cmd-and-args -- netstat -tnl</span>  </span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address    Foreign Address     State </span><br><span class="line">tcp        0      0 0.0.0.0:8080     0.0.0.0:*          LISTEN</span><br></pre></td></tr></table></figure>

<h3 id="容器环境变量"><a href="#容器环境变量" class="headerlink" title="容器环境变量"></a>容器环境变量</h3><p>容器环境变量需要应用程序支持通过环境变量进行配置，否则用户要在制作Docker镜像时通过entrypoint脚本完成环境变量到程序配置文件的同步。<br>向Pod对象中容器环境变量传递数据的方法有两种：env和envFrom，这里重点介绍第一种方式，第二种方式将在介绍ConfigMap和Secret资源时进行说明。<br>通过环境变量的配置容器化应用时，需要在容器配置段中嵌套使用env字段，它的值是一个由环境变量构成的列表。每个环境变量通常由name和value字段构成。</p>
<ul>
<li>name <string>：环境变量的名称，必选字段。</string></li>
<li>value <string>：传递给环境变量的值，通过$(VAR_NAME)引用，逃逸格式为$$(VAR_NAME)默认值为空。</string></li>
</ul>
<p>示例中使用镜像demoapp中的应用服务器支持通过HOST与PORT环境变量分别获取监听的地址和端口，它们的默认值分别为0.0.0.0和80，下面的配置保存在清单文件pod-using-env.yaml中，它分别为HOST和PORT两个环境变量传递了一个不同的值，以改变容器监听的地址和端口。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-env</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br></pre></td></tr></table></figure>

<p>下面将清单文件中定义的Pod对象创建至集群中，并查看应用程序监听的地址和端口来验证配置结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-env.yaml</span></span><br><span class="line">pod/pod-using-env created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> pod-using-env -- netstat -tnl</span></span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address      Foreign Address    State       </span><br><span class="line">tcp        0      0 127.0.0.1:8080     0.0.0.0:*         LISTEN</span><br></pre></td></tr></table></figure>

<h3 id="Pod的创建与删除过程"><a href="#Pod的创建与删除过程" class="headerlink" title="Pod的创建与删除过程"></a>Pod的创建与删除过程</h3><h4 id="Pod资源对象的创建过程。"><a href="#Pod资源对象的创建过程。" class="headerlink" title="Pod资源对象的创建过程。"></a>Pod资源对象的创建过程。</h4><p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220122090224890.png" alt="image-20220122090224890"></p>
<blockquote>
<p>1）用户通过kubectl或其他API客户端提交Pod Spec给API Server。<br>2）API Server尝试着将Pod对象的相关信息存入etcd中，待写入操作执行完成，API Server即会返回确认信息至客户端。<br>3）Scheduler（调度器）通过其watcher监测到API Server创建了新的Pod对象，于是为该Pod对象挑选一个工作节点并将结果信息更新至API Server。<br>4）调度结果信息由API Server更新至etcd存储系统，并同步给Scheduler。<br>5）相应节点的kubelet监测到由调度器绑定于本节点的Pod后会读取其配置信息，并由本地容器运行时创建相应的容器启动Pod对象后将结果回存至API Server。<br>6）API Server将kubelet发来的Pod状态信息存入etcd系统，并将确认信息发送至相应的kubelet。<br>另一方面，Pod可能曾用于处理生产数据或向用户提供服务等，Kubernetes可删除宽限期确保终止操作能够以平滑方式优雅完成，从而用户也可以在正常提交删除操作后获知何时开始终止并最终完成。删除时，用户提交请求后系统即会进行强制删除操作的宽限期倒计时，并将TERM信号发送给Pod对象中每个容器的主进程。宽限期倒计时结束后，这些进程将收到强制终止的KILL信号，Pod对象也随即由API Server删除。如果在等待进程终止的过程中kubelet或容器管理器发生了重启，则终止操作会重新获得一个满额的删除宽限期并重新执行删除操作。</p>
</blockquote>
<h4 id="Pod的终止过程"><a href="#Pod的终止过程" class="headerlink" title="Pod的终止过程"></a>Pod的终止过程</h4><p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220122090439625.png" alt="image-20220122090439625"></p>
<p>如图4-10所示，一个典型的Pod对象终止流程如下。</p>
<blockquote>
<p>1）用户发送删除Pod对象的命令。<br>2）API服务器中的Pod对象会随着时间的推移而更新，在宽限期内（默认为30秒），Pod被视为dead。<br>3）将Pod标记为Terminating状态。<br>4）（与第3步同时运行）kubelet在监控到Pod对象转为Terminating状态的同时启动Pod关闭过程。<br>5）（与第3步同时运行）端点控制器监控到Pod对象的关闭行为时将其从所有匹配到此端点的Service资源的端点列表中移除。<br>6）如果当前Pod对象定义了preStop钩子句柄，在其标记为terminating后即会以同步方式启动执行；如若宽限期结束后，preStop仍未执行完，则重新执行第2步并额外获取一个时长为2秒的小宽限期。<br>7）Pod对象中的容器进程收到TERM信号。<br>8）宽限期结束后，若存在任何一个仍在运行的进程，Pod对象即会收到SIGKILL信号。<br>9）Kubelet请求API Server将此Pod资源的宽限期设置为0从而完成删除操作，它变得对用户不再可见。<br>默认情况下，所有删除操作的宽限期都是30秒，不过kubectl delete命令可以使用–grace-period=<seconds>选项自定义其时长，使用0值则表示直接强制删除指定的资源，不过此时需要同时为命令使用–force选项。</seconds></p>
</blockquote>
<h2 id="暴露容器服务"><a href="#暴露容器服务" class="headerlink" title="暴露容器服务"></a>暴露容器服务</h2><p>不考虑通过Service资源进行服务暴露的情况下，服务于集群外部的客户端的常用方式有两种：一种是在其运行的节点上进行端口映射，由节点IP和选定的协议端口向Pod内的应用容器进行DNAT转发；另一种是让Pod共享其所在的工作节点的网络名称空间，应用进程将直接监听工作节点IP地址和协议端口。</p>
<h3 id="其他容器端口映射"><a href="#其他容器端口映射" class="headerlink" title="其他容器端口映射"></a>其他容器端口映射</h3><p>其他Kubernetes系统的网络模型中，各Pod的IP地址处于同一网络平面，无论是否为容器指定了要暴露的端口都不会影响集群中其他节点之上的Pod客户端对其进行访问，这意味着，任何在非本地回环接口lo上监听的端口都可直接通过Pod网络被请求。从这个角度来说，容器端口只是信息性数据，它仅为集群用户提供了一个快速了解相关Pod对象的可访问端口的途径，但显式指定容器的服务端口可额外为其赋予一个名称以方便按名称调用。定义容器端口的ports字段的值是一个列表，由一到多个端口对象组成，它的常用嵌套字段有如下几个。</p>
<ul>
<li>containerPort <integer>：必选字段，指定在Pod对象的IP地址上暴露的容器端口，有效范围为(0,65536)；使用时，需要指定为容器应用程序需要监听的端口。</integer></li>
<li>name <string>：当前端口的名称标识，必须符合IANA_SVC_NAME规范且在当前Pod内要具有唯一性；此端口名可被Service资源按名调用。</string></li>
<li>protocol <string>：端口相关的协议，其值仅支持TCP、SCTP或UDP三者之一，默认为TCP。</string></li>
</ul>
<p>需要借助于Pod所在节点将容器服务暴露至集群外部时，还需要使用hostIP与hostPort两个字段来指定占用的工作节点地址和端口。如图4-11所示的Pod A与Pod C可分别通过各自所在节点上指定的hostIP和hostPort服务于客户端请求。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220122092123020.png" alt="image-20220122092123020"></p>
<ul>
<li>hostPort <integer>：主机端口，它将接收到的请求通过NAT机制转发至由container-Port字段指定的容器端口。</integer></li>
<li>hostIP <string>：主机端口要绑定的主机IP，默认为主机之上所有可用的IP地址；该字段通常使用默认值。</string></li>
</ul>
<p>下面的资源配置清单示例（pod-using-hostport.yaml）中定义的demo容器指定了要暴露容器上TCP协议的80端口，并将之命名为http，该容器可通过工作节点的10080端口接入集群外部客户端的请求。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-hostport</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">protocol:</span> <span class="string">TCP</span></span><br><span class="line">      <span class="attr">hostPort:</span> <span class="number">10080</span></span><br></pre></td></tr></table></figure>

<p>在集群中创建配置清单中定义的Pod对象后，需获取其被调度至的目标节点，例如下面第二个命令结果中的k8s-node02.ilinux.io/172.29.9.12，而后从集群外部向该节点的10080端口发起Web请求进行访问测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-hostport.yaml</span> </span><br><span class="line">pod/pod-using-hostport</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/ pod-using-hostport | grep <span class="string">&quot;^Node:&quot;</span></span></span><br><span class="line">Node:         k8s-node02.ilinux.io/172.29.9.12</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl 172.29.9.12:10080</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 172.29.0.1, ServerName: pod-using-hostport, ServerIP: 10.244.2.9!</span><br></pre></td></tr></table></figure>

<p>注意，hostPort与NodePort类型的Service对象暴露端口的方式不同，NodePort是通过所有节点暴露容器服务，而hostPort则能经由Pod对象所在节点的IP地址进行。</p>
<h3 id="配置Pod使用节点网络"><a href="#配置Pod使用节点网络" class="headerlink" title="配置Pod使用节点网络"></a>配置Pod使用节点网络</h3><p>同一个Pod对象的各容器运行于一个独立、隔离的Network、UTS和IPC名称空间中，共享同一个网络协议栈及相关的网络设备，但也有些特殊的Pod对象需要运行于所在节点的名称空间中，执行系统级的管理任务（例如查看和操作节点的网络资源甚至是网络设备等），或借助节点网络资源向集群外客户端提供服务等，如图4-12中的右图所示。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220122092551676.png" alt="image-20220122092551676"></p>
<p>由kubeadm部署的Kubernetes集群中的kube-apiserver、kube-controller-manager、kube-scheduler，以及kube-proxy和kube-flannel等通常都是第二种类型的Pod对象。网络名称空间是Pod级别的属性，用户配置的Pod对象，仅需要设置其spec.hostNetwork的属性为true即可创建共享节点网络名称空间的Pod对象，如下面保存在pod-using-hostnetwork.yaml文件中的配置清单所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pod-using-hostnetwork</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>

<p>将上面配置清单中定义的pod-using-hostnetwork创建于集群上，并查看主机名称或网络接口的相关属性信息以验证它是否能共享使用工作节点的网络名称空间。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f pod-using-hostnetwork.yaml</span> </span><br><span class="line">pod/pod-using-hostnetwork created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> -it pod-using-hostnetwork -- hostname</span></span><br><span class="line">k8s-node01.ilinux.io</span><br></pre></td></tr></table></figure>

<p>上面第二个命令的结果显示出的主机名称，表示该Pod已然共享了其所在节点的UTS名称空间，以及Network和IPC名称空间。这意味着，Pod对象中运行容器化应用可在其所在的工作节点的IP地址之上监听，这可以通过直接向k8s-node01.ilinux.io节点发起请求来验证。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">curl k8s-node01.ilinux.io</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 172.29.9.1, ServerName: k8s-node01.ilinux.io, ServerIP: 172.29.9.11!</span><br></pre></td></tr></table></figure>

<h2 id="容器安全上下文"><a href="#容器安全上下文" class="headerlink" title="容器安全上下文"></a>容器安全上下文</h2><p>Kubernetes为安全运行Pod及容器运行设计了安全上下文机制，该机制允许用户和管理员定义Pod或容器的特权与访问控制，以配置容器与主机以及主机之上的其他容器间的隔离级别。安全上下文就是一组用来决定容器是如何创建和运行的约束条件，这些条件代表创建和运行容器时使用的运行时参数。需要提升容器权限时，用户通常只应授予容器执行其工作所需的访问权限，以“最小权限法则”来抑制容器对基础架构及其他容器产生的负面影响。<br>Kubernetes支持用户在Pod及容器级别配置安全上下文，并允许管理员通过Pod安全策略在集群全局级别限制用户在创建和运行Pod时可设定的安全上下文。本节仅描述Pod和容器级别的配置，Pod安全策略的话题将在第9章展开。<br>Pod和容器的安全上下文设置包括以下几个方面。</p>
<ul>
<li>自主访问控制（DAC）：传统UNIX的访问控制机制，它允许对象（OS级别，例如文件等）的所有者基于UID和GID设定对象的访问权限。</li>
<li>Linux功能：Linux为突破系统上传统的两级用户（root和普通用户）授权模型，而将内核管理权限打散成多个不同维度或级别的权限子集，每个子集称为一种“功能”或“能力”，例如CAP_NET_ADMIN、CAP_SYS_TIME、CAP_SYS_PTRACE和CAP_SYS_ADMIN等，从而允许进程仅具有一部分内核管理功能就能完成必要的管理任务。</li>
<li>seccomp：全称为secure computing mode，是Linux内核的安全模型，用于为默认可发起的任何系统调用进程施加控制机制，人为地禁止它能够发起的系统调用，有效降低了程序被劫持时的危害级别。</li>
<li>AppArmor：全称为Application Armor，意为“应用盔甲”，是Linux内核的一个安全模块，通过加载到内核的配置文件来定义对程序的约束与控制。</li>
<li>SELinux：全称为Security-Enhanced Linux，意为安全加强的Linux，是Linux内核的一个安全模块，提供了包括强制访问控制在内的访问控制安全策略机制。</li>
<li>Privileged模式：即特权模式容器，该模式下容器中的root用户拥有所有的内核功能，即具有真正的管理员权限，它能看到主机上的所有设备，能够挂载文件系统，甚至可以在容器中运行容器；容器默认运行于非特权（unprivileged）模式。</li>
<li>AllowPrivilegeEscalation：控制是否允许特权升级，即进程是否能够获取比父进程更多的特权；运行于特权模式或具有CAP_SYS_ADMIN能力的容器默认允许特权升级。<br>这些安全上下文相关的特性多数嵌套定义在Pod或容器的securityContext字段中，而且有些特性对应的嵌套字段还不止一个。而seccomp和AppArmor的安全上下文则需要以资源注解的方式进行定义，而且仅能由管理员在集群级别进行Pod安全策略配置。</li>
</ul>
<h3 id="配置格式速览"><a href="#配置格式速览" class="headerlink" title="配置格式速览"></a>配置格式速览</h3><p>安全上下文可分别设置Pod级别和容器级别。但有些参数并不适合通用设定，例如特权模式、特权升级、只读根文件系统和内核能力等，它们只可用于容器之上。但也有参数仅可用于Pod级别进行通用设定，例如设置内核参数的sysctl和设置存储卷新件文件默认属组的fsgroup等。下面以Pod资源的配置格式给出了这些配置选项，以便于读者快速预览和了解安全上下文的用法。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span> &#123;<span class="string">…</span>&#125;</span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span>         <span class="comment"># Pod级别的安全上下文，对内部所有容器均有效</span></span><br><span class="line">    <span class="string">runAsUser</span> <span class="string">&lt;integer&gt;</span>    <span class="comment"># 以指定的用户身份运行容器进程，默认由镜像中的USER指定</span></span><br><span class="line">    <span class="string">runAsGroup</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 以指定的用户组运行容器进程，默认使用的组随容器运行时设定</span></span><br><span class="line">    <span class="string">supplementalGroups</span>  <span class="string">&lt;[]integer&gt;</span>  <span class="comment"># 为容器中1号进程的用户添加的附加组</span></span><br><span class="line">    <span class="string">fsGroup</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 为容器中的1号进程附加一个专用组，其功能类似于sgid</span></span><br><span class="line">    <span class="string">runAsNonRoot</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否以非root身份运行</span></span><br><span class="line">    <span class="string">seLinuxOptions</span> <span class="string">&lt;Object&gt;</span>  <span class="comment"># SELinux的相关配置</span></span><br><span class="line">    <span class="string">sysctls</span>  <span class="string">&lt;[]Object&gt;</span>         <span class="comment"># 应用到当前Pod名称空间级别的sysctl参数设置列表</span></span><br><span class="line">    <span class="string">windowsOptions</span> <span class="string">&lt;Object&gt;</span>     <span class="comment"># Windows容器专用的设置</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">securityContext:</span>            <span class="comment"># 容器级别的安全上下文，仅在当前容器生效</span></span><br><span class="line">      <span class="string">runAsUser</span> <span class="string">&lt;integer&gt;</span>       <span class="comment"># 以指定的用户身份运行容器进程</span></span><br><span class="line">      <span class="string">runAsGroup</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 以指定的用户组运行容器进程</span></span><br><span class="line">      <span class="string">runAsNonRoot</span> <span class="string">&lt;boolean&gt;</span>    <span class="comment"># 是否以非root身份运行</span></span><br><span class="line">      <span class="string">allowPrivilegeEscalation</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否允许特权升级</span></span><br><span class="line">      <span class="string">capabilities</span> <span class="string">&lt;Object&gt;</span>     <span class="comment"># 为当前容器添加（add）或删除（drop）内核能力</span></span><br><span class="line">        <span class="string">add</span>  <span class="string">&lt;[]string&gt;</span>         <span class="comment"># 添加由列表定义的各内核能力</span></span><br><span class="line">        <span class="string">drop</span>  <span class="string">&lt;[]string&gt;</span>        <span class="comment"># 移除由列表定义的各内核能力</span></span><br><span class="line">      <span class="string">privileged</span> <span class="string">&lt;boolean&gt;</span>      <span class="comment"># 是否运行为特权容器</span></span><br><span class="line">      <span class="string">procMount</span> <span class="string">&lt;string&gt;</span>        <span class="comment"># 设置容器的procMount类型，默认为DefaultProcMount；</span></span><br><span class="line">      <span class="string">readOnlyRootFilesystem</span> <span class="string">&lt;boolean&gt;</span> <span class="comment"># 是否将根文件系统设置为只读模式</span></span><br><span class="line">      <span class="string">seLinuxOptions</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># SELinux的相关配置</span></span><br><span class="line">      <span class="string">windowsOptions</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># Windows容器专用的设置</span></span><br></pre></td></tr></table></figure>

<p>Kubernetes默认以非特权模式创建并运行容器，同时禁用了其他与管理功能相关的内核能力，但未额外设定其他上下文参数。</p>
<h3 id="管理容器进程的运行身份"><a href="#管理容器进程的运行身份" class="headerlink" title="管理容器进程的运行身份"></a>管理容器进程的运行身份</h3><p>制作Docker镜像时，Dockerfile支持以USER指令明确指定运行应用进程时的用户身份。对于未通过USER指令显式定义运行身份的镜像，创建和启动容器时，其进程的默认用户身份为容器中的root用户和root组，该用户有着其他一些附加的系统用户组，例如sys、daemon、wheel和bin等。然而，有些应用程序的进程需要以特定的专用用户身份运行，或者以指定的用户身份运行时才能获得更好的安全特性，这种需求可以在Pod或容器级别的安全上下文中使用runAsUser得以解决，必要时可同时使用runAsGroup设置进程的组身份。<br>下面的资源清单（securitycontext-runasuer-demo.yaml）配置以1001这个UID和GID的身份来运行容器中的demoapp应用，考虑到非特权用户默认无法使用1024以下的端口号，文件中通过环境变量改变了应用监听的端口。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-runasuser-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br></pre></td></tr></table></figure>

<p>下面的命令先将配置清单中定义的Pod对象securitycontext-runasuser-demo创建到集群上，随后的两条命令验证了容器用户身份确为配置中预设的UID和GID。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f securitycontext-runasuser-demo.yaml</span> </span><br><span class="line">pod/securitycontext-runasuser-demo created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-runasuser-demo -- <span class="built_in">id</span></span> </span><br><span class="line">uid=1001 gid=1001</span><br><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-runasuser-demo -- ps aux</span></span><br><span class="line">PID    USER      TIME  COMMAND</span><br><span class="line">  1    1001      0:00  python3 /usr/local/bin/demo.py</span><br></pre></td></tr></table></figure>

<p>还可在上面的配置清单中的安全上下文定义中，同时使用supplement-Groups选项定义主进程用户的其他附加用户组，这对于有着复杂权限模型的应用是一个非常有用的选项。<br>另外，若运行容器时使用的镜像文件中已经使用USER指令指定了非root用户的运行身份，我们也可以在安全上下文中使用runAsNonRoot参数定义容器必须使用指定的非root用户身份运行，而无须使用runAsUser参数额外指定用户。</p>
<h3 id="管理容器的内核功能"><a href="#管理容器的内核功能" class="headerlink" title="管理容器的内核功能"></a>管理容器的内核功能</h3><p>传统UNIX仅实现了特权和非特权两类进程，前者是指以0号UID身份运行的进程，而后者则是从属非0号UID用户的进程。Linux内核从2.2版开始将附加于超级用户的权限分割为多个独立单元，这些单元是线程级别的，它们可配置在每个线程之上，为其赋予特定的管理能力。Linux内核常用的功能包括但不限于如下这些。</p>
<ul>
<li>CAP_CHOWN：改变文件的UID和GID。</li>
<li>CAP_MKNOD：借助系统调用mknod()创建设备文件。</li>
<li>CAP_NET_ADMIN：网络管理相关的操作，可用于管理网络接口、netfilter上的iptables规则、路由表、透明代理、TOS、清空驱动统计数据、设置混杂模式和启用多播功能等。</li>
<li>CAP_NET_BIND_SERVICE：绑定小于1024的特权端口，但该功能在重新映射用户后可能会失效。</li>
<li>AP_NET_RAW：使用RAW或PACKET类型的套接字，并可绑定任何地址进行透明代理。</li>
<li>CAP_SYS_ADMIN：支持内核上的很大一部分管理功能。</li>
<li>CAP_SYS_BOOT：重启系统。</li>
<li>CAP_SYS_CHROOT：使用chroot()进行根文件系统切换，并能够调用setns()修改Mount名称空间。</li>
<li>CAP_SYS_MODULE：装载内核模块。</li>
<li>CAP_SYS_TIME：设定系统时钟和硬件时钟。</li>
<li>CAP_SYSLOG：调用syslog()执行日志相关的特权操作等。</li>
</ul>
<p>系统管理员可以通过get命令获取程序文件上的内核功能，并可使用setcap命令为程序文件设定内核功能或取消（-r选项）其已有的内核功能。而为Kubernetes上运行的进程设定内核功能则需要在Pod内容器上的安全上下文中嵌套capabilities字段，添加和移除内核能力还需要分别在下一级嵌套中使用add或drop字段。这两个字段可接受以内核能力名称为列表项，但引用各内核能力名称时需移除CAP_前缀，例如可使用NET_ADMIN和NET_BIND_SERVICE这样的功能名称。<br>下面的配置清单（securitycontext-capabilities-demo.yaml）中定义的Pod对象的demo容器，在安全上下文中启用了内核功能NET_ADMIN，并禁用了CHOWN。demo容器的镜像未定义USER指令，它将默认以root用户的身份运行容器应用。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-capabilities-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>,<span class="string">&quot;-c&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&quot;/sbin/iptables -t nat -A PREROUTING -p tcp --dport 8080 -j </span></span><br><span class="line"><span class="string">    REDIRECT --to-port 80 &amp;&amp; /usr/bin/python3 /usr/local/bin/demo.py&quot;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span> [<span class="string">&#x27;NET_ADMIN&#x27;</span>]</span><br><span class="line">        <span class="attr">drop:</span> [<span class="string">&#x27;CHOWN&#x27;</span>]</span><br></pre></td></tr></table></figure>

<p>容器中的root用户将默认映射为系统上的普通用户，它实际上并不具有管理网络接口、iptables规则和路由表等相关的权限，但内核功能NET_ADMIN可以为其开放此类权限。但容器中的root用户默认就具有修改容器文件系统上的文件从属关系的能力，而禁用CHOWN功能则关闭了这种操作权限。下面创建该Pod对象并运行在集群上，来验证清单中的配置。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f securitycontext-capabilities-demo.yaml                               </span><br><span class="line">pod/securitycontext-capabilities-demo created</span><br></pre></td></tr></table></figure>

<p>而后，检查Pod网络名称空间中netfilter之上的规则，清单中的iptables命令添加的规则位于NAT表的PREROUTING链上。下面的命令结果表示iptables命令已然生成的规则，NET_ADMIN功能启用成功。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-capabilities-demo -- iptables -t nat -nL PREROUTING</span> </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot    opt    source      destination         </span><br><span class="line">REDIRECT   tcp  --  0.0.0.0/0         0.0.0.0/0       tcp dpt:8080 redir ports 80</span><br></pre></td></tr></table></figure>

<p>接着，下面用于检查demo容器中的root用户是否能够修改容器文件系统上文件的属主和属组的命令结果表示，其CHOWN功能已然成功关闭。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> securitycontext-capabilities-demo -- <span class="built_in">chown</span> 200.200 /etc/hosts</span></span><br><span class="line">chown: /etc/hosts: Operation not permitted</span><br><span class="line">command terminated with exit code 1</span><br></pre></td></tr></table></figure>

<p>内核的各项功能均可按其原本的意义在容器的安全上下文中按需打开或关闭，但SYS_ADMIN功能拥有内核中的许多管理权限，实在太过强大，出于安全方面的考虑，用户应该基于最小权限法则组合使用内核功能完成容器运行。</p>
<h3 id="特权模式容器"><a href="#特权模式容器" class="headerlink" title="特权模式容器"></a>特权模式容器</h3><p>相较于内核功能，SYS_ADMIN赋予了进程很大一部分的系统级管理功能，特权（privileged）容器几乎将宿主机内核的完整权限全部开放给了容器进程，它提供的是远超SYS_ADMIN的授权，包括写操作到/proc和/sys目录以及管理硬件设备等，因而仅应该用到基础架构类的系统级管理容器之上。例如，使用kubeadm部署的集群中，kube-proxy中的容器就运行于特权模式。<br>下面的第一个命令从kube-system名称空间中取出一个kube-proxy相关的Pod对象名称，第二个命令则用于打印该Pod对象的配置清单，限于篇幅，这里仅列出了其中一部分内容：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">pod-name=$(kubectl get pods -l k8s-app=kube-proxy -n kube-system \</span></span><br><span class="line"><span class="language-bash">         -o jsonpath=&#123;.items[0].metadata.name&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods <span class="variable">$pod</span>-name -n kube-system -o yaml</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash">从命令结果中截取的启动容器应用的命令及传递的参数</span></span><br><span class="line">containers:</span><br><span class="line">  - command:</span><br><span class="line">    - /usr/local/bin/kube-proxy</span><br><span class="line">    - --config=/var/lib/kube-proxy/config.conf</span><br><span class="line">    - --hostname-override=$(NODE_NAME)</span><br><span class="line">    image: ……</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    name: kube-proxy</span><br><span class="line">    resources: &#123;&#125;</span><br><span class="line">    securityContext:</span><br><span class="line">      privileged: true</span><br></pre></td></tr></table></figure>

<p>上面保留的命令结果的最后两行是定义特权容器的格式，唯一用到的privileged字段只能嵌套在容器的安全上下文中，它使用布尔型值，true表示启用特权容器机制。</p>
<h3 id="在Pod上使用sysctl"><a href="#在Pod上使用sysctl" class="headerlink" title="在Pod上使用sysctl"></a>在Pod上使用sysctl</h3><p>Linux系统上的sysctl接口允许在运行时修改内核参数，管理员可通过/proc/sys/下的虚拟文件系统接口来修改或查询这些与内核、网络、虚拟内存或设备等各子系统相关的参数。Kubernetes也允许在Pod上独立安全地设置支持名称空间级别的内核参数，它们默认处于启用状态，而节点级别内核参数则被认为是不安全的，它们默认处于禁用状态。<br>截至目前，仅kernel.shm_rmid_forced、net.ipv4.ip_local_port_range和net.ipv4.tcp_syncookies这3个内核参数被Kubernetes视为安全参数，它们可在Pod安全上下文的sysctl参数内嵌套使用，而余下的绝大多数的内核参数都是非安全参数，需要管理员手动在每个节点上通过kubelet选项逐个启用后才能配置到Pod上。例如，在各工作节点上编辑/etc/default/kubelet文件，添加如下内容以允许在Pod上使用指定的两个非安全的内核参数，并重启kubelet服务使之生效。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">KUBELET_EXTRA_ARGS=&#x27;--allowed-unsafe-sysctls=net.core.somaxconn,net.ipv4.ip_unprivileged_port_start&#x27;</span></span><br></pre></td></tr></table></figure>

<p>net.core.somaxconn参数定义了系统级别入站连接队列最大长度，默认值是128；而net.ipv4.ip_unprivileged_port_start参数定义的是非特权用户可以使用的内核端口起始值，默认为1024，它限制了非特权用户所能够使用的端口范围。<br>下面配置清单（securitycontext-sysctls-demo.yaml）中定义的Pod对象在安全上下文中通过sysctls字段嵌套使用了一个安全的内核参数kernel.shm_rmid_forced，以及一个已经启用的非安全内核参数net.ipv4.ip_unprivileged_port_start，它将该非安全内核参数的值设置为0来允许非特权用户使用11024以内端口的权限。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">securitycontext-sysctls-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">sysctls:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">kernel.shm_rmid_forced</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">net.ipv4.ip_unprivileged_port_start</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;0&quot;</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br></pre></td></tr></table></figure>

<p>尽管上面配置清单设定了以非特权用户1001的身份运行容器应用，但受上面内核参数的影响，非管理员用户也具有了监听80端口的权限，因而不会遇到无法监听特权端口的情形。下面将配置清单中定义的资源创建在集群之上，来验证设定的结果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f securitycontext-sysctls-demo.yaml</span> </span><br><span class="line">pod/securitycontext-sysctls-demo created</span><br></pre></td></tr></table></figure>

<p>下面的命令结果显示，以普通用户身份运行的demo容器成功监听了TCP协议的80端口。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl exec securitycontext-sysctls-demo -- netstat -tnlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address  Foreign Address    State   PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:80        0.0.0.0:*      LISTEN    1/python3</span><br></pre></td></tr></table></figure>

<p>在Pod对象之上启用非安全内核参数，其配置结果可能会存在无法预料的结果，在正式使用之前一定要经过充分测试。例如，在某一Pod之上同时配置启用前面示例的两个非安全内核参数可能存在生效结果异常的情况。<br>本节中介绍了设置Pod与容器安全上下文配置方法及几种常用使用方式。从示例中我们可以看出，设置特权容器和添加内核功能等，以及在Pod上共享宿主机的Network和PID名称空间等，对于多项目或多团队共享的Kubernetes集群存在着不小的安全隐患，这就要求管理员应该在集群级别使用Pod安全策略（PodSecurityPolicy），来精心管控这些与安全相关配置的运用能力。</p>
<h2 id="容器应用的管理接口"><a href="#容器应用的管理接口" class="headerlink" title="容器应用的管理接口"></a>容器应用的管理接口</h2><h3 id="健康状态监测接口"><a href="#健康状态监测接口" class="headerlink" title="健康状态监测接口"></a>健康状态监测接口</h3><p>监测容器自身运行的API包括分别用于健康状态检测、指标、分布式跟踪和日志等实现类型，如图4-13所示。即便没有完全实现，至少容器化应用也应该提供用于健康状态检测（liveness和readiness）的API，以便编排系统能更准确地判定应用程序的运行状态。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220122102525490.png" alt="image-20220122102525490"></p>
<p>Kubelet仅能在控制循环中根据容器主进程的运行状态来判断其健康与否，主进程以非0状态码退出代表处于不健康状态，其他均为正常状态。然而，有些异常场景中，仍处于运行状态的进程内部的业务处理机制可能已然处于僵死状态或陷入死循环等，无法正常处理业务请求，对于这种状态的判断便要依赖应用自身专用于健康状态监测的接口。<br>存活状态（liveness）检测用于定期检测容器是否正常运行，就绪状态（readiness）检测用于定期检测容器是否可以接收流量，它们能够通过减少运维问题和提高服务质量来使服务更健壮和更具弹性。Kubernetes在Pod内部的容器资源上提供了livenessProbe和readinessProbe两个字段，分别让用户自定义容器应用的存活状态和就绪状态检测。</p>
<ul>
<li>存活状态检测：用于判定容器是否处于“运行”状态；若此类检测未通过，kubelet将杀死容器并根据其restartPolicy决定是否将其重启；未定义存活性检测的容器的默认状态为Success。</li>
<li>就绪状态检测：用于判断容器是否准备就绪并可对外提供服务；未通过该检测时，端点控制器（例如Service对象）会将其IP地址从所有匹配到此Pod对象的Service对象的端点列表中移除；检测通过之后，会再次将其IP添加至端点列表中；未定义就绪状态检测的容器的默认状态为Success。<br>容器探测是Pod对象生命周期中的一项重要日常任务，它由kubelet周期性执行。kubelet可在活动容器上分别执行由用户定义的启动状态检测（startupProbe）、存活状态检测（livenessProbe）和就绪状态检测（readinessProbe），定义在容器上的存活状态和就绪状态操作称为检测探针，它要通过容器的句柄（handler）进行定义。Kubernetes定义了用于容器探测的3种句柄。</li>
<li>ExecAction：通过在容器中执行一个命令并根据其返回的状态码进行的诊断操作称为Exec探测，状态码为0表示成功，否则即为不健康状态。</li>
<li>TCPSocketAction：通过与容器的某TCP端口尝试建立连接进行诊断，端口能够成功打开即为正常状态，否则为不健康状态。</li>
<li>HTTPGetAction：通过向容器IP地址的某指定端口的指定path发起http GET请求进行诊断，响应码为2xx或3xx即为成功，否则为失败。<br>上面的每种探测方式都可能存在3种返回结果：Success（成功）、Failure（失败）或Unknown（未知），仅第一种结果表示成功通过检测。<br>另外，Kubernetes自v1.16版本起还支持启动状态（startup）检测。将传统模式开发的大型应用程序迁移至容器编排平台运行时，可能需要相当长的时间进行启动后的初始化，但其初始过程是否正确完成的检测机制和探测参数都可能有别于存活状态检测，例如需要更长的间隔周期和更高的错误阈值等。该类检测的结果处理机制与存活状态检测相同，检测失败时kubelet将杀死容器并根据其restartPolicy决定是否将其重启，而未定义时的默认状态为Success。需要注意的是，一旦定义了启动检测探针，则必须等启动检测成功完成之后，存活探针和就绪探针才可启动。</li>
</ul>
<h3 id="容器存活状态检测"><a href="#容器存活状态检测" class="headerlink" title="容器存活状态检测"></a>容器存活状态检测</h3><p>存活性探测是隶属于容器级别的配置，kubelet可基于它判定何时需要重启容器。目前，Kubernetes在容器上支持的存活探针有3种类型：ExecAction、TCPSocketAction和HTTPGetAction。</p>
<h4 id="1-存活探针配置格式"><a href="#1-存活探针配置格式" class="headerlink" title="1. 存活探针配置格式"></a>1. 存活探针配置格式</h4><p>Pod配置格式中，spec.containers.livenessProbe字段用于定义此类检测，配置格式如下所示。但一个容器之上仅能定义一种类型的探针，即exec、httpGet和tcpSocket三者互斥，它们不可在一个容器同时使用。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">…</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="string">exec</span> <span class="string">&lt;Object&gt;</span>                   <span class="comment"># 命令式探针</span></span><br><span class="line">      <span class="string">httpGet</span> <span class="string">&lt;Object&gt;</span>                <span class="comment"># http GET类型的探针</span></span><br><span class="line">      <span class="string">tcpSocket</span> <span class="string">&lt;Object&gt;</span>              <span class="comment"># tcp Socket类型的探针</span></span><br><span class="line">      <span class="string">initialDelaySeconds</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 发起初次探测请求的延后时长</span></span><br><span class="line">      <span class="string">periodSeconds</span> <span class="string">&lt;integer&gt;</span>         <span class="comment"># 请求周期</span></span><br><span class="line">      <span class="string">timeoutSeconds</span> <span class="string">&lt;integer&gt;</span>        <span class="comment"># 超时时长</span></span><br><span class="line">      <span class="string">successThreshold</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 成功阈值</span></span><br><span class="line">      <span class="string">failureThreshold</span> <span class="string">&lt;integer&gt;</span>      <span class="comment"># 失败阈值</span></span><br></pre></td></tr></table></figure>

<p>探针之外的其他字段用于定义探测操作的行为方式，用户没有明确定义这些属性字段时，它们会使用各自的默认值:</p>
<ul>
<li>initialDelaySeconds <integer>：首次发出存活探测请求的延迟时长，即容器启动多久之后开始第一次探测操作，显示为delay属性；默认为0秒，即容器启动后便立刻进行探测；该参数值应该大于容器的最大初始化时长，以避免程序永远无法启动。</integer></li>
<li>timeoutSeconds <integer>：存活探测的超时时长，显示为timeout属性，默认为1秒，最小值也为1秒；应该为此参数设置一个合理值，以避免因应用负载较大时的响应延迟导致Pod被重启。</integer></li>
<li>periodSeconds <integer>：存活探测的频度，显示为period属性，默认为10秒，最小值为1秒；需要注意的是，过高的频率会给Pod对象带来较大的额外开销，而过低的频率又会使得对错误反应不及时。</integer></li>
<li>successThreshold <integer>：处于失败状态时，探测操作至少连续多少次的成功才被认为通过检测，显示为#success属性，仅可取值为1。</integer></li>
<li>failureThreshold：处于成功状态时，探测操作至少连续多少次的失败才被视为检测不通过，显示为#failure属性，默认值为3，最小值为1；尽量设置宽容一些的失败计数，能有效避免一些场景中的服务级联失败。<br>使用kubectl describe命令查看配置了存活性探测的Pod对象的详细信息时，其相关容器中会输出类似如下一行内容，它给出了探测方式及其额外的配置属性delay、timeout、period、success和failure及其各自的相关属性值。</li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Liveness:</span>  <span class="string">……</span> <span class="string">delay=0s</span> <span class="string">timeout=1s</span> <span class="string">period=10s</span> <span class="comment">#success=1 #failure=3</span></span><br></pre></td></tr></table></figure>

<h4 id="exec探针"><a href="#exec探针" class="headerlink" title="exec探针"></a>exec探针</h4><p>exec类型的探针通过在目标容器中执行由用户自定义的命令来判定容器的健康状态，命令状态返回值为0表示“成功”通过检测，其余值均为“失败”状态。spec.containers.livenessProbe.exec字段只有一个可用属性command，用于指定要执行的命令。<br>demoapp应用程序通过/livez输出内置的存活状态检测接口，服务正常时，它以200响应码返回OK，否则为5xx响应码，我们可基于exec探针使用HTTP客户端向该path发起请求，并根据命令的结果状态来判定容器健康与否。系统刚启动时，对该路径的请求将会延迟大约不到5秒的时长，且默认响应值为OK。它还支持由用户按需向该路径发起POST请求，并向参数livez传值来自定义其响应内容。下面是定义在资源清单文件liveness-exec-demo.yaml中的示例。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-exec-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>, <span class="string">&#x27;-c&#x27;</span>, <span class="string">&#x27;[ &quot;$(curl -s 127.0.0.1/livez)&quot; == &quot;OK&quot; ]&#x27;</span>]</span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>

<p>该配置清单中定义的Pod对象为demo容器定义了exec探针，它通过在容器本地执行测试命令来比较curl -s 127.0.0.1/livez的返回值是否为OK以判定容器的存活状态。命令成功执行则表示容器正常运行，否则3次检测失败之后则将其判定为检测失败。首次检测在容器启动5秒之后进行，请求间隔也是5秒。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f liveness-exec-demo.yaml</span> </span><br><span class="line">pod/liveness-exec-demo created</span><br></pre></td></tr></table></figure>

<p>创建完成后，Pod中的容器demo会正常运行，存活检测探针也不会遇到检测错误而导致容器重启。若要测试存活状态检测的效果，可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> liveness-exec-demo -- curl -s -X POST -d <span class="string">&#x27;livez=FAIL&#x27;</span> 127.0.0.1/livez</span></span><br></pre></td></tr></table></figure>

<p>而后经过1个检测周期，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令结果中的事件可知，容器因健康状态检测失败而被重启。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/liveness-exec-demo</span></span><br><span class="line">……</span><br><span class="line">Events: </span><br><span class="line">Warning  Unhealthy  17s (x3 over 27s)  kubelet, k8s-node03.ilinux.io  Liveness probe failed:</span><br><span class="line">Normal   Killing     17s             kubelet, k8s-node03.ilinux.io  Container demo failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure>

<p>另外，下面输出信息中的Containers一段中还清晰显示了容器健康状态检测及状态变化的相关信息：容器当前处于Running状态，但前一次是为Terminated，原因是退出码为137的错误信息，它表示进程是被外部信号所终止。137事实上由两部分数字之和生成：128+signum，其中signum是导致进程终止的信号的数字标识，9表示SIGKILL，这意味着进程是被强行终止的。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Containers:</span></span><br><span class="line">  <span class="attr">demo:</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">    <span class="attr">State:</span>          <span class="string">Running</span></span><br><span class="line">      <span class="attr">Started:</span>      <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:30:02</span> <span class="string">+0800</span></span><br><span class="line">    <span class="attr">Last State:</span>     <span class="string">Terminated</span></span><br><span class="line">      <span class="attr">Reason:</span>       <span class="string">Error</span></span><br><span class="line">      <span class="attr">Exit Code:</span>    <span class="number">137</span></span><br><span class="line">      <span class="attr">Started:</span>      <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:22:20</span> <span class="string">+0800</span></span><br><span class="line">      <span class="attr">Finished:</span>     <span class="string">Thu,</span> <span class="number">29</span> <span class="string">Aug</span> <span class="number">2020 14:30:02</span> <span class="string">+0800</span></span><br><span class="line">    <span class="attr">Ready:</span>          <span class="literal">True</span></span><br><span class="line">    <span class="attr">Restart Count:</span>  <span class="number">1</span></span><br><span class="line"><span class="string">……</span></span><br></pre></td></tr></table></figure>

<p>待容器重启完成后，/livez的响应内容会重置镜像中默认定义的OK，因而其存活状态检测不会再遇到错误，这模拟了一种典型的通过“重启”应用而解决问题的场景。需要特别说明的是，exec指定的命令运行在容器中，会消耗容器的可用计算资源配额，另外考虑到探测操作的效率等因素，探测操作的命令应该尽可能简单和轻量。</p>
<h4 id="HTTP探针"><a href="#HTTP探针" class="headerlink" title="HTTP探针"></a>HTTP探针</h4><p>HTTP探针是基于HTTP协议的探测（HTTPGetAction），通过向目标容器发起一个GET请求，并根据其响应码进行结果判定，2xx或3xx类的响应码表示检测通过。HTTP探针可用配置字段有如下几个。<br>▪host <string>：请求的主机地址，默认为Pod IP；也可以在httpHeaders使用“Host:”来定义。<br>▪port <string>：请求的端口，必选字段。<br>▪httpHeaders &lt;[]Object&gt;：自定义的请求报文头部。<br>▪path <string>：请求的HTTP资源路径，即URL path。<br>▪scheme：建立连接使用的协议，仅可为HTTP或HTTPS，默认为HTTP。<br>下面是一个定义在资源清单文件liveness-httpget-demo.yaml中的示例，它使用HTTP探针直接对/livez发起访问请求，并根据其响应码来判定检测结果。</string></string></string></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-httpget-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>

<p>上面清单文件中定义的httpGet测试中，请求的资源路径为/livez，地址默认为Pod IP，端口使用了容器中定义的端口名称http，这也是明确为容器指明要暴露的端口的用途之一。下面测试其效果，首先创建此Pod对象：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f liveness-httpget-demo.yaml</span><br><span class="line">pod/liveness-httpget-demo created</span><br></pre></td></tr></table></figure>

<p>首次检测为延迟5秒，这刚好超过了demoapp的/livez接口默认会延迟响应的时长。镜像中定义的默认响应是以200状态码响应、以OK为响应结果，存活状态检测会成功完成。为了测试存活状态检测的效果，同样可以手动将/livez的响应内容修改为OK之外的其他值，例如FAIL。<br>~$ kubectl exec liveness-httpget-demo – curl -s -X POST -d ‘livez=FAIL’ 127.0.0.1/livez<br>而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl describe pods/liveness-httpget-demo</span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  7s (x3 over 27s)  kubelet, k8s-node01.ilinux.io  Liveness probe failed: HTTP probe failed with statuscode: 520</span><br><span class="line">  Normal   Killing    7s          kubelet, k8s-node01.ilinux.io  Container demo failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure>

<p>一般来说，HTTP探针应该针对专用的URL路径进行。<br>这种检测方式仅对分层架构中的当前一层有效，例如，它能检测应用程序工作正常与否的状态，但重启操作却无法解决其后端服务（例如数据库或缓存服务）导致的故障。此时，容器可能会被反复重启，直到后端服务恢复正常。其他两种检测方式也存在类似的问题。</p>
<h4 id="TCP探针"><a href="#TCP探针" class="headerlink" title="TCP探针"></a>TCP探针</h4><p>TCP探针是基于TCP协议进行存活性探测（TCPSocketAction），通过向容器的特定端口发起TCP请求并尝试建立连接进行结果判定，连接建立成功即为通过检测。相比较来说，它比基于HTTP协议的探测要更高效、更节约资源，但精准度略低，毕竟连接建立成功未必意味着页面资源可用。<br>spec.containers.livenessProbe.tcpSocket字段用于定义此类检测，它主要有以下两个可用字段：<br>1）host <string>：请求连接的目标IP地址，默认为Pod自身的IP；<br>2）port <string>：请求连接的目标端口，必选字段，可以名称调用容器上显式定义的端口。<br>下面是一个定义在资源清单文件liveness-tcpsocket-demo.yaml中的示例，它向Pod对象的TCP协议的80端口发起连接请求，并根据连接建立的状态判定测试结果。为了能在容器中通过iptables阻止接收对80端口的请求以验证TCP检测失败，下面的配置还在容器上启用了特殊的内核权限NET_ADMIN。</string></string></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">liveness-tcpsocket-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">20</span></span><br></pre></td></tr></table></figure>

<p>按照配置，将该清单中的Pod对象创建在集群之上，20秒之后即会进行首次的tcpSocket检测。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f liveness-tcpsocket-demo.yaml</span>                                      </span><br><span class="line">pod/liveness-tcpsocket-demo created</span><br></pre></td></tr></table></figure>

<p>容器应用demoapp启动后即监听于TCP协议的80端口，tcpSocket检测也就可以成功执行。为了测试效果，可使用下面的命令在Pod的Network名称空间中设置iptables规则以阻止对80端口的请求：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> liveness-tcpsocket-demo -- iptables -A INPUT -p tcp --dport 80 -j REJECT</span></span><br></pre></td></tr></table></figure>

<p>而后经过至少1个检测周期后，可通过Pod对象的描述信息来获取相关的事件状态，例如，由下面命令的结果中的事件可知，容器因健康状态检测失败而被重启。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl describe pods/liveness-httpget-demo</span><br><span class="line">……</span><br><span class="line">Events:</span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  3s (x3 over 23s)  kubelet, k8s-node03.ilinux.io  Liveness probe failed: dial tcp 10.244.3.19:80: i/o timeout</span><br><span class="line">    Normal   Killing    3s      kubelet, k8s-node03.ilinux.io  Container demo     failed liveness probe, will be restarted</span><br></pre></td></tr></table></figure>

<p>不过，重启容器并不会导致Pod资源的重建操作，网络名称空间的设定附加在pause容器之上，因而添加的iptables规则在应用重启后依然存在，它是一个无法通过重启而解决的问题。若需要手消除该问题，删除添加至Pod中的iptables规则即可。</p>
<h3 id="Pod的重启策略"><a href="#Pod的重启策略" class="headerlink" title="Pod的重启策略"></a>Pod的重启策略</h3><p>Pod对象的应用容器因程序崩溃、启动状态检测失败、存活状态检测失败或容器申请超出限制的资源等原因都可能导致其被终止，此时是否应该重启则取决于Pod上的restartPolicy（重启策略）字段的定义，该字段支持以下取值。<br>1）Always：无论因何原因、以何种方式终止，kubelet都将重启该Pod，此为默认设定。<br>2）OnFailure：仅在Pod对象以非0方式退出时才将其重启。<br>3）Never：不再重启该Pod。<br>需要注意的是，restartPolicy适用于Pod对象中的所有容器，而且它仅用于控制在同一个节点上重新启动Pod对象的相关容器。首次需要重启的容器，其重启操作会立即进行，而再次重启操作将由kubelet延迟一段时间后进行，反复的重启操作的延迟时长依次为10秒、20秒、40秒、80秒、160秒和300秒，300秒是最大延迟时长。<br>事实上，一旦绑定到一个节点，Pod对象将永远不会被重新绑定到另一个节点，它要么被重启，要么被终止，直到节点故障、被删除或被驱逐。</p>
<h4 id="容器就绪状态检测"><a href="#容器就绪状态检测" class="headerlink" title="容器就绪状态检测"></a>容器就绪状态检测</h4><p>Pod对象启动后，应用程序通常需要一段时间完成其初始化过程，例如加载配置或数据、缓存初始化等，甚至有些程序需要运行某类预热过程等，因此通常应该避免在Pod对象启动后立即让其处理客户端请求，而是需要等待容器初始化工作执行完成并转为“就绪”状态，尤其是存在其他提供相同服务的Pod对象的场景更是如此。<br>与存活探针不同的是，就绪状态检测是用来判断容器应用就绪与否的周期性（默认周期为10秒钟）操作，它用于检测容器是否已经初始化完成并可服务客户端请求。与存活探针触发的操作不同，检测失败时，就绪探针不会杀死或重启容器来确保其健康状态，而仅仅是通知其尚未就绪，并触发依赖其就绪状态的其他操作（例如从Service对象中移除此Pod对象），以确保不会有客户端请求接入此Pod对象。<br>就绪探针也支持Exec、HTTP GET和TCP Socket这3种探测方式，且它们各自的定义机制与存活探针相同。因而，将容器定义中的livenessProbe字段名替换为readinessProbe，并略做适应性修改即可定义出就绪性检测的配置来，甚至有些场景中的就绪探针与存活探针的配置可以完全相同。<br>demoapp应用程序通过/readyz暴露了专用于就绪状态检测的接口，它于程序启动约15秒后能够以200状态码响应、以OK为响应结果，也支持用户使用POST请求方法通过readyz参数传递自定义的响应内容，不过所有非OK的响应内容都被响应以5xx的状态码。一个简单的示例如下面的配置清单（readiness-httpget-demo.yaml）所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">readiness-httpget-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">      <span class="attr">timeoutSeconds:</span> <span class="number">2</span></span><br><span class="line">      <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br><span class="line">      <span class="attr">failureThreshold:</span> <span class="number">3</span></span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure>

<p>下面来测试该Pod就绪探针的作用。按照配置，将Pod对象创建在集群上约15秒后启动首次探测，在该探测结果成功返回之前，Pod将一直处于未就绪状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">~ $ kubectl apply -f readiness-httpget-demo.yaml</span><br><span class="line">pod/readiness-httpget-demo created</span><br></pre></td></tr></table></figure>

<p>接着运行kubectl get -w命令监视其资源变动信息，由如下命令结果可知，尽管Pod对象处于Running状态，但直到就绪检测命令执行成功后Pod资源才转为“就绪”。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods/readiness-httpget-demo -w</span></span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">readiness-httpget-demo   0/1     Running   0          10s</span><br><span class="line">readiness-httpget-demo   1/1     Running   0          20s</span><br></pre></td></tr></table></figure>

<p>Pod运行过程中的某一时刻，无论因何原因导致的就绪状态检测的连续失败都会使得该Pod从就绪状态转变为“未就绪”，并且会从各个通过标签选择器关联至该Pod对象的Service后端端点列表中删除。为了测试就绪状态检测效果，下面修改/readyz响应以非OK内容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> readiness-httpget-demo -- curl -s -XPOST -d <span class="string">&#x27;readyz=FAIL&#x27;</span> 127.0.0.1/readyz</span></span><br></pre></td></tr></table></figure>

<p>而后在至少1个检测周期之后，通过该Pod的描述信息可以看到就绪检测失败相关的事件描述，命令及结果如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl describe pods/readiness-httpget-demo</span></span><br><span class="line">……</span><br><span class="line">Warning  Unhealthy  4s (x11 over 54s)  kubelet, k8s-node03.ilinux.io  Readiness probe failed: HTTP probe failed with statuscode: 521</span><br></pre></td></tr></table></figure>

<p>未定义就绪性检测的Pod对象在进入Running状态后将立即“就绪”，这在容器需要时间进行初始化的场景中可能会导致客户请求失败。因此，生产实践中，必须为关键性Pod资源中的容器定义就绪探针。</p>
<h3 id="容器生命周期"><a href="#容器生命周期" class="headerlink" title="容器生命周期"></a>容器生命周期</h3><p>容器生命周期接口工作示意图:</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220123090651488.png" alt="image-20220123090651488"></p>
<p>容器需要处理来自平台的最重要事件是SIGTERM信号，任何需要“干净”关闭进程的应用程序都需要捕捉该信号进行必要处理，例如释放文件锁、关闭数据库连接和网络连接等，而后尽快终止进程，以避免宽限期过后强制关闭信号SIGKILL的介入。SIGKILL信号是由底层操作系统接收的，而非应用进程，一旦检测到该信号，内核将停止为相应进程提供内核资源，并终止进程正在使用的所有CPU线程，类似于直接切断了进程的电源。<br>但是，容器应用很可能是功能复杂的分布式应用程序的一个组件，仅依赖信号终止进程很可能不足以完成所有的必要操作。因此，容器还需要支持postStart和preStop事件，前者常用于为程序启动前进行预热，后者则一般在“干净”地关闭应用之前释放占用的资源。<br>生命周期钩子函数lifecycle hook是编程语言（例如Angular）中常用的生命周期管理组件，它实现了程序运行周期中的关键时刻的可见性，并赋予用户为此采取某种行动的能力。类似地，容器生命周期钩子使它能够感知自身生命周期管理中的事件，并在相应时刻到来时运行由用户指定的处理程序代码。Kubernetes同样为容器提供了postStart和preStop两种生命周期钩子。</p>
<ul>
<li>postStart：在容器创建完成后立即运行的钩子句柄（handler），该钩子定义的事件执行完成后容器才能真正完成启动过程，如图4-15中的左图所示；不过Kubernetes无法确保它一定会在容器的主应用程序（由ENTRYPOINT定义）之前运行。</li>
<li>preStop：在容器终止操作执行之前立即运行的钩子句柄，它以同步方式调用，因此在其完成之前会阻塞删除容器的操作；这意味着该钩子定义的事件成功执行并退出，容器终止操作才能真正完成，如图4-15中的右图所示。</li>
</ul>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220123090725537.png" alt="image-20220123090725537"></p>
<p>钩子句柄的实现方式类似于容器探针句柄的类型，同样有exec、httpGet和tcpSocket这3种，它们各自的配置格式和工作逻辑也完全相同，exec在容器中执行用户定义的一个或多个命令，httpGet在容器中向指定的本地URL发起HTTP连接请求，而tcpSocket则试图与指定的端口建立TCP连接。<br>postStart和preStop句柄定义在容器的lifecycle字段中，其内部一次仅支持嵌套使用一种句柄类型。下面的配置清单（lifecycle-demo.yaml）示例中同时使用了postStart和preStop钩子处理相应的事件。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">lifecycle-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">        <span class="attr">scheme:</span> <span class="string">HTTP</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp </span></span><br><span class="line"><span class="string">          --dport 8080 -j REDIRECT --to-ports 80&#x27;</span>]</span><br><span class="line">      <span class="attr">preStop:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;while killall python3; do sleep 1; done&#x27;</span>]</span><br><span class="line">  <span class="attr">restartPolicy:</span> <span class="string">Always</span></span><br></pre></td></tr></table></figure>

<p>示例中的demo容器通过postStart执行iptables命令设置端口重定向规则，将发往该Pod IP的8080端口的所有请求重定向至80端口，从而让容器应用能够同时从8080端口接收请求。demo容器又借助preStop执行killall命令，它假设该命令能够更优雅地终止基于Python3运行的容器应用demoapp。将清单中的Pod对象创建于集群中便可展开后续的测试：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f lifecycle-demo.yaml</span> </span><br><span class="line">pod/lifecycle-demo created</span><br></pre></td></tr></table></figure>

<p>而后可获取容器内网络名称空间中PREROUTING链上的iptables规则，验证postStart钩子事件的执行结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> lifecycle-demo -- iptables -t nat -nL PREROUTING</span>  </span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt     source    destination         </span><br><span class="line">REDIRECT   tcp  --  0.0.0.0/0    0.0.0.0/0    tcp dpt:8080 redir ports 80</span><br></pre></td></tr></table></figure>

<p>上面的配置清单中有意同时添加了httpGet类型的存活探针，我们可以人为地将探针检测结果置为失败状态，以促使kubelet重启demo容器验证preStop钩子事件的执行。不过，该示例中给出的操作是终止容器应用，那么容器成功重启即验证了相应脚本的运行完成。</p>
<h2 id="多容器Pod"><a href="#多容器Pod" class="headerlink" title="多容器Pod"></a>多容器Pod</h2><p>容器设计模式中的单节点多容器模型中，初始化容器和Sidecar容器是目前使用较多的模式，尤其是服务网格的发展极大促进了Sidecar容器的应用。</p>
<h3 id="初始化容器"><a href="#初始化容器" class="headerlink" title="初始化容器"></a>初始化容器</h3><p>初始化代码要首先运行，且只能运行一次，它们常用于验证前提条件、基于默认值或传入的参数初始化对象实例的字段等。Pod中的初始化容器（Init Container）功能与此类似，它们为那些有先决条件的应用容器完成必要的初始设置，例如设置特殊权限、生成必要的iptables规则、设置数据库模式，以及获取最新的必要数据等。<br>有很多场景都需要在应用容器启动之前进行部分初始化操作，如等待其他关联组件服务可用、基于环境变量或配置模板为应用程序生成配置文件、从配置中心获取配置等。初始化容器的典型应用需求有如下几种。</p>
<ul>
<li>用于运行需要管理权限的工具程序，例如iptables命令等，出于安全等方面的原因，应用容器不适合拥有运行这类程序的权限。</li>
<li>提供主容器镜像中不具备的工具程序或自定义代码。</li>
<li>为容器镜像的构建和部署人员提供了分离、独立工作的途径，部署人员使用专用的初始化容器完成特殊的部署逻辑，从而使得他们不必协同起来制作单个镜像文件。</li>
<li>初始化容器和应用容器处于不同的文件系统视图中，因此可分别安全地使用敏感数据，例如Secrets资源等。</li>
<li>初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得以满足。</li>
</ul>
<p>Pod对象中的所有初始化容器必须按定义的顺序串行运行，直到它们全部成功结束才能启动应用容器，因而初始化容器通常很小，以便它们能够以轻量的方式快速运行。某初始化容器运行失败将会导致整个Pod重新启动（重启策略为Never时例外），初始化容器也必将再次运行，因此需要确保所有初始化容器的操作具有幂等性，以避免无法预知的副作用。<br>Pod资源的spec.initContainers字段以列表形式定义可用的初始化容器，其嵌套可用字段类似于spec.containers。下面的资源清单（init-container-demo.yaml）在Pod对象上定义了一个名为iptables-init的初始化容器示例。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">init-container-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span>   <span class="comment"># 定义初始化容器</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">iptables-init</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/admin-box:latest</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT </span></span><br><span class="line"><span class="string">    --to-port 80&#x27;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">http</span></span><br><span class="line">      <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>示例中，应用容器demo默认监听TCP协议的80端口，但我们又期望该Pod能够在TCP协议的8080端口通过端口重定向方式为客户端提供服务，因此需要在其网络名称空间中添加一条相应的iptables规则。但是，添加该规则的iptables命令依赖于内核中的网络管理权限，出于安全原因，我们并不期望应用容器拥有该权限，因而使用了拥有网络管理权限的初始化容器来完成此功能。下面先把配置清单中定义的资源创建于集群之上：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f init-container-demo.yaml</span> </span><br><span class="line">pod/init-container-demo created</span><br></pre></td></tr></table></figure>

<p>随后，在Pod对象init-container-demo的描述信息中的初始化容器信息段可以看到如下内容，它表明初始化容器启动后大约1秒内执行完成返回0状态码并成功退出。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">Command:</span></span><br><span class="line">  <span class="string">/bin/sh</span></span><br><span class="line">  <span class="string">-c</span></span><br><span class="line"><span class="attr">Args:</span></span><br><span class="line">  <span class="string">iptables</span> <span class="string">-t</span> <span class="string">nat</span> <span class="string">-A</span> <span class="string">PREROUTING</span> <span class="string">-p</span> <span class="string">tcp</span> <span class="string">--dport</span> <span class="number">8080</span> <span class="string">-j</span> <span class="string">REDIRECT</span> <span class="string">--to-port</span> <span class="number">80</span></span><br><span class="line"><span class="attr">State:</span>          <span class="string">Terminated</span></span><br><span class="line">  <span class="attr">Reason:</span>       <span class="string">Completed</span></span><br><span class="line">  <span class="attr">Exit Code:</span>    <span class="number">0</span></span><br><span class="line">  <span class="attr">Started:</span>      <span class="string">Sun,</span> <span class="number">30</span> <span class="string">Aug</span> <span class="number">2020 11:44:28</span> <span class="string">+0800</span></span><br><span class="line">  <span class="attr">Finished:</span>     <span class="string">Sun,</span> <span class="number">30</span> <span class="string">Aug</span> <span class="number">2020 11:44:29</span> <span class="string">+0800</span></span><br><span class="line"><span class="attr">Ready:</span>          <span class="literal">True</span></span><br><span class="line"><span class="attr">Restart Count:</span>  <span class="number">0</span></span><br></pre></td></tr></table></figure>

<p>这表明，向Pod网络名称空间中添加iptables规则的操作已经完成，我们可通过应用容器来请求查看这些规则，但因缺少网络管理权限，该查看请求会被拒绝：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> init-container-demo -- iptables -t nat -vnL</span></span><br><span class="line">iptables v1.8.3 (legacy): can&#x27;t initialize iptables table `nat&#x27;: Permission denied (you must be root)</span><br><span class="line">Perhaps iptables or your kernel needs to be upgraded.</span><br><span class="line">command terminated with exit code 3</span><br></pre></td></tr></table></figure>

<p>另一方面，应用容器中的服务却可以正常通过Pod IP的8080端口接收并响应，如下面的命令及执行结果所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/init-container-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl http://<span class="variable">$&#123;podIP&#125;</span>:8080</span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 10.244.0.0, ServerName: init-container-demo, …</span><br></pre></td></tr></table></figure>

<p>由此可见，初始化容器及容器的postStop钩子都能完成特定的初始化操作，但postStop必须在应用容器内部完成，它依赖的条件（例如管理权限）也必须为应用容器所有，这无疑会为应用容器引入安全等方面的风险。另外，考虑到应用容器镜像内部未必存在执行初始化操作的命令或程序库，使用初始化容器也就成了不二之选。</p>
<h3 id="Sidecar容器"><a href="#Sidecar容器" class="headerlink" title="Sidecar容器"></a>Sidecar容器</h3><p>Sidecar容器是Pod中与主容器松散耦合的实用程序容器，遵循容器设计模式，并以单独容器进程运行，负责运行应用的非核心功能，以扩展、增强主容器。Sidecar模式最著名的用例是充当服务网格中的微服务的代理应用（例如Istio中的数据控制平面Envoy），其他典型使用场景包括日志传送器、监视代理和数据加载器等。<br>下面的配置清单（sidecar-container-demo.yaml）中定义了两个容器：一个是运行demoapp的主容器demo，一个运行envoy代理的Sidecar容器proxy。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">sidecar-container-demo</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /etc/envoy/envoy.yaml https://</span></span><br><span class="line"><span class="string">          raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_Practical_2rd/</span></span><br><span class="line"><span class="string">          master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">HOST</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;127.0.0.1&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;8080&quot;</span></span><br></pre></td></tr></table></figure>

<p>Envoy程序是服务网格领域著名的数据平面实现，它在Istio服务网格中以Sidecar的模式同每一个微服务应用程序单独组成一个Pod，负责代理该微服务应用的所有通信事件，并为其提供限流、熔断、超时、重试等多种高级功能。这里我们将demoapp视作一个微服务应用，配置Envoy为其代理并调度入站（Ingress）流量，因而在示例中demo容器基于环境变量被配置为监听127.0.0.1地址上一个特定的8080端口，而proxy容器将监听Pod所有IP地址上的80端口，以接收客户端请求。proxy容器上的postStart事件用于为Envoy代理下载一个适用的配置文件，以便将proxy接收到的所有请求均代理至demo容器。<br>下面说明整个测试过程。先将配置清单中定义的对象创建到集群之上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f sidecar-container-demo.yaml</span> </span><br><span class="line">pod/sidecar-container-demo created</span><br></pre></td></tr></table></figure>

<p>随后，等待Pod中的两个容器成功启动且都转为就绪状态，可通过各Pod内端口监听的状态来确认服务已然正常运行。下面命令的结果表示，Envoy已经正常运行并监听了TCP协议的80端口和9901端口（Envoy的内置管理接口）。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> sidecar-container-demo -c proxy -- netstat -tnlp</span>    </span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address    Foreign Address   State    PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:9901       0.0.0.0:*       LISTEN      1/envoy</span><br><span class="line">tcp        0      0 0.0.0.0:80         0.0.0.0:*       LISTEN      1/envoy</span><br><span class="line">tcp        0      0 127.0.0.1:8080     0.0.0.0:*       LISTEN      -</span><br></pre></td></tr></table></figure>

<p>接下来，我们向Pod的80端口发起HTTP请求，若它能以demoapp的页面响应，则表示代理已然成功运行，甚至可以根据响应头部来判断其是否有代理服务Envoy发来的代理响应，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">podIP=$(kubectl get pods/sidecar-container-demo -o jsonpath=&#123;.status.podIP&#125;)</span></span><br><span class="line"><span class="meta">$ </span><span class="language-bash">curl http://<span class="variable">$podIP</span></span></span><br><span class="line">iKubernetes demoapp v1.0 !! ClientIP: 127.0.0.1, ServerName: sidecar-container-demo, ……</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">curl -I http://<span class="variable">$podIP</span></span></span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">content-type: text/html; charset=utf-8</span><br><span class="line">content-length: 108</span><br><span class="line">server: envoy</span><br><span class="line">date: Sun, 22 May 2020 06:43:04 GMT</span><br><span class="line">x-envoy-upstream-service-time: 3</span><br></pre></td></tr></table></figure>

<p>虽然Sidecar容器可以称得上是Pod中的常规容器，但直到v1.18版本，Kubernetes才将其添加作为内置功能。在此之前，Pod中的各应用程序彼此间没有区别，用户无从预测和控制容器的启动及关闭顺序，但多数场景都要求Sidecar容器必须要先于普通应用容器启动以做一些准备工作，例如分发证书、创建存储卷或获取一些数据等，且它们需要晚于其他应用容器终止。Kubernetes从v1.18版本开始支持用户在生命周期字段中将容器标记为Sidecar，这类容器全部转为就绪状态后，普通应用容器方可启动。因而，这个新特性根据生命周期将Pod的容器重新划分成了初始化容器、Sidecar容器和应用容器3类。<br>所有的Sidecar容器都是应用容器，唯一不同之处是，需要手动为Sidecar容器在lifecycle字段中嵌套定义type类型的值为Sidecar。配置格式如下所示：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">proxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">Sidecar</span></span><br><span class="line">    <span class="string">……</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="string">……</span></span><br></pre></td></tr></table></figure>

<p>另外，可能也有一些场景需要Sidecar容器启动晚于普通应用容器，这种特殊的应用需求，目前可通过OpernKruise项目中的SidecarSet提供的PostSidecar模型来解决。将来，该项目或许支持以DAG的方式来灵活编排容器的启动顺序。</p>
<h2 id="资源需求与资源限制"><a href="#资源需求与资源限制" class="headerlink" title="资源需求与资源限制"></a>资源需求与资源限制</h2><h3 id="资源需求与限制"><a href="#资源需求与限制" class="headerlink" title="资源需求与限制"></a>资源需求与限制</h3><p>在Kubernetes上，可由容器或Pod请求与消费的“资源”主要是指CPU和内存（RAM），它可统称为计算资源，另一种资源是事关可用存储卷空间的存储资源。<br>相比较而言，CPU属于可压缩型资源，即资源额度可按需弹性变化，而内存（当前）则是不可压缩型资源，CPU和内存资源的配置主要在Pod对象中的容器上进行，并且每个资源存在如图4-16所示的需求和限制两种类型。</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220123091411880.png" alt="image-20220123091411880"></p>
<ul>
<li>资源需求：定义需要系统预留给该容器使用的资源最小可用值，容器运行时可能用不到这些额度的资源，但用到时必须确保有相应数量的资源可用。</li>
<li>资源限制：定义该容器可以申请使用的资源最大可用值，超出该额度的资源使用请求将被拒绝；显然，该限制需要大于等于requests的值，但系统在某项资源紧张时，会从容器回收超出request值的那部分。</li>
</ul>
<p>在Kubernetes系统上，1个单位的CPU相当于虚拟机上的1颗虚拟CPU（vCPU）或物理机上的一个超线程（Hyperthread，或称为一个逻辑CPU），它支持分数计量方式，一个核心（1 core）相当于1000个微核心（millicores，以下简称为m），因此500m相当于是0.5个核心，即1/2个核心。内存的计量方式与日常使用方式相同，默认单位是字节，也可以使用E、P、T、G、M和K为单位后缀，或Ei、Pi、Ti、Gi、Mi和Ki形式的单位后缀。</p>
<h3 id="容器资源需求"><a href="#容器资源需求" class="headerlink" title="容器资源需求"></a>容器资源需求</h3><p>下面的配置清单示例（resource-requests-demo.yaml）中的自主式Pod要求为stress容器确保128MiB的内存及1/5个CPU核心（200m）资源可用。Pod运行stress-ng镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时stress容器也会尽可能多地占用CPU资源，另外再启动一个专用的CPU压力测试进程（-c 1）。stress-ng是一个多功能系统压力测试具，master/worker模型，master为主进程，负载生成和控制子进程，worker是负责执行各类特定测试的子进程，例如测试CPU的子进程，以及测试RAM的子进程等。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">stress-pod</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">stress</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/stress-ng</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/usr/bin/stress-ng&quot;</span>, <span class="string">&quot;-m 1&quot;</span>, <span class="string">&quot;-c 1&quot;</span>, <span class="string">&quot;-metrics-brief&quot;</span>]</span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;128Mi&quot;</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">&quot;200m&quot;</span></span><br></pre></td></tr></table></figure>

<p>上面的配置清单中，stress容器请求使用的CPU资源大小为200m，这意味着一个CPU核心足以确保其以期望的最快方式运行。另外，配置清单中期望使用的内存大小为128MiB，不过其运行时未必真的会用到这么多。考虑到内存为非压缩型资源，当超出时存在因OOM被杀死的可能性，于是请求值是其理想中使用的内存空间上限。<br>接下来创建并运行此Pod对象以对其资源限制效果进行检查。因为显示结果涉及资源占用比例等，因此同样的测试配置对不同的系统环境来说，其结果也会有所不同，作者为测试资源需求和资源限制功能而使用的系统环境中，每个节点的可用CPU核心数为8，物理内存空间为16GB。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create -f resource-requests-demo.yaml</span></span><br></pre></td></tr></table></figure>

<p>而后在Pod资源的容器内运行top命令，观察CPU及内存资源占用状态，如下所示。其中{stress-ng-vm}是执行内存压测的子进程，它默认使用256MB的内存空间，{stress-ng-cpu}是执行CPU压测的专用子进程。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- top</span></span><br><span class="line">Mem: 2884676K used, 13531796K free, 27700K shrd, 2108K buff, 1701456K cached</span><br><span class="line">CPU:  25% usr   0% sys   0% nic  74% idle   0% io   0% irq   0% sirq</span><br><span class="line">Load average: 0.57 0.60 0.71 3/435 15</span><br><span class="line">PID  PPID USER  STAT   VSZ %VSZ CPU %CPU COMMAND</span><br><span class="line">9     8  root     R     262m   2%   6  13% &#123;stress-ng-vm&#125; /usr/bin/stress-ng</span><br><span class="line">7     1  root     R     6888   0%   3  13% &#123;stress-ng-cpu&#125; /usr/bin/stress-ng</span><br><span class="line">1     0  root     S     6244   0%   1   0% /usr/bin/stress-ng -c 1 -m 1 --met</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>top命令的输出结果显示，每个测试进程的CPU占用率为13%（实际12.5%），{stress-ng-vm}的内存占用量为262MB（VSZ），此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用范围内尽量多地占用相关的资源。两个测试线程分布于两个CPU核心，以满载的方式运行，系统共有8个核心，因此其使用率为25%（2/8）。另外，节点上的内存资源充裕，所以，尽管容器的内存用量远超128MB，但它依然可以运行。一旦资源紧张时，节点仅保证该容器有1/5个CPU核心（其需求中的定义）可用。在有着8个核心的节点上来说，它的占用率为2.5%，于是每个进程占比为1.25%，多占用的资源会被压缩。内存为非可压缩型资源，该Pod对象在内存资源紧张时可能会因OOM被杀死。<br>对于压缩型的资源CPU来说，若未定义容器的资源请求用量，以确保其最小可用资源量，该Pod占用的CPU资源可能会被其他Pod对象压缩至极低的水平，甚至到该Pod对象无法被调度运行的境地。而对于非压缩型内存资源来说，资源紧缺情形下可能导致相关的容器进程被杀死。因此，在Kubernetes系统上运行关键型业务相关的Pod时，必须要使用requests属性为容器明确定义资源需求。当然，我们也可以为Pod对象定义较高的优先级来改变这种局面。<br>集群中的每个节点都拥有定量的CPU和内存资源，调度器将Pod绑定至节点时，仅计算资源余量可满足该Pod对象需求量的节点才能作为该Pod运行的可用目标节点。也就是说，Kubernetes的调度器会根据容器的requests属性定义的资源需求量来判定哪些节点可接收并运行相关的Pod对象，而对于一个节点的资源来说，每运行一个Pod对象，该Pod对象上所有容器requests属性定义的请求量都要给予预留，直到节点资源被绑定的所有Pod对象瓜分完毕为止。</p>
<h3 id="容器资源限制"><a href="#容器资源限制" class="headerlink" title="容器资源限制"></a>容器资源限制</h3><p>一旦定义资源限制，分配资源时，可压缩型资源CPU的控制阀可自由调节，容器进程也就无法获得超出其CPU配额的可用值。但是，若进程申请使用超出limits属性定义的内存资源时，该进程将可能被杀死。不过，该进程随后仍可能会被其控制进程重启，例如，当Pod对象的重启策略为Always或OnFailure时，或者容器进程存在有监视和管理功能的父进程等。<br>下面的配置清单文件（resource-limits-demo.yaml）中定义使用simmemleak镜像运行一个Pod对象，它模拟内存泄漏操作不断地申请使用内存资源，直到超出limits属性中memory字段设定的值而被杀死。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: memleak-pod</span><br><span class="line">  labels:</span><br><span class="line">    app: memleak</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: simmemleak</span><br><span class="line">    image: ikubernetes/simmemleak</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    resources:</span><br><span class="line">      requests:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;1&quot;</span><br><span class="line">      limits:</span><br><span class="line">        memory: &quot;64Mi&quot;</span><br><span class="line">        cpu: &quot;1&quot;</span><br></pre></td></tr></table></figure>

<p>下面将配置清单中定义的Pod对象创建到集群中，测试资源限制的实施效果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f resource-limits-demo.yaml</span></span><br><span class="line">pod/memleak-pod created</span><br></pre></td></tr></table></figure>

<p>Pod资源的默认重启策略为Always，于是在simmemleak容器因内存资源达到硬限制而被终止后会立即重启，因此用户很难观察到其因OOM而被杀死的相关信息。不过，多次因内存资源耗尽而重启会触发Kubernetes系统的重启延迟机制（退避算法），即每次重启的时间间隔会不断地拉长，因而用户看到Pod对象的相关状态通常为CrashLoopBackOff。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get pods -l app=memleak</span> </span><br><span class="line">NAME      READY     STATUS      RESTARTS   AGE</span><br><span class="line">memleak-pod   0/1       CrashLoopBackOff   1         24s</span><br></pre></td></tr></table></figure>

<p>Pod对象的重启策略在4.5.3节介绍过，这里不再赘述。我们可通过Pod对象的详细描述了解其相关状态，例如下面的命令及部分结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~]$ </span><span class="language-bash">kubectl describe pods memleak-pod</span></span><br><span class="line">Name:         memleak-pod</span><br><span class="line">……</span><br><span class="line">Last State:     Terminated</span><br><span class="line">      Reason:       OOMKilled</span><br><span class="line">      Exit Code:    137</span><br><span class="line">      Started:      Mon, 31 Aug 2020 12:42:50 +0800</span><br><span class="line">      Finished:     Mon, 31 Aug 2020 12:42:50 +0800</span><br><span class="line">    Ready:          False</span><br><span class="line">    Restart Count:  3</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>上面的命令结果中，OOMKilled表示容器因内存耗尽而被终止，因此为limits属性中的memory设置一个合理值至关重要。与资源需求不同的是，资源限制并不影响Pod对象的调度结果，即一个节点上的所有Pod对象的资源限制数量之和可以大于节点拥有的资源量，即支持资源的过载使用（overcommitted）。不过，这么一来，一旦内存资源耗尽，几乎必然地会有容器因OOMKilled而终止。<br>另外需要说明的是，Kubernetes仅会确保Pod对象获得它们请求的CPU时间额度，它们能否取得额外（throttled）的CPU时间，则取决于其他正在运行作业的CPU资源占用情况。例如对于总数为1000m的CPU资源来说，容器A请求使用200m，容器B请求使用500m，在不超出它们各自最大限额的前下，则余下的300m在双方都需要时会以2 : 5（200m : 500m）的方式进行配置。</p>
<h3 id="容器可见资源"><a href="#容器可见资源" class="headerlink" title="容器可见资源"></a>容器可见资源</h3><p>在容器中运行top等命令观察资源可用量信息时，容器可用资源受限于requests和limits属性中的定义，但容器中可见的资源量依然是节点级别的可用总量。例如，为前面定义的stress-pod添加如下limits属性定义。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">limits:</span></span><br><span class="line">   <span class="attr">memory:</span> <span class="string">&quot;512Mi&quot;</span></span><br><span class="line">   <span class="attr">cpu:</span> <span class="string">&quot;400m&quot;</span></span><br></pre></td></tr></table></figure>

<p>重新创建stress-pod对象，并在其容器内分别列出容器可见的内存和CPU资源总量，命令及结果如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- <span class="built_in">cat</span> /proc/meminfo | grep ^MemTotal</span></span><br><span class="line">MemTotal:       16416472 kB</span><br><span class="line"><span class="meta">$ </span><span class="language-bash">kubectl <span class="built_in">exec</span> stress-pod -- <span class="built_in">cat</span> /proc/cpuinfo | grep -c ^processor</span></span><br><span class="line">8</span><br></pre></td></tr></table></figure>

<p>命令结果中显示其可用内存资源总量为16416472 kB（16GB），CPU核心数为8个，这是节点级的资源数量，而非由容器的limits属性所定义的512MiB和400m。<br>较为典型的是在Pod中运行Java应用程序时，若未使用-Xmx选项指定JVM的堆内存可用总量，则会默认设置为主机内存总量的一个空间比例（例如30%），这会导致容器中的应用程序申请内存资源时很快达到上限，而转为OOMKilled状态。另外，即便使用了-Xmx选项设置其堆内存上限，但该设置对非堆内存的可用空间不产生任何限制作用，仍然存在达到容器内存资源上限的可能性。<br>另一个典型代表是在Pod中运行Nginx应用时，其配置参数worker_processes的值设置为auto，则会创建与可见CPU核心数量等同的worker进程数，若容器的CPU可用资源量远小于节点所需资源量时，这种设置在较大的访问负荷下会产生严重的资源竞争，并且会带来更多的内存资源消耗。一种较为妥当的解决方案是使用Downward API将limits定义的资源量暴露给容器，这将在后面的章节中予以介绍。</p>
<h3 id="Pod服务质量类别"><a href="#Pod服务质量类别" class="headerlink" title="Pod服务质量类别"></a>Pod服务质量类别</h3><p>前面曾提到，Kubernetes允许节点的Pod对象过载使用资源，这意味着节点无法同时满足绑定其上的所有Pod对象以资源满载的方式运行。因而在内存资源紧缺的情况下，应该以何种次序终止哪些Pod对象就变成了问题。事实上，Kubernetes无法自行对此做出决策，它需要借助于Pod对象的服务质量和优先级等完成判定。根据Pod对象的requests和limits属性，Kubernetes把Pod对象归类到BestEffort、Burstable和Guaranteed这3个服务质量类别（Quality of Service，QoS）类别下。</p>
<ul>
<li>Guaranteed：Pod对象为其每个容器都设置了CPU资源需求和资源限制，且二者具有相同值；同时为每个容器都设置了内存资需求和内存限制，且二者具有相同值。这类Pod对象具有最高级别服务质量。</li>
<li>Burstable：至少有一个容器设置了CPU或内存资源的requests属性，但不满足Guaranteed类别的设定要求，这类Pod对象具有中等级别服务质量。</li>
<li>BestEffort：不为任何一个容器设置requests或limits属性，这类Pod对象可获得的服务质量为最低级别。<br>一旦内存资源紧缺，BestEffort类别的容器将首当其冲地被终止，因为系统不为其提供任何级别的资源保证，但换来的好处是，它们能够做到尽可能多地占用资源。若此时系统上已然不存任何BestEffort类别的容器，则接下来将轮到Burstable类别的Pod被终止。Guaranteed类别的容器拥有最高优先级，它们不会被杀死，除非其内存资源需求超限，或者OOM时没有其他更低优先级的Pod对象存在。</li>
</ul>
<p>每个运行状态的容器都有其OOM评分，评分越高越优先被杀死。OOM评分主要根据两个维度进行计算：由服务质量类别继承而来的默认分值，以及容器的可用内存资源比例，而同等类别的Pod对象的默认分值相同。下面的代码片段取自pkg/kubelet/qos/policy.go源码文件，它们定义的是各种类别的Pod对象的OOM调节（Adjust）分值，即默认分值。其中，Guaranteed类别Pod资源的Adjust分值为–998，而BestEffort类别的默认分值为1000，Burstable类别的Pod资源的Adjust分值经由相应的算法计算得出。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">const (</span><br><span class="line">    PodInfraOOMAdj        int = -998</span><br><span class="line">    KubeletOOMScoreAdj    int = -999</span><br><span class="line">    DockerOOMScoreAdj     int = -999</span><br><span class="line">    KubeProxyOOMScoreAdj  int = -999</span><br><span class="line">    guaranteedOOMScoreAdj int = -998</span><br><span class="line">    besteffortOOMScoreAdj int = 1000</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>因此，同等级别优先级的Pod资源在OOM时，与自身的requests属性相比，其内存占用比例最大的Pod对象将先被杀死。例如，图4-17中的同属于Burstable类别的Pod A将先于Pod B被杀死，虽然其内存用量小，但与自身的requests值相比，它的占用比例为95%，要大于Pod B的80%</p>
<p><img src="/blog/2022/02/09/Kubernetes%E5%9F%BA%E7%A1%80/image-20220123092311772.png" alt="image-20220123092311772"></p>
<p>需要特别说明的是，OOM是内存耗尽时的处理机制，与可压缩型资源CPU无关，因此CPU资源的需求无法得到保证时，Pod对象仅仅是暂时获取不到相应的资源来运行而已。</p>
<h2 id="综合应用案例"><a href="#综合应用案例" class="headerlink" title="综合应用案例"></a>综合应用案例</h2><p>下面的配置清单（all-in-one.yaml）中定义的Pod对象all-in-one将前面的用到的大多数配置整合在一起：它有一个初始化容器和两个应用容器，其中sidecar-proxy为Sidecar容器，负责为主容器demo代理服务客户端请求。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">all-in-one</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">default</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">initContainers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">iptables-init</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/admin-box:latest</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;iptables -t nat -A PREROUTING -p tcp --dport 8080 -j REDIRECT </span></span><br><span class="line"><span class="string">    --to-port 80&#x27;</span>]</span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">capabilities:</span></span><br><span class="line">        <span class="attr">add:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">NET_ADMIN</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">sidecar-proxy</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">envoyproxy/envoy-alpine:v1.13.1</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&#x27;sleep 3 &amp;&amp; envoy -c /etc/envoy/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">      <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">          <span class="attr">command:</span> [<span class="string">&#x27;/bin/sh&#x27;</span>,<span class="string">&#x27;-c&#x27;</span>,<span class="string">&#x27;wget -O /etc/envoy/envoy.yaml https://</span></span><br><span class="line"><span class="string">          raw.githubusercontent.com/iKubernetes/Kubernetes_Advanced_</span></span><br><span class="line"><span class="string">          Practical_2rd/master/chapter4/envoy.yaml&#x27;</span>]</span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">tcpSocket:</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">demo</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">ikubernetes/demoapp:v1.0</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">env:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">PORT</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&#x27;8080&#x27;</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/livez&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">readinessProbe:</span></span><br><span class="line">      <span class="attr">httpGet:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">&#x27;/readyz&#x27;</span></span><br><span class="line">        <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">      <span class="attr">initialDelaySeconds:</span> <span class="number">15</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">runAsUser:</span> <span class="number">1001</span></span><br><span class="line">      <span class="attr">runAsGroup:</span> <span class="number">1001</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="number">0.5</span></span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;64Mi&quot;</span></span><br><span class="line">      <span class="attr">limits:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="number">2</span> </span><br><span class="line">        <span class="attr">memory:</span> <span class="string">&quot;1024Mi&quot;</span></span><br><span class="line">  <span class="attr">securityContext:</span></span><br><span class="line">    <span class="attr">supplementalGroups:</span> [<span class="number">1002</span>, <span class="number">1003</span>]</span><br><span class="line">    <span class="attr">fsGroup:</span> <span class="number">2000</span></span><br></pre></td></tr></table></figure>

<p>配置清单的Pod对象的各容器中，主容器demo在Pod的IP地址上监听TCP协议的8080端口，以接收并响应HTTP请求；Sidecar容器sidecar-proxy监听TCP协议的80端口，接收HTTP请求并将其代理至demo容器的8080端口；初始化容器在Pod的Network名称空间中添加了一条iptables重定向规则，该规则负责把所有发往Pod IP上8080端口的请求重定向至80端口，因而demo容器仅能从127.0.0.1的8080端口接收到请求。读者朋友可将清单中的Pod对象创建到集群上，并逐一测试其各项配置的效果。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/02/09/git%E4%BD%BF%E7%94%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/02/09/git%E4%BD%BF%E7%94%A8/" class="post-title-link" itemprop="url">git及CI/CD</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-02-09 14:41:10" itemprop="dateCreated datePublished" datetime="2022-02-09T14:41:10+08:00">2022-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-11 09:08:34" itemprop="dateModified" datetime="2022-02-11T09:08:34+08:00">2022-02-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/CI-CD/" itemprop="url" rel="index"><span itemprop="name">CI/CD</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="CI-CD的功能"><a href="#CI-CD的功能" class="headerlink" title="CI/CD的功能"></a>CI/CD的功能</h2><p>CI/CD 是一种通过在应用开发阶段引入<font color="red">自动化</font>来频<font color="red">频繁</font>向客户交付应用的方法。CI/CD 的核心概念是<font color="red">持续集成、持续交付和持续部署</font>。作为一个面向开发和运营团队的解决方案，CI/CD 主要针对在集成新代码时所引发的问题（亦称：“<a target="_blank" rel="noopener" href="https://www.solutionsiq.com/agile-glossary/integration-hell/">集成地狱</a>”）。</p>
<p>具体而言，<font color="red">CI/CD 可让持续自动化和持续监控贯穿于应用的整个生命周期（从集成和测试阶段，到交付和部署）</font>。这些关联的事务通常被统称为“CI/CD 管道”，由<a target="_blank" rel="noopener" href="https://www.redhat.com/zh/topics/devops">开发和运维团队</a>以敏捷方式协同支持。</p>
<h2 id="CI-是什么？CI-和-CD-有什么区别？"><a href="#CI-是什么？CI-和-CD-有什么区别？" class="headerlink" title="CI 是什么？CI 和 CD 有什么区别？"></a>CI 是什么？CI 和 CD 有什么区别？</h2><p>CI/CD 中的“CI”始终指持续集成，它属于开发人员的自动化流程。成功的 CI 意味着应用代码的新更改会定期构建、测试并合并到共享存储库中。该解决方案可以解决在一次开发中有太多应用分支，从而导致相互冲突的问题。</p>
<p>CI/CD 中的“CD”指的是持续交付和/或持续部署，这些相关概念有时会交叉使用。两者都事关管道后续阶段的自动化，但它们有时也会单独使用，用于说明自动化程度。</p>
<p>持续<em>交付</em>通常是指开发人员对应用的更改会自动进行错误测试并上传到存储库（如 <a target="_blank" rel="noopener" href="https://redhatofficial.github.io/#!/main">GitHub</a> 或容器注册表），然后由运维团队将其部署到实时生产环境中。这旨在解决开发和运维团队之间可见性及沟通较差的问题。因此，持续交付的目的就是确保尽可能减少部署新代码时所需的工作量。</p>
<p>持续<em>部署</em>（另一种“CD”）指的是自动将开发人员的更改从存储库发布到生产环境，以供客户使用。它主要为了解决因手动流程降低应用交付速度，从而使运维团队超负荷的问题。持续部署以持续交付的优势为根基，实现了管道后续阶段的自动化。</p>
<p><img src="/blog/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/ci-cd-flow-desktop_1-16443901866861.png" alt="CI/CD 流程"></p>
<p>CI/CD 既可能仅指持续集成和持续交付构成的关联环节，也可以指持续集成、持续交付和持续部署这三项构成的关联环节。更为复杂的是，有时“持续交付”也包含了持续部署流程。</p>
<p>归根结底，我们没必要纠结于这些语义，您只需记得 CI/CD 其实就是一个流程（通常形象地表述为管道），用于实现应用开发中的高度持续自动化和持续监控。因案例而异，该术语的具体含义取决于 CI/CD 管道的自动化程度。许多企业最开始先添加 CI，然后逐步实现交付和部署的自动化（例如作为<a target="_blank" rel="noopener" href="https://www.redhat.com/zh/topics/cloud-native-apps">云原生应用</a>的一部分）。</p>
<hr>
<h2 id="CI-持续集成（Continuous-Integration）"><a href="#CI-持续集成（Continuous-Integration）" class="headerlink" title="CI 持续集成（Continuous Integration）"></a>CI 持续集成（Continuous Integration）</h2><p><a target="_blank" rel="noopener" href="https://www.redhat.com/zh/solutions/cloud-native-development">现代应用开发</a>的目标是让多位开发人员同时处理同一应用的不同功能。但是，如果企业安排在一天内将所有分支源代码合并在一起（称为“<a target="_blank" rel="noopener" href="https://thedailywtf.com/articles/Happy_Merge_Day!">合并日</a>”），最终可能造成工作繁琐、耗时，而且需要手动完成。这是因为当一位独立工作的开发人员对应用进行更改时，有可能会与其他开发人员同时进行的更改发生冲突。如果每个开发人员都自定义自己的本地<a target="_blank" rel="noopener" href="https://www.redhat.com/zh/topics/middleware/what-is-ide">集成开发环境（IDE）</a>，而不是让团队就一个基于云的 IDE 达成一致，那么就会让问题更加雪上加霜。</p>
<p>持续集成（CI）可以帮助开发人员更加频繁地（有时甚至每天）将代码更改合并到共享分支或“主干”中。一旦开发人员对应用所做的更改被合并，系统就会通过自动构建应用并运行不同级别的自动化测试（通常是单元测试和集成测试）来验证这些更改，确保这些更改没有对应用造成破坏。这意味着测试内容涵盖了从类和函数到构成整个应用的不同模块。如果自动化测试发现新代码和现有代码之间存在冲突，CI 可以更加轻松地快速修复这些错误。</p>
<p><a target="_blank" rel="noopener" href="https://developers.redhat.com/blog/2017/09/06/continuous-integration-a-typical-process/">进一步了解技术细节</a></p>
<hr>
<h2 id="CD-持续交付（Continuous-Delivery）"><a href="#CD-持续交付（Continuous-Delivery）" class="headerlink" title="CD 持续交付（Continuous Delivery）"></a>CD 持续交付（Continuous Delivery）</h2><p>完成 CI 中构建及单元测试和集成测试的自动化流程后，持续交付可自动将已验证的代码发布到存储库。为了实现高效的持续交付流程，务必要确保 CI 已内置于开发管道。持续交付的目标是拥有一个可随时部署到生产环境的代码库。</p>
<p>在持续交付中，每个阶段（从代码更改的合并，到生产就绪型构建版本的交付）都涉及测试自动化和代码发布自动化。在流程结束时，运维团队可以快速、轻松地将应用部署到生产环境中。</p>
<hr>
<h2 id="CD-持续部署（Continuous-Deployment）"><a href="#CD-持续部署（Continuous-Deployment）" class="headerlink" title="CD 持续部署（Continuous Deployment）"></a>CD 持续部署（Continuous Deployment）</h2><p>对于一个成熟的 CI/CD 管道来说，最后的阶段是持续部署。作为持续交付——自动将生产就绪型构建版本发布到代码存储库——的延伸，持续部署可以自动将应用发布到生产环境。由于在生产之前的管道阶段没有手动门控，因此持续部署在很大程度上都得依赖精心设计的测试自动化。</p>
<p>实际上，持续部署意味着开发人员对应用的更改在编写后的几分钟内就能生效（假设它通过了自动化测试）。这更加便于持续接收和整合用户反馈。总而言之，所有这些 CI/CD 的关联步骤都有助于降低应用的部署风险，因此更便于以小件的方式（而非一次性）发布对应用的更改。不过，由于还需要编写自动化测试以适应 CI/CD 管道中的各种测试和发布阶段，因此前期投资还是会很大。</p>
<h2 id="版本控制系统"><a href="#版本控制系统" class="headerlink" title="版本控制系统"></a>版本控制系统</h2><h3 id="什么是版本控制系统"><a href="#什么是版本控制系统" class="headerlink" title="什么是版本控制系统"></a>什么是版本控制系统</h3><p>版本控制（Revision control）是一种在开发的过程中用于管理我们对文件、目录或工程等内容的修改历史，方便查看更改历史记录，备份<font color="red">以便恢复以前的版本</font>的软件工程技术</p>
<h3 id="版本控制系统解决的问题"><a href="#版本控制系统解决的问题" class="headerlink" title="版本控制系统解决的问题"></a>版本控制系统解决的问题</h3><p>1、追溯文件历史变更</p>
<p>2、多人团队协同开发</p>
<p>3、代码集中管理</p>
<h3 id="常见的版本控制系统（集中式VS分布式）"><a href="#常见的版本控制系统（集中式VS分布式）" class="headerlink" title="常见的版本控制系统（集中式VS分布式）"></a>常见的版本控制系统（集中式VS分布式）</h3><h4 id="Subversion集中式版本控制系统"><a href="#Subversion集中式版本控制系统" class="headerlink" title="Subversion集中式版本控制系统"></a>Subversion集中式版本控制系统</h4><ul>
<li><p>Subversion的特点概括起来主要由以下几条：</p>
<blockquote>
<ul>
<li>每个版本库有唯一的URL（官方地址），每个用户都从这个地址获取代码和数据；</li>
<li>获取代码的更新，也只能连接到这个唯一的版本库，同步以取得最新数据；</li>
<li>提交必须有网络连接（非本地版本库）；</li>
<li>提交需要授权，如果没有写权限，提交会失败；</li>
<li>提交并非每次都能够成功。如果有其他人先于你提交，会提示“改动基于过时的版本，先更新再提交”… 诸如此类；</li>
<li>冲突解决是一个提交速度的竞赛：手快者，先提交，平安无事；手慢者，后提交，可能遇到麻烦的冲突解决。</li>
</ul>
</blockquote>
</li>
</ul>
<p><img src="/blog/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/20190113235821878.png" alt="在这里插入图片描述"></p>
<p><font color="red">好处：</font>每个人都可以一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限。</p>
<p><font color="blue">缺点：</font>中央服务器的单点故障。</p>
<p>若是宕机一小时，那么在这一小时内，谁都无法提交更新、还原、对比等，也就无法协同工作。如果中央服务器的磁盘发生故障，并且没做过备份或者备份得不够及时的话，还会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，被客户端提取出来的某些快照数据除外，但这样的话依然是个问题，你不能保证所有的数据都已经有人提取出来。</p>
<p>Subversion<font color="red">原理上只关心文件内容的具体差异</font>。每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容。</p>
<h4 id="Git属于分布式的版本控制系统"><a href="#Git属于分布式的版本控制系统" class="headerlink" title="Git属于分布式的版本控制系统"></a>Git属于分布式的版本控制系统</h4><p>Git记录版本历史<font color="red">只关心文件数据的整体是否发生变化</font>。Git <font color="red">不保存文件内容前后变化的差异数据</font>。</p>
<p>实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一连接。</p>
<p>在分布式版本控制系统中，客户端并不只提取最新版本的文件快照，而是把原始的代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程。</p>
<p>另外，因为Git在本地磁盘上就保存着所有有关当前项目的历史更新，并且Git中的绝大多数操作都只需要访问本地文件和资源，不用连网，所以处理起来速度飞快。用SVN的话，没有网络或者断开VPN你就无法做任何事情。但用Git的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程的镜像仓库。换作其他版本控制系统，这么做几乎不可能，抑或是非常麻烦。</p>
<ul>
<li>Git具有以下特点：</li>
</ul>
<blockquote>
<ul>
<li>Git中每个克隆(clone)的版本库都是平等的。你可以从任何一个版本库的克隆来创建属于你自己的版本库，同时你的版本库也可以作为源提供给他人，只要你愿意。</li>
<li>Git的每一次提取操作，实际上都是一次对代码仓库的完整备份。</li>
<li>提交完全在本地完成，无须别人给你授权，你的版本库你作主，并且提交总是会成功。</li>
<li>甚至基于旧版本的改动也可以成功提交，提交会基于旧的版本创建一个新的分支。</li>
<li>Git的提交不会被打断，直到你的工作完全满意了，PUSH给他人或者他人PULL你的版本库，合并会发生在PULL和PUSH过程中，不能自动解决的冲突会提示您手工完成。</li>
<li>冲突解决不再像是SVN一样的提交竞赛，而是在需要的时候才进行合并和冲突解决。</li>
<li>Git 也可以模拟集中式的工作模式</li>
<li>Git版本库统一放在服务器中</li>
<li>可以为 Git 版本库进行授权：谁能创建版本库，谁能向版本库PUSH，谁能够读取（克隆）版本库</li>
<li>团队的成员先将服务器的版本库克隆到本地；并经常的从服务器的版本库拉（PULL）最新的更新；</li>
<li>团队的成员将自己的改动推（PUSH）到服务器的版本库中，当其他人和版本库同步（PULL）时，会自动获取改变</li>
<li>Git 的集中式工作模式非常灵活</li>
<li>你完全可以在脱离Git服务器所在网络的情况下，如移动办公／出差时，照常使用代码库</li>
<li>你只需要在能够接入Git服务器所在网络时，PULL和PUSH即可完成和服务器同步以及提交</li>
<li>Git提供 rebase 命令，可以让你的改动看起来是基于最新的代码实现的改动</li>
<li>Git 有更多的工作模式可以选择，远非 Subversion可比</li>
</ul>
</blockquote>
<h2 id="git基本使用"><a href="#git基本使用" class="headerlink" title="git基本使用"></a>git基本使用</h2><h3 id="配置git"><a href="#配置git" class="headerlink" title="配置git"></a>配置git</h3><ul>
<li>通常只需要配置你是谁，邮箱是什么。就可以知道是谁提交了什么内容</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# git config --global user.name &quot;fanyang&quot;</span><br><span class="line">[root@localhost ~]# git config --global user.email &quot;fanyang@163.com&quot;</span><br><span class="line">[root@localhost ~]# git config --global color.ui true</span><br><span class="line">[root@localhost ~]# cat .gitconfig	</span><br></pre></td></tr></table></figure>

<p><img src="/blog/2022/02/09/git%E4%BD%BF%E7%94%A8/git%E4%BD%BF%E7%94%A8/image-20201119202333081.png" alt="image-20201119202333081"></p>
<h3 id="git如何提交目录文件到本地仓库"><a href="#git如何提交目录文件到本地仓库" class="headerlink" title="git如何提交目录文件到本地仓库"></a>git如何提交目录文件到本地仓库</h3><p>1、首先创建git仓库，这个目录里的所有文件都可以被git管理起来，每个文件的修改、删除、GIt都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">创建git工作目录</span></span><br><span class="line">[root@localhost ~]# mkdir /git</span><br><span class="line">[root@localhost ~]# cd /git</span><br><span class="line"><span class="meta"># </span><span class="language-bash">初始化该目录为git仓库</span></span><br><span class="line">[root@localhost git]# git  init</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建新的文件</span></span><br><span class="line">[root@localhost git]<span class="comment"># touch file&#123;1..3&#125;</span></span><br><span class="line">[root@localhost git]<span class="comment"># ls</span></span><br><span class="line">file1  file2  file3</span><br><span class="line"><span class="comment"># 查看git状态</span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line"><span class="comment"># 有三个未提交的文件</span></span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to include <span class="keyword">in</span> what will be committed)</span><br><span class="line"></span><br><span class="line">	file1</span><br><span class="line">	file2</span><br><span class="line">	file3</span><br><span class="line"></span><br><span class="line">nothing added to commit but untracked files present (use <span class="string">&quot;git add&quot;</span> to track)</span><br><span class="line"><span class="comment"># 添加本地所有文件到本地git缓存</span></span><br><span class="line">[root@localhost git]<span class="comment"># git add .</span></span><br><span class="line"><span class="comment"># 查看git状态</span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use <span class="string">&quot;git rm --cached &lt;file&gt;...&quot;</span> to unstage)</span><br><span class="line"><span class="comment"># 有三个新的文件</span></span><br><span class="line">	new file:   file1</span><br><span class="line">	new file:   file2</span><br><span class="line">	new file:   file3</span><br><span class="line"><span class="comment"># 添加git描述</span></span><br><span class="line">[root@localhost git]<span class="comment"># git commit -m &quot;新增file&#123;1..3&#125;到git仓库&quot;</span></span><br><span class="line">[master (root-commit) 8acf856] 新增file&#123;1..3&#125;到git仓库</span><br><span class="line"> 3 files changed, 0 insertions(+), 0 deletions(-)</span><br><span class="line"> create mode 100644 file1</span><br><span class="line"> create mode 100644 file2</span><br><span class="line"> create mode 100644 file3</span><br><span class="line"><span class="comment"># 修改file1</span></span><br><span class="line">[root@localhost git]<span class="comment"># echo 1 &gt;file1 </span></span><br><span class="line">[root@localhost git]<span class="comment"># git status</span></span><br><span class="line">On branch master</span><br><span class="line">Changes not staged <span class="keyword">for</span> commit:</span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to update what will be committed)</span><br><span class="line">  (use <span class="string">&quot;git checkout -- &lt;file&gt;...&quot;</span> to discard changes <span class="keyword">in</span> working directory)</span><br><span class="line"></span><br><span class="line">	modified:   file1</span><br><span class="line"></span><br><span class="line">no changes added to commit (use <span class="string">&quot;git add&quot;</span> and/or <span class="string">&quot;git commit -a&quot;</span>)</span><br><span class="line">[root@localhost git]<span class="comment"># git add .</span></span><br><span class="line">[root@localhost git]<span class="comment"># git commit -m &quot;修改file1&quot;</span></span><br><span class="line">[master 53 30aef] 修改file1</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="文件改名后重新提交到本地git仓库"><a href="#文件改名后重新提交到本地git仓库" class="headerlink" title="文件改名后重新提交到本地git仓库"></a>文件改名后重新提交到本地git仓库</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git mv file1 file </span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	renamed:    file1 -&gt; file</span><br></pre></td></tr></table></figure>

<h3 id="对比文件差异"><a href="#对比文件差异" class="headerlink" title="对比文件差异"></a>对比文件差异</h3><h4 id="对比本地文件和暂存区文件的差异"><a href="#对比本地文件和暂存区文件的差异" class="headerlink" title="对比本地文件和暂存区文件的差异"></a>对比本地文件和暂存区文件的差异</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git diff file</span><br><span class="line">[root@localhost git]# echo test &gt;&gt; file</span><br><span class="line">[root@localhost git]# git diff file</span><br><span class="line">diff --git a/file b/file</span><br><span class="line">index d00491f..c0f2f8d 100644</span><br><span class="line">--- a/file</span><br><span class="line">+++ b/file</span><br><span class="line">@@ -1 +1,2 @@</span><br><span class="line"> 1</span><br><span class="line">+test</span><br></pre></td></tr></table></figure>

<h4 id="对比暂存区文件和本地git仓库文件差异"><a href="#对比暂存区文件和本地git仓库文件差异" class="headerlink" title="对比暂存区文件和本地git仓库文件差异"></a>对比暂存区文件和本地git仓库文件差异</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git diff file</span><br><span class="line">[root@localhost git]# git diff file --cache file</span><br><span class="line">fatal: option &#x27;--cache&#x27; must come before non-option arguments</span><br><span class="line">[root@localhost git]# git diff file --cached file</span><br><span class="line">fatal: option &#x27;--cached&#x27; must come before non-option arguments</span><br><span class="line">[root@localhost git]# git diff --cached file</span><br><span class="line">diff --git a/file b/file</span><br><span class="line">new file mode 100644</span><br><span class="line">index 0000000..c0f2f8d</span><br><span class="line">--- /dev/null</span><br><span class="line">+++ b/file</span><br><span class="line">@@ -0,0 +1,2 @@</span><br><span class="line">+1</span><br><span class="line">+test</span><br><span class="line">[root@localhost git]# </span><br><span class="line">[root@localhost git]# git commit -m &quot;修改file文件&quot;</span><br><span class="line">[master 8b1ecec] 修改file文件</span><br><span class="line"> 2 files changed, 2 insertions(+), 1 deletion(-)</span><br><span class="line"> create mode 100644 file</span><br><span class="line"> delete mode 100644 file1</span><br><span class="line">[root@localhost git]# git diff --cached file</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="文件回滚"><a href="#文件回滚" class="headerlink" title="文件回滚"></a>文件回滚</h3><h4 id="操作导致文件被清空（本地目录与暂存区间的撤销）"><a href="#操作导致文件被清空（本地目录与暂存区间的撤销）" class="headerlink" title="操作导致文件被清空（本地目录与暂存区间的撤销）"></a>操作导致文件被清空（本地目录与暂存区间的撤销）</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">使用以前提交到暂存区的内容覆盖本地目录</span></span><br><span class="line">[root@localhost git]# echo &gt; file</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   file</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br><span class="line">[root@localhost git]# git checkout file</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br></pre></td></tr></table></figure>

<h4 id="本地文件误操作提交至暂存区"><a href="#本地文件误操作提交至暂存区" class="headerlink" title="本地文件误操作提交至暂存区"></a>本地文件误操作提交至暂存区</h4><p>本地<font color="red">仓库</font>覆盖暂存区—–&gt; 暂存区覆盖本地<font color="red">目录</font></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# echo ddd &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use &quot;git reset HEAD &lt;file&gt;...&quot; to unstage)</span><br><span class="line"></span><br><span class="line">	modified:   file</span><br><span class="line"></span><br><span class="line">[root@localhost git]# git reset HEAD file</span><br><span class="line">Unstaged changes after reset:</span><br><span class="line">M	file</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">Changes not staged for commit:</span><br><span class="line">  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)</span><br><span class="line">  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)</span><br><span class="line"></span><br><span class="line">	modified:   file</span><br><span class="line"></span><br><span class="line">no changes added to commit (use &quot;git add&quot; and/or &quot;git commit -a&quot;)</span><br><span class="line">[root@localhost git]# git checkout file</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br></pre></td></tr></table></figure>

<h4 id="多次提交到本地仓库后回滚"><a href="#多次提交到本地仓库后回滚" class="headerlink" title="多次提交到本地仓库后回滚"></a>多次提交到本地仓库后回滚</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# echo test &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add  .</span><br><span class="line">[root@localhost git]# ls</span><br><span class="line">file  file2  file3</span><br><span class="line">[root@localhost git]# git commit -m &quot;test&quot;</span><br><span class="line">[master c691c60] test</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line">[root@localhost git]# echo test1 &gt;&gt; file</span><br><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git commit -m &quot;test1&quot;</span><br><span class="line">[master 3595d2b] test1</span><br><span class="line"> 1 file changed, 1 insertion(+)</span><br><span class="line">[root@localhost git]# git log --oneline </span><br><span class="line">3595d2b (HEAD -&gt; master) test1</span><br><span class="line">c691c60 test</span><br><span class="line">8b1ecec 修改file文件</span><br><span class="line">5330aef 修改file1</span><br><span class="line">8acf856 新增file&#123;1..3&#125;到git仓库</span><br><span class="line">[root@localhost git]# git reset --hard c691c60</span><br><span class="line">HEAD is now at c691c60 test</span><br><span class="line">[root@localhost git]# git status </span><br><span class="line">On branch master</span><br><span class="line">nothing to commit, working tree clean</span><br><span class="line">[root@localhost git]# cat file</span><br><span class="line">1</span><br><span class="line">test</span><br><span class="line">test</span><br></pre></td></tr></table></figure>

<h4 id="git-回退后，恢复回退前版本"><a href="#git-回退后，恢复回退前版本" class="headerlink" title="git 回退后，恢复回退前版本"></a>git 回退后，恢复回退前版本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git reflog </span><br><span class="line">c691c60 (HEAD -&gt; master) HEAD@&#123;0&#125;: reset: moving to c691c60</span><br><span class="line">3595d2b HEAD@&#123;1&#125;: commit: test1</span><br><span class="line">c691c60 (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: test</span><br><span class="line">8b1ecec HEAD@&#123;3&#125;: commit: 修改file文件</span><br><span class="line">5330aef HEAD@&#123;4&#125;: commit: 修改file1</span><br><span class="line">8acf856 HEAD@&#123;5&#125;: commit (initial): 新增file&#123;1..3&#125;到git仓库</span><br><span class="line">[root@localhost git]# git re</span><br><span class="line">rebase         reflog         remote         repack         replace        request-pull   reset          revert </span><br><span class="line">[root@localhost git]# git reset --hard c691c60</span><br><span class="line">[root@localhost git]# git log --oneline </span><br><span class="line">c691c60 (HEAD -&gt; master) test</span><br><span class="line">8b1ecec 修改file文件</span><br><span class="line">5330aef 修改file1</span><br><span class="line">8acf856 新增file&#123;1..3&#125;到git仓库</span><br></pre></td></tr></table></figure>

<h3 id="git-分支管理"><a href="#git-分支管理" class="headerlink" title="git 分支管理"></a>git 分支管理</h3><h4 id="查看、创建、切换分支"><a href="#查看、创建、切换分支" class="headerlink" title="查看、创建、切换分支"></a>查看、创建、切换分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看分支</span></span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* master</span><br><span class="line"><span class="meta"># </span><span class="language-bash">创建分支</span></span><br><span class="line">[root@localhost git]# git branch dev</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">  dev</span><br><span class="line">* master</span><br><span class="line"><span class="meta"># </span><span class="language-bash">切换分支</span></span><br><span class="line">[root@localhost git]# git checkout dev</span><br><span class="line">Switched to branch &#x27;dev&#x27;</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* dev</span><br></pre></td></tr></table></figure>

<h4 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h4><p>master合并到dev—-&gt;测试合并后的dev分支—–&gt;dev分支合并到master</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git merge master</span><br><span class="line">[root@localhost git]# git merge dev</span><br></pre></td></tr></table></figure>

<h4 id="删除分支"><a href="#删除分支" class="headerlink" title="删除分支"></a>删除分支</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git branch dev -d</span><br><span class="line">Deleted branch dev (was 06b4943).</span><br><span class="line">[root@localhost git]# git branch </span><br><span class="line">* masterl,;</span><br></pre></td></tr></table></figure>

<h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><h4 id="创建标签"><a href="#创建标签" class="headerlink" title="创建标签"></a>创建标签</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">对当前分支当前的版本打标签</span></span><br><span class="line">[root@localhost git]# git tag -a &quot;v1.0&quot; -m &quot;第一个版本&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看当前分支有哪些标签</span></span><br><span class="line">[root@localhost git]# git tag</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看标签内容</span></span><br><span class="line">[root@localhost git]# git show v1.0 </span><br><span class="line"><span class="meta"># </span><span class="language-bash">对指定的<span class="built_in">id</span>打标签</span></span><br><span class="line">[root@localhost git]# git tag -a &quot;v1.0&quot; c691c60 -m &quot;未发布的版本&quot;</span><br><span class="line">fatal: tag &#x27;v1.0&#x27; already exists</span><br></pre></td></tr></table></figure>

<h4 id="删除标签"><a href="#删除标签" class="headerlink" title="删除标签"></a>删除标签</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git tag -d v0.9 </span><br><span class="line">Deleted tag &#x27;v0.9&#x27; (was 8d74209)</span><br></pre></td></tr></table></figure>

<h3 id="git-操作远程仓库"><a href="#git-操作远程仓库" class="headerlink" title="git 操作远程仓库"></a>git 操作远程仓库</h3><h4 id="关联远程仓库"><a href="#关联远程仓库" class="headerlink" title="关联远程仓库"></a>关联远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git remote add origin git@172.18.128.4:root/git-test.git</span><br><span class="line">[root@localhost git]# git remote -v</span><br><span class="line">origin	git@172.18.128.4:root/git-test.git (fetch)</span><br><span class="line">origin	git@172.18.128.4:root/git-test.git (push)</span><br></pre></td></tr></table></figure>

<h4 id="将本地仓库内容推送到远程仓库"><a href="#将本地仓库内容推送到远程仓库" class="headerlink" title="将本地仓库内容推送到远程仓库"></a>将本地仓库内容推送到远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git add .</span><br><span class="line">[root@localhost git]# git commit -m &quot;修改file文件&quot;</span><br><span class="line">[root@localhost git]# git push -u origin master </span><br></pre></td></tr></table></figure>

<h4 id="删除远程仓库"><a href="#删除远程仓库" class="headerlink" title="删除远程仓库"></a>删除远程仓库</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git]# git remote remove origin </span><br><span class="line"><span class="meta"># </span><span class="language-bash">origin ：用户名称</span></span><br></pre></td></tr></table></figure>

<h3 id="新用户加入需要做的"><a href="#新用户加入需要做的" class="headerlink" title="新用户加入需要做的"></a>新用户加入需要做的</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost ~]# mkdir /test</span><br><span class="line">[root@localhost ~]# cd /test</span><br><span class="line">[root@localhost test]# git init </span><br><span class="line">Initialized empty Git repository in /test/.git/</span><br><span class="line">[root@localhost test]# git status </span><br><span class="line"><span class="meta"># </span><span class="language-bash">On branch master</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash"><span class="comment"># Initial commit</span></span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">nothing to commit (create/copy files and use <span class="string">&quot;git add&quot;</span> to track)</span></span><br><span class="line">[root@localhost git-test]#     git config --global user.name &quot;Your Name&quot;</span><br><span class="line">[root@localhost git-test]#     git config --global user.email you@example.co</span><br><span class="line">[root@localhost test]# git remote add origin git@172.18.128.4:root/git-test.git</span><br><span class="line">[root@localhost test]# git remote -v</span><br><span class="line">origin	git@172.18.128.4:root/git-test.git (fetch)</span><br><span class="line">origin	git@172.18.128.4:root/git-test.git (push)</span><br><span class="line">[root@localhost test]# git clone git@172.18.128.4:root/git-test.git</span><br><span class="line">Cloning into &#x27;git-test&#x27;...</span><br><span class="line"></span><br><span class="line">[root@localhost test]# ls</span><br><span class="line">git-test</span><br><span class="line">[root@localhost test]# cd git-test/</span><br><span class="line">[root@localhost git-test]# ls</span><br><span class="line">file  file2  file3  file5  file7  file8</span><br><span class="line">[root@localhost git-test]# touch file9</span><br><span class="line">[root@localhost git-test]# git add .</span><br><span class="line">[root@localhost git-test]# git commit -m &quot;new file&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">On branch master</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Your branch is ahead of <span class="string">&#x27;origin/master&#x27;</span> by 1 commit.</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">  (use <span class="string">&quot;git push&quot;</span> to publish your <span class="built_in">local</span> commits)</span></span><br><span class="line"><span class="meta">#</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">nothing to commit, working directory clean</span></span><br><span class="line">[root@localhost git-test]# git push origin master </span><br><span class="line">Counting objects: 3, done.</span><br><span class="line">Compressing objects: 100% (2/2), done.</span><br><span class="line">Writing objects: 100% (2/2), 233 bytes | 0 bytes/s, done.</span><br><span class="line">Total 2 (delta 1), reused 0 (delta 0)</span><br><span class="line">To git@172.18.128.4:root/git-test.git</span><br><span class="line">   06b4943..b3fdc9f  master -&gt; master</span><br></pre></td></tr></table></figure>

<h3 id="同步远程仓库中否代码"><a href="#同步远程仓库中否代码" class="headerlink" title="同步远程仓库中否代码"></a>同步远程仓库中否代码</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@localhost git-test]# git pull origin master </span><br></pre></td></tr></table></figure>


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/01/16/Kubernetes-magedu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/blog/2022/01/16/Kubernetes-magedu/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-01-16 10:41:45" itemprop="dateCreated datePublished" datetime="2022-01-16T10:41:45+08:00">2022-01-16</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-02-09 20:48:39" itemprop="dateModified" datetime="2022-02-09T20:48:39+08:00">2022-02-09</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description">myBlog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">10</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/muse.js"></script>


<script src="/blog/js/next-boot.js"></script>




  















  

  

</body>
</html>
