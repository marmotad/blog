<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/blog/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/blog/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/blog/images/logo.svg" color="#222">

<link rel="stylesheet" href="/blog/css/main.css">


<link rel="stylesheet" href="/blog/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"marmotad.github.io","root":"/blog/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="资源指标与集群监控Kubernetes的自动化应用编排机制为用户带来便利的同时，其控制平面及容器运行时也增加了IT基础架构栈的复杂性。为了能在生产中可靠地运行Kubernetes，我们必须有针对性地增强传统监控策略，以提供Kubernetes编排和容器运行时引入的其他基础架构层的可见性。监控系统是信息系统的基础设施，它为用户提供了快速了解系统资源分配和利用状态的有效途径。对于Kubernetes来">
<meta property="og:type" content="article">
<meta property="og:title" content="资源指标与集群监控">
<meta property="og:url" content="https://marmotad.github.io/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/index.html">
<meta property="og:site_name" content="marmotad">
<meta property="og:description" content="资源指标与集群监控Kubernetes的自动化应用编排机制为用户带来便利的同时，其控制平面及容器运行时也增加了IT基础架构栈的复杂性。为了能在生产中可靠地运行Kubernetes，我们必须有针对性地增强传统监控策略，以提供Kubernetes编排和容器运行时引入的其他基础架构层的可见性。监控系统是信息系统的基础设施，它为用户提供了快速了解系统资源分配和利用状态的有效途径。对于Kubernetes来">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212525523.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212556227.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212649174.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212703787.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212807683.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212847926.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213502184.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213545953.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213638957.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213805440.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213826221.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213949834.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214139110.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214200311.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214839129.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214856555.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224215038317.png">
<meta property="og:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224215106410.png">
<meta property="article:published_time" content="2022-03-02T13:57:32.000Z">
<meta property="article:modified_time" content="2022-03-02T14:05:15.030Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Kubernetes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://marmotad.github.io/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212525523.png">

<link rel="canonical" href="https://marmotad.github.io/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>资源指标与集群监控 | marmotad</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/blog/atom.xml" title="marmotad" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/blog/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">marmotad</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">blog</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/blog/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/blog/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://marmotad.github.io/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/blog/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="myBlog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="marmotad">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          资源指标与集群监控
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-03-02 21:57:32 / 修改时间：22:05:15" itemprop="dateCreated datePublished" datetime="2022-03-02T21:57:32+08:00">2022-03-02</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="资源指标与集群监控"><a href="#资源指标与集群监控" class="headerlink" title="资源指标与集群监控"></a>资源指标与集群监控</h1><p>Kubernetes的自动化应用编排机制为用户带来便利的同时，其控制平面及容器运行时也增加了IT基础架构栈的复杂性。为了能在生产中可靠地运行Kubernetes，我们必须有针对性地增强传统监控策略，以提供Kubernetes编排和容器运行时引入的其他基础架构层的可见性。<br>监控系统是信息系统的基础设施，它为用户提供了快速了解系统资源分配和利用状态的有效途径。对于Kubernetes来说，监控系统虽然以附件的形式存在，却也是必不可少的核心组件，甚至被Kubernetes某些内置的特性所依赖。最初，Kubernetes指标采集系统围绕Heapster构建，而在新一代监控架构体系中，Kubernetes将资源指标的规范及其实现分离开来，从而预留出极大的可扩展空间。本章主要说明如何为Kubernetes集群提供资源监控机制，并利用资源指标实现HPA等功能。</p>
<h2 id="资源监控与资源指标"><a href="#资源监控与资源指标" class="headerlink" title="资源监控与资源指标"></a>资源监控与资源指标</h2><p>监控应用程序的当前状态是帮助预测问题并发现生产环境中资源瓶颈的最有效方法之一，但这也是目前几乎所有软件开发组织面临的最大挑战之一：当今主流的微服务架构又使得系统监控和日志记录变得更加复杂，数量众多的分布式的、多样化的应用以网格化通信模型协作，致使单点故障甚至可能会中断整个系统，而单点故障的识别变得越来越困难。<br>监控只是微服务体系众多挑战中的一个，要确保应用可用性与性能及完成部署等任务必然会推动团队创建或使用编排工具来托管所有服务和主机，这也是Kubernetes这一类编排系统迅速流行的原因之一。Kubernetes有效降低了分布式应用环境的复杂性，但这么一来，问题又转变成如何有效地监控Kubernetes系统自身，并高效、全面地输出指标数据。<br>我们已经了解到，基于诸如CPU和内存资源的使用率指标自动缩放工作负载规模是Kubernetes最强大的特性之一，而启用该功能的前提便是建立一种收集和存储这些指标的方法，曾经这几乎必然要用到Heapster。然而，通过Heapster收集指标数据并据此缩放工作负载的方法支持的指标类型有限且扩展难度较大，幸运的是，Kubernetes新的指标API实现了一种更为一致和高效的指标数据提供方式，这为以自定义指标为基础的自动缩放功能提供了可行的实现。</p>
<h3 id="资源监控与Heapster"><a href="#资源监控与Heapster" class="headerlink" title="资源监控与Heapster"></a>资源监控与Heapster</h3><p>Kubernetes系统上的关键指标大体可以分为两个主要组成部分：集群系统本身的指标和容器应用相关的指标。对于集群系统本身相关的监控层面而言，监控整个Kubernetes集群的健康状况是最核心的需求，包括所有工作节点是否运行正常、系统资源容量大小、每个工作节点上运行的容器化应用的数量以及整个集群的资源利用率等，它们通常可分为如下一些可衡量的指标。</p>
<blockquote>
<p>1）节点资源状态：多数指标都与节点上的系统资源利用状况有关，例如网络带宽、磁盘空间、CPU和内存的利用率等，它们也是管理员能够评估集群规模合理性的重要标准。<br>2）节点数量：在公有云服务商以实例数量计费的场景中，根据集群整体应用的资源需求规模实时调整集群节点规模是常见的弹性伸缩应用场景之一，而实时了解集群中的可用节点数量也给了用户计算所需支付费用的参考标准。<br>3）活动Pod对象的数量：正在运行的Pod对象数量常用于评估可用节点的数量是否足够，以及在节点发生故障时它们是否能够承接整个工作负载等。<br>编排运行容器化应用是Kubernetes平台的核心价值所在，通常以Pod资源形式存在的容器化应用才是集群上计算资源的消耗主力，这些应用的监控需求通常可以大体分为3类：应用编排指标、容器指标和应用程序指标。<br>1）应用编排指标：用于监视特定应用程序相关的Pod对象的部署过程、当前副本数量、期望的副本数量、部署过程进展状态、健康状态监测及网络服务器的可用性等，这些指标数据需要经由Kubernetes系统接口获取。<br>2）容器指标：包括容器的资源需求、资源限制以及CPU、内存、磁盘空间、网络带宽等资源的实际占用状况等。<br>3）应用程序指标：应用程序内置的指标，通常与其所处理的业务规则相关，例如，关系型数据库应用程序可能会内置用于暴露索引状态的指标，以及表和关系的统计指标等。</p>
</blockquote>
<p>监控集群所有节点的常用方法之一是通过DaemonSet控制器在各节点部署一个采集监控指标数据的代理程序，由代理程序将节点级采集的各种指标数据上报至监控服务端，以统一进行数据的处理、存储和展示。这种部署方式给了管理员很大的自主选择空间，但也必定难以形成统一之势。<br>于是，早期版本的Kubernetes系统特地在kubelet程序中集成了工具程序cAdvisor，用于对节点上的资源与容器进行实时监控及指标数据采集，支持的指标包括CPU、内存、网络吞吐量及文件系统等资源的使用率，并可通过TCP协议的4194端口提供一个Web UI（kubeadm的默认部署没启用此功能）。需要快速了解某特定节点上的cAdvisor运行是否正常，以及了解单节点的资源利用状态时，可直接访问cAdvisor Web UI，URL是http://<Node_IP>:4194/，cAdvisor默认的界面如图15-1所示。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212525523.png" alt="image-20220224212525523">cAdvisor的问题同样在于其仅能收集单个节点及其相关Pod资源的指标数据。事实上，将各工作节点采集的指标数据予以汇集，并通过一个统一接口向外暴露不仅能让用户使用更便捷，而且这些指标数据也是Kubernetes系统某些组件所依赖的，这些组件包括kubectl top命令、Horizontal Pod Autoscalers（即HPA)资源以及Dashboard等。例如，在未实现资源指标采集及统一输出的Kubernetes环境上运行kubectl top命令将会返回相应的错误提示，具体如下所示。</Node_IP></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl top nodes</span></span><br><span class="line">Error from server (NotFound): the server could not find the requested resource (get services http:heapster:)</span><br></pre></td></tr></table></figure>

<p>Heapster曾与ClusterDNS、Ingress和Dashboard一起并称为Kubernetes的4大核心附件，其组件结构与系统数据流向如图15-2所示。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212556227.png" alt="image-20220224212556227"></p>
<p>然而，Heapster支持的每个存储后端的代码都直接驻留在其代码仓库中，成为核心代码库的一部分，结果是必然会被烂尾的驻留代码所拖累，这甚至成为用户使用Heapster最常见的挫败原因。更重要的是，若Heapster没有将Prometheus作为数据接收器，则会暴露rometheus格式的指标，这通常会引起不小的混淆和麻烦。<br>换句话说，Heapster既为那些依赖于指标数据的系统组件提供了指标API接口（非标准格式的Kubernetes API），又提供了指标API接口背后的实现方式，即收集和存储指标数据以响应对API的请求。这种以耦合度较高的方式实现核心系统组件的做法给系统变更引入了不少的不确定性和隐患。另外，Heapster假设数据存储是一个原始的时间序列数据库，这些数据库都有着一个可直接写入的路径。这使得它与Prometheus系统基本上不兼容，因为Prometheus采用拉取式模型。Kubernetes生态系统的整体组件几乎原生支持Prometheus系统，于是Heapster的这部分功能逐渐被Prometheus所取代。<br>为了避免重蹈Heapster的覆辙，资源指标API和自定义指标API被Kubernetes特地设计为纯粹的API定义而非具体的实现，它们作为聚合API集成到Kubernetes集群中，从而能够在API保持不变的情况下切换具体的实现方案，大大降低了二者的耦合程度。最终，Heapster用于提供核心指标API的功能也被采用聚合方式的指标API服务器Metrics Server所取代。</p>
<h3 id="新一代监控体系与指标系统"><a href="#新一代监控体系与指标系统" class="headerlink" title="新一代监控体系与指标系统"></a>新一代监控体系与指标系统</h3><p>我们知道，Kubernetes通过API Aggregator（聚合器）为开发人员提供了轻松扩展API资源的能力，为集群添加指标数据API的自定义指标API、资源指标API（简称为指标API）和外部指标API都属于这种类型的扩展。<br>资源指标API是调度程序、HPA、VPA和kubectl top命令等核心系统组件可选甚至是强制依赖的基础功能，尽管是以扩展方式实现，但它提供的是Kubernetes系统必备的“核心指标”，因而并不适合与类似于Prometheus一类的第三方监控系统集成。这类的指标由轻量级且基于易失性存储器的Metrics Server收集，并通过metrics.k8s.io这一API群组公开。另一方面，自定义指标API或外部指标API为用户提供了按需进行指标扩展的接口，它支持用户将自定义指标类型的API Server直接聚合进主API服务器中，因此具有更广泛的使用场景。简单总结起来，新一代的Kubernetes监控系统架构主要由核心指标流水线和监控指标流水线共同组成。<br>（1）核心指标管道<br>由kubelet、资源评估器、Metrics Server以及提供关键指标API的API Server共同组成，如图15-3所示，它们为Kubernetes系统提供核心指标。Metrics Server通过服务发现机制发现集群上的所有节点，而后自动采集每个节点上kubelet的CPU和内存使用状态。kubelet完全能够基于容器运行时接口获取容器的统计信息，同时为了能够兼容较旧版本的Docker，它也支持从内部集成的cAdvisor采集资源指标信息，这些指标数据经由kubelet守护进程监听TCP的10250端口，并以只读方式对外提供，最终由Metrics Server完成聚合后对外公开。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212649174.png" alt="image-20220224212649174">截至目前，核心指标管道中的指标主要包括CPU累计使用、内存即时利用率、Pod资源占用率及容器的磁盘占用率等。这些度量标准的核心系统组件包括调度逻辑（基于指标数据的调度程序和应用规模的水平缩放），以及部分UI组件（例如kubectl top命令和Dashboard）等。<br>（2）监控管道<br>监控指标相关的管道负责从系统收集各种指标数据，并提供给终端用户、存储系统以及HPA控制器等使用。事实上，监控管道会收集包含核心指标（未必是Kubernetes可解析的格式）在内的大多数指标，核心指标集合之外的其他指标通常称为非核心指标。Kubernetes自定义指标API允许用户扩展任意数量的、特定于应用程序的指标，例如队列长度、每秒入站请求数等。Kubernetes系统自身既不会提供这类组件，也不为这些指标提供相关的解释，它依赖于用户使用的第三方整体解决方案。<br>Kubernetes系统上能同时使用资源指标API和自定义指标API的代表组件是HPA v2控制器（HorizontalPodAutoscaler)，它能够基于观察到的指标自动缩放Deployment或ReplicaSet一类控制器编排的应用程序的规模。而传统的HPA v1控制器仅支持根据CPU利用率进行应用规模缩放，但CPU利用率并不总是最适合自动调整应用程序规模的度量指标，也不应该是唯一指标。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212703787.png" alt="image-20220224212703787"></p>
<p>从根本上来讲，资源指标API与自定义指标API都仅是API的定义和规范，它们自身都未提供具体的实现。目前，资源指标API的主流实现是Metrics Service项目，而自定义指标API则以构建在Prometheus之上的k8s-prometheus-adapter接受最为广泛。</p>
<h2 id="资源指标与应用"><a href="#资源指标与应用" class="headerlink" title="资源指标与应用"></a>资源指标与应用</h2><p>如前所述，Kubernetes自v1.8版本起，资源利用率指标可由客户端通过指标API直接调用，这些客户端包括但不限于终端用户、kubectl top命令和HPA等。另外，虽然通过指标API能够查询某节点或Pod的当前资源利用情况，但该API本身并不存储任何指标数据，它仅提供资源利用率的实时监测数据而无法提供过去指定时刻的指标监测记录结果。本节来介绍Metrics Server的部署及基于该指标体系的kubectl top命令的用法。</p>
<h3 id="部署Metrics-Server"><a href="#部署Metrics-Server" class="headerlink" title="部署Metrics Server"></a>部署Metrics Server</h3><p>资源指标API与Kubernetes系统的其他API并无特别不同之外，它同样可经API Server的URL路径（/apis/metrics.k8s.io/）进行存取，于是也拥有同样级别的安全性、稳定性及可靠性。只有在Kubernetes集群中部署Metrics Server应用后，核心指标API才真正可用。图15-5为资源指标API架构简图。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212807683.png" alt="image-20220224212807683">Metrics Server是集群级别资源利用率数据的聚合器，它受Heapster项目启发，且在功能和特性上完全可视作一个仅服务于指标数据的简化版的Heapster。Metrics Server通过Kubernetes聚合器（kube-aggregator)注册到主API Server之上，而后基于kubelet的Summary API收集每个节点上的指标数据，将它们存储在内存中并以指标API格式提供，如图15-6所示。</p>
<p>Metrics Server基于内存存储，重启后数据将全部丢失，而且它仅能留存最近收集到的指标数据，因此，如果用户希望访问历史数据，就不得不借助第三方的监控系统（例如Prometheus等）或自行开发实现这样的功能。<br>如前所述，核心指标是Kubernetes多个核心组件的基础依赖，因此Metric Server应该在集群创建之初便作为核心组件部署运行在集群中。Metrics Server通常仅需要在集群中运行单个实例即可，它会在启用时自动初始化与各节点的连接，因而出于安全方面的考虑，它仅应该运行在普通节点之上，且需要根据目标Kubernetes集群的环境等因素定制几个配置选项。</p>
<ul>
<li>–tls-cert-file和–tls-private-key-file：metrics-server服务进程使用的证书和私钥，未指定时将由程序自动生成自签证书，生产环境建议自行指定。</li>
<li>–secure-port=<port>：metrics-server服务进程对外提供服务的端口，默认为443，以非管理员账户运行时建议修改为1024及以上的端口号，例如4443等。</port></li>
<li>–metric-resolution=<duration>：从kubelet抓取指标数据的时间间隔，默认为60秒。</duration></li>
<li>–kubelet-insecure-tls：不验证kubelet签发证书的CA，对于kubelet使用自签证书的测试环境较为有用，但不建议在生产环境使用。</li>
<li>–kubelet-preferred-address-types：与kubelet通信时倾向于使用的地址类型顺序，默认为Hostname、InternalDNS、InternalIP、ExternalDNS和ExternalIP。</li>
<li>–kubelet-port：kubelet监听的能够提供指标数据的端口号，默认为10250。</li>
</ul>
<p>对于使用kubeadm部署的Kubernetes集群来说，若未指定签署节点证书的CA，也未给每个节点配置自定义证书，则各kubelet通常是在Bootstrap过程中生成自签证书，这类证书无法由Metrics Server完成CA验证，因此需要使用–kubelet-insecure-tls选项来禁用这种验证功能。<br>另外，类似于本书这种通过hosts文件解析节点名称的集群，若节点的DNS域（ilinux.io）与Kubernetes集群使用的DNS域（默认为cluster.local）不相同，则CoreDNS通常无法解析各节点的主机名称，这类问题的解决方案有两种，下面逐一介绍。方案1：配置Metrics Server通过IP地址与各节点通信，这可以在部署Metrics Server的清单中向容器传递类似如下参数（metrics-server-deploy.yaml)实现。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224212847926.png" alt="image-20220224212847926"></p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">metrics-server</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">k8s.gcr.io/metrics-server/metrics-server:v0.3.7</span></span><br><span class="line">        <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">        <span class="attr">args:</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">--cert-dir=/tmp</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">--secure-port=4443</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">--kubelet-preferred-address-types=InternalIP</span></span><br><span class="line">          <span class="bullet">-</span> <span class="string">--kubelet-insecure-tls</span></span><br></pre></td></tr></table></figure>

<p>方案2：为CoreDNS添加类似hosts解析的资源记录，从而让CoreDNS能够直接解析各节点的名称。方法是修改kube-system名称空间中与CoreDNS相关的configmap/coredns资源，关键配置类似如下示例（coredns-configmap.yaml）。但这种方式仅建议在测试环境中使用。事实上，对于能够通过外部DNS解析节点名称的场景来说，节点名称解析通常能够正常进行，而无须过多设置。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">coredns</span></span><br><span class="line">  <span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="attr">Corefile:</span> <span class="string">|</span></span><br><span class="line"><span class="string">    .:53 &#123;</span></span><br><span class="line"><span class="string">        hosts &#123;</span></span><br><span class="line"><span class="string">          172.29.9.1 k8s-master01.ilinux.io</span></span><br><span class="line"><span class="string">          172.29.9.11 k8s-node01.ilinux.io</span></span><br><span class="line"><span class="string">          172.29.9.12 k8s-node02.ilinux.io</span></span><br><span class="line"><span class="string">          172.29.9.13 k8s-node03.ilinux.io</span></span><br><span class="line"><span class="string">          fallthrough</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        ……</span></span><br><span class="line"><span class="string"></span>  <span class="string">&#125;</span></span><br></pre></td></tr></table></figure>

<p>本节的Metrics Server部署示例将采用前一种方案，我们将项目提供的部署清单下载至本地，并按照第一种方案修改deployment/metrics-server的清单后部署到Kubernetes集群上即可。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.7/components.yaml \</span></span><br><span class="line"><span class="language-bash">    -O metrics-server-deploy.yaml</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f metrics-server-deploy.yaml</span></span><br></pre></td></tr></table></figure>

<p>该部署清单会在kube-system名称空间中创建出多种类型的资源对象，包括RBAC相关的RoleBinding、ClusterRole和ClusterroleBinding对象以及Service Account对象，以实现在启用了RBAC授权插件的集群上对metrics-server开放资源访问的许可。另外，它还会通过一个APIService对象创建Metrics API相关的群组（metrics.k8s.io），从而将Metrics Server提供的API聚合进主API Server，并且在该群组中提供两个用于指标获取的nodes和pods资源类型，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl api-versions | grep metrics</span></span><br><span class="line">metrics.k8s.io/v1beta1</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl api-resources --api-group=<span class="string">&#x27;metrics.k8s.io&#x27;</span></span></span><br><span class="line">NAME  SHORTNAMES     APIGROUP         NAMESPACED   KIND</span><br><span class="line">nodes                metrics.k8s.io   false        NodeMetrics</span><br><span class="line">pods                 metrics.k8s.io   true         PodMetrics</span><br></pre></td></tr></table></figure>

<p>待Metrics Server相关的Pod对象正常运行后，即可测试由其注册的API群组及资源的可用性。我们知道，kubectl get –raw命令可基于资源URL路径测试资源指标API服务的可用状态，例如以下命令应返回集群中所有节点的资源使用情况的指标列表，如集群中所有节点的CPU及内存资源的占用情况。在使用时，也可以直接给定具体的节点标识，从而仅列出特定节点的相关信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get --raw <span class="string">&quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</span> | jq.</span></span><br><span class="line">&#123;</span><br><span class="line">  &quot;kind&quot;: &quot;NodeMetricsList&quot;,</span><br><span class="line">  &quot;apiVersion&quot;: &quot;metrics.k8s.io/v1beta1&quot;,</span><br><span class="line">  &quot;metadata&quot;: &#123;</span><br><span class="line">    &quot;selfLink&quot;: &quot;/apis/metrics.k8s.io/v1beta1/nodes&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">……</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>提示<br>jq是处理JSON数据的命令行工具，其功能类似于sed对文本信息的处理，在Ubuntu或CentOS系统上都是由名为jq的程序包所提供。<br>此外，Pod对象的资源消耗信息也可以经由资源指标API直接列出，例如要获取集群上所有Pod对象的相关资源消耗数据，可使用如下格式的命令。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get --raw <span class="string">&quot;/apis/metrics.k8s.io/v1beta1/pods&quot;</span> | jq</span></span><br></pre></td></tr></table></figure>

<p>上面的两个命令能正常返回nodes和pods的指标数据意味着Metrics Server部署完成，随之那些依赖核心资源指标的控制器、调度器及UI工具也将在功能上得到进一步完善。</p>
<h3 id="显示资源使用信息"><a href="#显示资源使用信息" class="headerlink" title="显示资源使用信息"></a>显示资源使用信息</h3><p>kubectl top命令可显示节点和Pod对象的资源使用信息，它基于集群中的资源指标API来收集各项指标数据。它有node和pod两个子命令，分别用于显示Node对象和Pod对象的相关资源占用率。<br>列出Node资源占用率的命令语法格式为kubectl top node [-l label | NAME]，例如下面命令结果显示了各节点累计CPU资源占用时长与百分比，以及内容空间占用量与占用比例。必要时，我们也可以在命令中直接给出要查看的特定节点的标识，并使用标签选择器进行节点过滤。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl top nodes</span></span><br><span class="line">NAME                     CPU(cores)   CPU%      MEMORY(bytes)   MEMORY%   </span><br><span class="line">k8s-master01.ilinux.io   187m         4%        2406Mi          62%       </span><br><span class="line">k8s-node01.ilinux.io     368m         9%        1820Mi          48%       </span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>而名称空间级别的Pod对象资源占用率的使用方式会略有不同，相关命令会限定名称空间并使用标签选择器过滤出目标Pod对象。命令的语法格式为kubectl top pod [NAME | -l label] [–all-namespaces] [–containers=false|true]，例如下面显示了kube-system名称空间中标签为k8s-app=calico-node的所有Pod资源及其容器的资源占用状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl top pod -l k8s-app=calico-node -n kube-system</span></span><br><span class="line">NAME              CPU(cores)   MEMORY(bytes)   </span><br><span class="line">calico-node-5qx7x   22m          24Mi </span><br><span class="line">calico-node-bx2r7   24m          22Mi </span><br><span class="line">calico-node-fnpbk   27m          59Mi </span><br><span class="line">calico-node-lzbt5   26m           20Mi</span><br></pre></td></tr></table></figure>

<p>由此可见，kubectl top命令为用户提供了简洁、快速获取Node对象及Pod对象占用系统资源状况的接口，事实上它是集群日常维护中常用的命令之一。<br>另外，Kubernetes Dashboard能够根据核心指标生成节点及相关Pod资源的使用状况，如图15-7所示。图中显示了harbor名称空间中各Pod资源的CPU和内存的占用状态。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213502184.png" alt="image-20220224213502184"></p>
<h2 id="自定义指标与Prometheus"><a href="#自定义指标与Prometheus" class="headerlink" title="自定义指标与Prometheus"></a>自定义指标与Prometheus</h2><p>除了核心资源指标，Kubernetes系统还有很多其他类型的资源指标，例如各类资源相关的指标、更全面的节点与容器指标及应用程序自身暴露的指标等。尽管CNCF孵化的Prometheus项目是一个指标监控系统，但它不能与API Server的自定义API服务器进行聚合，因而无法直接服务于Kubernetes的自定义指标API或外部指标API，二者之间还需要一个中间层。Prometheus是第一个开发了Kubernetes自定义资源指标适配器的监控系统，该适配器名为Kubernetes Custom Metrics Adapter，由托管在GitHub上的k8s-prometheus-adapter项目提供。它负责以聚合API Server的形式服务于custom.metrics.k8s.io这一API群组，并将对自定义指标的查询请求转发至后端的Prometheus Server，如图15-8所示。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213545953.png" alt="image-20220224213545953">自定义指标API的目的是为最终用户和Kubernetes系统组件提供可以依赖的稳定的、版本化的API接口，但其可用的实现与可用指标则依赖第三方或用户的自行实现，目前主要基于Prometheus收集和存储指标数据，并借助k8s-prometheus-adapter或kube-metrics-adapter等适配器项目将这些指标数据查询接口转换为标准的Kubernetes自定义指标API是较为流行的解决方案。</p>
<h3 id="Prometheus基础"><a href="#Prometheus基础" class="headerlink" title="Prometheus基础"></a>Prometheus基础</h3><p>Prometheus是一个开源的服务监控系统和时序数据库，目前已经成为Kubernetes生态圈中的核心监控系统，而且越来越多的项目（如etcd等)都提供了对Prometheus的原生支持，这足以证明社区对它的认可程度。<br>图15-9给出了Prometheus系统的整体工作架构，其中Prometheus Server基于服务发现机制或静态配置获取要监视的目标，并通过每个目标上的指标暴露器Exporter来采集指标数据。Prometheus Server内置了一个基于文件的时间序列存储来持久化存储指标数据，用户可使用PromDash或PromQL接口来检索数据，也可按需将告警需求发往Alertmanager完成告警内容发送。另外，一些短期运行的作业的生命周期过短，难以有效地将必要的指标数据传递到Server端，它们一般会采用推送方式输出指标数据，Prometheus借助Pushgateway接收这些推送的数据，进而由Server端进行抓取。</p>
<h4 id="客户端库"><a href="#客户端库" class="headerlink" title="客户端库"></a><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213638957.png" alt="image-20220224213638957">客户端库</h4><p>该组件并未显示在图15-9中，因为它通常内嵌在各类Exporter中，甚至是应用程序内部。应用程序自己并不会直接生成指标数据，需要开发人员将相关的客户端库添加至应用程序中，构建出测量系统来完成。Prometheus为Go、Python、Java（或Scala）和Ruby等主流编程语言提供了各自适用的客户端库，还有适用于Bash、C、C++、C#、Node.js、Haskell、Erlang、Perl、PHP和Rust等多种编程语言的第三方库可用。通常，三两行代码即能将客户端库整合进应用程序中，实现直接测量机制。<br>客户端库主要负责处理所有的细节问题，例如线程安全和记账，以及生成文本格式的指标以数据响应HTTP请求等。客户端库通常会额外提供一些指标，例如CPU使用率和垃圾回收统计信息等，具体的实现则取决于库和相关的运行时环境。<br>另外，Prometheus是一个开放的生态系统，客户端库并不局限于输出Prometheus文本格式的指标，它也可以为其他监控系统生成适用格式的指标数据。类似地，在监控系统尚未完全统一至Prometheus的环境中，用户也可以把其他监控系统的指标数据导入到客户端库，进而提供给Prometheus使用。</p>
<h4 id="指标暴露器Exporters"><a href="#指标暴露器Exporters" class="headerlink" title="指标暴露器Exporters"></a>指标暴露器Exporters</h4><p>对于不可由用户直接控制的应用代码来说，为其添加客户端库以进行直接测量着实难以实现。操作系统内核就是一个典型的示例，它显然不大可能易于添加自定义代码，并通过HTTP协议输出Prometheus格式的指标。不过这一类的程序一般都会以某种接口输出其内在的指标，但这些指标可能有着特殊的格式，例如Linux内核的特有指标格式，或者SNMP指标格式等。需要对这些指标进行适当的解析和处理以转换为合规的目标格式，Exporter（指标暴露器）是完成此类转换功能的应用程序。<br>Exporter独立运行于要获取测量指标的应用程序之外，负责接收来自Prometheus Server的指标获取请求，它通过目标应用程序（真正的目标）内置的指标接口获取指标数据，并将这些指标数据转换为合用的目标格式后响应给Prometheus。因此，Exporter更像是“一对一”的代理，它作为Prometheus Server的target存在，在应用程序的指标接口和Prometheus的文本指标格式之间转换数据格式。不过，Exporter不存储也不缓存任何数据。<br>与添加客户端库至应用程序中的直接测量机制相比较而言，Exporter实现的是一种可称之为“自定义收集器”的测量机制，它也通常被称为常量指标（ConstMetrics)。好在，随着Prometheus社区的发展，Exporter程序的可用性越来越丰富，用户几乎无须任何自定义开发即能找到适用于各种主流应用程序的Exporter。</p>
<h4 id="服务发现"><a href="#服务发现" class="headerlink" title="服务发现"></a>服务发现</h4><p>具体的监控场景中，Prometheus需要了解每一个支持直接测量的目标，以及每一个Exporter的接入位置。然而，动态系统环境中，需要纳入监控控制系统中的目标会动态变化，用户难以全部事先静态定义出这些目标来，这就要借助服务发现功能进行动态生成。<br>一般来说，各类动态的系统环境自身便存在一个服务发现功能来完成系统内各组件间的协作，例如OpenStack系统和Kubernetes系统各有其内置的服务注册和服务发现功能，Prometheus通过此类的服务发现功能来动态管理监控目标。Prometheus还可以直接与一些开源的服务发现工具进行集成，例如在微服务架构的应用程序中经常用到的Consul等。另外，除了集成到这些平台级的公有云、私有云、容器云以及专门的服务发现功能之外，Prometheus还支持基于DNS服务以及文件系统的文件等完成监控目标的动态发现。<br>即便基于动态发现及静态配置相结合的方式能够很好地生成机器列表和服务列表，这也并不意味着它们能够正确反映出实际的系统体系。例如，不同服务平台上标识服务的元数据机制略有不同，甚至是差别巨大。Prometheus为此提供了重新打标（relabeling）的机制，以基于元数据标签（以双下划线“__”起始的标签）进行实例（instance）标签重写，然后将重写的标签显式附加到实例之上。</p>
<h4 id="指标数据抓取"><a href="#指标数据抓取" class="headerlink" title="指标数据抓取"></a>指标数据抓取</h4><p>待监控目标确定之后，Prometheus即可以拉取的方式通过HTTP请求采集相应的指标数据，这个过程在Prometheus中也称为“指标抓取”。响应报文中的指标数据经Prometheus解析后以时间序列的格式保存在内存上，并定时存储至内置的TSDB（Time-Series Database）存储系统中，同时存入的还有其他几个有用的指标，例如抓取操作的成功状态以及耗费的时长等。指标抓取是一个周期性的例行操作，默认为15秒，用户可根据需要自定义为10秒～60秒区间内的任意值。<br>事实上，指标采集还存在另一种称为推送的工作模型，关于二者孰优孰劣的争论由来已久且难以定论。不过，相比较而言，拉取的方式可以提供类似如下的工作特性：</p>
<ul>
<li>只要Exporter处于运行状态，用户可在任何位置搭建监控系统。</li>
<li>可以更便捷地查看目标实例的健康状态，并能够快速定位故障。</li>
<li>更有利于构建团队的DevOps文化。</li>
<li>具有松耦合的架构体系，应用程序的运行并不受制于Prometheus Server的运行状态。</li>
</ul>
<p>Prometheus也间接支持指标数据的推送工作模式，例如，一些短期运行作业的生命周期过短，难以有效地将必要的指标数据传送到Server端，于是它们一般会采用推送方式输出指标数据。Prometheus为此专门提供了Pushgateway组件来接收这些推送的数据，而Prometheus Server则把Pushgateway作为target以完成相应的指标数据抓取。</p>
<h4 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h4><p>为了解决此前版本中存储系统相关的问题，Prometheus 2.0引入了重写的新式TSDB，该版本以每两个小时为一个时间窗口将时序数据分割为数据块，并为每个数据块维护一个基于发布清单（posting list）构建的逆序索引，而数据则通过mmap机制进行访问。当前时间窗口内的数据保存在内存中，并在满两个小时后同步至磁盘中。为了避免Prometheus程序崩溃或重启时丢失数据，它使用预写日志（write ahead log）对当前窗口中的数据进行持久化。<br>显然，选择将时间序列数据库存储在本地磁盘中，其所能够存储的数据量也就受限于可用的磁盘空间上限，即使监控系统通常仅会用到近期采集的样本数据，考虑到目标系统规模和规划的前瞻性，长期存储指标数据必然成为规划监控系统时的一个重要考虑因素。而考虑分布式系统在可靠存储方面存在诸多挑战，因此Prometheus自身没有试图支持任何存储意义上的分布式集群。不过，Prometheus 2.0起也支持将数据通过适配器存储在第三方的存储服务中，因此PromQL的查询请求可同时运行在本地存储和远程数据集之上。</p>
<h4 id="PromQL和可视化接口"><a href="#PromQL和可视化接口" class="headerlink" title="PromQL和可视化接口"></a>PromQL和可视化接口</h4><p>Prometheus内置了众多的HTTP API，用户既可以通过它们请求原始数据，又可以进行PromQL查询，并可基于数据与查询结果生成图形和仪表板。PromQL是一种表达式语言，它支持使用多维时间序列标签进行过滤，允许用户实时选择和汇聚时间序列数据，广泛应用于Prometheus的数据查询、可视化和告警处理等使用场景当中。<br>用户可以使用Prometheus开箱即用的“表达式浏览器”，以图形或表格格式显示每个表达式的结果，也可以直接通过HTTP API进行系统调用。但表达式浏览器仅适合进行临时查询和数据浏览，它并非通用的仪表板系统。最为常用的通用可视化接口是Grafana，它是一款开源的度量分析与可视化套件，通过访问时序数据库（Prometheus就是其中一种）获取数据，并以较美观的形式展示自定义报表和图形等。Grafana具有多种功能，包括对Prometheus作为数据源的官方支持，甚至支持同时与多个Prometheus服务器通信，即使在单个图形中也可以。</p>
<h4 id="记录规则和告警规则"><a href="#记录规则和告警规则" class="headerlink" title="记录规则和告警规则"></a>记录规则和告警规则</h4><p>尽管PromQL和存储引擎足够强大和高效，但是可视化接口在运行时针对数以百计的主机上的指标进行聚合计算仍然会存在些许滞后性。记录规则通过把那些使用频繁或者计算量较大的表达式预先进行周期性计算，并将其结果保存为一组新的时间序列，以结果直接响应客户端查询，从而大大提升其响应速度。<br>告警规则也是一种预先定义在配置文件中的表达式，只不过它定义的是告警触发条件，Prometheus周期性地评估这些条件表达式，并在满足触发条件时根据用户指定的配置将告警通知发送至外部的告警服务以作出进一步处理。<br>记录规则和告警规则保存在配置文件上的规则组中，并以配置的时间间隔周期性按顺序运行。8. 告警管理器Alertmanager<br>Alertmanager接收Prometheus Server上由告警规则触发的告警通知，并将其实例化为某种形式的告警消息发送过程。Alertmanager内置提供了多种形式第三方告警通知机制，例如E-mail、Pagerduty和OpsGenie等，也支持Webhook机制，用户可通过该机制对告警操作进行个性化扩展。<br>由此可见，Prometheus的告警功能可分为两个部分：一部分是Prometheus Server中的告警规则，它负责将告警通知发送到Alertmanager；而Alertmanager就是相应的另一部分，它负责管理告警操作，包括静默、抑制、分组、路由和去重等，并将告警发送给客户端应用程序。9. 推送网关Pushgateway<br>Prometheus Pushgateway的设计目标是为了允许临时任务或批处理作业向Prometheus暴露其指标。由于这类的工作任务可能只存活较短的时间，可能会错过Prometheus Server的抓取周期，因此需要将这些指标推送到Pushgateway，并由Prometheus通过Pushgateway进行抓取。但Pushgateway并不能将Prometheus变成基于推送的监控系统，它仅是个指标缓存服务，也不支持类似statsd的语义，而仅仅是将应用程序的指标原样暴露给Prometheus Server。对于计算机级别的指标，通常更适合使用Node Exporter进行指标暴露，Pushgateway仅适用于服务级别指标。另外，当通过单个Pushgateway监视多个实例时，Pushgateway既可能变成单个故障点，又可能成为潜在的性能瓶颈。</p>
<h3 id="Prometheus核心概念"><a href="#Prometheus核心概念" class="headerlink" title="Prometheus核心概念"></a>Prometheus核心概念</h3><p>Prometheus将采集而来的所有数据在底层均存储为时序格式，它把采集到的数据存储为相关时间序列的样本，它包括一个float64格式的值和相关的毫秒级时间戳。数据存储模型代表Prometheus解析指标数据的物理表现形式，在此基础上，它还使用指标类型来表达数据投射到现实中的真实意义。</p>
<h4 id="数据模型"><a href="#数据模型" class="headerlink" title="数据模型"></a>数据模型</h4><p>Prometheus的每个时间序列都由其“指标名称”和可选的“标签”作为标识符，指标名称用于表达指标自身的含义，即监控目标上某个可测量属性的基本含义，而标签则用来体现某个指标再次细分的维度特征。一个时间序列也可称为一个测度，它的格式如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;metric name&gt;&#123;&lt;label name&gt;=&lt;label value&gt;, ...&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>指标名称：指标名称用于表达监控目标的某个一般类型的特性，这些名称应尽量做到见名知义，例如前面用到的http_request_total便能看出它代表http协议的总请求数。指标名称由ASCII字符、数字、下划线和冒号组成，且必须匹配正则表达式“[a-zA-Z_:][a-zA-Z0-9_:]”。</li>
<li>标签：标签用于描述指标的特定维度，同一指标名称的不同标签用于标识该指标的不同维度，例如{status=”200”, method=”GET”}和{status=”404”, method=”GET”}。标签名称只能使用ASCII字母、数字和下划线表示，且它们必须匹配正则表达式[a-zA-Z_:][a-zA-Z0-9_:]*，不过以__起始的标签名称由Prometheus保留使用，而标签值可以使用任何兼容Unicode的字符。<br>PromQL支持基于定义的指标维度进行过滤和聚合，而更改任何标签值，包括添加或删除标签，都会创建一个新的时间序列，这意味着应该尽可能地保持标签的稳定性，否则很可能创建新的时间序列，更甚者会生成一个动态的数据环境，并使得监控的数据源难以跟踪，从而导致建立在该指标之上的图形、告警及记录规则变得无效。_</li>
</ul>
<h4 id="指标类型"><a href="#指标类型" class="headerlink" title="指标类型"></a>指标类型</h4><p>目前，Prometheus的客户端库支持Counter（计数器）、Gauge（仪表盘）、Histogram（柱状图或直方图）和Summary（摘要）4种指标类型。Prometheus Server并不使用类型信息，而是将所有数据展平为时间序列。另外，相较于Counter和Gauge来说，Histogram和Summary是更复杂的指标类型，因为单个Histogram或Summary指标会创建多个时间序列且难以正确使用。<br>Counter代表一个随着时间而不断累积的数据，因而具有单调递增的特征，其名称通常以_total为后缀，例如HTTP已接收的总请求数http_request_total等。对于这类的时序数据，可基于时间点的数据进行计算，得到指定时间区间内的数据变化量或者速率，PromQL内置的聚合函数（例如rate()和topk()等）即可完成此类功能以实现数据分析。<br>Gauge表示可增可减的数据量，一般代表采样那个时间点的瞬时值，侧重于反映系统某个测度的当前状态，例如CPU利用率、内存空间利用率、磁盘空间利用率、系统上总的进程数或并发请求数等，多数的监控指标均属于此种类型。<br>Histogram是一种对数据分布情况的图形表示，由一系列高度不等的长条图（bar）或线段表示，用于显示单个测度值的分布。它一般用横轴表示某个指标维度的数据取值区间，用纵轴表示样本统计的频率或频数，从而能够以二维图的形式展现数值的分布状况。为了构建Histogram，首先需要将值的范围进行分段，即将所有值的整个可用范围分成一系列连续、相邻（相邻处可以是等同值）但不重叠的间隔，而后统计每个间隔中有多少值。<br>对Prometheus来说，Histogram会在一定时间范围内对数据进行采样（通常是请求持续时长或响应大小等），并将其计入可配置的bucket（观测桶）中。换句话说，Histogram事先将特定测度可能的取值范围分隔为多个样本空间，并通过对落入bucket内的观测值进行计数以及求和等操作。但Prometheus取值间隔的划分方式与前述通用方式略有不同，它采用的是累积区间间隔机制，即每个bucket中的样本均包含了前面所有bucket中的样本。<br>Histogram类型的指标有一个基础指标名称<basename>，它会暴露多个时间序列。</basename></p>
<ul>
<li><basename><em>bucket{le=”<upper inclusive bound>“}：bucket的上边界（upper inclusive bound），即样本统计区间，最大区间（包含所有样本）的名称为<basename></basename></upper></em> bucket{le= “+Inf”}。</basename></li>
<li><basename><em>sum：所有样本观测值的总和。</em></basename></li>
<li><basename>_count：总的观测次数，它自身本质上是一个Counter类型的指标。</basename></li>
</ul>
<p>累积间隔机制生成的测度数据需要使用内置的histogram_quantile()函数（即Histogram指标）来计算相应的分位数（quantile），即某个bucket的样本数在所有样本数中占据的比例。但累积间隔机制也能带来不少的优势，例如在抓取指标时可以根据需要动态任意丢弃除了le=”+Inf”之外的bucket，且无须修改应用代码，甚至于即便丢弃了所有bucket，用户仍可根据_sum和_count指标计算出样本平均值。<br>如前所述，指标类型是客户端库的特性，而Histogram在客户端仅是进行简单的桶划分和分桶计数，分位数计算由Prometheus Server基于样本数据进行估算，因而其结果未必准确，甚至不合理的bucket划分会导致较大的误差。<br>相应地，Summary是一种类似于Histogram的指标类型，但它在客户端一段时间内（默认为10分钟）的每个采样点直接进行统计计算并存储了分位数数值，Server端直接抓取相应值即可。但Summary不支持sum或avg一类的聚合运算，而且其分位数由客户端计算并生成，Server端无法获取客户端未定义的分位数，而Histogram可通过PromQL任意定义，有着较好的灵活性。3. Job和Instance<br>Prometheus将任意一个可以抓取测度数据的端点（监控目标target）称之为Instance（实例），这通常是对应于单个进程的叫法，例如envoy或node_exporter，而诸如水平扩展集群中的多个端点的Instance的集合称为Job（作业）。于是，Prometheus基于Job配置抓取target之上的测度数据时，会自动在抓取的时序标识上添加类似下面两个标签以区别监控的目标实例。</p>
<ul>
<li>Job：target所属的已配置Job的名称。</li>
<li>Instance：实例标识，由<host>:<port>组成。<br>如果在抓取的数据中已经存在上面两个标签中的任何一个，则其行为取决于honor_labels配置选项的定义。而对每一个Instance而言，Prometheus会按照以下时序来存储所采集的数据样本。</port></host></li>
<li>up{job=”<job-name>“, instance=”<instance-id>“}：值为1表示实例工作正常，而0则表示数据抓取失败。</instance-id></job-name></li>
<li>scrape_duration_seconds{job=”<job-name>“, instance=”<instance-id>“}：数据抓取的持续时长。</instance-id></job-name></li>
<li>scrape_samples_post_metric_relabeling{job=”<job-name>“, instance=”<instance-id>“}：重新定义标签操作后剩余的样本数。</instance-id></job-name></li>
<li>scrape_samples_scraped{job=”<job-name>“, instance=”<instance-id>“}：监控目标暴露的样本数。</instance-id></job-name></li>
<li>scrape_series_added{job=”<job-name>“, instance=”<instance-id>“}：Prometheus v2.10的新功能，表示此抓取中新增序列的大致数量。</instance-id></job-name></li>
</ul>
<h3 id="Prometheus查询语言"><a href="#Prometheus查询语言" class="headerlink" title="Prometheus查询语言"></a>Prometheus查询语言</h3><p>Prometheus基于指标名称以及附属的标签集唯一定义一条时间序列，基于PromQL表达式，用户可以针对指定的特征及其细分的维度进行过滤、聚合、统计等运算，从而产生期望的计算结果。<br>PromQL是Prometheus Server内置数据查询语言，使用表达式来表述查询需求，根据使用的指标、标签以及时间范围，表达式的查询请求可灵活地覆盖在一个或多个时间序列的一定范围内的样本之上，甚至是只包含单个时间序列的单个样本。PromQL的一个表达式或子表达式可针对以下4种类型的数据之一进行计算并返回结果。</p>
<ul>
<li>即时向量：通常会涉及一组时间序列，但它仅包含这组每个时间序列上具有相同时间戳的单个样本，类似于下图中的“即时向量选择器”（instant vector selector）选出的样本。</li>
<li>范围向量：一组时间序列，每个时间序列包含随时间变化的一系列样本，它类似于下图中的“范围向量选择器”（range vector selector）选出的样本。</li>
<li>标量：一个简单的数字浮点值。</li>
<li>字符串：一个简单的字符串值，目前尚未使用。</li>
</ul>
<p>表达式的返回值类型也是上述4种数据类型之一，不过有些使用场景要求表达式返回值必须满足特定的条件，例如需要将返回值绘制成图形时，仅支持即时向量类型的数据；而对于诸如rate一类的速率函数来说，其要求使用的却又必须是范围向量型的数据。</p>
<p>PromQL的查询操作需要针对有限时间序列上的样本数据进行，挑选出目标时间序列是构建表达式时最为关键的一步。用户可使用向量选择器表达式来挑选出给定指标名称下的所有时间序列或部分时间序列的即时（当前）样本值或至过去某个时间范围内的样本值，前者称为即时向量选择器，后者称为范围向量选择器。如图15-10所示。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213805440.png" alt="image-20220224213805440">1）即时向量选择器：格式为metric_name{labelMatcher}，metric_name代表指标名称，而labelMatcher表示标签匹配表达式（支持“=”、“!=”、“=<del>”和“!</del>”4种操作），二者可同时定义，也可省略其中之一，例如http_requests_total{ job=”kubernetes-pods”}等。<br>2）范围向量选择器：相较于即时向量选择器挑选出的每个时间序列一次仅能返回一个样本来说，范围向量选择器挑选出的每个时间序列一次可以返回多个样本，而且它的表示格式也仅仅是在即时向量选择器之后附加一个中括号，并在其中指定时长范围（<instant_query>[range]）。例如，http_requests_total{ job=”kubernetes-pods”}[2m]等，其中的m表示分钟，其他的时间单位还有s（秒）、h（时）、d（日）、w（周）和y（月）等。<br>由于范围向量选择器返回的是范围向量型数据，它不能用于表达式浏览器中图形绘制功能，否则表达式浏览器会返回“Error executing query: invalid expression type “range vector” for range query, must be Scalar or instant Vector”一类的错误。事实上，范围向量选择几乎总是结合速率类的函数rate一同使用。<br>但是，单个指标的价值不大，监控场景中往往需要联合并可视化一组指标，这种联合通常是指聚合操作，例如，将计数、求和、平均值、分位数、标准差及方差等统计函数应用于时间序列的样本之上，生成具有统计学意义的结果。同时，将查询结果事先按照某种分类机制进行分组（groupby)，并将查询结果按组进行聚合计算也是较为常见的需求，例如分组统计、分组求平均值、分组求和等。<br>一般聚合操作由聚合函数针对一组值进行计算并返回单个值或少量几个值作为结果。Prometheus内置提供的11个聚合函数也称为聚合运算符，这些运算符仅应用于单个即时向量的元素，运算返回值也是具有少量元素的新向量或标量。这些聚合运行符既可以基于向量表达式返回结果中的时间序列的所有标签维度进行分组聚合，也可以仅基于指定的标签维度进行分组聚合。PromQL中的聚合操作语法如下：</instant_query></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;aggr-op&gt;([parameter,] &lt;vector expression&gt;) [without|by (&lt;label list&gt;)]</span><br></pre></td></tr></table></figure>

<p>PromQL的聚合计算中用于分组的关键字是without和by，前者从结果向量中删除由without子句指定的标签，未指定的标签用于分组操作。by子句的功能与without刚好相反，结果向量仅使用by子句中指定的标签进行聚合，在结果向量中出现但未被by子句指定的标签则会被忽略。显然，为了保留上下文信息，使用by子句时需要显式指定其结果中原本出现的job、instance等一类的标签。<br>聚合函数用于对分组后的结果进行某种计算操作，事实上，各函数工作机制的不同之处也仅在于计算操作本身。PromQL内置的聚合函数分别针对分组内的、由各时间序列结果生成的即时向量进行如下计算。</p>
<ul>
<li>sum：对样本值求和。</li>
<li>avg：对样本值求平均值，这是进行指标数据分析的标准方法。</li>
<li>count：对分组内的时间序列进行数量统计。</li>
<li>stddev：对样本值求标准差，以帮助用户了解数据的波动大小（或称之为离开程度）。</li>
<li>stdvar：对样本值求方差，它是求取标准差过程中的中间状态。</li>
<li>min：求取样本值中的最小者。</li>
<li>max：求取样本值中的最大者。</li>
<li>topk：逆序返回分组内的样本值最大的前k个时间序列及其值。</li>
<li>bottomk：顺序返回分组内的样本值最小的前k个时间序列及其值。</li>
<li>quantile：分位数用于评估数据的分布状态，该函数会返回分组内指定的分位数的值，即数值落在小于等于指定的分位区间的比例。</li>
<li>count_values：对分组内的时间序列的样本值进行数量统计。</li>
</ul>
<p>PromQL是实现数据查询、数据可视化及基于Alertmanager实现告警功能的基础，也是掌握并灵活使用Prometheus监控系统的基本前提。</p>
<h3 id="监控Kubernetes"><a href="#监控Kubernetes" class="headerlink" title="监控Kubernetes"></a>监控Kubernetes</h3><p>与传统IT基础设施中的监控系统相比，面向Kubernetes平台上容器化应用的监控策略需要提供针对编排工具及容器运行时的增强配置，以便提供这类新增基础架构层的可见性，如图15-11所示。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213826221.png" alt="image-20220224213826221"></p>
<p>不难发现，这个新增的基础设施层中待监控的核心目标主要由Kubernetes系统组件、附加组件、计算和存储资源、Kubernetes资源对象及容器运行时所组成。</p>
<ul>
<li>Kubernetes系统组件：主要包括控制平面组件API Server、Controller Manager、Scheduler和etcd，以及各节点上的kubelet、kube-proxy等；这些组件均暴露了/healthz端点以支持健康状态检测，也提供了/metrics端点以暴露内部的关键指标。</li>
<li>附加组件：用于扩展Kubernetes功能的CoreDNS、Dashboard、Ingress Controller、Cluster Log，甚至是监控组件自身等。</li>
<li>计算和存储资源：统计Node及Pod级别的计算资源使用状况，通常由Metrics Server统一收集并通过Metrics API提供。</li>
<li>Kubernetes资源对象：Kubernetes多种API的抽象，来确保应用容器（如Deployment、Pod、PVC/PV和Node等）PVC/PV和Node等）的可用性，kube-state-metrics服务器能够从API Server中获取这些API对象的状态和健康信息，并将它们暴露为指标格式。</li>
</ul>
<p>任何被监控目标都需要事先纳入监控系统中才能进行时序数据采集、存储、告警及相关的展示等，上述这些新的监控目标既可以通过配置信息以静态形式指定，也可以让Prometheus通过服务发现机制进行动态管理（增、删等），对于变动频繁的系统环境（例如容器云环境）来说，这种动态管理机制尤为有用。Kubernetes集群中，除了从此前配置的Pod对象的容器应用获得资源指标数据以外，Prometheus还支持通过多个监控目标采集Kubernetes监控架构体系中所谓的“非核心指标数据”。Kubernetes中的Prometheus数据源如图15-12所示。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224213949834.png" alt="image-20220224213949834"></p>
<blockquote>
<p>1）监控代理程序：例如node_exporter，负责收集标准的主机指标数据，包括平均负载、CPU、内存、磁盘、网络及诸多其他维度的数据。<br>2）kubelet（cAdvisor）：收集容器指标数据，它们也是Kubernetes“核心指标”，每个容器的相关指标数据主要有CPU利用率（user和system）及限额、文件系统读/写限额、内存利用率及限额、网络报文发送/接收/丢弃速率等。<br>3）Kubernetes API Server：收集API Server的性能指标数据，包括控制工作队列的性能、请求速率与延迟时长、etcd缓存工作队列及缓存性能、普通进程状态（文件描述符、内存、CPU等）、Golang状态（垃圾回收、内存和线程等）。<br>4）kube-state-metrics：该组件根据Kubernetes API Server中的资源派生出多种资源指标，它们主要是资源类型相关的计数器和元数据信息，包括指定类型的对象总数、资源限额、容器状态（ready/restart/running/terminated/waiting)以及Pod资源的标签系列等。</p>
</blockquote>
<p>Prometheus支持基于Kubernetes API Server的服务发现机制，从而动态发现和监控集群中的所有可被监控的对象。Kubernetes资源通常需要添加下列注解信息才能被Prometheus系统自动发现并抓取其内置的指标数据，但我们此前部署的大多数Pod和Service等资源并未添加类似注解信息，必要时，只能依照规则进行修改。</p>
<blockquote>
<p>1）prometheus.io/scrape：用于标识是否需要采集指标数据，布尔型值（true或false）。<br>2）prometheus.io/path：抓取指标数据时使用的URL路径，一般为/metrics。<br>3）prometheus.io/port：抓取指标数据时使用的套接字端口，例如8080。</p>
</blockquote>
<p>需要特别说明的是，若仅希望基于Prometheus为Kubernetes生成自定义指标，可部署Prometheus Server、kube-state-metrics和自定义资源指标API（或外部资源指标API）的一种实现（例如k8s-prometheus-adapter或kube-metrics-adapter等）即可，这种场景甚至都不需要启用数据持久化功能。但若要配置完整功能的监控系统，我们还需部署Alertmanager和PushGateway组件，并应该在每个主机上部署node_exporter，以及为Prometheus的时序数据提供展示接口的第三方组件Grafana等。<br>Helm Hub中主要提供了3种风格的Prometheus监控系统的部署方式。<br>一是直接部署Prometheus监控系统，借助stable/prometheus和stale/grafana协同完成该任务。<br>二是部署Prometheus Operator，而后由该Operator编排运行多个独立的Prometheus监控系统，stable/prometheus-operator和bitnami/preometheus-operator都能分别实现该功能。<br>三是借助Thanos部署高可用的Prometheus Server和Alertmanager组件，再联合其他必要组件构建一个完整的监控系统。<br>另外，我们也可以自行编写所需的资源清单，完成Prometheus系统的整体部署，不过不建议这种部署方式。为了便于大家了解组件间的拼接关系，我们下面选择使用上述的第一种方式来说明以Prometheus为中心的完整监控系统的部署方法，并通过该系统为Kubernetes提供自定义的资源指标。</p>
<h4 id="部署Prometheus"><a href="#部署Prometheus" class="headerlink" title="部署Prometheus"></a>部署Prometheus</h4><p>我们先借助Helm stable仓库中的Prometheus Chart部署Prometheus Server、Alertmanager、Pushgateway和kube-state-metrics这4个组件，并通过各自指定的URL将前3个组件经由Ingress暴露到集群外部。下面的值文件（prometheus-values-with-longhorn-volumes.yaml)给出了需要用到的关键配置的定义。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">serviceAccounts:</span>    <span class="comment"># SA相关的值</span></span><br><span class="line">  <span class="attr">server:</span>           <span class="comment"># Prometheus Server专用的SA</span></span><br><span class="line">    <span class="attr">create:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">prometheus</span></span><br><span class="line"><span class="attr">alertmanager:</span>       <span class="comment"># Altermanager相关的值</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span>     <span class="comment"># 是否启用该组件</span></span><br><span class="line">  <span class="attr">useClusterRole:</span> <span class="literal">true</span>  </span><br><span class="line">  <span class="attr">ingress:</span>          <span class="comment"># Ingress相关的配置</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span>   <span class="comment"># 是否启用Ingress</span></span><br><span class="line">    <span class="attr">hosts:</span>          <span class="comment"># 关联到Altermanager的虚拟主机名称列表</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">prom.ilinux.io/alertmgr</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">alertmgr.ilinux.io</span></span><br><span class="line">  <span class="attr">persistentVolume:</span> <span class="comment"># 持久卷相关的值</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span>   <span class="comment"># 是否启用；禁用时将启用emptyDir</span></span><br><span class="line">    <span class="attr">accessModes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">    <span class="attr">existingClaim:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">    <span class="attr">size:</span> <span class="string">2Gi</span></span><br><span class="line">    <span class="attr">storageClass:</span> <span class="string">&quot;longhorn&quot;</span></span><br><span class="line">  <span class="attr">replicaCount:</span> <span class="number">1</span>   <span class="comment"># 副本数，大于1时需要启用下面的StatefulSet控制器</span></span><br><span class="line">  <span class="attr">statefulSet:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">podManagementPolicy:</span> <span class="string">OrderedReady</span>    <span class="comment"># Pod管理策略</span></span><br><span class="line"><span class="attr">kubeStateMetrics:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span>       <span class="comment"># 是否启用kubeStateMetrics，启用时需要配置该子Chart的专用值</span></span><br><span class="line"><span class="attr">kube-state-metrics:</span>   <span class="comment"># 依赖的kube-state-metrics Chart的自定义值</span></span><br><span class="line">  <span class="attr">prometheusScrape:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">autosharding:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">false</span>    <span class="comment"># 是否自动分片</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">1</span></span><br><span class="line">  <span class="attr">service:</span></span><br><span class="line">    <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">nodeExporter:</span>     <span class="comment"># Node Exporter组件相关的值</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">tolerations:</span>    <span class="comment"># 添加特定的容忍度，以便node_exporter可以部署到Master节点</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;node-role.kubernetes.io/master&quot;</span> <span class="comment"># Master节点专用的污点之一</span></span><br><span class="line">      <span class="attr">operator:</span> <span class="string">&quot;Exists&quot;</span></span><br><span class="line">      <span class="attr">value:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">      <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br><span class="line">  <span class="attr">service:</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span>         <span class="comment"># 专用注解，指定可以抓取该服务的指标</span></span><br><span class="line">    <span class="attr">clusterIP:</span> <span class="string">None</span></span><br><span class="line">    <span class="attr">hostPort:</span> <span class="number">9100</span></span><br><span class="line">    <span class="attr">servicePort:</span> <span class="number">9100</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="attr">server:</span>   <span class="comment"># Prometheus Server的专用配置</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">useExistingClusterRoleName:</span> <span class="string">cluster-admin</span> <span class="comment"># 使用现有的ClusterRole</span></span><br><span class="line">  <span class="attr">baseURL:</span> <span class="string">&quot;http://prom.ilinux.io&quot;</span>          <span class="comment"># 能够访问到该Server的外部URL</span></span><br><span class="line">  <span class="attr">global:</span>                                   <span class="comment"># Server的全局配置参数</span></span><br><span class="line">    <span class="attr">scrape_interval:</span> <span class="string">1m</span>                     <span class="comment"># 数据抓取时间间隔</span></span><br><span class="line">    <span class="attr">scrape_timeout:</span> <span class="string">10s</span>                     <span class="comment"># 抓取操作的超时时长</span></span><br><span class="line">    <span class="attr">evaluation_interval:</span> <span class="string">1m</span>                 <span class="comment"># 指标值的评估时间间隔</span></span><br><span class="line">  <span class="attr">ingress:</span>   <span class="comment"># Ingress的配置</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">hosts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">prom.ilinux.io</span></span><br><span class="line">  <span class="attr">persistentVolume:</span>   <span class="comment"># 持久卷的相关值</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span>     <span class="comment"># 禁用时将使用emptyDir</span></span><br><span class="line">    <span class="attr">accessModes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">    <span class="attr">size:</span> <span class="string">8Gi</span></span><br><span class="line">    <span class="attr">storageClass:</span> <span class="string">&quot;longhorn&quot;</span>   <span class="comment"># 指定的存储类，需要事先存在，否则Pod将被挂起</span></span><br><span class="line">  <span class="attr">emptyDir:</span></span><br><span class="line">    <span class="attr">sizeLimit:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">replicaCount:</span> <span class="number">1</span>              <span class="comment"># 副本数，多于1时需要由StatefulSet控制器编排</span></span><br><span class="line">  <span class="attr">statefulSet:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">podManagementPolicy:</span> <span class="string">OrderedReady</span></span><br><span class="line">    <span class="attr">headless:</span>   <span class="comment"># StatefulSet专用的headless Service</span></span><br><span class="line">      <span class="attr">servicePort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">gRPC:</span></span><br><span class="line">        <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">        <span class="attr">servicePort:</span> <span class="number">10901</span></span><br><span class="line">  <span class="attr">service:</span>   <span class="comment"># Prometheus Server对客户端提供服务的Service</span></span><br><span class="line">    <span class="attr">clusterIP:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">servicePort:</span> <span class="number">80</span></span><br><span class="line">    <span class="attr">sessionAffinity:</span> <span class="string">None</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">    <span class="attr">gRPC:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">servicePort:</span> <span class="number">10901</span></span><br><span class="line">    <span class="attr">statefulsetReplica:</span></span><br><span class="line">      <span class="attr">enabled:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">replica:</span> <span class="number">0</span></span><br><span class="line">  <span class="attr">retention:</span> <span class="string">&quot;15d&quot;</span></span><br><span class="line"><span class="attr">pushgateway:</span>      <span class="comment"># Pushgateway相关的值</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span>   <span class="comment"># 是否启用该组件</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">pushgateway</span></span><br><span class="line">  <span class="attr">ingress:</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">annotations:</span></span><br><span class="line">      <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">hosts:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">pushgw.ilinux.io</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">prom.ilinux.io/pushgateway</span></span><br><span class="line">  <span class="attr">replicaCount:</span> <span class="number">1</span>      <span class="comment"># 副本数量，无状态，由Deployment控制器编排即可</span></span><br><span class="line">  <span class="attr">persistentVolume:</span>    <span class="comment"># 持久卷配置</span></span><br><span class="line">    <span class="attr">enabled:</span> <span class="literal">true</span>      <span class="comment"># 是否启用持久卷</span></span><br><span class="line">    <span class="attr">accessModes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">    <span class="attr">existingClaim:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/data</span></span><br><span class="line">    <span class="attr">size:</span> <span class="string">2Gi</span></span><br><span class="line">    <span class="attr">storageClass:</span> <span class="string">&quot;longhorn&quot;</span></span><br></pre></td></tr></table></figure>

<p>随后，我们便可基于上述自定义的值文件完成相关组件的部署测试与部署操作，为了便于管理，我们把这些组件部署在专用的名称空间monitoring之中，如下面的命令所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create namespace monitoring</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">helm install prom -f prom-values-with-longhorn-volumes.yaml stable/prometheus \</span></span><br><span class="line"><span class="language-bash">-n monitoring --dry-run</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">helm install prom -f prom-values-with-longhorn-volumes.yaml stable/prometheus -n monitoring</span></span><br></pre></td></tr></table></figure>

<p>Prometheus Server需要在集群级别读取/metrics这一非资源型URL的权限，因此，我们需要将Prometheus Server Pod的ServiceAccount（值文件中显式定义的prometheus）绑定到ClusterRole资源之上。简单起见，这里选择直接将serviceaccount/prometheus绑定到clusterrole/cluster-admin之上。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl create clusterrolebinding prometheus-cluster-admin \</span></span><br><span class="line"><span class="language-bash">      --clusterrole=cluster-admin --serviceaccount=monitoring:prometheus</span></span><br></pre></td></tr></table></figure>

<p>待monitoring名称空间中的所有Pod就绪之后，随后即可通过kube-state-metrics的服务接口来查看Prometheus Server是否能够正常派生出Kubernetes系统相关的指标，来验证系统工作正常与否。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">首先，使用如下命令获取kube-state-metrics的服务名称及端口号</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get svc -l app.kubernetes.io/name=kube-state-metrics -n monitoring</span> </span><br><span class="line">NAME               TYPE    CLUSTER-IP  EXTERNAL-IP  PORT(S)    AGE</span><br><span class="line">prom-kube-state-metrics  ClusterIP   10.103.39.6   &lt;none&gt;        8080/TCP   2m</span><br><span class="line"><span class="meta"># </span><span class="language-bash">其次，启动一个临时客户端Pod，以便进行测试</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client --image=<span class="string">&quot;ikubernetes/admin-toolbox:v1.0&quot;</span> -it --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">If you don&#x27;t see a command prompt, try pressing enter.</span><br><span class="line"><span class="meta"># </span><span class="language-bash">最后，在Pod的交互式接口中向monitoring名称空间中的prom-kube-state-metrics服务的</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"> 8080端口的/metrics路径发起访问请求，若获得相应的指标响应，则表示该服务运行正常</span></span><br><span class="line">[root@client /]# curl -s http://prom-kube-state-metrics.monitoring:8080/metrics</span><br><span class="line"><span class="meta"># </span><span class="language-bash">HELP kube_certificatesigningrequest_labels Kubernetes labels converted to Prometheus labels.</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">TYPE kube_certificatesigningrequest_labels gauge</span></span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<h4 id="Web-UI及表达式浏览器"><a href="#Web-UI及表达式浏览器" class="headerlink" title="Web UI及表达式浏览器"></a>Web UI及表达式浏览器</h4><p>Prometheus Server内置了一个称为Web UI的HTTP Server，它默认监听于0.0.0.0:9090套接字之上。根据前面的部署设定，我们于集群之外，在Web浏览器上通过<a target="_blank" rel="noopener" href="http://prom.ilinux.io/%E5%8D%B3%E5%8F%AF%E5%90%91%E8%AF%A5UI%E5%8F%91%E8%B5%B7%E8%AE%BF%E9%97%AE%E8%AF%B7%E6%B1%82%EF%BC%8C%E9%BB%98%E8%AE%A4%E8%AF%B7%E6%B1%82%E7%9A%84%E9%A1%B5%E9%9D%A2%E4%BC%9A%E8%B7%B3%E8%BD%AC%E8%87%B3%E7%BB%8F%E7%94%B1/graph%E8%B7%AF%E5%BE%84%E5%B1%95%E7%A4%BA%E7%9A%84%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B5%8F%E8%A7%88%E5%99%A8%EF%BC%8C%E5%A6%82%E5%9B%BE15-13%E6%89%80%E7%A4%BA%E3%80%82%E9%99%A4%E4%BA%86/graph%EF%BC%88%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B5%8F%E8%A7%88%E5%99%A8%EF%BC%89%EF%BC%8C%E8%AF%A5UI%E8%BF%98%E6%8F%90%E4%BE%9B%E4%BA%86/alters%EF%BC%88%E5%91%8A%E8%AD%A6%EF%BC%89%E3%80%81/status%EF%BC%88%E7%8A%B6%E6%80%81%EF%BC%89%E3%80%81/config%EF%BC%88%E9%85%8D%E7%BD%AE%EF%BC%89%E5%92%8C/targets%EF%BC%88%E7%9B%91%E6%8E%A7%E7%9B%AE%E6%A0%87%EF%BC%89%E7%AD%89%E5%87%A0%E4%B8%AA%E8%B7%AF%E5%BE%84%EF%BC%8C%E8%BF%99%E4%BA%9B%E8%B7%AF%E5%BE%84%E9%83%BD%E8%83%BD%E5%A4%9F%E9%80%9A%E8%BF%87%E4%B8%BB%E9%A1%B5%E9%9D%A2%E7%9A%84%E5%AF%BC%E8%88%AA%E5%88%B0%E8%BE%BE%EF%BC%8C%E4%B9%9F%E5%8F%AF%E9%80%9A%E8%BF%87%E5%90%84%E5%8A%9F%E8%83%BD%E5%AF%B9%E5%BA%94%E7%9A%84PATH%E7%9B%B4%E6%8E%A5%E8%BF%9B%E8%A1%8C%E8%AE%BF%E9%97%AE%E3%80%82">http://prom.ilinux.io/即可向该UI发起访问请求，默认请求的页面会跳转至经由/graph路径展示的表达式浏览器，如图15-13所示。除了/graph（表达式浏览器），该UI还提供了/alters（告警）、/status（状态）、/config（配置）和/targets（监控目标）等几个路径，这些路径都能够通过主页面的导航到达，也可通过各功能对应的PATH直接进行访问。</a></p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214139110.png" alt="image-20220224214139110"></p>
<p>表达式浏览器页面顶部的文本框用于接收用户键入的PromQL查询表达式，而后点击Execute按钮运行便可分别在Graph和Console面板显示图形与表格形式的查询结果。图15-13给出了速率函数rate基于范围向量选择器authenticated_user_requests[5m])计算生成的新数据序列。事实上，表达式浏览器提供的图形展示界面过于简陋且无法保存查询，通常仅用于帮助用户构建查询表达式，真正的数据可视化工作由其他专用的工具完成，这其中又以Grafana最为知名。<br>另外，Prometheus内置的HTTP Server还通过/metrics暴露Prometheus Server程序自身内置指标，PrometheusServer默认的配置文件也会抓取自身的这些指标，实现自我监控。再者，它还提供了如下几个用于管理功能的API。</p>
<ul>
<li>Prometheus Server的健康状态检测：GET /-/healthy，200响应码表示健康。</li>
<li>Prometheus Server的就绪状态检测：GET /-/ready，200响应码表示就绪。</li>
<li>重载配置文件和规则文件：PUT /-/reload或POST /-/reload，默认处于禁用状态，需要使用–web.enable-lifecycle选项启用。</li>
<li>关闭Prometheus Server进程：PUT /-/quit或POST /-/quit，默认处于禁用状态，需要使用–web.enable-lifecycle选项启用。</li>
</ul>
<p>另外，/targets路径也是常用的页面之一，它负责展示当前Prometheus Server已经纳入的监控目标，这些目标可能来自静态配置文件，也可能来自动态的服务发现。而服务发现自身则经由/service-discovery这一路径提供。<br>尽管使用表达式浏览器能够便捷地了解单个指标的相关数据走势，以及调试和编写所需要的PromQL表达式，但它无法有效地保存查询语句，并以丰富、直观的图形满足用户进行数据展示的需要，因而通常还要借助Grafana项目实现该功能。3. 部署Grafana<br>Grafana是一款开源、通用的指标时序数据可视化工具，常用于展示基础设施的时序数据及分析应用程序运行状态，它通过类似Prometheus这类后端系统加载时序数据，基于查询条件设置聚合规则，而后通过Dashboard组件进行展示，如图15-14所示。目前，除了Prometheus，Grafana还能够支持Graphite、Elasticsearch、InfluxDB、OpenTSDB和AWS Cloudwatch等多种类型的数据源。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214200311.png" alt="image-20220224214200311">Panel（面板）是Dashboard的原子可视化单元，目前支持graph、singlestat、Heatmap和table这4种类型，图15-14中第一栏主要是singlestat类型的面板，而第二栏则属于graph类型。每个Panel通过其专用的Query Editor（查询编辑器）定义针对指定数据源的专属查询表达式，定期（刷新时间间隔）从数据源加载数据进行展示。这意味着，一个Dashboard中的多个Panel所展示的内容可能会来自不同的数据源。<br>Dashboard的定义遵循JSON规范，便于存储、传输及分享，Grafana项目甚至为社区提供了一个专用于Dashboard分享的仓库<a target="_blank" rel="noopener" href="https://grafana.com/dashboards%E3%80%82%E7%9B%AE%E5%89%8D%EF%BC%8C%E8%AF%A5%E4%BB%93%E5%BA%93%E4%B8%AD%E6%94%B6%E5%BD%95%E4%BA%86%E7%94%B1Grafana%E9%A1%B9%E7%9B%AE%E6%88%96%E7%A4%BE%E5%8C%BA%E5%85%B1%E5%90%8C%E7%BB%B4%E6%8A%A4%E7%9A%84%E6%95%B0%E9%87%8F%E4%BC%97%E5%A4%9A%E7%9A%84Dashboard%EF%BC%8C%E5%AE%83%E4%BB%AC%E4%B8%BA%E7%BB%9D%E5%A4%A7%E5%A4%9A%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF%E6%8F%90%E4%BE%9B%E4%BA%86%E5%8F%AF%E7%94%A8%E5%AE%9E%E7%8E%B0%EF%BC%8C%E5%9B%A0%E6%AD%A4%E9%83%A8%E7%BD%B2%E5%AE%8CGrafana%E5%90%8E%E7%9A%84%E7%AC%AC%E4%B8%80%E4%BB%BB%E5%8A%A1%E9%80%9A%E5%B8%B8%E6%98%AF%E5%88%B0%E4%BB%93%E5%BA%93%E4%B8%AD%E6%A3%80%E7%B4%A2%E5%B9%B6%E5%AE%89%E8%A3%85%E9%80%82%E7%94%A8%E7%9A%84Dashboard%E3%80%82">https://grafana.com/dashboards。目前，该仓库中收录了由Grafana项目或社区共同维护的数量众多的Dashboard，它们为绝大多数的使用场景提供了可用实现，因此部署完Grafana后的第一任务通常是到仓库中检索并安装适用的Dashboard。</a><br>以Kubernetes为部署环境时，Helm Hub的stable仓库中提供了独立可用的grafana Chart，它支持通过值文件设定Grafana的基础配置，也允许用户直接设定要加载的数据源和Dashboard等。下面的自定义值文件示例（grafana-values.yaml)列出了部分经常会用到的自定义值。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Service相关值，多数为默认值，额外启用指定注解以支持相应指标数据的抓取</span></span><br><span class="line"><span class="attr">service:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">targetPort:</span> <span class="number">3000</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">portName:</span> <span class="string">service</span></span><br><span class="line"><span class="comment">## 为Pod添加支持指标数据抓取的相关注解</span></span><br><span class="line"><span class="attr">podAnnotations:</span></span><br><span class="line">  <span class="attr">prometheus.io/scrape:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">prometheus.io/port:</span> <span class="string">&quot;3000&quot;</span>   <span class="comment"># 指定Pod中的应用能够输出指标的端口</span></span><br><span class="line"><span class="comment">## 为Grafana启用Ingress，以便将其发布到Kubernetes集群之外</span></span><br><span class="line"><span class="attr">ingress:</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">kubernetes.io/ingress.class:</span> <span class="string">nginx</span>   <span class="comment"># Ingress控制器类型，可按需修改</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">/</span></span><br><span class="line">  <span class="attr">hosts:</span>   <span class="comment"># 虚拟主机名称列表</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">grafana.ilinux.io</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">promui.ilinux.io</span></span><br><span class="line">  <span class="attr">tls:</span> []</span><br><span class="line">  <span class="comment">#  - secretName: chart-example-tls</span></span><br><span class="line">  <span class="comment">#    hosts:</span></span><br><span class="line">  <span class="comment">#      - chart-example.local</span></span><br><span class="line"><span class="comment">## </span></span><br><span class="line"><span class="attr">persistence:</span>   <span class="comment"># 持久卷相关的值</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">pvc</span></span><br><span class="line">  <span class="attr">enabled:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">storageClassName:</span> <span class="string">&quot;longhorn&quot;</span>   <span class="comment"># 指定的存储类需要事先存在，否则请禁用持久卷</span></span><br><span class="line">  <span class="attr">accessModes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">ReadWriteOnce</span></span><br><span class="line">  <span class="attr">size:</span> <span class="string">10Gi</span></span><br><span class="line"><span class="comment">## Grafana UI的管理员用户名和密码，安全起见，应该使用自定义的Secret加载密码</span></span><br><span class="line"><span class="attr">adminUser:</span> <span class="string">admin</span></span><br><span class="line"><span class="attr">adminPassword:</span> <span class="string">MageEdu.com</span></span><br><span class="line"><span class="attr">admin:</span>  <span class="comment"># 从指定的Secret中加载用户名和密码</span></span><br><span class="line">  <span class="attr">existingSecret:</span> <span class="string">&quot;&quot;</span></span><br><span class="line">  <span class="attr">userKey:</span> <span class="string">admin-user</span></span><br><span class="line">  <span class="attr">passwordKey:</span> <span class="string">admin-password</span></span><br><span class="line"><span class="comment">## 为Grafana启用的插件列表</span></span><br><span class="line"><span class="attr">plugins:</span> []</span><br><span class="line">  <span class="comment"># - digrich-bubblechart-panel</span></span><br><span class="line">  <span class="comment"># - grafana-clock-panel</span></span><br><span class="line"><span class="comment">## 部署后默认添加的数据源，也可以在部署完成之后手动添加</span></span><br><span class="line"><span class="attr">datasources:</span></span><br><span class="line">  <span class="attr">datasources.yaml:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">datasources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">Prometheus</span>        <span class="comment"># 数据源名称</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">prometheus</span>        <span class="comment"># 数据源类型</span></span><br><span class="line">      <span class="comment">## 数据源的访问端点，应该与部署的Prometheus服务名称保持一致</span></span><br><span class="line">      <span class="attr">url:</span> <span class="string">http://prom-prometheus-server.monitoring.svc.cluster.local</span></span><br><span class="line">      <span class="attr">access:</span> <span class="string">proxy</span></span><br><span class="line">      <span class="attr">isDefault:</span> <span class="literal">true</span>         <span class="comment"># 是否为默认的数据源</span></span><br><span class="line"><span class="comment">## 部署后默认添加的Dashboard Provider，也可以在UI界面中交互式添加</span></span><br><span class="line"><span class="attr">dashboardProviders:</span></span><br><span class="line">  <span class="attr">dashboardproviders.yaml:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="number">1</span></span><br><span class="line">    <span class="attr">providers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">&#x27;default&#x27;</span>         <span class="comment"># Dashboard Provider的名称</span></span><br><span class="line">      <span class="attr">orgId:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">folder:</span> <span class="string">&#x27;Kube-Summary&#x27;</span>  <span class="comment"># 目录名称</span></span><br><span class="line">      <span class="attr">type:</span> <span class="string">file</span></span><br><span class="line">      <span class="attr">disableDeletion:</span> <span class="literal">false</span></span><br><span class="line">      <span class="attr">editable:</span> <span class="literal">true</span></span><br><span class="line">      <span class="attr">options:</span></span><br><span class="line">        <span class="attr">path:</span> <span class="string">/var/lib/grafana/dashboards/default</span></span><br><span class="line"><span class="comment">## 在default provider中在线添加的几个Dashboard的定义</span></span><br><span class="line"><span class="attr">dashboards:</span></span><br><span class="line">  <span class="attr">default:</span></span><br><span class="line">    <span class="attr">Kubernetes-cluster-monitoring:</span>   <span class="comment"># Dashboard标识</span></span><br><span class="line">      <span class="attr">gnetId:</span> <span class="number">315</span>                    <span class="comment"># Dashboard HUB中的Dashboard ID</span></span><br><span class="line">      <span class="attr">revision:</span> <span class="number">3</span>                    <span class="comment"># Dashboard的版本号</span></span><br><span class="line">      <span class="attr">datasource:</span> <span class="string">Prometheus</span>         <span class="comment"># 该Dashboard使用的数据源名称</span></span><br><span class="line">    <span class="attr">Kubernetes-Nodes:</span></span><br><span class="line">      <span class="attr">gnetId:</span> <span class="number">5219</span></span><br><span class="line">      <span class="attr">revision:</span> <span class="number">8</span></span><br><span class="line">      <span class="attr">datasource:</span> <span class="string">Prometheus</span></span><br><span class="line">    <span class="attr">Kubernetes-Cluster:</span></span><br><span class="line">      <span class="attr">gnetId:</span> <span class="number">7249</span></span><br><span class="line">      <span class="attr">revision:</span> <span class="number">1</span></span><br><span class="line">      <span class="attr">datasource:</span> <span class="string">Prometheus</span></span><br></pre></td></tr></table></figure>

<p>上面的值文件启用了Ingress和持久存储卷的定义，它们完全适配本书示例中一直使用的Kubernetes环境。为了便于组织和管理，我们选择把它与Prometheus部署在同一名称空间。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">helm install ui -f grafana-values.yaml stable/grafana -n monitoring --dry-run</span></span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">helm install ui -f grafana-values.yaml stable/grafana -n monitoring</span></span><br></pre></td></tr></table></figure>

<p>待Grafana的Pod就绪后即可通过部署命令最后提示的方式进行访问，例如可以通过Ingress中指定的域名grafana.ilinux.io进行访问，以值文件中指定的用户名和密码登录后，等值文件中定义安装的Dashboard加载成功后即可使用，其中的一个Dashboard的展示效果如图15-14所示。</p>
<h3 id="自定义指标适配器"><a href="#自定义指标适配器" class="headerlink" title="自定义指标适配器"></a>自定义指标适配器</h3><p>Prometheus并非Kubernetes系统的聚合API服务器，其PromQL接口无法直接作为自定义指标数据源，我们还需要一个专门的中间层将PromQL的指标转换为符合Kubernetes系统聚合API格式的指标。这些自定义指标再经由Kubernetes系统上的custom.metrics.k8s.io或external.metrics.k8s.io API提供给相应的客户端使用，例如HPAv2等。目前最流行的中间层解决方案是托管在GitHub上的k8s-prometheus-adapter项目，另外可选的还有kube-metrics-adapter等，我们将以前者为例进行说明。<br>Helm Hub的stable仓库中名为kubernetes-adapter的项目便是用于部署k8s-prometheus-adapter的Chart，部署时需要自定义的通常只是与后端的Prometheus服务相关的参数，下面的配置内容（prometheus-adapter-values.yaml）能够让部署的k8s-prometheus-adapter实例适配到15.3.4节部署的Prometheus环境。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后端Prometheus服务的URL及端口，要与实际环境保持一致</span></span><br><span class="line"><span class="attr">prometheus:</span>  </span><br><span class="line">  <span class="attr">url:</span> <span class="string">http://prom-prometheus-server.monitoring.svc.cluster.local</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">80</span></span><br><span class="line">  <span class="attr">path:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">replicas:</span> <span class="number">1</span>       <span class="comment"># 副本数量，无状态应用，可按需定义副本数</span></span><br><span class="line"><span class="attr">logLevel:</span> <span class="number">4</span>       <span class="comment"># 日志级别</span></span><br><span class="line"><span class="comment"># 列表显示各指标数据更新时间间隔</span></span><br><span class="line"><span class="attr">metricsRelistInterval:</span> <span class="string">1m</span></span><br><span class="line"><span class="attr">listenPort:</span> <span class="number">6443</span>  <span class="comment"># 监听的端口号</span></span><br><span class="line"><span class="comment"># k8s-prometheus-adapter服务的相关定义</span></span><br><span class="line"><span class="attr">service:</span></span><br><span class="line">  <span class="attr">annotations:</span> &#123;&#125;</span><br><span class="line">  <span class="attr">port:</span> <span class="number">443</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">ClusterIP</span></span><br><span class="line"><span class="comment"># 定义将Prometheus指标暴露为Kubernetes自定义指标的规则</span></span><br><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="attr">default:</span> <span class="literal">true</span> <span class="comment"># 是否加载默认规则</span></span><br><span class="line">  <span class="attr">custom:</span> []    <span class="comment"># 自定义规则列表，需要暴露应用上自定义的指标时，通常需要于此处配置规则实现</span></span><br><span class="line">  <span class="attr">existing:</span>     <span class="comment"># 通过指定的ConfigMap加载预定义规则覆盖其他所有类型的规则</span></span><br><span class="line">  <span class="attr">external:</span> []  <span class="comment"># 外部规则</span></span><br><span class="line"><span class="comment"># k8s-prometheus-adapter证书的相关定义，未启用时将生成自签证书</span></span><br><span class="line"><span class="attr">tls:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span></span><br><span class="line">  <span class="attr">ca:</span> <span class="string">|-</span>        <span class="comment"># CA证书的内容，Base64编码格式</span></span><br><span class="line">  <span class="attr">key:</span> <span class="string">|-</span></span><br><span class="line">  <span class="attr">certificate:</span> <span class="string">|-</span></span><br></pre></td></tr></table></figure>

<p>为了便于管理，我们选择将k8s-prometheus-adapter同Prometheus部署在同一名称空间中，实例名称为adapter，具体的命令如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">helm install adapter -f prometheus-adapter-values.yaml stable/prometheus-adapter -n monitoring</span></span><br></pre></td></tr></table></figure>

<p>部署过程中会在API Server的kube-aggregator上注册新的API群组custom.metrics.k8s.io，待相关Pod对象转为正常运行状态即可通过kubectl api-versions命令确认其API接口注册的结果，正确的结果需要与下面命令的输出相同。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl api-versions | grep custom</span></span><br><span class="line">custom.metrics.k8s.io/v1beta1</span><br></pre></td></tr></table></figure>

<p>待适配器实例能够正常从Prometheus Server查询指标数据后，直接向Kubernetes的API群组custom.metrics.k8s.io发送请求，即可列出其可用的所有自定义指标，例如下面使用kubectl get –raw命令进行测试，结合使用jq过滤命令结果，从而仅列出指标名称，命令及结果如下所示。需要注意的是，不同的Kubernetes系统环境上，输出的指标会存在一定的不同。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get --raw <span class="string">&quot;/apis/custom.metrics.k8s.io/v1beta1&quot;</span> | jq <span class="string">&#x27;.resources[].name&#x27;</span></span></span><br><span class="line">&quot;ingresses.extensions/kube_ingress_tls&quot;</span><br><span class="line">&quot;jobs.batch/kube_pod_labels&quot;</span><br><span class="line">&quot;jobs.batch/kube_pod_restart_policy&quot;</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>我们也可以直接通过API接口查看指定的Pod对象的相应指标及值，例如使用类似如下命令列出kube-system名称空间中的所有Pod对象的文件系统占用率：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get --raw \</span></span><br><span class="line"><span class="language-bash"><span class="string">&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/kube-system/pods/*/cpu_usage&quot;</span> | jq .</span></span><br></pre></td></tr></table></figure>

<p>该命令会列出相应名称空间中所有Pod对象的内存资源占用状况，一个数据项的显示结果足以让我们了解其响应格式，例如下面的内容便截取自命令结果中etcd的Pod对象的相关输出：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;describedObject&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;kind&quot;:</span> <span class="string">&quot;Pod&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;namespace&quot;:</span> <span class="string">&quot;kube-system&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;etcd-k8s-master01.ilinux.io&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;apiVersion&quot;:</span> <span class="string">&quot;/v1&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;metricName&quot;:</span> <span class="string">&quot;cpu_usage&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;timestamp&quot;:</span> <span class="string">&quot;2020-09-10T02:57:43Z&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;value&quot;:</span> <span class="string">&quot;48m&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;selector&quot;:</span> <span class="literal">null</span></span><br><span class="line">&#125;<span class="string">,</span></span><br></pre></td></tr></table></figure>

<p>事实上，k8s-prometheus-adapter适配器自身就是客户端与Prometheus Server之间的代理网关，它将Kubernetes自定义指标API（custom.metrics.k8s.io）中的每一个指标与一个特定的PromQL表达式建立起对应关系，客户端对该自定义指标的查询请求也将由适配器相应转换为PromQL语句，转发给后端的Prometheus Server，Prometheus Server的响应报文再以适配器指标格式响应给客户端。<br>基于上述值文件部署的k8s-prometheus-adapter适配器仅能根据默认规则输出内置的各类基础指标，因而无法将各种应用上特有的自定义指标也通过Kubernetes的自定义指标API进行暴露。例如，有些基于HTTP/HTTPS的应用上很可能会提供http_requests_total以及http_requests_per_second一类的指标，若要将它们经由Kubernetes的自定义指标API提供给客户端使用，就需要在适配器上设置自定义规则来完成。<br>k8s-prometheus-adapter适配器通过规则来定义公开哪些指标以及指标数据的生成的方式，各规则彼此间各自独立执行，因而它们必须存在互斥关系。每个规则可由发现机制、关联方式、指标命名和查询语句4个部分组成。<br>1）发现机制：定义适配器如何从Prometheus中为当前规则查找待暴露的指标，使用seriesQuery来指定传递给Prometheus的查询条件，且能够使用seriesFilters进一步缩小指标范围。下面的条件表示从每个名称空间查询所有Pod上的http_requests_total指标，其中的kubernetes_namespace代表名称空间的名称标识，而kubernetes_pod_name代表Pod自身名称标识，它们是适配器中固定的Go模板变量。</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seriesQuery<span class="punctuation">:</span> &#x27;http_requests_total<span class="punctuation">&#123;</span>kubernetes_namespace!=<span class="string">&quot;&quot;</span><span class="punctuation">,</span>kubernetes_pod_name!=<span class="string">&quot;&quot;</span><span class="punctuation">&#125;</span>&#x27;</span><br></pre></td></tr></table></figure>

<p>2）关联方式：定义上面发现机制中指定的指标可以附加到Kubernetes的哪些资源上，即暴露哪些资源的指定指标。关联方式使用resources字段进行定义，支持两种格式：一种是嵌套使用template字段以Go模板的形式限定目标资源，使用Group代表资源群组，使用Resouce代表资源类型；另一种是嵌套使用overrides字段将特定的资源标签转为Kubernetes资源类型。<br>例如，下面的示例把具体的名称空间的名称统一为固定的资源类型标识namespace（也可以是namespaces），把具体的Pod名称统一为固定的资源类型标识pod（也可以是pods），它们都隶属于core群组，因而无须指定群组名称。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">overrides:</span></span><br><span class="line">    <span class="attr">kubernetes_namespace:</span> &#123;<span class="attr">resource:</span> <span class="string">&quot;namespace&quot;</span>&#125;</span><br><span class="line">    <span class="attr">kubernetes_pod_name:</span> &#123;<span class="attr">resource:</span> <span class="string">&quot;pod&quot;</span>&#125;</span><br></pre></td></tr></table></figure>

<p>3）指标命名：定义如何将Prometheus的指标名称转换为所需的自定义指标名称，它由name字段进行定义，并嵌套使用match字段选定要转换的指标（默认为“.*”），使用as字段指定要使用的名称，支持正则表达式的分组引用机制，例如$0或${0}等。例如，下面的示例表示把所有指标名称中的_total后缀修改为_per_second。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span></span><br><span class="line">  <span class="attr">matches:</span> <span class="string">&quot;^(.*)_total&quot;</span></span><br><span class="line">  <span class="attr">as:</span> <span class="string">&quot;$&#123;1&#125;_per_second&quot;</span></span><br></pre></td></tr></table></figure>

<p>4）查询语句：定义具体发往PromQL的查询语句，在metricsQuery字段以Go模板格式进行定义，并在具体执行时基于目标对象的信息进行模板渲染后转为具体PromQL语句。模板固定以Series引用发现机制中指定的指标名称；以LabelMatchers引用资源标签匹配条件列表，目前该匹配条件的默认值是资源类型及其所属的名称空间，因而集群级别的资源无此条件；以GroupBy引用分组条件列表，目前该分组条件默认为资源类型。例如，下面的语句代表以指定的指标查询满足标签选择条件的、监控对象上的Prometheus指标，而后将其速率值进行分组求和：</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">metricsQuery<span class="punctuation">:</span> &#x27;sum(rate(&lt;&lt;.Series&gt;&gt;<span class="punctuation">&#123;</span>&lt;&lt;.LabelMatchers&gt;&gt;<span class="punctuation">&#125;</span><span class="punctuation">[</span><span class="number">2</span>m<span class="punctuation">]</span>)) by (&lt;&lt;.GroupBy&gt;&gt;)&#x27;</span><br></pre></td></tr></table></figure>

<p>上面提到的标签及标签匹配条件是指Prometheus上下文中的标签。<br>下面定义的规则示例中，处于注释状态的自定义规则用于暴露所有名称空间中各Pod对象上以http_requests_为前缀的指标，各指标保留原有名称。而处于启用状态的自定义规则，通过获取各名称空间中所有Pod上的以http_requests_为前缀的指标，并以新的https_requests_per_second名称进行暴露。由于两条规则间的发现条件存在“包含”关系，因而不能同时启用。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">rules:</span></span><br><span class="line">  <span class="attr">default:</span> <span class="literal">true</span>   <span class="comment"># 是否加载默认规则</span></span><br><span class="line">  <span class="attr">custom:</span></span><br><span class="line"><span class="comment">#  - seriesQuery: &#x27;&#123;__name__=~&quot;^http_requests_.*&quot;,kubernetes_</span></span><br><span class="line"><span class="string">namespace!=&quot;&quot;,kubernetes_pod_name!=&quot;&quot;&#125;&#x27;</span></span><br><span class="line"><span class="comment">#    resources:</span></span><br><span class="line"><span class="comment">#      overrides:</span></span><br><span class="line"><span class="comment">#        kubernetes_namespace: &#123;resource: &quot;namespace&quot;&#125;</span></span><br><span class="line"><span class="comment">#        kubernetes_pod_name: &#123;resource: &quot;pod&quot;&#125;</span></span><br><span class="line"><span class="comment">#    metricsQuery: &#x27;&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;&#x27;</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">seriesQuery:</span> <span class="string">&#x27;http_requests_total&#123;kubernetes_namespace!=&quot;&quot;,kubernetes_</span></span><br><span class="line"><span class="string">  pod_name!=&quot;&quot;&#125;&#x27;</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">      <span class="attr">overrides:</span></span><br><span class="line">        <span class="attr">kubernetes_namespace:</span> &#123;<span class="attr">resource:</span> <span class="string">&quot;namespace&quot;</span>&#125;</span><br><span class="line">        <span class="attr">kubernetes_pod_name:</span> &#123;<span class="attr">resource:</span> <span class="string">&quot;pod&quot;</span>&#125;</span><br><span class="line">    <span class="attr">name:</span></span><br><span class="line">      <span class="attr">matches:</span> <span class="string">&quot;^(.*)_total&quot;</span></span><br><span class="line">      <span class="attr">as:</span> <span class="string">&quot;$&#123;1&#125;_per_second&quot;</span></span><br><span class="line">    <span class="attr">metricsQuery:</span> <span class="string">&#x27;rate(&lt;&lt;.Series&gt;&gt;&#123;&lt;&lt;.LabelMatchers&gt;&gt;&#125;[2m])&#x27;</span></span><br><span class="line">  <span class="attr">existing:</span></span><br><span class="line">  <span class="attr">external:</span> []</span><br></pre></td></tr></table></figure>

<p>将上面的自定义规则替换到前面定义的值文件中，并更新monitoring名称空间中的adapter，则Helm Release即可生效。为了便于各个示例资源清单相分离，这里将完整的值文件保存在一个名为prometheus-adapter-values-with-custom-rules.yaml的单独值文件中，用到的更新命令如下所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">helm upgrade adapter -f prometheus-adapter-values-with-custom-rules.yaml \</span></span><br><span class="line"><span class="language-bash">        stable/prometheus-adapter -n monitoring</span></span><br></pre></td></tr></table></figure>

<p>对于stable/prometheus-adapter来说，更新操作会导致重建相关的Pod对象，待新对象就绪后即可尝试通过自定义指标API请求http_requests_per_second指标，操作的示例命令及其结果如下所示。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">~$</span> <span class="string">kubectl</span> <span class="string">get</span> <span class="string">--raw</span> <span class="string">&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/http_requests_per_second&quot;</span> <span class="string">|</span> <span class="string">jq</span> <span class="string">.</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="attr">&quot;kind&quot;:</span> <span class="string">&quot;MetricValueList&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;apiVersion&quot;:</span> <span class="string">&quot;custom.metrics.k8s.io/v1beta1&quot;</span>,</span><br><span class="line">  <span class="attr">&quot;metadata&quot;:</span> &#123;</span><br><span class="line">    <span class="attr">&quot;selfLink&quot;:</span> <span class="string">&quot;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/http_requests_per_second&quot;</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="attr">&quot;items&quot;:</span> [</span><br><span class="line">    &#123;</span><br><span class="line">      <span class="attr">&quot;describedObject&quot;:</span> &#123;</span><br><span class="line">        <span class="attr">&quot;kind&quot;:</span> <span class="string">&quot;Pod&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;namespace&quot;:</span> <span class="string">&quot;default&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;name&quot;:</span> <span class="string">&quot;metrics-app-5fb75796d4-274ww&quot;</span>,</span><br><span class="line">        <span class="attr">&quot;apiVersion&quot;:</span> <span class="string">&quot;/v1&quot;</span></span><br><span class="line">      &#125;,</span><br><span class="line">      <span class="attr">&quot;metricName&quot;:</span> <span class="string">&quot;http_requests_per_second&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;timestamp&quot;:</span> <span class="string">&quot;2020-09-13T04:45:31Z&quot;</span>,</span><br><span class="line">      <span class="attr">&quot;value&quot;:</span> <span class="string">&quot;100m&quot;</span>,    <span class="comment"># m是千分之一单位，因此100m是指每秒0.1个请求</span></span><br><span class="line">      <span class="attr">&quot;selector&quot;:</span> <span class="literal">null</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">……</span></span><br></pre></td></tr></table></figure>

<p>HPA v2能够基于自定义指标API中的某项指标弹性缩放由其编排的某控制器应用的基本前提是，配置可用的自定义指标API及特定应用上选定的合理指标。</p>
<h2 id="自动弹性缩放"><a href="#自动弹性缩放" class="headerlink" title="自动弹性缩放"></a>自动弹性缩放</h2><p>Deployment、ReplicaSet、Replication Controller或StatefulSet控制器资源管控的Pod副本数量支持手动运行时调整，从而可以更好地匹配业务规模的实际需求，但这种调整的方式需要用户深度参与监控容器应用的资源压力并计算出合理的值进行调整，存在一定程度的滞后性。为此，Kubernetes提供了多种自动弹性缩放工具。<br>1）HPA：一种支持控制器对象下Pod规模弹性缩放的工具，如图15-15所示。目前，HPA有两个版本的实现：HPAv1和HPAv2，HPAv1仅支持把CPU指标数据作为评估基准，而新版本能够使用资源指标API、自定义指标API和外部指标API中的指标。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214839129.png" alt="image-20220224214839129">2）CA：全称Cluster Autoscaler，是集群规模自动弹性缩放工具，能自动增减GCP、AWS或Azure集群上部署的Kubernetes集群节点数量，GA版本自Kubernetes v1.8起可用。<br>3）VPA：是Pod应用垂直缩放工具，它通过调整Pod对象的CPU和内存资源需求量完成扩展或收缩。<br>4）AR：全称Addon Resizer，是一个简化版本的Pod应用垂直缩放工具，它基于集群中的节点数量来调整附加组件的资源需求量。</p>
<h3 id="HPA控制器概述"><a href="#HPA控制器概述" class="headerlink" title="HPA控制器概述"></a>HPA控制器概述</h3><p>尽管Cluster Autoscaler高度依赖基础云计算环境，但HPA、VPA和AR能够独立于IaaS或PaaS云环境运行。HPA作为Kubernetes API资源和控制器实现，它基于采集到的资源指标数据来调整控制器的行为，控制器会定期调整ReplicaSets或Deployment控制器对象中的副本数，以使得观察到的平均CPU利用率与用户指定的目标相匹配。<br>HPA自身是控制循环的一个实现，其周期由kube-controller-manager守护进程的–horizontal-pod-autoscaler-sync-period选项定义，默认为30秒。在每个周期内，Controller Manager将根据每个HPA对象定义中指定的指标查询相应的资源利用率，并根据用户定义的阈值自主进行应用规模缩放相关的决策。目前，HPAv2支持资源指标API（针对每个Pod资源指标）和自定义指标API（针对所有其他指标），而HPAv1仅支持前者。<br>1）对于每个资源指标（例如CPU），控制器将从HPA定位到的每个Pod的资源指标API中获取指标数据；设置了目标利用率标准时，HPA控制器计算实际利用率（utilized/requests），而未设置标准的场景则直接使用其初始值。而后，控制器获取所有目标Pod对象的利用率或初始值的均值（取决于指定的目标类型），并生成一个用于缩放所需副本数的比例。对于那些未定义资源需求量的Pod对象，HPA控制器将无法定义该容器的CPU利用率，并且不会对该指标采取任何操作。<br>2)针对每个Pod对象的自定义指标，HPA控制器的处理逻辑与每个Pod资源指标处理机制类似，只是它仅能够处理初始值而非利用率。<br>由于指标的动态变动特性，使用HPA控制器管理Pod对象副本规模时可能会导致副本数量频繁波动，这种现象有时也称为“抖动”。故此，从Kubernetes 1.6版本开始允许集群管理员通过调整kube-controller-manager的选项值定义副本数变动延迟时长来缓解此问题。目前，默认的缩容延迟时长为5分钟，而扩容延迟时长为3分钟。<br>15.4.2　HPA v1控制器<br>HPA是标准的API资源类型，其基于资源配置清单的管理方式同其他资源相同。但它还有一个kubectl autoscale命令可以命令式命令快速创建HPA控制器。例如，我们先创建deployment/demoapp和相应service/demoapp资源，前者在Pod模板中为容器定义了如下资源需求和限制，完整的资源定义请求可参考demoapp.yaml。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">resources:</span></span><br><span class="line">  <span class="attr">requests:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">&quot;256Mi&quot;</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;50m&quot;</span></span><br><span class="line">  <span class="attr">limits:</span></span><br><span class="line">    <span class="attr">memory:</span> <span class="string">&quot;256Mi&quot;</span></span><br><span class="line">    <span class="attr">cpu:</span> <span class="string">&quot;50m&quot;</span></span><br></pre></td></tr></table></figure>

<p>而后，我们为deployment/demoapp创建HPA控制器资源，它要求Pod副本的最低数量是2，一旦CPU资源占用比例超过资源需求的60%，即按需自动扩缩容。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl autoscale deploy demoapp --min=2 --max=5 --cpu-percent=60</span></span><br></pre></td></tr></table></figure>

<p>通过命令创建的HPA对象隶属于autoscaling/v1群组，因此它仅支持基于CPU利用率的弹性伸缩机制，可从Metrics Service获得相关的指标数据。下面的命令用于显示HPA控制器的当前状态。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get hpa demoapp -o yaml</span></span><br><span class="line">……</span><br><span class="line">spec:</span><br><span class="line">  maxReplicas: 5                        # 最大副本数</span><br><span class="line">  minReplicas: 2                        # 最小副本数</span><br><span class="line">  scaleTargetRef:                       # 控制的目标资源</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: demoapp</span><br><span class="line">  targetCPUUtilizationPercentage: 60    # 调整应用规模的目标CPU资源的占用阈值</span><br><span class="line">status:</span><br><span class="line">  currentCPUUtilizationPercentage: 2    # 当前CPU占用比例</span><br><span class="line">  currentReplicas: 2                    # 当前副本数</span><br><span class="line">  desiredReplicas: 2                    # 期望的副本数</span><br></pre></td></tr></table></figure>

<p>HPA控制器会试图让Pod对象的相应资源占用率无限接近设定的目标值。例如，向service/demoapp发起持续性的压力测试访问请求，各Pod对象的CPU利用率将持续上升，直到超过目标利用率的60%，而后触发增加Pod对象副本数量操作。待其资源占用率下降到必须要降低Pod对象的数量，以使得资源占用率靠近目标设定值时，即触发Pod副本的终止操作，如图15-16所示。</p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224214856555.png" alt="image-20220224214856555"></p>
<p><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224215038317.png" alt="image-20220224215038317"></p>
<p>下面的命令是一段时间内hpa/demoapp资源控制下Pod数量的变动情况，包括因CPU资源占用比超出阈值而增加Pod副本数量，以及CPU资源占用比例下降到阈值以内而降低Pod副本数量的过程，但变动并非在设定的资源阈值到达后马上发生，而是延迟一段时间以免发生资源数量频繁波动。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get hpa -w</span></span><br><span class="line">NAME      REFERENCE            TARGETS   MINPODS   MAXPODS   REPLICAS   AGE</span><br><span class="line">demoapp   Deployment/demoapp   2%/60%    2         5         2          5m23s</span><br><span class="line">demoapp   Deployment/demoapp   79%/60%   2         5         2          5m36s</span><br><span class="line">demoapp   Deployment/demoapp   79%/60%   2         5         3          5m51s</span><br><span class="line">demoapp   Deployment/demoapp   85%/60%   2         5         3          6m37s</span><br><span class="line">demoapp   Deployment/demoapp   84%/60%   2         5         3          7m38s</span><br><span class="line">demoapp   Deployment/demoapp   68%/60%   2         5         5          8m39s</span><br><span class="line">demoapp   Deployment/demoapp   7%/60%    2         5         5          9m40s</span><br><span class="line">demoapp   Deployment/demoapp   24%/60%   2         5         5          10m</span><br><span class="line">demoapp   Deployment/demoapp   24%/60%   2         5         5          12m</span><br><span class="line">demoapp   Deployment/demoapp   12%/60%   2         5         2          14m</span><br><span class="line">demoapp   Deployment/demoapp   24%/60%   2         5         2          15m</span><br></pre></td></tr></table></figure>

<p>当然，用户可以通过资源配置清单定义HPAv1控制器资源，其spec字段嵌套使用的属性字段主要有maxReplicas、minReplicas、scaleTargetRef和targetCPUUtilizationPercentage几个，其使用方式请参考相应的文档。尽管CPU资源占用率可以作为规模伸缩的评估标准，但多数时候，Pod对象面临访问压力时未必会直接反映到CPU之上。</p>
<h3 id="HPA-v2控制器"><a href="#HPA-v2控制器" class="headerlink" title="HPA v2控制器"></a>HPA v2控制器</h3><p>HPAv2控制器支持基于核心指标CPU和内存资源以及基于任意自定义指标资源占用状态实现应用规模的自动弹性缩放，它从Metrics Service请求查看核心指标，从k8s-prometheus-adapter一类的自定义指标API获取自定义指标数据，如图15-17所示。<br><img src="/blog/2022/03/02/%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7/image-20220224215106410.png" alt="image-20220224215106410">截至目前，API群组autoscaling的版本在Kubernetes v1.20中晋升为稳定版v1，而在Kubernetes v1.19及之前的版本中，该API群组的主流版本为v2beta2。本节仍以v2beta2为例来介绍HPA（v2)资源的使用，完整的资源规范及简要说明如下。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling/v2beta2</span>    <span class="comment"># API群组及版本号</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HorizontalPodAutoscaler</span>      <span class="comment"># 资源类型标识</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="string">name</span> <span class="string">&lt;string&gt;</span></span><br><span class="line">  <span class="string">namespace</span> <span class="string">&lt;string&gt;</span>               <span class="comment"># 名称空间级别的资源类型</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">behavior</span> <span class="string">&lt;Object&gt;</span>      <span class="comment"># 配置扩缩容策略，默认遵循HPAScalingRules</span></span><br><span class="line">    <span class="string">scaleDown</span> <span class="string">&lt;Object&gt;</span>   <span class="comment"># 缩容策略，默认为300秒的固定窗口，最低缩容至最少Pod数</span></span><br><span class="line">      <span class="string">policies</span> <span class="string">&lt;[]Object&gt;</span>    <span class="comment"># 自定义策略列表</span></span><br><span class="line">        <span class="string">periodSeconds</span>  <span class="string">&lt;integer&gt;</span>    <span class="comment"># 自定义的操作窗口大小，取值范围(0,1800]</span></span><br><span class="line">        <span class="string">type</span> <span class="string">&lt;string&gt;</span>       <span class="comment"># 策略类型</span></span><br><span class="line">        <span class="string">value</span>  <span class="string">&lt;integer&gt;</span>    <span class="comment"># 目标数量</span></span><br><span class="line">      <span class="string">selectPolicy</span> <span class="string">&lt;string&gt;</span> <span class="comment"># 指定要使用的策略，默认为编号最大的策略</span></span><br><span class="line">      <span class="string">stabilizationWindowSeconds</span>  <span class="string">&lt;integer&gt;</span>   <span class="comment"># 稳定的窗口大小</span></span><br><span class="line">    <span class="string">scaleUp</span>  <span class="string">&lt;Object&gt;</span>    <span class="comment"># 扩容策略，默认为300秒的固定窗口，最多扩容至最大Pod数</span></span><br><span class="line">  <span class="string">maxReplicas</span> <span class="string">&lt;integer&gt;</span>        <span class="comment"># 最大副本数</span></span><br><span class="line">  <span class="string">minReplicas</span> <span class="string">&lt;integer&gt;</span>        <span class="comment"># 最小副本数</span></span><br><span class="line">  <span class="string">scaleTargetRef</span> <span class="string">&lt;Object&gt;</span>      <span class="comment"># 要自动扩缩容的目标资源，仅能引用同一名称空间的资源</span></span><br><span class="line">    <span class="string">apiVersion</span>   <span class="string">&lt;string&gt;</span>      <span class="comment"># 待扩缩容的资源隶属的API群组及版本号</span></span><br><span class="line">    <span class="string">kind</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 待扩缩容的资源类型</span></span><br><span class="line">    <span class="string">name</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 待扩缩容的资源名称</span></span><br><span class="line">  <span class="string">metrics</span> <span class="string">&lt;[]Object&gt;</span>           <span class="comment"># 要评估的指标</span></span><br><span class="line">    <span class="string">type</span> <span class="string">&lt;string&gt;</span>              <span class="comment"># 指标类型，可以是如下4种类型之一</span></span><br><span class="line">    <span class="string">pods</span> <span class="string">&lt;Object&gt;</span>              <span class="comment"># Pod对象上的指标</span></span><br><span class="line">      <span class="string">metric</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 引用的指标</span></span><br><span class="line">        <span class="string">name</span> <span class="string">&lt;string&gt;</span>          <span class="comment"># 指标名称</span></span><br><span class="line">        <span class="string">selector</span> <span class="string">&lt;Object&gt;</span>   </span><br><span class="line">      <span class="string">target</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 指标的目标值定义</span></span><br><span class="line">        <span class="string">type</span> <span class="string">&lt;string&gt;</span>   <span class="comment"># 数据类型，有Utilization、Value和AverageValue这3种</span></span><br><span class="line">        <span class="string">value</span> <span class="string">&lt;string&gt;</span>         <span class="comment"># 指标的直接值</span></span><br><span class="line">        <span class="string">averageValue</span> <span class="string">&lt;string&gt;</span>  <span class="comment"># 所有Pod的同一指标值的平均数</span></span><br><span class="line">        <span class="string">averageUtilization</span> <span class="string">&lt;integer&gt;</span>   <span class="comment"># 资源类型指标的利用率</span></span><br><span class="line">    <span class="string">resource</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 根据指定的资源指标扩缩容</span></span><br><span class="line">      <span class="string">name</span> <span class="string">&lt;string&gt;</span>            <span class="comment"># 指标名称</span></span><br><span class="line">      <span class="string">target</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 格式同pods类型</span></span><br><span class="line">    <span class="string">object</span>  <span class="string">&lt;Object&gt;</span>           <span class="comment"># 根据指定Kubernetes资源对象的指标进行扩缩容</span></span><br><span class="line">      <span class="string">describedObject</span> <span class="string">&lt;Object&gt;</span> <span class="comment"># 参考的资源对象</span></span><br><span class="line">      <span class="string">metric</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 引用的指标，格式同pods类型</span></span><br><span class="line">      <span class="string">target</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 指标的目标值定义</span></span><br><span class="line">    <span class="string">external</span>  <span class="string">&lt;Object&gt;</span>         <span class="comment"># 外部指标</span></span><br><span class="line">      <span class="string">metric</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 引用的指标，格式同pods类型</span></span><br><span class="line">      <span class="string">target</span> <span class="string">&lt;Object&gt;</span>          <span class="comment"># 指标的目标值定义</span></span><br></pre></td></tr></table></figure>

<p>提示<br>查看非默认版本中资源类型内置文档的命令是kubectl explain –api-version= ‘VERSION’。<br>除了基于CPU资源的占用率调整应用规模，HPAv2还支持内存资源评估，甚至是二者同时进行。下面的示例代码（hpa-v2-resources.yaml）中定义了一个HPAv2控制器的资源，它使用资源指标API获取CPU和内存资源两个资源指标的使用状况，并与它们各自的设定目标进行比较，计算得出所需要的副本数量，两个指标计算结果中数值较大的值是要调整为的Pod副本数。</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">autoscaling/v2beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">HorizontalPodAutoscaler</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">scaleTargetRef:</span></span><br><span class="line">    <span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line">    <span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">demoapp</span></span><br><span class="line">  <span class="attr">minReplicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">maxReplicas:</span> <span class="number">5</span></span><br><span class="line">  <span class="attr">metrics:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Resource</span>  <span class="comment"># 指标类型</span></span><br><span class="line">    <span class="attr">resource:</span>       <span class="comment"># 具体的资源指标定义</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">cpu</span></span><br><span class="line">      <span class="attr">target:</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">Utilization</span></span><br><span class="line">        <span class="attr">averageUtilization:</span> <span class="number">60</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">type:</span> <span class="string">Resource</span></span><br><span class="line">    <span class="attr">resource:</span></span><br><span class="line">      <span class="attr">name:</span> <span class="string">memory</span></span><br><span class="line">      <span class="attr">target:</span></span><br><span class="line">        <span class="attr">type:</span> <span class="string">AverageValue</span></span><br><span class="line">        <span class="attr">averageValue:</span> <span class="string">30Mi</span></span><br></pre></td></tr></table></figure>

<p>将配置清单中的资源创建到集群中即可进行应用规模缩放测试，其测试方式与15.4.2节中的过程类似，这里不再给出其具体过程。尽管HPAv2目前仍处于beta阶段，将来它有可能会发生部分改变，但目前的特性足以支撑起它的各项核心功能。<br>HPA v2beta2的资源规范中，.spec.metrics能够嵌套定义多个评估指标，每个指标单独计算其所需的副本数，所有指标计算结果中的最大值为最终采用的副本数量。计算时，以资源占用率为例，目标控制器资源上所有Pod资源的资源占用率之和除以目标占用率所得的结果即为目标Pod副本数，因此在请求稳定的情况下，增加Pod副本数量必然会降低各Pod对象的资源占用率，反之亦然。metrics字段值是对象列表，它由要引用的各指标的数据源及其类型构成的对象组成。</p>
<ul>
<li>external：用于引用非附属于任何对象的全局指标，甚至可以引用集群之外的组件指标数据，例如消息队列的长度等。</li>
<li>object：引用集群中某单一对象的特定指标，例如Ingress对象上的hits-per-second等。</li>
<li>pods：引用当前应用的Pod对象的特定指标，例如transactions-processed-per-second等，计算方式是各Pod对象的同一指标数据取平均值后与目标值进行比较。</li>
<li>resource：引用资源指标，即当前应用的Pod对象中容器的CPU或内存资源指标，其中占用率的评估基准是limits和requests定义的值。</li>
<li>type：即指标源的类型，其值可以为External、Objects、Pods或Resource，它们分别对应于上面指标来源。</li>
</ul>
<p>基于非核心资源指标定义HPAv2资源时，上述几种指标来源中，以pods或object相关的指标调用居多。下面通过一个示例说明其用法，并测试其扩缩容的效果。<br>镜像文件ikubernetes/metrics-app在运行时会启动一个简单的Web服务器，它通过/metrics路径输出了http_requests_total和http_requests_per_second两个指标。资源清单示例metrics-app.yaml定义的deployment/metrics-app资源期望基于该镜像在default名称空间中运行两个Pod副本，而service/metrics-app资源用于发布该资源。基于该资源清单把资源创建到集群中，而后启动一个临时测试客户端Pod，在命令行向创建的Service端点的/metrics发起访问请求，即可看到它输出的与Prometheus兼容的指标与数据，如下面的命令及结果所示。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f metrics-app.yaml</span></span><br><span class="line">deployment.apps/metrics-app created</span><br><span class="line">service/metrics-app created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl run client --image=<span class="string">&quot;ikubernetes/admin-toolbox:v1.0&quot;</span> -it --<span class="built_in">rm</span> --<span class="built_in">command</span> -- /bin/sh</span></span><br><span class="line">[root@client /]#  curl metrics-app.default.svc.cluster.local/metrics</span><br><span class="line"><span class="meta"># </span><span class="language-bash">HELP http_requests_total The amount of requests <span class="keyword">in</span> total</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">TYPE http_requests_total counter</span></span><br><span class="line">http_requests_total 6</span><br><span class="line"><span class="meta"># </span><span class="language-bash">HELP http_requests_per_second The amount of requests per second the latest ten seconds</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">TYPE http_requests_per_second gauge</span></span><br><span class="line">http_requests_per_second 0.2</span><br></pre></td></tr></table></figure>

<p>命令结果中返回了所有的指标及其数据，每个指标还附带了通过注释行提供的帮助信息和类型说明。<br>下面的资源配置清单示例（metrics-app-hpa.yaml）用于自动弹性缩放前面基于metrics-app.yaml创建的deployment/metrics-app相关的Pod对象副本数量，其缩放标准是接入HTTP请求报文的速率，具体的数据则需经相关Pod对象的http_requests指标的平均数据与目标速率5（即5个请求/秒）进行比较来判定。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">kind: HorizontalPodAutoscaler</span><br><span class="line">apiVersion: autoscaling/v2beta2</span><br><span class="line">metadata:</span><br><span class="line">  name: metrics-app-hpa</span><br><span class="line">spec:</span><br><span class="line">  scaleTargetRef:</span><br><span class="line">    apiVersion: apps/v1</span><br><span class="line">    kind: Deployment</span><br><span class="line">    name: metrics-app</span><br><span class="line">  minReplicas: 2</span><br><span class="line">  maxReplicas: 10</span><br><span class="line">  metrics:</span><br><span class="line">  - type: Pods</span><br><span class="line">    pods:</span><br><span class="line">      metric:</span><br><span class="line">        name: http_requests_per_second # 评估的自定义指标名称</span><br><span class="line">      target:</span><br><span class="line">        type: AverageValue</span><br><span class="line">        averageValue: 5                # 阈值为每秒5个请求</span><br><span class="line">  behavior:</span><br><span class="line">    scaleDown:</span><br><span class="line">      stabilizationWindowSeconds: 120</span><br></pre></td></tr></table></figure>

<p>注意<br>metrics-app提供的http_requests_per_second指标并不会由k8s-prometheus-adapter适配器默认进行公开，它通常要在适配器上通过自定义规则进行暴露，具体方法请参考15.3.5节。<br>为了测试其效果，需要将上面示例清单中的资源hpa/metrics-app-hpa创建到集群上，并了解其当前的指标状况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl apply -f metrics-app-hpa.yaml</span></span><br><span class="line">horizontalpodautoscaler.autoscaling/metrics-app-hpa created</span><br><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get hpa/metrics-app-hpa</span></span><br><span class="line">NAME                      REFERENCE     TARGETS   MINPODS  MAXPODS  REPLICAS…</span><br><span class="line">metrics-app-hpa  Deployment/metrics-app  16m/5     2          10         2   …</span><br></pre></td></tr></table></figure>

<p>随后，我们启动一到多个测试客户端对service/metrics-app发起持续性测试请求，模拟压力访问以便其指标数据能满足扩展规模之需，同时监控hpa/metrics-app-hpa资源的状态变动即可了解自动规则伸缩的效果。下面的命令及结果便取自测试执行过程中，它清晰地反映了HPAv2的工作效果。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">~$ </span><span class="language-bash">kubectl get hpa/metrics-app-hpa -w</span></span><br><span class="line">NAME              REFERENCE                TARGETS   ……  REPLICAS    AGE</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   92m/5     ……     2        61s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   6874m/5   ……     2        3m1s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   8615m/5   ……     3        3m16s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   8749m/5   ……     4        3m31s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   5736m/5   ……     4        5m18s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   5974m/5   ……     5        5m34s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   7028m/5   ……     6        7m6s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   6896m/5   ……     7        7m36s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   7311m/5   ……     10       9m23s</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   1027m/5   ……     10       12m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   972m/5    ……     8        13m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   959m/5   ……      6        13m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   997m/5   ……      5        13m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   1258m/5   ……     4        14m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   1489m/5   ……     3        14m</span><br><span class="line">metrics-app-hpa   Deployment/metrics-app   2816m/5   ……     2        15m</span><br></pre></td></tr></table></figure>

<p>借助自定义指标自动弹性伸缩应用规模的机制，赋予了不同应用程序根据核心指标控制自身规模的能力，这是Kubernetes系统除敏捷部署（Deployment等控制器）功能之外又一极具特色的特性，运用得当能有效降低系统维护成本。</p>
<p>15.5　本章小结<br>本章详细讲解了第一代指标API及其实现方案Heapster，第二代监控架构体系、资源指标API、自定义指标API及其各自的解决方案，最后又说明了Dashboard的部署机制。<br>▪核心资源指标API是HPA控制器、Dashboard和调度器依赖的基础组件，它们分别在指标数据的基础上实现应用规模弹性缩放、指标数据展示和Pod对象的调度。<br>▪新一代监控系统将指标划分为核心指标和自定义指标，并把API的定义同其实现分离开来。资源指标API的标准实现是Metrics Server，而自定义指标API的主要实现是Prometheus及相应的适配器，例如k8s-prometheus-adapter等。<br>▪HPA第一代仅支持CPU指标数据，而第二代可基于各种核心指标和自定义指标实现应用规模的自动变更</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/blog/tags/Kubernetes/" rel="tag"># Kubernetes</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/blog/2022/03/02/%E5%BA%94%E7%94%A8%E7%AE%A1%E7%90%86-helm%E3%80%81Kustomizeimage/" rel="prev" title="应用管理(helm、Kustomize)">
      <i class="fa fa-chevron-left"></i> 应用管理(helm、Kustomize)
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7"><span class="nav-number">1.</span> <span class="nav-text">资源指标与集群监控</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87"><span class="nav-number">1.1.</span> <span class="nav-text">资源监控与资源指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7%E4%B8%8EHeapster"><span class="nav-number">1.1.1.</span> <span class="nav-text">资源监控与Heapster</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%96%B0%E4%B8%80%E4%BB%A3%E7%9B%91%E6%8E%A7%E4%BD%93%E7%B3%BB%E4%B8%8E%E6%8C%87%E6%A0%87%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.1.2.</span> <span class="nav-text">新一代监控体系与指标系统</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B5%84%E6%BA%90%E6%8C%87%E6%A0%87%E4%B8%8E%E5%BA%94%E7%94%A8"><span class="nav-number">1.2.</span> <span class="nav-text">资源指标与应用</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2Metrics-Server"><span class="nav-number">1.2.1.</span> <span class="nav-text">部署Metrics Server</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%98%BE%E7%A4%BA%E8%B5%84%E6%BA%90%E4%BD%BF%E7%94%A8%E4%BF%A1%E6%81%AF"><span class="nav-number">1.2.2.</span> <span class="nav-text">显示资源使用信息</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E4%B8%8EPrometheus"><span class="nav-number">1.3.</span> <span class="nav-text">自定义指标与Prometheus</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prometheus%E5%9F%BA%E7%A1%80"><span class="nav-number">1.3.1.</span> <span class="nav-text">Prometheus基础</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%BA%93"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">客户端库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E6%9A%B4%E9%9C%B2%E5%99%A8Exporters"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">指标暴露器Exporters</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">服务发现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E6%95%B0%E6%8D%AE%E6%8A%93%E5%8F%96"><span class="nav-number">1.3.1.4.</span> <span class="nav-text">指标数据抓取</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8"><span class="nav-number">1.3.1.5.</span> <span class="nav-text">数据存储</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#PromQL%E5%92%8C%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8E%A5%E5%8F%A3"><span class="nav-number">1.3.1.6.</span> <span class="nav-text">PromQL和可视化接口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%B0%E5%BD%95%E8%A7%84%E5%88%99%E5%92%8C%E5%91%8A%E8%AD%A6%E8%A7%84%E5%88%99"><span class="nav-number">1.3.1.7.</span> <span class="nav-text">记录规则和告警规则</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prometheus%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="nav-number">1.3.2.</span> <span class="nav-text">Prometheus核心概念</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">数据模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E6%A0%87%E7%B1%BB%E5%9E%8B"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">指标类型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Prometheus%E6%9F%A5%E8%AF%A2%E8%AF%AD%E8%A8%80"><span class="nav-number">1.3.3.</span> <span class="nav-text">Prometheus查询语言</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%91%E6%8E%A7Kubernetes"><span class="nav-number">1.3.4.</span> <span class="nav-text">监控Kubernetes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%83%A8%E7%BD%B2Prometheus"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">部署Prometheus</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Web-UI%E5%8F%8A%E8%A1%A8%E8%BE%BE%E5%BC%8F%E6%B5%8F%E8%A7%88%E5%99%A8"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">Web UI及表达式浏览器</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87%E9%80%82%E9%85%8D%E5%99%A8"><span class="nav-number">1.3.5.</span> <span class="nav-text">自定义指标适配器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E5%BC%B9%E6%80%A7%E7%BC%A9%E6%94%BE"><span class="nav-number">1.4.</span> <span class="nav-text">自动弹性缩放</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#HPA%E6%8E%A7%E5%88%B6%E5%99%A8%E6%A6%82%E8%BF%B0"><span class="nav-number">1.4.1.</span> <span class="nav-text">HPA控制器概述</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HPA-v2%E6%8E%A7%E5%88%B6%E5%99%A8"><span class="nav-number">1.4.2.</span> <span class="nav-text">HPA v2控制器</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description">myBlog</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/blog/archives/">
        
          <span class="site-state-item-count">13</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/blog/lib/anime.min.js"></script>
  <script src="/blog/lib/velocity/velocity.min.js"></script>
  <script src="/blog/lib/velocity/velocity.ui.min.js"></script>

<script src="/blog/js/utils.js"></script>

<script src="/blog/js/motion.js"></script>


<script src="/blog/js/schemes/muse.js"></script>


<script src="/blog/js/next-boot.js"></script>




  















  

  

</body>
</html>
